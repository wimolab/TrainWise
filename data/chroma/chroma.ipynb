{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b84617fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from uuid import uuid4\n",
    "\n",
    "from loaders import HuggingFaceBlogLoader, HuggingFaceDocsLoader, ArxivLoader\n",
    "from data.utils import load_dataframe_from_sheet, get_docs\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "FIRECRAWL_API_KEY = os.getenv(\"FIRECRAWL_API_KEY\")\n",
    "GOOGLE_SHEETS_ID = os.getenv(\"GOOGLE_SHEETS_ID\")\n",
    "GOOGLE_SHEETS_NAME = os.getenv(\"GOOGLE_SHEETS_NAME\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0aba1a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_dataframe_from_sheet(GOOGLE_SHEETS_ID, GOOGLE_SHEETS_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b0ff27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Hugging Face blogs: 100%|██████████| 5/5 [00:06<00:00,  1.21s/it]\n",
      "Cleaning Hugging Face blogs: 100%|██████████| 5/5 [00:00<00:00, 10412.87it/s]\n"
     ]
    }
   ],
   "source": [
    "hf_blogs_urls = get_docs(df, source=\"Hugging Face\", doc_type=\"blog\", urls_only=True)\n",
    "hf_blogs_loader = HuggingFaceBlogLoader(firecrawl_api_key=FIRECRAWL_API_KEY, urls=hf_blogs_urls)\n",
    "hf_blogs_docs = hf_blogs_loader.load()\n",
    "hf_blogs_docs = hf_blogs_loader.clean(hf_blogs_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79f5f8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Hugging Face docs: 100%|██████████| 23/23 [00:28<00:00,  1.26s/it]\n",
      "Cleaning Hugging Face docs: 100%|██████████| 23/23 [00:00<00:00, 4419.71it/s]\n"
     ]
    }
   ],
   "source": [
    "hf_docs_urls = get_docs(df, source=\"Hugging Face\", doc_type=\"docs\", urls_only=True)\n",
    "hf_docs_loader = HuggingFaceDocsLoader(firecrawl_api_key=FIRECRAWL_API_KEY, urls=hf_docs_urls)\n",
    "hf_docs_docs = hf_docs_loader.load()\n",
    "hf_docs_docs = hf_docs_loader.clean(hf_docs_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2afa2512",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Arxiv papers: 100%|██████████| 3/3 [01:30<00:00, 30.00s/it]\n",
      "Cleaning Arxiv papers: 100%|██████████| 3/3 [00:00<00:00, 743.67it/s]\n"
     ]
    }
   ],
   "source": [
    "arxiv_urls = get_docs(df, source=\"arxiv\", doc_type=\"paper\", urls_only=True)\n",
    "arxiv_loader = ArxivLoader(urls=arxiv_urls)\n",
    "arxiv_docs = arxiv_loader.load()\n",
    "arxiv_docs = arxiv_loader.clean(arxiv_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cf81e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents loaded: 31\n"
     ]
    }
   ],
   "source": [
    "all_docs = hf_blogs_docs + hf_docs_docs + arxiv_docs\n",
    "uuids = [str(uuid4()) for _ in range(len(all_docs))]\n",
    "\n",
    "print(f\"Total documents loaded: {len(all_docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "587430df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a6ef5b0ffc342c9bf168e7144e83c98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc6b627270a44ec589bd49fbb597fa5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8866800227e045f3a9492ecf2d5d6deb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c756ae063fd4de79cb5f65277b7e644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "278fe92c3a0846269a5b646c6cbb295b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aa3e35f01cc48f595fba8d9a13344c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cancellation requested; stopping current tasks.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/trainwise/lib/python3.13/site-packages/huggingface_hub/file_download.py:626\u001b[39m, in \u001b[36mxet_get\u001b[39m\u001b[34m(incomplete_path, xet_file_data, headers, expected_size, displayed_filename, _tqdm_bar)\u001b[39m\n\u001b[32m    624\u001b[39m     progress.update(progress_bytes)\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mdownload_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxet_download_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconnection_info\u001b[49m\u001b[43m.\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    629\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconnection_info\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccess_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconnection_info\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexpiration_unix_epoch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    630\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_refresher\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_refresher\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprogress_updater\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mprogress_updater\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_text_splitters\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformersTokenTextSplitter\n\u001b[32m      3\u001b[39m tokenizer = AutoTokenizer.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33mnomic-ai/modernbert-embed-base\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m text_splitter = \u001b[43mSentenceTransformersTokenTextSplitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_huggingface_tokenizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokens_per_chunk\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunk_overlap\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m chunks = text_splitter.split_documents(all_docs)\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTotal chunks created: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(chunks)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/trainwise/lib/python3.13/site-packages/langchain_text_splitters/base.py:188\u001b[39m, in \u001b[36mTextSplitter.from_huggingface_tokenizer\u001b[39m\u001b[34m(cls, tokenizer, **kwargs)\u001b[39m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_huggingface_tokenizer_length\u001b[39m(text: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m    186\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tokenizer.tokenize(text))\n\u001b[32m--> \u001b[39m\u001b[32m188\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlength_function\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_huggingface_tokenizer_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/trainwise/lib/python3.13/site-packages/langchain_text_splitters/sentence_transformers.py:39\u001b[39m, in \u001b[36mSentenceTransformersTokenTextSplitter.__init__\u001b[39m\u001b[34m(self, chunk_overlap, model_name, tokens_per_chunk, **kwargs)\u001b[39m\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[32m     38\u001b[39m \u001b[38;5;28mself\u001b[39m.model_name = model_name\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m \u001b[38;5;28mself\u001b[39m._model = \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[38;5;28mself\u001b[39m.tokenizer = \u001b[38;5;28mself\u001b[39m._model.tokenizer\n\u001b[32m     41\u001b[39m \u001b[38;5;28mself\u001b[39m._initialize_chunk_configuration(tokens_per_chunk=tokens_per_chunk)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/trainwise/lib/python3.13/site-packages/sentence_transformers/SentenceTransformer.py:327\u001b[39m, in \u001b[36mSentenceTransformer.__init__\u001b[39m\u001b[34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[39m\n\u001b[32m    309\u001b[39m has_modules = is_sentence_transformer_model(\n\u001b[32m    310\u001b[39m     model_name_or_path,\n\u001b[32m    311\u001b[39m     token,\n\u001b[32m   (...)\u001b[39m\u001b[32m    314\u001b[39m     local_files_only=local_files_only,\n\u001b[32m    315\u001b[39m )\n\u001b[32m    316\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    317\u001b[39m     has_modules\n\u001b[32m    318\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._get_model_type(\n\u001b[32m   (...)\u001b[39m\u001b[32m    325\u001b[39m     == \u001b[38;5;28mself\u001b[39m._model_config[\u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    326\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     modules, \u001b[38;5;28mself\u001b[39m.module_kwargs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_sbert_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    339\u001b[39m     modules = \u001b[38;5;28mself\u001b[39m._load_auto_model(\n\u001b[32m    340\u001b[39m         model_name_or_path,\n\u001b[32m    341\u001b[39m         token=token,\n\u001b[32m   (...)\u001b[39m\u001b[32m    349\u001b[39m         has_modules=has_modules,\n\u001b[32m    350\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/trainwise/lib/python3.13/site-packages/sentence_transformers/SentenceTransformer.py:2305\u001b[39m, in \u001b[36mSentenceTransformer._load_sbert_model\u001b[39m\u001b[34m(self, model_name_or_path, token, cache_folder, revision, trust_remote_code, local_files_only, model_kwargs, tokenizer_kwargs, config_kwargs)\u001b[39m\n\u001b[32m   2300\u001b[39m         module = module_class.load(local_path)\n\u001b[32m   2302\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2303\u001b[39m     \u001b[38;5;66;03m# Newer modules that support the new loading method are loaded with the new style\u001b[39;00m\n\u001b[32m   2304\u001b[39m     \u001b[38;5;66;03m# i.e. with many keyword arguments that can optionally be used by the modules\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2305\u001b[39m     module = \u001b[43mmodule_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2306\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2307\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Loading-specific keyword arguments\u001b[39;49;00m\n\u001b[32m   2308\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodule_config\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpath\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2309\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2310\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2311\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2312\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2313\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Module-specific keyword arguments\u001b[39;49;00m\n\u001b[32m   2314\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2318\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2319\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2321\u001b[39m modules[module_config[\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m]] = module\n\u001b[32m   2322\u001b[39m module_kwargs[module_config[\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m]] = module_config.get(\u001b[33m\"\u001b[39m\u001b[33mkwargs\u001b[39m\u001b[33m\"\u001b[39m, [])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/trainwise/lib/python3.13/site-packages/sentence_transformers/models/Transformer.py:365\u001b[39m, in \u001b[36mTransformer.load\u001b[39m\u001b[34m(cls, model_name_or_path, subfolder, token, cache_folder, revision, local_files_only, trust_remote_code, model_kwargs, tokenizer_kwargs, config_kwargs, backend, **kwargs)\u001b[39m\n\u001b[32m    334\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    335\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\n\u001b[32m    336\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    350\u001b[39m     **kwargs,\n\u001b[32m    351\u001b[39m ) -> Self:\n\u001b[32m    352\u001b[39m     init_kwargs = \u001b[38;5;28mcls\u001b[39m._load_init_kwargs(\n\u001b[32m    353\u001b[39m         model_name_or_path=model_name_or_path,\n\u001b[32m    354\u001b[39m         subfolder=subfolder,\n\u001b[32m   (...)\u001b[39m\u001b[32m    363\u001b[39m         backend=backend,\n\u001b[32m    364\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/trainwise/lib/python3.13/site-packages/sentence_transformers/models/Transformer.py:88\u001b[39m, in \u001b[36mTransformer.__init__\u001b[39m\u001b[34m(self, model_name_or_path, max_seq_length, model_args, tokenizer_args, config_args, cache_dir, do_lower_case, tokenizer_name_or_path, backend)\u001b[39m\n\u001b[32m     85\u001b[39m     config_args = {}\n\u001b[32m     87\u001b[39m config, is_peft_model = \u001b[38;5;28mself\u001b[39m._load_config(model_name_or_path, cache_dir, backend, config_args)\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_peft_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[38;5;66;03m# Get the signature of the auto_model's forward method to pass only the expected arguments from `features`,\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m# plus some common values like \"input_ids\", \"attention_mask\", etc.\u001b[39;00m\n\u001b[32m     92\u001b[39m model_forward_params = \u001b[38;5;28mlist\u001b[39m(inspect.signature(\u001b[38;5;28mself\u001b[39m.auto_model.forward).parameters)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/trainwise/lib/python3.13/site-packages/sentence_transformers/models/Transformer.py:196\u001b[39m, in \u001b[36mTransformer._load_model\u001b[39m\u001b[34m(self, model_name_or_path, config, cache_dir, backend, is_peft_model, **model_args)\u001b[39m\n\u001b[32m    194\u001b[39m         \u001b[38;5;28mself\u001b[39m._load_mt5_model(model_name_or_path, config, cache_dir, **model_args)\n\u001b[32m    195\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m         \u001b[38;5;28mself\u001b[39m.auto_model = \u001b[43mAutoModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m backend == \u001b[33m\"\u001b[39m\u001b[33monnx\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    200\u001b[39m     \u001b[38;5;28mself\u001b[39m.auto_model = load_onnx_model(\n\u001b[32m    201\u001b[39m         model_name_or_path=model_name_or_path,\n\u001b[32m    202\u001b[39m         config=config,\n\u001b[32m    203\u001b[39m         task_name=\u001b[33m\"\u001b[39m\u001b[33mfeature-extraction\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    204\u001b[39m         **model_args,\n\u001b[32m    205\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/trainwise/lib/python3.13/site-packages/transformers/models/auto/auto_factory.py:604\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    602\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    603\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    606\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    608\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    609\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    610\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/trainwise/lib/python3.13/site-packages/transformers/modeling_utils.py:277\u001b[39m, in \u001b[36mrestore_default_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    275\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    279\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/trainwise/lib/python3.13/site-packages/transformers/modeling_utils.py:4900\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4890\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   4891\u001b[39m     gguf_file\n\u001b[32m   4892\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4893\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m ((\u001b[38;5;28misinstance\u001b[39m(device_map, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdisk\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map.values()) \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdisk\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map)\n\u001b[32m   4894\u001b[39m ):\n\u001b[32m   4895\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   4896\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mOne or more modules is configured to be mapped to disk. Disk offload is not supported for models \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4897\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mloaded from GGUF files.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4898\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m4900\u001b[39m checkpoint_files, sharded_metadata = \u001b[43m_get_resolved_checkpoint_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4901\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4902\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4903\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4904\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgguf_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgguf_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4905\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4906\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4907\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4908\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4909\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4910\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4911\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4912\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4913\u001b[39m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4914\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_auto_class\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   4917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransformers_explicit_filename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransformers_explicit_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4918\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4920\u001b[39m is_sharded = sharded_metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4921\u001b[39m is_quantized = hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/trainwise/lib/python3.13/site-packages/transformers/modeling_utils.py:1037\u001b[39m, in \u001b[36m_get_resolved_checkpoint_files\u001b[39m\u001b[34m(pretrained_model_name_or_path, subfolder, variant, gguf_file, from_tf, from_flax, use_safetensors, cache_dir, force_download, proxies, local_files_only, token, user_agent, revision, commit_hash, is_remote_code, transformers_explicit_filename)\u001b[39m\n\u001b[32m   1022\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1023\u001b[39m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[32m   1024\u001b[39m     cached_file_kwargs = {\n\u001b[32m   1025\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcache_dir\u001b[39m\u001b[33m\"\u001b[39m: cache_dir,\n\u001b[32m   1026\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mforce_download\u001b[39m\u001b[33m\"\u001b[39m: force_download,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1035\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m: commit_hash,\n\u001b[32m   1036\u001b[39m     }\n\u001b[32m-> \u001b[39m\u001b[32m1037\u001b[39m     resolved_archive_file = \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcached_file_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1039\u001b[39m     \u001b[38;5;66;03m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001b[39;00m\n\u001b[32m   1040\u001b[39m     \u001b[38;5;66;03m# result when internet is up, the repo and revision exist, but the file does not.\u001b[39;00m\n\u001b[32m   1041\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m resolved_archive_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m filename == _add_variant(SAFE_WEIGHTS_NAME, variant):\n\u001b[32m   1042\u001b[39m         \u001b[38;5;66;03m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/trainwise/lib/python3.13/site-packages/transformers/utils/hub.py:322\u001b[39m, in \u001b[36mcached_file\u001b[39m\u001b[34m(path_or_repo_id, filename, **kwargs)\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcached_file\u001b[39m(\n\u001b[32m    265\u001b[39m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    266\u001b[39m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    267\u001b[39m     **kwargs,\n\u001b[32m    268\u001b[39m ) -> Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    269\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    270\u001b[39m \u001b[33;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[32m    271\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    320\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    321\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m     file = \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m     file = file[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[32m    324\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/trainwise/lib/python3.13/site-packages/transformers/utils/hub.py:479\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    477\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) == \u001b[32m1\u001b[39m:\n\u001b[32m    478\u001b[39m         \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m         \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    493\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    494\u001b[39m         snapshot_download(\n\u001b[32m    495\u001b[39m             path_or_repo_id,\n\u001b[32m    496\u001b[39m             allow_patterns=full_filenames,\n\u001b[32m   (...)\u001b[39m\u001b[32m    505\u001b[39m             local_files_only=local_files_only,\n\u001b[32m    506\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/trainwise/lib/python3.13/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/trainwise/lib/python3.13/site-packages/huggingface_hub/file_download.py:1007\u001b[39m, in \u001b[36mhf_hub_download\u001b[39m\u001b[34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[39m\n\u001b[32m    987\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[32m    988\u001b[39m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[32m    989\u001b[39m         local_dir=local_dir,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1004\u001b[39m         local_files_only=local_files_only,\n\u001b[32m   1005\u001b[39m     )\n\u001b[32m   1006\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1007\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[32m   1009\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[32m   1016\u001b[39m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1017\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1019\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1020\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[32m   1022\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/trainwise/lib/python3.13/site-packages/huggingface_hub/file_download.py:1168\u001b[39m, in \u001b[36m_hf_hub_download_to_cache_dir\u001b[39m\u001b[34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[39m\n\u001b[32m   1165\u001b[39m \u001b[38;5;66;03m# Local file doesn't exist or etag isn't a match => retrieve file from remote (or cache)\u001b[39;00m\n\u001b[32m   1167\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[32m-> \u001b[39m\u001b[32m1168\u001b[39m     \u001b[43m_download_to_tmp_and_move\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.incomplete\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1170\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1171\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1172\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1173\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1174\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1175\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1176\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1177\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1178\u001b[39m \u001b[43m        \u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1179\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1180\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(pointer_path):\n\u001b[32m   1181\u001b[39m         _create_symlink(blob_path, pointer_path, new_blob=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/trainwise/lib/python3.13/site-packages/huggingface_hub/file_download.py:1720\u001b[39m, in \u001b[36m_download_to_tmp_and_move\u001b[39m\u001b[34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download, etag, xet_file_data)\u001b[39m\n\u001b[32m   1718\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m xet_file_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m is_xet_available():\n\u001b[32m   1719\u001b[39m     logger.debug(\u001b[33m\"\u001b[39m\u001b[33mXet Storage is enabled for this repo. Downloading file from Xet Storage..\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1720\u001b[39m     \u001b[43mxet_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1721\u001b[39m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1722\u001b[39m \u001b[43m        \u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1723\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1724\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1725\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisplayed_filename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1726\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1727\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1728\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m xet_file_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m constants.HF_HUB_DISABLE_XET:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/trainwise/lib/python3.13/site-packages/huggingface_hub/file_download.py:621\u001b[39m, in \u001b[36mxet_get\u001b[39m\u001b[34m(incomplete_path, xet_file_data, headers, expected_size, displayed_filename, _tqdm_bar)\u001b[39m\n\u001b[32m    610\u001b[39m     displayed_filename = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdisplayed_filename[:\u001b[32m40\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m(…)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    612\u001b[39m progress_cm = _get_progress_bar_context(\n\u001b[32m    613\u001b[39m     desc=displayed_filename,\n\u001b[32m    614\u001b[39m     log_level=logger.getEffectiveLevel(),\n\u001b[32m   (...)\u001b[39m\u001b[32m    618\u001b[39m     _tqdm_bar=_tqdm_bar,\n\u001b[32m    619\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m621\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m progress_cm \u001b[38;5;28;01mas\u001b[39;00m progress:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprogress_updater\u001b[39m(progress_bytes: \u001b[38;5;28mfloat\u001b[39m):\n\u001b[32m    624\u001b[39m         progress.update(progress_bytes)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/trainwise/lib/python3.13/site-packages/tqdm/std.py:1138\u001b[39m, in \u001b[36mtqdm.__exit__\u001b[39m\u001b[34m(self, exc_type, exc_value, traceback)\u001b[39m\n\u001b[32m   1135\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m   1136\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1138\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_value, traceback):\n\u001b[32m   1139\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1140\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from langchain_text_splitters import SentenceTransformersTokenTextSplitter\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nomic-ai/modernbert-embed-base\")\n",
    "\n",
    "\n",
    "text_splitter = SentenceTransformersTokenTextSplitter.from_huggingface_tokenizer(\n",
    "    tokenizer=tokenizer,\n",
    "    tokens_per_chunk=1500,\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(all_docs)\n",
    "print(f\"Total chunks created: {len(chunks)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ee5a35ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "—Large models represent a groundbreaking advance-\n",
      "ment in multiple application fields, enabling remarkable achieve-\n",
      "ments across various tasks. However, their unprecedented scale\n",
      "comes with significant computational costs. These models, often\n",
      "consisting of billions of parameters, require vast amounts of\n",
      "computational resources for execution. Especially, the expansive\n",
      "scale and computational demands pose considerable challenges\n",
      "when customizing them for particular downstream tasks, particu-\n",
      "larly over the hardware platforms constrained by computational\n",
      "capabilities.\n",
      "Parameter Efficient Fine-Tuning (PEFT) provides a practical\n",
      "solution by efficiently adjusting the large models over the various\n",
      "downstream tasks. In particular, PEFT refers to the process of\n",
      "adjusting the parameters of a pre-trained large model to adapt\n",
      "it to a specific task or domain while minimizing the number\n",
      "of additional parameters introduced or computational resources\n",
      "required. This approach is particularly important when dealing\n",
      "with large-scale language models with high parameter counts,\n",
      "as fine-tuning these models from scratch can be computationally\n",
      "expensive and resource-intensive, posing considerable challenges\n",
      "in the supporting system platform design.\n",
      "In this survey, we present comprehensive studies of various\n",
      "PEFT algorithms, examining their performance and computa-\n",
      "tional overhead. Moreover, we provide an overview of applica-\n",
      "tions developed using different PEFT algorithms and discuss\n",
      "common techniques employed to mitigate computation costs\n",
      "for PEFT. In addition to providing an extensive survey from\n",
      "an algorithmic standpoint, we also examine various real-world\n",
      "system designs to investigate the implementation costs associated\n",
      "with different PEFT approaches. This survey serves as a valuable\n",
      "resource for researchers aiming to understand both the PEFT al-\n",
      "gorithm and its system implementation, offering detailed insights\n",
      "into recent advancements and practical applications.\n",
      "Index Terms—Large Language Model, Parameter-Efficient\n",
      "Fine-tuning, Computer System, Distributed System.\n",
      "I. INTRODUCTION\n",
      "Large Models (LMs) have recently captured considerable\n",
      "public interest. Their ability to understand context and nuances\n",
      "enables them to proficiently handle diverse tasks across mul-\n",
      "tiple domains, including natural language processing (NLP),\n",
      "computer vision (CV), etc. In the field of NLP, Large Lan-\n",
      "guage Models (LLMs) have achieved significant advance-\n",
      "ments across various tasks including text generation [1], [2],\n",
      "translation [3], [4], personalized chat-bots [5], [6], [7], and\n",
      "summarization [8], demonstrating remarkable proficiency.\n",
      "* Corresponding author\n",
      "Earlier studies [1] have suggested that LLMs exhibit high\n",
      "levels of generalization, enabling them to apply their acquired\n",
      "knowledge to new tasks not included in their original training.\n",
      "This capability is commonly known as zero-shot learning.\n",
      "Nevertheless, fine-tuning remains essential to further enhance\n",
      "LLMs for optimal performance on new user datasets and tasks.\n",
      "Due to its scale, a widely adopted strategy for fine-tuning\n",
      "LLMs involves adjusting a limited number of LLM parame-\n",
      "ters while keeping the remainder unchanged. This technique,\n",
      "termed Parameter-Efficient-Fine-Tuning (PEFT), involves se-\n",
      "lectively adjusting a small proportion of their parameters while\n",
      "keeping the rest unaltered. Furthermore, the application of\n",
      "PEFT extends beyond the realm of NLP and quickly attracts\n",
      "interest in the CV community for handling fine-tuning vision\n",
      "models with large parameters, such as Vision Transformers\n",
      "(ViT) and diffusion models, as well as disciplinary models\n",
      "such as vision-language models.\n",
      "In this survey, we systematically review and categorize\n",
      "recent advancements in PEFT algorithms as well as the system\n",
      "implementation costs associated with various PEFT algorithms\n",
      "across diverse scenarios. Figure 1 presents the overview con-\n",
      "tent for this survey. In section II, we present some fundamental\n",
      "concepts for LLM and PEFT, including computational flow\n",
      "for LLM, basic knowledge of PEFT, commonly used datasets\n",
      "and tasks, and evaluation benchmarks. We categorize all\n",
      "types of PEFT algorithms in Section III according to their\n",
      "computational flow. In Section III-A, we detail additive algo-\n",
      "rithms that either introduce new weight parameters or modify\n",
      "activations. Algorithms that only require fine-tuning of existing\n",
      "parameters are categorized as selective approaches, which are\n",
      "introduced in Section III-B. In Section III-C, we explore\n",
      "reparameterized PEFT, which constructs a (low- dimensional)\n",
      "reparameterization of original model parameters for training\n",
      "while transforming the weights back to maintain the inference\n",
      "speed. Additionally, there exist algorithms that combine the\n",
      "above techniques, and we have classified these as hybrid\n",
      "approaches, elaborating on them in Section III-D. We also\n",
      "investigate strategies for further reducing the computational\n",
      "complexity of different PEFT algorithms, including KV-cache\n",
      "management, pruning, quantization, and memory optimization,\n",
      "in Section IV.\n",
      "In Section V, we expand the scope of this survey beyond\n",
      "the computational perspective to involve various potential\n",
      "application scenarios. Specifically, we explore innovations that\n",
      "arXiv:2403.14608v7  [cs.LG]  16 Sep 2024\n",
      "2\n",
      "Background\n",
      "Computational \n",
      "flow for LLM\n",
      "PEFT \n",
      "Taxonomy\n",
      "Selective\n",
      "PEFT\n",
      "Additive\n",
      "PEFT\n",
      "System Design\n",
      "Challenge\n",
      "System Design \n",
      "for PEFT\n",
      "Centralized PEFT \n",
      "Serving System\n",
      "PEFT for \n",
      "LLMs\n",
      "Distributed PEFT \n",
      "Training System\n",
      "Hybrid\n",
      "PEFT\n",
      "PEFT\n",
      "overview\n",
      "Reparameterized \n",
      "PEFT\n",
      "Efficient PEFT\n",
      "Design\n",
      "KV-cache \n",
      "Management for \n",
      "PEFT Efficiency\n",
      "PEFT Pruning\n",
      "PEFT \n",
      "Quantization\n",
      "Memory-efficient \n",
      "PEFT\n",
      "Parallel PEFT \n",
      "Training System\n",
      "Apply PEFT for \n",
      "other Applications\n",
      "PEFT for \n",
      "ViTs\n",
      "PEFT for \n",
      "VLAs\n",
      "PEFT for \n",
      "Diffusion Models\n",
      "Downstream\n",
      "tasks\n",
      "Section 2\n",
      "Section 3\n",
      "Section 4\n",
      "Section 5\n",
      "Section 6\n",
      "2.1\n",
      "2.2\n",
      "2.3\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "3.4\n",
      "4.1\n",
      "4.2\n",
      "4.3\n",
      "4.4\n",
      "5.1\n",
      "5.2\n",
      "5.3\n",
      "5.4\n",
      "6.1\n",
      "6.2\n",
      "6.3\n",
      "6.4\n",
      "Fig. 1: A content overview covered in the survey.\n",
      "applying PEFT techniques to different model architecture,\n",
      "including LLMs (Section V-A), Vision Transformer (Sec-\n",
      "tion V-B), Vision-Language alignment models (Section V-C),\n",
      "and Diffusion models (Section V-D), for varied downstream\n",
      "tasks, underscoring PEFT’s versatility and applicability in a\n",
      "range of scenarios. After that, in Section VI, we explore the\n",
      "system design challenge for PEFT methods. The discussion\n",
      "includes three advanced system solutions for practical PEFT\n",
      "deployment: PEFT query serving (Section VI-B), distributed\n",
      "tuning (Section VI-C), and concurrent PEFT tuning (Sec-\n",
      "tion VI-D). Finally, in Section VII, we summarize our survey\n",
      "and propose several potential future directions from both\n",
      "algorithmic and systemic perspectives, aiming to offer valuable\n",
      "insights for further research and development in the field.\n",
      "II. BACKGROUND\n",
      "In this section, we first discussed the computation flow of\n",
      "LLM, including its fundamental components, computational\n",
      "complexity, and the flow of computations it involves as a case\n",
      "study. We then provide a brief overview of different PEFT\n",
      "algorithms in section II-B.\n",
      "A. Computation flow for LLaMA\n",
      "In order to gain a deeper understanding of LLM and other\n",
      "Transformer-based models, we employ LLaMA-7B, a cutting-\n",
      "edge open-source LLM model, to scrutinize the architecture\n",
      "of LLM as well as Transformer. As shown in Figure 2 (a),\n",
      "LLaMA consists of three major components: an embedding\n",
      "block, a stack of decoder blocks, and a head block which\n",
      "consists of linear and softmax layer. The embedding layer’s\n",
      "primary role is to transform unstructured textual information,\n",
      "into chunks of discrete numerical vectors (tokens) to facilitate\n",
      "subsequent processing. The embedded tokens are then deliv-\n",
      "ered to the decoder layers for further processing. Each LLaMA\n",
      "decoder is composed of two fundamental components: Multi-\n",
      "head Self-Attention (MSA) and Feedforward Network (FFN).\n",
      "In the MSA module, each of the tokens will be clustered by\n",
      "an attention map obtained by a dot production between two\n",
      "linear mappings of the input tokens. Then the grouped tokens\n",
      "will be further processed by a Feedforward Neural network.\n",
      "Additionally, Root Mean Square Layer Normalization (RM-\n",
      "SNorm) [9] is adopted in LLaMA as a replacement for Layer\n",
      "Normalization to ensure efficient training.\n",
      "LLM distinguishes itself from other deep neural network\n",
      "(DNN) models such as convolutional neural networks (CNN)\n",
      "in two significant ways. Firstly, LLM exhibits an inherent\n",
      "autoregressive nature, necessitating multiple iterations to com-\n",
      "plete the generation task. Moreover, LLM incorporates an\n",
      "attention mechanism, a component with computational com-\n",
      "plexity that scales quadratically with the length of the inputs.\n",
      "On the other hand, the inherent computation characteristic of\n",
      "LLM lies in the attention blocks inside each decoder layer.\n",
      "Figure 2 (c) depicts the high-level overview of the computation\n",
      "flow in the attention block.\n",
      "During the inference process, each decoder takes a three-\n",
      "dimensional tensor x P Rbˆlˆd as the input tokens. The\n",
      "input tokens are first multiplied with three weight matrices\n",
      "WQ, WK, and WV , producing the output referred to as\n",
      "query(Q), key(K) and value(V ). Given the MSA module’s\n",
      "inability to recognize positional data and the inherent auto-\n",
      "regressive nature of LLMs, the query and key will undergo\n",
      "a process using Rotary Positional Embedding [10] (RoPE,\n",
      "denoted as Rp.q in Eq 1) to encode the position information.\n",
      "Subsequently, the key and value will be combined with prior\n",
      "tokens.\n",
      "After the positional embedding, the intermediate activation\n",
      "will then undergo a series of multiplication, softmax, and\n",
      "residual addition to generate MSA output as described in Eq 9.\n",
      "To be noted here, dk in the equation refers to the number of\n",
      "feature dimensions in the multi-head attention mechanism.\n",
      "Q, K, V “ RpWqxq, RpWkxq, Wvx\n",
      "(1)\n",
      "SApxq “ Softmaxp QKT\n",
      "?dhead\n",
      "qV\n",
      "(2)\n",
      "MSApxq “ rSA1pxq; SA2pxq; . . . ; SAkpxqsWo\n",
      "(3)\n",
      "The SA output will then be forwarded to the FFN blocks\n",
      "for further processing. The FFN block will have another three\n",
      "3\n",
      "<BOS>\n",
      "FFN\n",
      "SA\n",
      "LLaMA\n",
      "ML\n",
      "ML\n",
      "is\n",
      "LLaMA\n",
      "LLaMA\n",
      "is\n",
      "awesome\n",
      "LLaMA\n",
      "awesome\n",
      "<EOS>\n",
      "Decoder\n",
      "Decoder\n",
      "Decoder\n",
      "Linear &\n",
      "Softmax\n",
      "…\n",
      "Q\n",
      "K\n",
      "V\n",
      "LoRA\n",
      "FC\n",
      "FC\n",
      "ReLU\n",
      "Adapter\n",
      "Decoder\n",
      "SA\n",
      "FFN\n",
      "Wdown\n",
      "Wup\n",
      "Input tokens\n",
      "Prompt\n",
      "(c)\n",
      "Embedding\n",
      "(b)\n",
      "(a)\n",
      "Fig. 2: (a) LLaMA architecture. (b) LLaMA auto-regressive pattern. (c) Three common PEFT operations. All the learnable\n",
      "components are highlighted in red, while the frozen components are highlighted in grey. LoRA is applied on all the Query, Key,\n",
      "and Value blocks. The adapter targets the FFN module. Soft-Prompt focused on tuning the input activation of each decoder.\n",
      "We only show one decoder for illustration simplicity.\n",
      "matrices Wup, Wdown, and Wgate and the computation can be\n",
      "illustrated by:\n",
      "FFNLLaMapxq “ WuppSiLUpWgatexq d pWdownxqq ` x,\n",
      "(4)\n",
      "where x denotes the input of the FFN layer, and SiLU\n",
      "is the nonlinear function used in LLaMA. In the original\n",
      "Transformer, the FFN block can be demonstrated by:\n",
      "FFNT ransfomerpxq “ WuppReLUpWdownxqq ` x.\n",
      "(5)\n",
      "The output of the last decoder layer will be sent to a\n",
      "linear layer, which then generates a probability distribution\n",
      "spanning the complete vocabulary to predict the next token in\n",
      "the sequence. The produced token will then be concatenated\n",
      "with the previous tokens and used as the input for the next\n",
      "round of processing. This generating process repeats in an\n",
      "auto-regressive manner until a full sequence of tokens, referred\n",
      "to as a completion, is produced (Figure 2 (b)). For training, the\n",
      "computation flow is similar to that for inference, except that\n",
      "the generated sentences are directly compared to the ground\n",
      "truth output and generate the training loss. Gradients will then\n",
      "be computed across the LLM weights to minimize this training\n",
      "loss.\n",
      "To analyze the computation cost and memory overhead\n",
      "in LLM, we also set a series of parameters used in later\n",
      "section III. Table I shows the parameter size and computation\n",
      "dimension in the LLaMA-7B model as a starting example.\n",
      "LLM models generate tokens (words) one for each round,\n",
      "depicted in Fig 2, based on the previous prompt (input) and\n",
      "previously generated sequence. This process will be repeated\n",
      "until the model outputs hits and termination token. To accel-\n",
      "erate the inference process in LLM models, people take the\n",
      "strategy of storing the previous Keys and Values in the Key-\n",
      "Value cache (KV-cache), so they don’t need to recalculate\n",
      "them for each new token. Mathematically, we can represent\n",
      "the total decoders’ KV-cache memory cost in equation 6. In\n",
      "the equation, l and b are the context length and batch size\n",
      "and L refers to the number of layers. The dhead is the head\n",
      "dimension and nhead is the number of heads.\n",
      "Size “ L ˆ 2 ˆ b ˆ l ˆ dhead ˆ nhead\n",
      "(6)\n",
      "B. Overview on Parameter Efficient Fine Tuning\n",
      "Fine-tuning remains essential to enhance LLM performance\n",
      "on unseen user datasets and tasks. With the size of the model\n",
      "growing (e.g. 1.5B in GPT-2 to 175B in GPT-3), standard\n",
      "full fine-tuning paradigm requires thousands of GPU work\n",
      "in parallel, which is highly inefficient and unsustainable. A\n",
      "type of algorithm has been raised namely Parameter-efficient\n",
      "fine-tuning (PEFT) which aims to tune minimal parameters\n",
      "to achieve better performance over full tuning on downstream\n",
      "tasks.\n",
      "In parallel developments, large-scale pre-trained models in\n",
      "vision and multimodal domains have also demonstrated their\n",
      "effective representational learning capabilities, enabling adap-\n",
      "tation from large datasets to smaller ones or across various data\n",
      "modalities through fine-tuning. Consequently, this capability\n",
      "has made PEFT increasingly attractive to the wider research\n",
      "community.\n",
      "We categorized the PEFT algorithms into additive, selec-\n",
      "tive, reparameterized, and hybrid fine-tuning based on their\n",
      "operations. As Figure 3 depicts, three major additive fine-\n",
      "tuning algorithms are normally used: (1) Adapter; (2) Soft\n",
      "Prompt; (3) Others. They differ from each other in terms of the\n",
      "different additional tunable modules or parameters. Selective\n",
      "fine-tuning, on the other hand, doesn’t require any additional\n",
      "parameters, it selects a small subset of parameters from the\n",
      "backbone model and only makes them tunable while keeping\n",
      "the majority of parameters untouched during fine-tuning on\n",
      "downstream tasks. We categorized selective fine-tuning based\n",
      "4\n",
      "TABLE I: Configuration parameters and computation operation for LLaMA-7B architecture\n",
      "Operation\n",
      "Weights Symbol\n",
      "Weights Dimension\n",
      "Input Tensor Dimension\n",
      "Complexity\n",
      "Eq. 1\n",
      "WQ, WK, WV\n",
      "d ˆ k ˆ d\n",
      "k\n",
      "b ˆ l ˆ d\n",
      "Oplq\n",
      "Eq. 2\n",
      "-\n",
      "-\n",
      "b ˆ l ˆ 3 ˆ k ˆ d\n",
      "k\n",
      "Opl2q\n",
      "Eq. 3\n",
      "Wo\n",
      "d ˆ d\n",
      "b ˆ l ˆ d\n",
      "Oplq\n",
      "Eq. 4\n",
      "Wup, Wdown, Wgate\n",
      "d ˆ 4d\n",
      "b ˆ l ˆ d OR l ˆ b ˆ 4d\n",
      "Oplq\n",
      "on the grouping of chosen parameters: (1) Unstructural Mask-\n",
      "ing; and (2) Structural Masking. Reparametrization represents\n",
      "transforming model parameters between two equivalent forms.\n",
      "Specifically, reparametrized fine-tuning introduces addi-\n",
      "tional low-rank trainable parameters during training, which\n",
      "are then integrated with the original model for inference. This\n",
      "approach is categorized into two main strategies: (1) Low-\n",
      "rank Decomposition, and (2) LoRA Derivatives. Hybrid fine-\n",
      "tuning explores the design spaces of different PEFT methods\n",
      "and combines their advantages.\n",
      "C. Downstream Tasks for LLM Evaluation\n",
      "Two types of tasks have been widely used for LLM eval-\n",
      "uation, the first type is the General Language Understand-\n",
      "ing Evaluation (GLUE) [11] benchmark, which integrates\n",
      "nine sentence or sentence-pair language understanding tasks\n",
      "(CoLA, SST-2, MRPC, STS-B, QQP, MNLI, QNLI, RTE, and\n",
      "WNLI), chosen for their diversity in dataset sizes, text genres,\n",
      "and difficulty levels, and is based on established existing\n",
      "datasets. It also includes a diagnostic dataset specifically\n",
      "designed to evaluate and analyze model performance across\n",
      "various linguistic phenomena inherent in natural language.\n",
      "Additionally, it features a public leaderboard to track perfor-\n",
      "mance on the benchmark and a dashboard to visualize model\n",
      "performance on the diagnostic set.\n",
      "The other type of dataset that has been used in recent\n",
      "LLM papers is common sense reasoning which integrated\n",
      "into our study caters to a variety of research facets: (1)\n",
      "OpenBookQA [12] is curated to foster research in advanced\n",
      "question-answering, delving into a profound understanding\n",
      "of both the subject matter and the language in which it\n",
      "is articulated. (2) PIQA [13] primarily emphasizes everyday\n",
      "scenarios, demonstrating a predilection for unconventional\n",
      "solutions. (3) Social IQA [14] emerges as a novel question-\n",
      "answering benchmark tailored for gauging social common-\n",
      "sense intelligence. (4) HellaSwag [15] serves as a dataset, the\n",
      "essence of which is to ascertain the capability of machines in\n",
      "aptly concluding sentences. (5) BoolQ [16] is a dataset dedi-\n",
      "cated to question-answering, particularly for binary responses\n",
      "(yes/no queries). (6) WinoGrande [17] is introduced as a fresh\n",
      "compilation, encompassing a substantial 44,000 problems. (7)\n",
      "ARC-easy [18] presents itself as a novel dataset constituting\n",
      "genuine grade-school level multiple-choice science questions,\n",
      "designed to invigorate research in intricate question-answering.\n",
      "(8) ARC-challenges [18], distinctively, encompasses solely\n",
      "those questions that were inaccurately addressed by both a\n",
      "retrieval-based algorithm and a word co-occurrence algorithm.\n",
      "Image recognition is the primary benchmark and application\n",
      "for vision models, exemplified by benchmarks such as fine-\n",
      "grained visual categorization (FGVC) and visual task adapta-\n",
      "tion benchmark (VTAB). Beyond image classification, video\n",
      "action recognition is another key application area, involving\n",
      "datasets like Kinetics-400 [19], SSv2 [20], and HMDB51 [21].\n",
      "Additionally, PEFT has been utilized for dense prediction\n",
      "tasks, using datasets like MSCOCO [22], ADE20K [23], and\n",
      "PASCAL VOC [24].\n",
      "D. Evaluation Benchmarks for PEFT\n",
      "To help readers evaluate the performance differences be-\n",
      "tween various PEFT methods under a unified standard, a com-\n",
      "prehensive benchmark is essential. Next, we discuss several\n",
      "commonly used benchmarks.\n",
      "From the algorithmic perspective, [25] benchmarks the\n",
      "performance of several PEFT algorithms across more than\n",
      "100 NLP tasks and conducts systematic experiments based on\n",
      "criteria such as performance, convergence, efficiency, combin-\n",
      "ability, scalability, and transferability. Similarly, [26] and [27]\n",
      "have also established targeted benchmarks to evaluate different\n",
      "PEFT algorithms.\n",
      "From the system perspective, three commonly used bench-\n",
      "marks are outlined below to evaluate system performance.\n",
      "The first benchmark is the ShareGPT dataset [28], which\n",
      "includes real-world interactions with OpenAI’s ChatGPT. It\n",
      "encompasses a broad spectrum of conversational queries and\n",
      "responses that are representative of typical user interactions\n",
      "with large language models (LLMs). This dataset is vital\n",
      "for evaluating the system’s ability to manage diverse and\n",
      "realistic conversational requirements, focusing on the accuracy\n",
      "of responses and efficiency in handling requests.\n",
      "The second benchmark involves the Microsoft Azure Func-\n",
      "tion Trace from the years 2019 and 2021 [29], containing\n",
      "logs from serverless computing activities via Azure Functions.\n",
      "While these logs are from a general serverless computing con-\n",
      "text rather than LLM-specific applications, they offer insights\n",
      "into the computational demands driven by events. These traces\n",
      "simulate the arrival patterns and workload intensities that LLM\n",
      "systems might face, including irregular and peak demands,\n",
      "thus acting as practical proxies for LLM inference tasks.\n",
      "The third benchmark is based on the Gamma process [30],\n",
      "a prevalent approach in simulations to model the timing of\n",
      "incoming requests in queueing and service systems. This\n",
      "method facilitates the creation of workloads with varied arrival\n",
      "rates and patterns, producing synthetic, yet realistic request\n",
      "5\n",
      "PEFT Methods for PLMs\n",
      "Additive\n",
      "Fine-tuning\n",
      "Adapter-based\n",
      "Fine-tuning\n",
      "Adapter Design\n",
      "Serial Adapter [31], Parallel Adapter [32], CIAT [33], CoDA [34]\n",
      "Multi-task Adaptation\n",
      "AdapterFusion [35], AdaMix [36], PHA [37], AdapterSoup [38], MerA [39], Hyperformer [40]\n",
      "Soft Prompt-based\n",
      "Fine-tuning\n",
      "Soft Prompt Design\n",
      "Prefix-tuning [41], Prefix-Propagation [42], p-tuning v2 [43], APT [44], p-tuning [45],\n",
      "prompt-tuning [46], Xprompt [47], IDPG [48], LPT [49], SPT [50], APrompt [51]\n",
      "Training Speedup\n",
      "SPoT [52], TPT [53], InfoPrompt [54], PTP [55], IPT [56], SMoP [57], DePT [58]\n",
      "Others\n",
      "(IA)3 [59], MoV [60], SSF [61], IPA [62]\n",
      "Selective\n",
      "Fine-tuning\n",
      "Unstructural\n",
      "Masking\n",
      "U-Diff pruning [63], U-BitFit [64], PaFi [65], FishMask [66], Fish-Dip [67], LT-SFT [68], SAM [69], Child-tuning [70]\n",
      "Structural Masking\n",
      "S-Diff pruning [63], S-BitFit [64], FAR [71], Bitfit [72], Xattn Tuning [73], SPT [74]\n",
      "Reparameterized\n",
      "Fine-tuning\n",
      "Low-rank\n",
      "Decomposition\n",
      "Intrinsic SAID [75], LoRA [76], Compacter [77], KronA [78], KAdaptation [79], HiWi [65], VeRA [80], DoRA [81]\n",
      "LoRA Derivatives\n",
      "Dynamic Rank\n",
      "DyLoRA [82], AdaLoRA [83], SoRA [84], CapaBoost [85], AutoLoRA [86]\n",
      "LoRA Improvement\n",
      "Laplace-LoRA [87], LoRA Dropout [88], PeriodicLoRA [89], LoRA+ [90], MoSLoRA [91]\n",
      "Multiple LoRA\n",
      "LoRAHub [92], MOELoRA [93], MoLORA [60], MoA [94], MoLE [95], MixLoRA [96]\n",
      "Hybrid\n",
      "Fine-tuning\n",
      "UniPELT [97], S4 [98], MAM Adapter [32], NOAH [99], AUTOPEFT [100], LLM-Adapters [101], S3PET [102]\n",
      "Fig. 3: Taxonomy of Parameter-Efficient Fine-Tuning Methods for Large Models.\n",
      "Output\n",
      "Combine\n",
      "Frozen\n",
      "Learnable\n",
      "Input\n",
      "Output\n",
      "Input\n",
      "(c) Reparameterization PEFT\n",
      "(a) Additive PEFT\n",
      "(b) Selective PEFT\n",
      "Merge\n",
      "Output\n",
      "Input\n",
      "Input (train)\n",
      "Fig. 4: Different types of PEFT algorithms.\n",
      "scenarios that a system could encounter during actual opera-\n",
      "tions. Such synthetic workloads are crucial for testing system\n",
      "performance under controlled conditions that resemble real-\n",
      "world user activity.\n",
      "III. PEFT TAXONOMY\n",
      "The PEFT strategies can be broadly classified into four\n",
      "categories: additive PEFT (Section III-A), which modifies\n",
      "the model architecture by injecting new trainable modules\n",
      "or parameters; selective PEFT (Section III-B), which makes\n",
      "a subset of parameters trainable during fine-tuning; repa-\n",
      "rameterized PEFT (Section III-C), which constructs a (low-\n",
      "dimensional) reparameterization of the original model param-\n",
      "eters for training, then equivalently transforms it back for\n",
      "inference; and hybrid PEFT (Section III-D), which combines\n",
      "advantages from different PEFT methods to build a unified\n",
      "PEFT model. An overview of different types of PEFT algo-\n",
      "rithms is depicted in Figure 4.\n",
      "A. Additive PEFT\n",
      "Standard full fine-tuning entails substantial computational\n",
      "expenses and could also potentially harm the model’s gener-\n",
      "alization ability. To mitigate this problem, a widely employed\n",
      "approach is to maintain the pre-trained backbone unchanged\n",
      "and introduce only a minimal number of trainable parameters\n",
      "that are strategically positioned within the model architecture.\n",
      "While fine-tuning for a specific downstream task, only the\n",
      "weights of these additional modules or parameters are updated,\n",
      "which results in a substantial reduction in storage, memory,\n",
      "and computational resource requirements. Due to their char-\n",
      "acteristic of adding parameters, these techniques can be termed\n",
      "as Additive Tuning, as shown in Figure 4 (a). Next, we discuss\n",
      "several popular Additive PEFT algorithms.\n",
      "1) Adapters: Adapter approaches involve the insertion of\n",
      "small adapter layers within Transformer blocks. Typically, an\n",
      "adapter layer consists of a down-projection matrix Wdown P\n",
      "Rrˆd, followed by a non-linear activation function σp¨q, and\n",
      "an up-projection matrix Wup P Rdˆr. In this context, d\n",
      "represents the dimension of the hidden layer, and r serves\n",
      "as the bottleneck dimension, which is a hyperparameter used\n",
      "in configuring the adapters. Denote hin as the input to the\n",
      "adapter, the computation within the adapter module (with\n",
      "residual) can be summarized as follows:\n",
      "Adapterpxq “ WupσpWdownxq ` x.\n",
      "(7)\n",
      "The concept of adapters in the field of NLP was initially\n",
      "introduced by Serial Adapter [31] as shown in Figure 5\n",
      "(a). In their approach, each Transformer block is enhanced\n",
      "by adding two adapter modules, with one positioned after\n",
      "the self-attention layer and the other after the FFN layer,\n",
      "respectively. Subsequent research has aimed to address the\n",
      "additional computational cost associated with adapter layers.\n",
      "A modified framework AdapterFusion [35] was proposed,\n",
      "where adapter layers are inserted only after the ’Add & Norm’\n",
      "step following the FFN layer to enhance the computational\n",
      "efficiency. The adapters mentioned above follow a sequen-\n",
      "tial design, placing adapter layers as bottlenecks within the\n",
      "Transformer blocks. This approach may potentially reduce the\n",
      "model’s parallelism and require a trade-off between inference\n",
      "efficiency and accuracy. In contrast, [32] introduced a parallel\n",
      "6\n",
      "Transformer \n",
      "Module\n",
      "Adapter\n",
      "Transformer \n",
      "Module\n",
      "(a) Serial Adapter \n",
      "(b) Parallel Adapter \n",
      "Transformer \n",
      "Module\n",
      "(c) CoDA\n",
      "all tokens\n",
      "all tokens\n",
      "top-k tokens\n",
      "all tokens\n",
      "(d) Adapter Layer\n",
      "Wdown\n",
      "Wup\n",
      "ReLU\n",
      "Adapter\n",
      "Adapter\n",
      "Fig. 5: Illustration of three representative adapter-based fine-tuning algorithms. Blue represents frozen, while yellow represents\n",
      "trainable.\n",
      "adapter (PA) approach as depicted in Figure 5 (b), which\n",
      "reorganizes the traditionally sequential adapter layers into a\n",
      "parallel side-network that runs alongside each Transformer\n",
      "sublayer. Similarly, CIAT [33], CoDA [34] and KronA [78]\n",
      "also adopts a parallel adapter design. Except for the parallel\n",
      "design, CoDA employs a sparse activation mechanism to\n",
      "improve the inference efficiency as shown in Figure 5 (c).\n",
      "Specifically, CoDA uses a soft top-k selection process that\n",
      "identifies k important tokens in each layer, which will be\n",
      "processed by both the frozen pre-trained Transformer layer and\n",
      "the adapter branch to maintain model accuracy. In contrast,\n",
      "those unimportant tokens are only processed by the adapter\n",
      "branch while skipping the heavy pre-trained layer, therefore\n",
      "optimizing for inference efficiency without compromising\n",
      "overall performance.\n",
      "To enhance the performance and generalization of adapters,\n",
      "various studies have implemented multi-task learning strate-\n",
      "gies, such as AdapterFusion [35], AdaMix [36], PHA [37],\n",
      "AdapterSoup [38], MerA [39], and Hyperformer [40].\n",
      "AdapterFusion keeps all pre-trained adapters in the model\n",
      "and employs a fusion module to merge the multi-task in-\n",
      "formation. Unlike AdapterFusion, MerA merges pretrained\n",
      "adapters into a single one through optimal transport based\n",
      "on weights and activations. This approach avoids introduc-\n",
      "ing any additional trainable parameters, thereby enhancing\n",
      "computational efficiency. Hyperformer stores the multi-task\n",
      "information in a shared hypernetwork, which generates task\n",
      "and layer-specific adapter parameters conditioned on task and\n",
      "layer ID embeddings. Given a new task, only an additional\n",
      "task embedding needs to be learned, therefore reducing the\n",
      "number of trained parameters.\n",
      "2) Soft Prompt: Alternatively, prompt tuning presents an\n",
      "additional approach for refining the model to achieve improved\n",
      "performance through fine-tuning. Instead of optimizing dis-\n",
      "crete token representations through in-context learning, there\n",
      "is a prevailing belief that the continuous embedding space\n",
      "of soft prompts inherently contains more information [103].\n",
      "Drawing inspiration from this concept, researchers directly\n",
      "prepend adjustable vectors, referred to as soft prompts, to the\n",
      "start of the input sequence. This can be represented as follows:\n",
      "Xplq “ rsplq\n",
      "1 , . . . , splq\n",
      "NS, xplq\n",
      "1 , . . . , xplq\n",
      "NXs\n",
      "(8)\n",
      "where Xplq is the sequence of input tokens for layer l,\n",
      "including soft prompt tokens splq\n",
      "i\n",
      "followed by the original input\n",
      "tokens xplq\n",
      "i . NS is the number of soft prompt tokens, and NX\n",
      "is the number of original input tokens.\n",
      "Prefix-tuning [41] introduces learnable vectors that are\n",
      "prepended to keys k and values v across all Transformer layers.\n",
      "To ensure stability during the optimization process, Prefix-\n",
      "tuning adopts a reparameterization strategy, which utilizes\n",
      "an MLP layer to generate these prefix vectors rather than\n",
      "optimizing them directly. After fine-tuning, only the prefix\n",
      "vectors are saved for inference. This technique has been\n",
      "adapted and improved in several studies [42], [43], [44]. For\n",
      "instance, p-tuning v2 [43] removes reparameterization and\n",
      "expands its usage to broader model scales and NLP tasks.\n",
      "APT (Adaptive Prefix Tuning) [44] enhances Prefix-tuning by\n",
      "introducing an adaptive gate mechanism to control the prefix\n",
      "importance in each layer. Concurrent work p-tuning [45]\n",
      "and prompt-tuning [46] apply learnable vectors only at the\n",
      "initial word embedding layer rather than all layers to enhance\n",
      "training and inference efficiency. It’s important to highlight\n",
      "that prompt-tuning demonstrates its effectiveness primarily in\n",
      "the context of large models, specifically those with over 11\n",
      "billion parameters [46]. Complementing this, Xprompt [47]\n",
      "eliminates the negative prompt tokens through a hierarchi-\n",
      "cally structured pruning, which closes the performance gap at\n",
      "smaller model scales. [104] provides some theoretical analysis\n",
      "towards prompt tuning, demonstrating its universality and\n",
      "limitations in limited-depth Transformers. IDPG (Instance-\n",
      "Dependent Prompt Generation) [48] improves prompt tuning\n",
      "by generating prompts based on each input sentence with\n",
      "a lightweight prompt generator. In a related approach, LPT\n",
      "(Late Prompt Tuning) [49] also leverages a prompt generator\n",
      "to obtain instance-aware prompt. Unlike previous work, LPT\n",
      "adds these prompts only after an intermediate layer, rather than\n",
      "at the initial or all layers. This strategic placement eliminates\n",
      "the gradient calculation below the intermediate layer, thereby\n",
      "significantly accelerating the training speed. Simultaneously,\n",
      "LPT can improve the overall performance due to the shorter\n",
      "backpropagation path preserves more task-related information.\n",
      "Inspired by LPT, SPT (Selective Prompt Tuning) [50] delves\n",
      "deeper into the importance of prompt inserting strategies.\n",
      "It introduces a learnable probabilistic gate in each layer to\n",
      "determine whether to use the prompt propagated from the pre-\n",
      "vious layer or inject a newly generated prompt. APrompt [51]\n",
      "employs another prompt inserting strategy. In addition to input\n",
      "prompts inserted at the beginning of the input sequence for\n",
      "each Transformer layer, APrompt also prepends additional\n",
      "learnable prompts to the respective query, key, and value\n",
      "7\n",
      "V\n",
      "K\n",
      "Q\n",
      "⊙\n",
      "lk\n",
      "lv\n",
      "lff\n",
      "⊙\n",
      "softmax\n",
      "Wdown\n",
      "𝛔\n",
      "⊙\n",
      "Wup\n",
      "(a) (IA)3\n",
      "⊙\n",
      "⊕\n",
      "Operation 1\n",
      "Operation 2\n",
      "(b) SSF\n",
      "scale\n",
      "shift\n",
      "Fig. 6: Illustration of (IA)3 and SSF. Blue represents frozen,\n",
      "while yellow represents trainable.\n",
      "matrices in the self-attention blocks to learn new attention\n",
      "patterns. Besides, APrompt incorporates the learning of a task-\n",
      "specific head.\n",
      "The concept of soft prompts has been employed for various\n",
      "downstream tasks [105], [106], although their training can\n",
      "be prone to instability and slow convergence. To address\n",
      "this, SPoT [52] uses a source prompt learned from one or\n",
      "multiple tasks to initialize prompts for new tasks. Similarly,\n",
      "the transfer of soft prompts from one task to initialize another\n",
      "is proposed in TPT (transferable prompt tuning) [53], which\n",
      "demonstrates that a better prompt initialization results in a\n",
      "large training convergence speedup. InfoPrompt [54] develops\n",
      "two mutual information-based loss functions, i.e., head loss\n",
      "and representation loss, to find better prompt initialization\n",
      "and learn sufficient task-relevant information, thereby also\n",
      "expediting convergence. PTP [55] delves into the root causes\n",
      "of training instability. It identifies the steep nature of the loss\n",
      "landscape in conventional prompt tuning, where minor varia-\n",
      "tions in input data can lead to significant loss fluctuations. To\n",
      "mitigate this, PTP introduces perturbation-based regularizers\n",
      "to smooth the loss landscape and consequently stabilize the\n",
      "training process. DePT [58] decomposes the soft prompt into\n",
      "a shorter soft prompt with a pair of low-rank matrices, which\n",
      "are optimized with two distinct learning rates. This strategy\n",
      "not only improves performance but also enhances training and\n",
      "inference efficiency. SMoP (Sparse Mixture-of-Prompts) [57]\n",
      "reduce the training and inference cost by utilizing short soft\n",
      "prompts. During training, multiple short soft prompts are\n",
      "trained, each tailored to specific subsets of the dataset. During\n",
      "inference, SMoP integrates a gating mechanism that routes\n",
      "each input instance to an appropriate short prompt. This\n",
      "technique not only increases efficiency in both training and\n",
      "inference stages but also retains performance comparable to\n",
      "those achieved with longer soft prompts. To further cut down\n",
      "the number of soft prompt parameters, IPT (Intrinsic Prompt\n",
      "Tuning) [56] identifies an intrinsic task subspace by training\n",
      "an auto-encoder on multiple tasks. Tuning on new tasks then\n",
      "requires adjusting only a few parameters within this subspace,\n",
      "significantly reducing the number of training parameters.\n",
      "3) Other Additive Methods: Apart from the methods men-\n",
      "tioned above, there appear other approaches that strategi-\n",
      "cally incorporate additional parameters during the fine-tuning\n",
      "process. For example, (IA)3 [59] introduces three learnable\n",
      "rescaling vectors: lk P Rdk, lv P Rdv, and lff P Rdff ,\n",
      "to rescale the key, value, and FFN activations, respectively,\n",
      "as depicted in Figure 6 (a). The operations within the self-\n",
      "attention block can be described as follows:\n",
      "SApxq “ SoftmaxpQplk d KT q\n",
      "?dhead\n",
      "qpplv d V q.\n",
      "(9)\n",
      "In FFN, the rescaling can be denoted as:\n",
      "FFNT ransfomerpxq “ Wupplff d σpWdownxqq,\n",
      "(10)\n",
      "where d is Hadamard product. Furthermore, the scale vectors\n",
      "lk and lv can be seamlessly integrated into the weight matrices\n",
      "of AQ and AW . This integration effectively eliminates the ex-\n",
      "tra computational costs during inference. A similar technique\n",
      "SSF [61] also performs linear transformation to the model\n",
      "activations, as illustrated in Figure 6 (b). Specifically, after\n",
      "each operation (i.e., MSA, FFN, and layer normalization) in\n",
      "the pre-trained model, an SSF-ADA layer is injected, which\n",
      "performs scaling and shifting to the features generated from\n",
      "the operation. During fine-tuning, only those SSF-ADA layers\n",
      "can be updated, while during inference, similar to (IA)3, these\n",
      "SSF-ADA layers can be merged into model weights, so no ad-\n",
      "ditional inference overhead would be incurred. IPA (Inference-\n",
      "Time Policy Adapters) [62] offers a novel approach to align\n",
      "LLMs, such as GPT-4, with user-specific requirements without\n",
      "modifying the base model’s parameters. This is particularly\n",
      "significant when dealing with models whose parameters are\n",
      "extremely large and often not directly accessible. IPA achieves\n",
      "this by combining (through multiplication and normalization)\n",
      "the output distribution of a base LLM (base policy) with that\n",
      "of a smaller-sized model (adapter policy) during the decoding\n",
      "phase. During training, the policy adapter’s parameters are\n",
      "fine-tuned using reinforcement learning, while the base pol-\n",
      "icy’s parameters remain fixed. During inference, IPA decodes\n",
      "with the combined distribution of the base model and the\n",
      "trained policy adapter, tailoring it to fulfill specific user-defined\n",
      "criteria.\n",
      "B. Selective PEFT\n",
      "Rather than additive PEFT, which increases the model\n",
      "complexity by adding more parameters, selective PEFT fine-\n",
      "tunes a subset of the existing parameters to enhance model\n",
      "performance over downstream tasks, as depicted in Figure 4\n",
      "(b).\n",
      "Specifically,\n",
      "given\n",
      "a\n",
      "model\n",
      "with\n",
      "parameters\n",
      "θ\n",
      "“\n",
      "tθ1, θ2, ..., θnu where each θi denotes an individual model\n",
      "parameter and n represents the total count of these parameters,\n",
      "the process of selective PEFT is represented by applying a\n",
      "binary mask M “ tm1, m2, ..., mnu to these parameters. Each\n",
      "mi in M is either 0 or 1, indicating whether the corresponding\n",
      "parameter θi is selected (1) or not selected (0) for fine-tuning.\n",
      "The updated parameter set θ1 after fine-tuning is given by:\n",
      "θ1\n",
      "i “ θi ´ η ¨ mi ¨ BL\n",
      "Bθi\n",
      "(11)\n",
      "where η represents the learning rate, and\n",
      "BL\n",
      "Bθi is the gradient\n",
      "of the loss function with respect to the parameter θi. In this\n",
      "formulation, only the parameters that are selected (i.e., mi “\n",
      "1) are updated during backpropagation.\n",
      "8\n",
      " \n",
      "(a) Unstructural \n",
      "Masking\n",
      "(b) Structural \n",
      "Masking\n",
      "Frozen\n",
      "Learnable\n",
      "Fig. 7: Illustration of two parameter masking methods.\n",
      "Diff pruning [63] is a representative work that applies\n",
      "a learnable binary mask to the model weights during fine-\n",
      "tuning. To achieve parameter efficiency, the mask is reg-\n",
      "ularized by a differentiable approximation of the L0-norm\n",
      "penalty. PaFi [65] simply select model parameters with the\n",
      "smallest absolute magnitude as trainable. FishMask [66] de-\n",
      "termines parameter importance using the approximate Fisher\n",
      "information. It then selects the top k parameters based on this\n",
      "information to form the mask M. Similarly, Fish-Dip [67]\n",
      "also uses Fisher information to calculate M, but the mask\n",
      "will be re-calculated dynamically in each train period. LT-\n",
      "SFT [68] introduces another technique to determine parameter\n",
      "importance inspired by the Lottery Ticket Hypothesis [107],\n",
      "[108], where the subset of parameters that change the most\n",
      "during an initial fine-tuning stage is selected to form the mask\n",
      "M. SAM [69] proposes a second-order approximation method,\n",
      "which approximates the original problem with an analytically\n",
      "solvable optimization function, to help decide the parameter\n",
      "mask. Child-tuning [70] proposes two approaches to select a\n",
      "child network during each training iteration, where only the\n",
      "parameters within this child network can be updated.\n",
      "However, the above unstructured parameter masking results\n",
      "in an uneven distribution of non-zero masks and diminished\n",
      "hardware efficiency when implementing PEFT. As shown in\n",
      "Figure 7, the structured mask organizes parameter masking\n",
      "in regular patterns, unlike unstructured ones that apply it\n",
      "randomly, thus enhancing computational and hardware effi-\n",
      "ciency during training. Therefore, various structured selective\n",
      "PEFT techniques have undergone extensive investigation. Diff\n",
      "pruning proposes a structured pruning strategy by partitioning\n",
      "the weight parameters into local groups and strategically elim-\n",
      "inating them together. Similarly, FAR [71] fine-tunes BERT\n",
      "models by grouping weights of the FFN in Transformer blocks\n",
      "into nodes, then ranking and selecting the learner nodes using\n",
      "L1 norm. To further reduce the memory access frequency,\n",
      "they also reconfigure the FFN by grouping the learner nodes.\n",
      "Bitfit [72] is proposed to only fine-tune the bias parameters\n",
      "of each DNN layer, and achieves competitive results for small\n",
      "models. However, this method fails to handle large models.\n",
      "[64] applies NAS to Bitfit, where S-BitFit keeps the structural\n",
      "nature in Bitfit that restricts NAS algorithm must choose\n",
      "whether δb “ 0 or not for each bias module. Similar to\n",
      "Bitfit that fine-tunes a specific module in Transformer, Xattn\n",
      "Tuning [73] fine-tunes only the cross-attention layers. SPT\n",
      "(sensitivity-aware visual parameter-efficient fine-tuning) [74]\n",
      "first identifies the sensitive parameters measured by the loss\n",
      "reduction when being tuned. This sensitivity is calculated\n",
      "using a first-order Taylor expansion, derived from a single\n",
      "forward and backward pass before fine-tuning in one shot.\n",
      "Next, SPT finds the weight matrices whose number of sensitive\n",
      "parameters exceeds a pre-defined threshold and then applies\n",
      "a selected PEFT technique (e.g., LoRA and Adapter) to these\n",
      "targeted weights to achieve structural tuning.\n",
      "C. Reparameterized PEFT\n",
      "Reparameterization stands for equivalently transforming a\n",
      "model’s architecture from one to another via transforming\n",
      "its parameters. In the context of PEFT, this often means\n",
      "constructing a low-rank parameterization to achieve the goal of\n",
      "parameter efficiency during training. For inference, the model\n",
      "can be converted to its original weight parameterization, en-\n",
      "suring unchanged inference speed. This procedure is depicted\n",
      "in Figure 4 (c).\n",
      "Earlier research studies [75] have shown that common\n",
      "pre-trained models exhibit an exceptionally low intrinsic di-\n",
      "mensionality. In other words, it is possible to find a low-\n",
      "dimensional reparameterization that is effective for fine-tuning\n",
      "as the entire parameter space. Intrinsic SAID [75] is the pi-\n",
      "oneering work in investigating the intrinsic dimension feature\n",
      "during the fine-tuning of LLMs. However, the most widely\n",
      "recognized reparameterization technique is LoRA (Low-Rank\n",
      "Adaptation) [76], [109], as shown in Figure 8 (a). For a given\n",
      "pre-trained weight matrix W0 P Rdˆk, LoRA introduces two\n",
      "trainable weight matrices, Wup P Rdˆr and Wdown P Rrˆk\n",
      "where the rank r ! minpd, kq, operating in parallel to W0.\n",
      "Let hin represent the input. Under normal conditions, the\n",
      "output through W0 is hout “ W0hin. Instead, LoRA modifies\n",
      "this output by introducing an incremental update ∆W that\n",
      "encapsulates task-specific knowledge:\n",
      "hout “ W0hin ` α\n",
      "r ∆Whin “ W0hin ` α\n",
      "r WupWdownhin,\n",
      "(12)\n",
      "where α denotes a scaling factor. At the onset of training,\n",
      "Wdown is initialized using a random Gaussian distribution,\n",
      "while Wup is initialized to zero, ensuring that ∆W initially\n",
      "holds a value of zero. LoRA is straightforward to implement\n",
      "and has been evaluated on models with up to 175 billion\n",
      "parameters. Fig 8 (c) used a single decoder as an example,\n",
      "the frozen and learnable components are highlighted in grey\n",
      "and red, respectively. Once fine-tuning is complete, LoRA’s\n",
      "adaptive weights seamlessly integrate with the pre-trained\n",
      "backbone weights. This integration ensures that LoRA main-\n",
      "tains the model’s efficiency, adding no extra burden during\n",
      "inference.\n",
      "In LoRA training, selecting an appropriate rank has always\n",
      "been a challenging issue. To address this, DyLoRA [82], as\n",
      "depicted in Figure 8 (b), trains the LoRA module on a range of\n",
      "ranks within a predefined training budget, rather than adhering\n",
      "to a single, fixed rank. Specifically, for a given rank range R “\n",
      "trmin, rmin`1, . . . , rmaxu, DyLoRA dynamically chooses a rank\n",
      "r P R at each iteration of the training process. Consequently,\n",
      "the matrices Wdown and Wup are tailored for the selected rank\n",
      "r, resulting in truncated versions WdownÓr “ Wdownr1 : r, :s\n",
      "and WupÓr “ Wupr:, 1 : rs, and the subsequent forward\n",
      "and backward pass during this iteration will be restricted\n",
      "9\n",
      "Pre-trained\n",
      "Weights\n",
      "W0 ∊ Rd×k\n",
      "Wdown ∊ Rr×k\n",
      "Wup ∊ Rd×r\n",
      "(a) LoRA\n",
      "×\n",
      "d\n",
      "d\n",
      "rmax\n",
      "r\n",
      "r\n",
      "rmax\n",
      "Wdown↓r\n",
      "Wup↓r\n",
      "Wup\n",
      "Wdown\n",
      "×\n",
      "(b) DyLoRA\n",
      "Pre-trained\n",
      "Weights\n",
      "Decompose\n",
      "W0 ∊ Rd×k\n",
      "Pre-trained\n",
      "Weights\n",
      "                 \n",
      "  \n",
      "Magnitude\n",
      "Direction\n",
      "m∊ R1×k\n",
      "1/॥V+ΔV॥c\n",
      "×\n",
      "V ∊ Rd×k\n",
      "Wup ∊ Rd×r\n",
      "Wdown ∊ Rr×k\n",
      "×\n",
      "(c) DoRA\n",
      "Fig. 8: Illustration of three representative reparameterized PEFT algorithms. Blue represents frozen, while yellow represents\n",
      "trainable.\n",
      "on WdownÓr and WupÓr instead of Wdown and Wup. With\n",
      "this dynamic and search-free approach, DyLoRA significantly\n",
      "reduces the training time required to find an optimal and fixed\n",
      "LoRA rank for specific tasks. AdaLoRA [83] reformulates\n",
      "the ∆W with a singular value decomposition (SVD), denoted\n",
      "as ∆W\n",
      "“ PΛQ, where P\n",
      "P Rdˆr and Q P Rrˆk are\n",
      "orthometric, Λ is a diagonal matrix containing singular values\n",
      "tλiu1ďiďr. All three weight matrices are made learnable.\n",
      "During training, the singular values are pruned iteratively\n",
      "based on their importance scores, which are constructed from\n",
      "the moving average of the magnitude of the gradient-weight\n",
      "product. To ensure the orthogonality between P and Q, i.e.,\n",
      "P T P “ QQT “ I, an additional regularizer term is included\n",
      "in the loss:\n",
      "RpP, Qq “\n",
      "››P T P ´ I\n",
      "››2\n",
      "F `\n",
      "››QQT ´ I\n",
      "››2\n",
      "F .\n",
      "(13)\n",
      "This adaptive approach enables the model to dynamically ad-\n",
      "just the rank within each LoRA module, effectively managing\n",
      "its parameter counts based on the significance of the weight\n",
      "matrices. However, according to SoRA [84], the importance\n",
      "scores used in AdaLoRA are heuristically constructed, which\n",
      "lacks rigorous theoretical motivation. Additionally, both mov-\n",
      "ing average operation and calculation of Eq. 13 introduce\n",
      "extra computation costs during training. To address this, SoRA\n",
      "eliminates the orthogonality premise of P and Q. Instead, a\n",
      "gating unit g P Rr between Wup and Wdown is directly applied\n",
      "and optimized:\n",
      "hout “ Wuppg d pWdownhinqq,\n",
      "(14)\n",
      "where d is Hadamard product. The gate g is updated using a\n",
      "variation of proximal gradient iteration for l1 loss [110], [111],\n",
      "which has a clear mathematical meaning and does not need\n",
      "the heuristic premise. After training, the zeroed-out gate units\n",
      "are pruned by removing the corresponding columns and rows\n",
      "in Wdown and Wup.\n",
      "Several subsequent studies have aimed to improve LoRA’s\n",
      "performance in various aspects. For instance, Laplace-\n",
      "LoRA [87] notices that fine-tuned LLMs often exhibit over-\n",
      "confidence. To enhance the calibration of fine-tuned LLMs,\n",
      "Laplace-LoRA utilizes a Bayesian approach, specifically a\n",
      "post-hoc Laplace approximation [112], [113], to the posterior\n",
      "over the LoRA parameters. LoRA Dropout [88] introduces\n",
      "random noises to the learnable low-rank matrices and in-\n",
      "creases parameter sparsity to reduce the risk of overfitting.\n",
      "LoRA+ [90] proposes to set different learning rates for the\n",
      "LoRA matrices Wdown and Wup, such that ηup “ ληdown with\n",
      "λ ą 1 fixed and tune ηdown. MoSLoRA (Mixture-of-Subspaces\n",
      "LoRA) [91] decomposes LoRA into subspaces via structural\n",
      "reparameterization, then employs a learnable mixer, trained\n",
      "jointly with the original LoRA weights, to fuse the subspaces.\n",
      "Similarly to LoRA, MoSLoRA can also be merged into the\n",
      "original weights.\n",
      "Thanks to the modular design of LoRA, many studies\n",
      "incorporate multiple LoRA modules in their frameworks to\n",
      "enhance performance. For example, LoRAHub aggregates\n",
      "various LoRA modules trained on different tasks. Given\n",
      "a handful of examples from a new task, LoRAHub can\n",
      "autonomously compose compatible LoRA modules without\n",
      "human intervention via a gradient-free method Shiwa [114].\n",
      "MOELoRA employs a Mixture-of-Experts (MOE) approach\n",
      "to train LoRA in a multi-task setting, resulting in multiple\n",
      "expert LoRA modules. To retrieve parameters for certain tasks,\n",
      "MOELoRA utilizes a task-motivated gate function that assigns\n",
      "contribution weights to each expert based on the task ID, and\n",
      "the final parameters are calculated through a weighted sum of\n",
      "all experts.\n",
      "In addition to LoRA, several other reparameterization tech-\n",
      "niques are emerging with significant potential. For instance,\n",
      "Compacter [77] introduces a light-weight adapter modules\n",
      "by parameterizing the Wdown and Wup as W “ řn\n",
      "i“1 Ai bBi,\n",
      "where Ai P Rnˆn, Bi P R\n",
      "r\n",
      "n ˆ d\n",
      "n , and b denotes the Kronecker\n",
      "product. They further decrease the parameter count by desig-\n",
      "nating Ai as shared parameters and reparameterizing Bi using\n",
      "the product of two low-rank matrices, effectively reducing the\n",
      "parameter complexity from Oprdq to Opr`dq. Related studies,\n",
      "such as KronA [78] and KAdaptation [79], also employ the\n",
      "Kronecker product to reparameterize adapter weights, aiming\n",
      "to achieve parameter reduction. HiWi [65] proposes an adapter\n",
      "fine-tuning method that applies an adapter directly to pre-\n",
      "trained parameters instead of hidden representations as:\n",
      "W 1 “ W ` σpWWdownqWup,\n",
      "(15)\n",
      "where W denotes the weights or biases within the Transformer\n",
      "block’s feed-forward layer. Notably, during inference, this\n",
      "method computes W 1 in advance, ensuring that the model’s\n",
      "10\n",
      "inference latency remains on par with that of traditional\n",
      "full fine-tuning. VeRA (Vector-based Random Matrix Adapta-\n",
      "tion) [80] employs a single pair of frozen low-rank matrices\n",
      "Wup and Wdown that are shared across all layers, and adapts\n",
      "these matrices by learning small, trainable scaling vectors\n",
      "represented as b and d (formally denoted by diagonal matrices\n",
      "Λb and Λd). Specifically, the reparameterization is given by:\n",
      "hout “ W0hin ` ΛbWupΛdWdownhin,\n",
      "(16)\n",
      "where both Wup and Wdown are initialized using a random\n",
      "Gaussian distribution. Similar to LoRA, the scaling vector\n",
      "b is initialized to zeros to ensure that the weight matrix is\n",
      "unaffected during the first forward pass. This method signif-\n",
      "icantly reduces the number of trainable parameters compared\n",
      "to LoRA yet maintains the same performance, enabling the\n",
      "fine-tuning of larger models on a single GPU. DoRA (Weight-\n",
      "Decomposed Low-Rank Adaptation) [81] presents a novel\n",
      "approach as illustrated in Figure 8 (c) by decomposing model\n",
      "weights W0 P Rdˆk into magnitude and direction as follows:\n",
      "W0 “ m V\n",
      "}V }c\n",
      "“ }W0}c\n",
      "W0\n",
      "}W0}c\n",
      ",\n",
      "(17)\n",
      "where m P R1ˆk is the magnitude vector, V\n",
      "P Rdˆk is\n",
      "the directional matrix, with } ¨ }c being the vector-wise norm\n",
      "of a matrix across each column. Subsequently, DoRA adopts\n",
      "a unique fine-tuning strategy for m and V . While both are\n",
      "tunable, only V undergoes LoRA reparameterization, defined\n",
      "as:\n",
      "W 1 “ m V ` ∆V\n",
      "}V ` ∆V }c\n",
      "“ m\n",
      "W0 ` WupWdown\n",
      "}W0 ` WupWdown}c\n",
      ",\n",
      "(18)\n",
      "where ∆V is the incremental directional update learned by\n",
      "LoRA, and the underlined parameters denote the trainable\n",
      "parameters. Through this methodology, DoRA consistently\n",
      "outperforms LoRA across various tasks and models, demon-\n",
      "strating its superiority.\n",
      "D. Hybrid PEFT\n",
      "The efficacy of various PEFT methods can significantly\n",
      "differ across different tasks. As a result, numerous studies aim\n",
      "to either combine the advantages of diverse PEFT approaches\n",
      "or seek to establish a unified perspective by analyzing the\n",
      "similarities among these methods. For instance, UniPELT [97]\n",
      "integrates LoRA, prefix-tuning, and adapters into each Trans-\n",
      "former block. To control which PEFT submodules should\n",
      "be activated, they also introduce a gating mechanism. This\n",
      "mechanism consists of three small FFNs that each produce\n",
      "a scalar value G P p0, 1q, which is then applied to the\n",
      "LoRA, prefix, and adapter matrices, respectively. Across var-\n",
      "ious setups, UniPELT has consistently shown improvements\n",
      "in accuracy ranging from 1% to 4%. S4 [98] explores design\n",
      "spaces for several PEFT methods (i.e., Adapter (A), Prefix\n",
      "(P), BitFit (B), and LoRA (L)) to uncover underlying design\n",
      "patterns. After a series of experiments, their findings include:\n",
      "(1) Applying the spindle grouping partitioning for Transformer\n",
      "layers, which results in four layer groups Gi for i P t1 . . . 4u.\n",
      "Layers in one group have similar behaviors together, which\n",
      "means should apply similar PEFT strategies. (2) Allocating\n",
      "the number of trainable parameters to layers uniformly. (3)\n",
      "Tuning all the groups. (4) Assigning different PEFT strategies\n",
      "in different groups. The resulting design space that has the\n",
      "best performance is:\n",
      "G1 : pA, Lq, G2 : pA, Pq, G3 : pA, P, Bq, G4 : pP, B, Lq\n",
      "MAM Adapter[32] explores the intrinsic similarity between\n",
      "three additive PEFT methods: adapters, prefix-tuning, and\n",
      "LoRA, which leads to the development of three variants:\n",
      "Parallel Adapter, which places adapter layers alongside spe-\n",
      "cific layers (SA or FFN) instead of after them; Multi-head\n",
      "Parallel Adapter, which divides the parallel adapter into\n",
      "multiple heads, each affecting the head attention output in\n",
      "SA; and Scaled Parallel Adapter, which adds a scaling term\n",
      "after the parallel adapter layer, similar to LoRA. Extensive\n",
      "experimentation revealed that the most effective configura-\n",
      "tion involves using prefix-tuning in the SA layer and the\n",
      "scaled parallel adapter in the FFN layer, which is called the\n",
      "MAM Adapter. LLM-Adapters [101] builds an easy-to-use\n",
      "framework that incorporates various PEFT techniques into\n",
      "LLMs. Through comprehensive benchmarking across multiple\n",
      "datasets, the study reveals several key insights: (1) The most\n",
      "effective locations for series adapters, parallel adapters, and\n",
      "LoRA are after the MLP layers, alongside the MLP layers, and\n",
      "simultaneously following the Attention layers and MLP layers,\n",
      "respectively. (2) Smaller LLMs utilizing PEFT can achieve\n",
      "competitive or even superior results on certain tasks when\n",
      "compared to their larger counterparts. (3) With appropriate\n",
      "in-distribution fine-tuning data, smaller models are capable of\n",
      "surpassing larger models in task-specific performance.\n",
      "Several studies leverage neural architecture search (NAS)\n",
      "to find better PEFT combination approaches. For example,\n",
      "NOAH [99] discovers that different PEFT configurations are\n",
      "specifically tailored for different tasks. To address this issue,\n",
      "NOAH employs NAS to identify the most effective PEFT con-\n",
      "figurations for each dataset. Specifically, NOAH’s searching\n",
      "space encompasses three PEFT methods: Adapter, LoRA, and\n",
      "Visual Prompt Tuning (VPT). It utilizes AutoFormer [115], a\n",
      "one-shot NAS algorithm, for the efficient discovery of optimal\n",
      "prompt modules. In a related vein, AUTOPEFT [100] first\n",
      "establishes a searching space that includes serial adapters,\n",
      "parallel adapters, and prefix tuning. After that, they propose\n",
      "an effective NAS method based on a high-dimensional multi-\n",
      "dimensional Bayesian optimisation [116]. Both NOAH and\n",
      "AUTOPEFT demonstrate the capability of NAS in enhancing\n",
      "PEFT configurations across a variety of tasks.\n",
      "IV. EFFICIENT PEFT DESIGN\n",
      "Processing latency and peak memory overhead are pivotal\n",
      "factors to consider from a computational standpoint. This\n",
      "section introduces a key characteristic in LLMs aimed at\n",
      "balancing between latency and memory usage (Section IV-A).\n",
      "Following this, we explore strategies for developing efficient\n",
      "PEFT methods to address computational challenges, including\n",
      "PEFT pruning (Section IV-B), PEFT quantization (Sec-\n",
      "tion IV-C), and memory-efficient PEFT techniques (Sec-\n",
      "tion IV-D), each designed to enhance model performance\n",
      "11\n",
      "Efficient PEFT Design\n",
      "PEFT Pruning\n",
      "AdapterDrop [117], SparseAdapter [118], SPLoRA [119], LoRAPruning [120], ProPETL [121]\n",
      "PEFT\n",
      "Quantization\n",
      "BI-Adapter [122], PEQA [123], QLoRA [124], LoftQ [125], LQ-LoRA [126], QA-LoRA [127], INT2.1 [128], QDyLoRA [129],\n",
      "BitDelta [130]\n",
      "Memory-efficient\n",
      "PEFT\n",
      "Side-Tuning [131], LST [132], Res-Tuning [133], MEFT [134], LoRA-FA [135], HyperTuning [136], PEFT Plug-in [137],\n",
      "MeZO [138], GaLore [139]\n",
      "Fig. 9: Taxonomy of Efficient PEFT Design.\n",
      "while minimizing resource consumption. It is noteworthy that\n",
      "quantization inherently addresses memory overhead concerns.\n",
      "However, given its distinct characteristics, we address these\n",
      "quantization methods separately rather than incorporating\n",
      "them under the memory-efficient PEFT section.\n",
      "A. KV-cache Management for PEFT Efficiency\n",
      "The core of the LLMs model lies in an auto-regressive\n",
      "Transformer model. When we consider the auto-regression\n",
      "characteristic, it becomes a major challenge in designing an\n",
      "inference system, because every time a new token is generated,\n",
      "the entire LLM model has to transfer all the weights from\n",
      "different memories to the memory of the graphics processor,\n",
      "which is very unfriendly to single-user task scheduling or\n",
      "multi-user workload balance. The challenging part of serving\n",
      "the auto-regressive paradigm is that all previous sequences\n",
      "have to be cached and saved for the next proceeding iteration;\n",
      "the cached activation generated from the previous sequences\n",
      "is stored as the Key-Value Cache (KV-cache). To effectively\n",
      "manage these challenges, S-LoRA [140] employs a Unified\n",
      "Paging mechanism within a unified memory pool that dynam-\n",
      "ically allocates and manages memory in a paged fashion. This\n",
      "sophisticated approach minimizes memory fragmentation and\n",
      "enhances the efficiency of KV-cache storage by allowing for\n",
      "flexible and efficient memory access patterns. These pages are\n",
      "managed such that the KV-cache associated with each adapter\n",
      "is segmented into manageable blocks, streamlining access and\n",
      "reducing the overhead associated with variable cache sizes. By\n",
      "dynamically adjusting to different KV-cache requirements, S-\n",
      "LoRA maintains high throughput and performance, ensuring\n",
      "that the system remains responsive and efficient even as it\n",
      "scales to serve thousands of adapters simultaneously. This\n",
      "efficient handling of KV-cache is crucial for supporting the\n",
      "auto-regressive nature of LLMs in high-demand environments,\n",
      "optimizing both single-user and multi-user workload balanc-\n",
      "ing.\n",
      "B. Pruning Strategies for PEFT\n",
      "The inclusion of pruning can substantially enhance the\n",
      "efficiency of PEFT methods. In particular, AdapterDrop [117]\n",
      "explores the removal of adapters from lower transformer\n",
      "layers and multi-task adapters in AdapterFusion [35], which\n",
      "shows that the pruning can improve the training and in-\n",
      "ference efficiency with minimal decrease in performance.\n",
      "SparseAdapter [118] investigates different pruning methods\n",
      "and finds that high sparsity ratio (80%) can outperform stan-\n",
      "dard adapters. Additionally, the Large-Sparse configuration,\n",
      "which increases the bottleneck dimension while maintaining\n",
      "a constant parameter budget (e.g., doubling dimensions with\n",
      "a 50% sparsity), substantially enhances the model’s capacity,\n",
      "resulting in improved performance. SPLoRA [119] adopts\n",
      "channel-based pruning to the LoRA weights Wdown and Wup.\n",
      "This pruning affects not only the source weights W0, but\n",
      "also the LoRA parameters Wup and Wdown. Similarly, Lo-\n",
      "RAPruning [120] adopts structured pruning not only to the\n",
      "pretrained model weights but also to the LoRA weights.\n",
      "In contrast to unstructured LoRA pruning methods, which\n",
      "primarily focus on sparsifying model weights while leaving\n",
      "LoRA weights dense, thus making weight merging challenging\n",
      "to achieve, LoRAPruning enables the weights to be merged\n",
      "easily. Additionally, this work also introduces a novel criterion\n",
      "that utilizes LoRA’s gradients as an approximation of the\n",
      "gradients for the pre-trained weights, enabling the estimation\n",
      "of weight importance. ProPETL [121] constructs a single\n",
      "shared prototype (e.g., adapter, prefix, or LoRA) across layers\n",
      "and tasks. In addition, ProPETL learns binary masks to prune\n",
      "different sub-networks in different layers and tasks. As a result,\n",
      "the parameters can be reused across layers and tasks, largely\n",
      "increasing the parameter efficiency.\n",
      "C. Quantization Strategies for PEFT\n",
      "Quantization serves as another popular technique for im-\n",
      "proving computational efficiency and reducing memory us-\n",
      "age. For example, by investigating the loss landscape of\n",
      "adapters, BI-Adapter [122] finds that adapters are resistant\n",
      "to noise in parameter space. Building on this insight, the\n",
      "authors introduce a clustering-based quantization approach.\n",
      "Remarkably, they demonstrate that a 1-bit quantization of\n",
      "adapters not only minimizes storage requirements but also\n",
      "achieves superior performance among all precision settings.\n",
      "PEQA (Parameter-Efficient and Quantization-aware Adapta-\n",
      "tion) [123] uses a two-stage pipeline to achieve parameter-\n",
      "efficient and quantization-aware fine-tuning. In the first stage,\n",
      "the pre-trained FFN weight matrix W P Rnˆm is quantized\n",
      "to W “ s ¨ W, where s P Rnˆ1 represents per-channel\n",
      "scales and W denotes the quantized weight. In the second\n",
      "stage, W remains fixed, and fine-tuning is only conducted\n",
      "on s. This approach not only ensures memory efficiency but\n",
      "also facilitates parameter efficiency. QLoRA [124] proposes\n",
      "several novel techniques, including a 4-bit NormalFloat, a\n",
      "Double Quantization, and a Paged Optimizers, to backprop-\n",
      "agate a 4-bit quantized pretrained language model into LoRA.\n",
      "12\n",
      "These techniques enable the fine-tuning for a 65B language\n",
      "model on a single 48GB GPU while maintaining similar\n",
      "performance to the full 16-bit fine-tuning. Similar to the\n",
      "original implementation [76], QLoRA attaches the fixed zero-\n",
      "initialized LoRA weights to the quantized pre-trained model\n",
      "as the training start point. However, when applying the ex-\n",
      "treme low-bit (e.g., 2-bit) quantization, the huge quantization\n",
      "error can adversely impact the initialization of LoRA fine-\n",
      "tuning, i.e., quantizationpW0q ` WdownWup ‰ W0 where\n",
      "Wdown “ 0, which will harm the fine-tuning performance as\n",
      "shown in the work by [134]. To solve this, several quanti-\n",
      "zation strategies are proposed to eliminate the quantization\n",
      "error. For example, LoftQ (LoRA-Fine-Tuning-aware Quanti-\n",
      "zation) [125] presents an innovative framework that provides\n",
      "a superior initialization point of quantized backbone weights\n",
      "and LoRA weights for subsequent LoRA fine-tuning. This\n",
      "approach addresses the discrepancies caused by quantization\n",
      "through the optimization of a Frobenius norm objective during\n",
      "network initialization, which takes both the LoRA weights\n",
      "and the quantized pre-trained backbone into consideration.\n",
      "LoftQ exhibits superior performance in 2-bit quantization over\n",
      "QLoRA, as well as greater generalization for downstream\n",
      "tasks. LQ-LoRA [126] uses an iterative algorithm inspired\n",
      "by robust principal components analysis [141], [142] which\n",
      "decomposes the weight W0 such that W0 « Q ` L1L2\n",
      "to resolve the inaccuracy caused by the quantization error,\n",
      "where Q is the quantized component which remains fixed\n",
      "and L1L2 is the trainable low-rank component. Moreover, this\n",
      "approach leverages integer linear programming to determine\n",
      "a mixed quantization strategy, enabling dynamic quantization\n",
      "configurations for each weight matrix while adhering to a\n",
      "predetermined total bit rate limit. QA-LoRA [127] address\n",
      "another limitation of QLoRA, which struggles to preserve its\n",
      "quantized property post-fine-tuning. In QLoRA, the quantized\n",
      "pre-trained weight (NF4) has to be recovered to FP16 to match\n",
      "the LoRA weight precision (FP16) during weight merging.\n",
      "Instead, QA-LoRA uses INT4 quantization and introduces\n",
      "group-wise operators to enable quantization during the infer-\n",
      "ence stage, therefore improving the efficiency and accuracy\n",
      "compared with QLoRA. BitDelta [130] introduces a novel 1-\n",
      "bit post-training quantization method that acts on the weight\n",
      "delta between a fine-tuned model and its underlying pre-\n",
      "trained model. Specifically, given the weight matrices Wfine\n",
      "and Wbase from the fine-tuned and base models respectively,\n",
      "the weight delta ∆“ Wfine ´ Wbase is binarized as ˆ∆“ α d\n",
      "Signp∆q. Here, α, a high-precision scalar, is initialized based\n",
      "on the mean absolute delta value α “\n",
      "1\n",
      "nm\n",
      "ř\n",
      "ij |Wij|, with\n",
      "Signp¨q indicating the sign of ∆. BitDelta further calibrates\n",
      "the scaling factors via distillation on a compact calibration\n",
      "dataset, while the binary matrices remain unchanged. This\n",
      "approach notably streamlines the deployment of multiple fine-\n",
      "tuned models on shared servers by utilizing a singular full-\n",
      "precision base model alongside efficiently batched 1-bit deltas.\n",
      "D. Memory-efficient PEFT Methods\n",
      "Fine-tuning the full LLMs necessitates substantial training\n",
      "memory owing to their considerable size. While most PEFT\n",
      "methods primarily target parameter efficiency, they still in-\n",
      "cur a significant memory overhead during training because\n",
      "gradient computation and backpropagation are still necessary\n",
      "for these methods. For example, prevalent PEFT techniques\n",
      "such as adapters and LoRA can only reduce memory usage\n",
      "to approximately 70% compared to full model fine-tuning ac-\n",
      "cording to some literature [132], [137]. From a computational\n",
      "perspective, memory efficiency also remains a critical factor\n",
      "that cannot be overlooked.\n",
      "To improve memory efficiency, various techniques have\n",
      "been developed to minimize the need for caching gradi-\n",
      "ents for the entire LLM during fine-tuning, thereby reducing\n",
      "memory usage. For example, both Side-Tuning [131] and\n",
      "LST (Ladder-Side Tuning) [132] introduce a learnable net-\n",
      "work branch parallel to the backbone model. By channeling\n",
      "the backpropagation exclusively through this parallel branch,\n",
      "it circumvents the need to store gradient information for\n",
      "the main model’s weights, thus markedly reducing memory\n",
      "requirements during training. Similarly, Res-Tuning [133]\n",
      "disentangles the PEFT tuners (e.g., prompt tuning, adapter)\n",
      "from the backbone model. On top of the disentanglement, a\n",
      "memory-efficient fine-tuning framework named Res-Tuning-\n",
      "Bypass is proposed, which generates a bypass network in\n",
      "parallel with the backbone model by removing the data flow\n",
      "from the decoupled tuners to the backbone. This eliminates the\n",
      "requirement for gradient caching within the backbone model\n",
      "during backpropagation. MEFT [134] (memory-efficient fine-\n",
      "tuning) is an approach inspired by the reversible model [143].\n",
      "During the training of a reversible model, intermediate ac-\n",
      "tivations are not required to be cached in the forward pass.\n",
      "During backpropagation, they can be recalculated from the\n",
      "final output. To save the memory during fine-tuning, MEFT\n",
      "investigates how to transform an LLM to its reversible counter-\n",
      "parts without additional pre-training. A critical aspect of this\n",
      "transformation is the careful initialization of newly introduced\n",
      "parameters in the pre-trained models. MEFT demonstrates the\n",
      "importance of parameter initialization and suggests that these\n",
      "parameters must be initialized in a manner that preserves the\n",
      "pre-trained model’s starting point, ensuring that the fine-tuning\n",
      "of the modified model achieves performance on par with full\n",
      "fine-tuning methods. With this key consideration, MEFT intro-\n",
      "duces three distinct methods, each significantly curtailing the\n",
      "memory demands traditionally required for storing activations.\n",
      "LoRA-FA [135] addresses a limitation about memory over-\n",
      "head in LoRA fine-tuning. During training, LoRA modules\n",
      "still require high activation memory consumption. This is be-\n",
      "cause, during backpropagation, large input activations must be\n",
      "stored during the forward pass to compute gradients. LoRA-FA\n",
      "resolves this issue by freezing both the pre-trained weights W0\n",
      "and the projection-down weights Wdown, and only updating the\n",
      "projection-up weights Wup. Consequently, the input activation\n",
      "hin no longer needs to be stored, as the intermediate activation\n",
      "Wdownhin is adequate for gradient computation for Wup. Given\n",
      "that r ! d, the memory requirement for activations in LoRA-\n",
      "FA can be significantly reduced.\n",
      "To further reduce memory usage during fine-tuning, some\n",
      "methods attempt to circumvent backpropagation within LLMs\n",
      "to address this issue. HyperTuning [136] employs a Hyper-\n",
      "13\n",
      "Model to generate PEFT parameters using only fewshot exam-\n",
      "ples. This approach demonstrates results comparable to those\n",
      "obtained through full model fine-tuning. PEFT Plug-in [137]\n",
      "first trains PEFT modules on small language models, which\n",
      "is more memory efficient compared to training on large ones.\n",
      "Subsequently, the research introduces a suite of techniques for\n",
      "seamlessly integrating these trained PEFT modules into LLMs\n",
      "during inference. This strategy effectively circumvents the\n",
      "necessity of gradient-based optimization directly on the larger\n",
      "models, resulting in substantial memory savings. However, it is\n",
      "important to note that both HyperModel and PEFT Plug-in still\n",
      "require additional model training, and this training cost cannot\n",
      "be entirely overlooked. MeZO [138] introduces a memory-\n",
      "efficient zeroth-order (ZO) optimizer for LLMs. Unlike con-\n",
      "ventional PEFT techniques, which rely on backpropagation to\n",
      "compute gradients for updating model parameters, MeZO fine-\n",
      "tunes LLMs through only forward passes. It accomplishes this\n",
      "by employing a ZO gradient estimator to calculate the gradient.\n",
      "Notably, MeZO implements an in-place solution for the classic\n",
      "ZO gradient estimator, effectively mitigating memory con-\n",
      "sumption during inference execution. This innovative approach\n",
      "allows for efficient fine-tuning of LLMs containing 30 billion\n",
      "parameters on a single GPU with 80GB of memory, all while\n",
      "maintaining performance that is comparable to fine-tuning\n",
      "using backpropagation. Furthermore, it can substantially de-\n",
      "crease storage demands in comparison to the traditional PEFT\n",
      "methods such as LoRA and Adapter.\n",
      "V. PEFT FOR DNNS OF OTHER APPLICATIONS\n",
      "In Section III, we outlined four categories of PEFT methods\n",
      "along with their improvements. Nonetheless, our discussion\n",
      "did not fully extend to the utilization or adaptation of PEFT\n",
      "techniques beyond traditional architectures (e.g., LLMs) or\n",
      "standard benchmarks (e.g., the GLUE dataset), where the ma-\n",
      "jority of the discussed PEFT methods are applied. Therefore,\n",
      "in this section, we will highlight and discuss several most\n",
      "representative works that leverage PEFT strategies for various\n",
      "downstream tasks. We do not aim to cover all PEFT applica-\n",
      "tion scenarios in this section. Our objective is to showcase the\n",
      "significant influence of PEFT within various research domains\n",
      "and demonstrate how to optimize and tailor general-purpose\n",
      "PEFT methods to achieve enhanced performance in specific\n",
      "models or tasks.\n",
      "Typically, fine-tuning happens when adapting a pre-trained\n",
      "backbone model to specialized downstream tasks. To this end,\n",
      "this section organizes the discussion around various model\n",
      "architectures, which include: LLM, Vision Transformer (ViT),\n",
      "Vision-Language Alignment Model (VLA), and Diffusion\n",
      "model. Within each architectural category, the discussion is\n",
      "further classified based on different downstream tasks.\n",
      "A. PEFT for LLMs – Beyond the Basics\n",
      "Instead of common tasks in NLP such as NLU and NLG,\n",
      "PEFT techniques boast a wide array of applications across\n",
      "diverse scenarios. PEFT has been successfully implemented in\n",
      "commonsense question answering [144], [145], multi-level im-\n",
      "plicit discourse relation recognition [146], out-of-distribution\n",
      "detection [147], privacy protection [148], [149], federated\n",
      "learning [150], and social biases mitigation [151]. In this\n",
      "section, we pay more focus on three representative downstream\n",
      "tasks: visual instruction following, continual learning, and\n",
      "context window extension.\n",
      "1) Visual Instruct Following: Several studies, including\n",
      "VL-BART [152], MiniGPT-4 [153], and LLaVA [154], have\n",
      "successfully extended the capabilities of LLMs, initially de-\n",
      "signed for pure text, to comprehend and generate responses to\n",
      "visual inputs. These enhanced models, namely visual instruct-\n",
      "following LLMs, can process both images and text to produce\n",
      "textual responses, which can be benchmarked on tasks such as\n",
      "image captioning [155], [156], [157], [158] and visual question\n",
      "answering (VQA) [159], [160], [161]. However, these methods\n",
      "fine-tune the entire LLM to learn the visual representations,\n",
      "which can be inefficient in both time and memory. Therefore, it\n",
      "is natural to apply PEFT techniques in the fine-tuning of visual\n",
      "instruct-following LLMs. An earlier work VL-Adapter [162]\n",
      "directly applies several PEFT methods (Adapter [31], Hy-\n",
      "performer [40] and Compacter [77]) on VL-BART [152]\n",
      "then benchmarks them on several image-text and video-text\n",
      "tasks. Results show that vanilla adapters are the best among\n",
      "them, which can achieve performance on par with full fine-\n",
      "tuning. However, considering the functionality gap between\n",
      "the encoders and decoders in VL-BART, directly assigning\n",
      "identical modular modifications will lead to suboptimal perfor-\n",
      "mance. Therefore, VL-PET [163] selectively integrates PEFT\n",
      "modules into different components of the encoder and decoder.\n",
      "They also introduce a granularity-controlled mechanism for\n",
      "finer-grained control.\n",
      "To adapt the recently prevalent LLaMA model, LLaMA-\n",
      "Adapter [164] prepends a set of learnable prompts (similar\n",
      "to prefix tuning) to the input tokens in LLaMA’s higher trans-\n",
      "former layers. To avoid the unstable fine-tuning with large loss\n",
      "values at early training stages, instead of the randomly initial-\n",
      "ized weights of other PEFT methods, LLaMA-Adapter adopts\n",
      "a zero-initialized attention mechanism, which learns a zero-\n",
      "initialized gating factor to adaptively control the contribution\n",
      "of adaptation prompts to the word tokens. This can maintain\n",
      "the fine-tuning starting point the same as the original model\n",
      "and progressively inject new knowledge into the model, where\n",
      "a similar idea can be found in MEFT [134] and LoftQ [125]\n",
      "discussed earlier. To represent visual information, LLaMA-\n",
      "Adapter extracts multi-scale global image features using a\n",
      "CLIP image encoder and then projects them to linguistic em-\n",
      "bedding space. After that, the feature is element-wisely added\n",
      "onto the adaptation prompts at all inserted transformer layers.\n",
      "LLaMA-Adapter only introduces 1.2M learnable parameters\n",
      "in LLaMA-7B and costs less than one hour for fine-tuning on\n",
      "8 A100 GPUs. A following work LLaMA-Adapter V2 [165]\n",
      "demonstrates that the simple multimodal fusion in LLaMA-\n",
      "Adapter cannot generalize to more challenging open-ended\n",
      "multimodal reasoning tasks, where the visual cues tend to\n",
      "dominate the adaptation prompts than the language instruction\n",
      "data. To address this, LLaMA-Adapter V2 decouples the learn-\n",
      "ing of instruction-following ability (to generate long language\n",
      "responses) and vision-language alignment to avoid interfer-\n",
      "ence between visual and language fine-tuning. Specifically,\n",
      "14\n",
      "LLaMA-Adapter V2 sets disjoint parameter groups which\n",
      "are respectively learned from image-text pairs and language\n",
      "instruction data. The visual adaptation prompts are inserted in\n",
      "the early stage of LLM, while the language adaptation prompts\n",
      "remain at the higher transformer layers similar to the LLaMA-\n",
      "Adapter. Additionally, LLaMA-Adapter V2 introduces more\n",
      "learnable parameters and several expert systems (e.g., caption-\n",
      "ing, detection, and OCR) to enhance multimodal performance.\n",
      "LayerNorm Tuning [166] adjust only the weights of the\n",
      "LayerNorm within each attention block. This straightforward\n",
      "technique can achieve comparable or even better performance\n",
      "than the finetuning, while offering about 10× more parameter\n",
      "efficiency than LoRA.\n",
      "2) Continual Learning: Continual Learning (CL) aims to\n",
      "learn a sequence of new tasks over time within one single\n",
      "model, which has broad application in scenarios such as\n",
      "dialogue systems [167], information extraction systems [168],\n",
      "and question answering systems [169]. The main challenge in\n",
      "CL is catastrophic forgetting [170]. A popular practice, called\n",
      "architecture-based methods, tackles the CL by maintaining\n",
      "task-specific parameters in the model for each new task. There-\n",
      "fore, it’s natural to leverage PEFT methods for CL tasks [171],\n",
      "[172], [173], [174]. For example, AdapterCL [171] pa-\n",
      "rameterizes each new task using residual adapters. During\n",
      "testing, since the task-id is not provided, AdapterCL uses\n",
      "an entropy-based classifier to select which adapter to use\n",
      "for accomplishing a specific task. CPT (Continual Prompt\n",
      "Tuning) [172] trains a soft prompt for each task. Instead of\n",
      "training soft prompts from scratch, CPT proposes a series\n",
      "of techniques (continual prompt initialization, query fusion,\n",
      "memory replay, and a memory-guided technique) to achieve\n",
      "knowledge transfer from preceding and subsequent tasks.\n",
      "O-LoRA (orthogonal low-rank adaptation) [175] employs a\n",
      "strategy of learning distinct tasks within separate low-rank\n",
      "vector subspaces that are kept orthogonal to each other in order\n",
      "to minimize interference. This approach can effectively reduce\n",
      "catastrophic forgetting during the acquisition of new tasks.\n",
      "3) Context Window Extension: LLMs are typically trained\n",
      "with a pre-defined context size. For example, LLaMA and\n",
      "LLaMA2 have pre-defined context sizes of 2048 and 4096\n",
      "tokens, respectively. The positional encoding RoPE has weak\n",
      "extrapolation properties [176], which means the performance\n",
      "drops obviously given an input length exceeds the pre-defined\n",
      "context length. To solve this, a naive solution is to fine-\n",
      "tune a pre-trained LLM to a longer context. However, this\n",
      "escalates computational costs quadratically with context size,\n",
      "straining memory and processing resources. To address this,\n",
      "LongLoRA [177] proposes to fine-tune a pre-trained LLM\n",
      "using LoRA to enlarge the context size. To reduce the\n",
      "perplexity gap between LoRA tuning and full fine-tuning,\n",
      "LongLoRA also opens embedding and normalization layers\n",
      "for training. In order to further improve training efficiency in\n",
      "a long context scenario, LongLoRA further introduces a novel\n",
      "shifted sparse attention (S2-Attn) as an efficient substitute for\n",
      "standard self-attention during training. A subsequent study\n",
      "LongQLoRA [178] combines the advantages of LongLoRA\n",
      "with QLoRA and Position Interpolation [10] to save GPU\n",
      "memory. This work successfully extends the context length\n",
      "of LLaMA2-13B from 4096 to 8192 on a single V100 with\n",
      "32GB memory. LLoCO [179] introduces a pipeline that\n",
      "learns contexts offline through the combination of context\n",
      "compression and LoRA. The process begins by compressing\n",
      "documents into compact contexts, then fine-tuning LLM us-\n",
      "ing LoRA on the compacted context to improve the LLM’s\n",
      "ability to accurately extract and utilize information from these\n",
      "compressed representations. During model serving, a standard\n",
      "RAG retriever selects both the compressed document and the\n",
      "most relevant LoRA module, and applies them to the LLM\n",
      "for inference. This approach effectively extends the context\n",
      "window of a 4k token LLaMA2-7B model to handle up to\n",
      "128k tokens.\n",
      "In addition to limited training-stage sequence length, real-\n",
      "world system memory constraints introduce another critical\n",
      "bottleneck to the context window. Specifically, the capacity\n",
      "of the KV-cache is curtailed by available system memory. For\n",
      "example, a 30B parameter LLM operating with an input length\n",
      "of 1024 and a batch size of 128 might necessitate up to 180GB\n",
      "for the KV-cache [180], thereby restricting the feasible size of\n",
      "the context window. In response to this, some strategies have\n",
      "resorted to quantizing the KV cache [181], [182], but quanti-\n",
      "zation will certainly compromise performance. To effectively\n",
      "counteract this issue without significant loss, GEAR [183]\n",
      "presents a novel approach by employing a low-rank matrix\n",
      "to capture the majority of coherent bases of quantization\n",
      "error, complemented by a sparse matrix that addresses errors\n",
      "from outlier entries, thus efficiently minimizing approximation\n",
      "errors.\n",
      "B. PEFT for ViTs\n",
      "ViT [184] has emerged as a powerful backbone model in the\n",
      "recent computer vision community. In the ViT model, images\n",
      "are treated as sequences of fixed-size patches analogous to how\n",
      "LLM uses discrete tokens. These patches undergo linear em-\n",
      "bedding and then receive positional encodings. Subsequently,\n",
      "they are processed through standard Transformer encoders.\n",
      "The training of ViT can be supervised [184], [185] or self-\n",
      "supervised [186], [187], and ViT can achieve superior perfor-\n",
      "mance when training with more data and using larger model\n",
      "size [188]. However, such scaling up inevitably escalates\n",
      "training and storage costs. Therefore, similar to LLMs, PEFT\n",
      "is widely implemented in various downstream tasks, such as\n",
      "dense prediction [189], continual learning [190], [191], deep\n",
      "metric learning [192]. Here, we focus on two typical tasks to\n",
      "showcase the involvement of PEFT: image classification and\n",
      "video recognition.\n",
      "1) Image Classification: Image classification on targeted\n",
      "visual datasets is a very common demand and has extensive\n",
      "applications, while pre-train then fine-tuning paradigm serves\n",
      "as a widespread strategy. A variety of methods leverage PEFT\n",
      "techniques to achieve efficient model tuning [193], [189],\n",
      "[194], [195]. For instance, AdaptFormer [194] inserts adapter\n",
      "modules in parallel to the FFN of the original ViT model for\n",
      "visual recognition tasks. VPT (Visual Prompt Tuning) [193]\n",
      "prepends a small amount of task-specific parameters into the\n",
      "input sequence of each Transformer layer. When applying\n",
      "15\n",
      "ViT to downstream tasks, only these added parameters and\n",
      "the classification head are set to trainable. [196] notices that\n",
      "compared with supervised ViT, VPT often underperforms with\n",
      "self-supervised ViT. Further analysis demonstrates that differ-\n",
      "ent pre-trained methods and downstream tasks have varying\n",
      "degrees of dependency on transformer blocks at different lo-\n",
      "cations. To tackle this issue, the research introduces adaptable\n",
      "gates for ViT blocks. These gates dynamically modulate the\n",
      "contribution of prompt tokens to ViT blocks, allowing for a\n",
      "more targeted adaptation of the model to the task at hand.\n",
      "2) Video Recognition: Several works consider the more\n",
      "challenging adaptation problem that transfers ViT to down-\n",
      "stream tasks that have a much larger domain gap. For example,\n",
      "ST-Adapter (Spatio-Temporal Adapter) [197] and AIM [198]\n",
      "both insert adapters layers into pre-trained ViT blocks. Their\n",
      "primary goal is to model spatial-temporal information, thereby\n",
      "enabling efficient adaptation of ViTs from image models\n",
      "to video tasks. Notably, both methodologies have exhibited\n",
      "performance that surpasses traditional full-model fine-tuning\n",
      "approaches.\n",
      "C. PEFT for VLAs\n",
      "Vision-language\n",
      "alignment\n",
      "models\n",
      "(VLA),\n",
      "such\n",
      "as\n",
      "CLIP [199], ALIGN [200], DeCLIP [201], and FLAVA [202],\n",
      "are designed to learn a good image and text features which can\n",
      "be aligned within a unified representation space. Each VLA\n",
      "typically consists of separate image and text encoders that\n",
      "extract respective features. Contrastive learning is leveraged in\n",
      "these models to effectively align the image and text features.\n",
      "Fine-tuning is leveraged to improve the performance of VLA\n",
      "in specific datasets or tasks, but fine-tuning the full model\n",
      "is computationally intensive. For instance, fine-tuning CLIP\n",
      "RN50x64 requires a batch size of 32,768 and 18 days of\n",
      "training on 592 V100 GPUs [199]. Moreover, full fine-tuning\n",
      "on smaller datasets often leads to catastrophic forgetting [170].\n",
      "In response to these challenges, and drawing inspiration from\n",
      "the success of PEFT techniques in NLP, a range of PEFT\n",
      "strategies have been proposed and implemented in VLA\n",
      "models, such as semantic segmentation [203], [204], [205],\n",
      "point cloud understanding [206], [207], [208], [209], video\n",
      "understanding [210], [211], [212], visual reasoning [213],\n",
      "[214], temporal action detection [215], to name a few. This\n",
      "section will focus on one common task that uses VLAs:\n",
      "open-vocabulary image classification.\n",
      "1) Open-vocabulary\n",
      "Image\n",
      "Classification:\n",
      "In\n",
      "open-\n",
      "vocabulary\n",
      "image\n",
      "classification,\n",
      "earlier\n",
      "works\n",
      "design\n",
      "class-specific prompts, e.g., a photo of a [CLASS], for each\n",
      "category, and rank images based on their similarity to these\n",
      "textual descriptions. CoOp (Context Optimization) [216]\n",
      "replaces the handcrafted text prompt with learnable vectors,\n",
      "while keeping the entire VLA fixes during training. CoCoOp\n",
      "(Conditional Context Optimization) [217] builds on this by\n",
      "tackling CoOp’s limitations in generalizing to unseen classes.\n",
      "It introduces a lightweight neural network that generates an\n",
      "input-specific context token, dynamically adapting the prompt\n",
      "based on each image, thereby enhancing generalizability,\n",
      "but at the cost of increased computational demands due to\n",
      "the instance-aware operation. ProGrad [218] addresses the\n",
      "over-fitting risk in CoOp in a few-shot setting by regularizing\n",
      "the soft prompt updates whose gradient is aligned to the\n",
      "general knowledge only updates the prompt whose gradient is\n",
      "aligned (or non-conflicting) to the general knowledge offered\n",
      "by the original prompt. MaPLe [219] notes that existing\n",
      "methods learn prompts either in the language or in the vision\n",
      "branch of CLIP, which is not efficient in leveraging the\n",
      "multimodal nature of VLAs. To address this, MaPLe proposes\n",
      "branch-aware hierarchical prompts that simultaneously adapt\n",
      "both language and vision branches, and achieves superior\n",
      "performance. TPT (test-time prompt tuning) [220] studies\n",
      "prompt tuning on the fly without additional training samples.\n",
      "Specifically, during inference, TPT first augments the input\n",
      "image into various views, which are then utilized to tune\n",
      "the learnable prompts. The primary training objective is to\n",
      "ensure the VLA can generate consistent responses when faced\n",
      "with these differing views. A following work DiffTPT [221]\n",
      "further enhances the data diversity of test samples through\n",
      "diffusion models.\n",
      "In another direction, several studies explore the usage of\n",
      "adapters in VLA. For example, CLIP-Adapter [222] inte-\n",
      "grates residual-style adapters after CLIP’s text and visual en-\n",
      "coders. Therefore, unlike CoOp and CoCoOp, CLIP-Adapter\n",
      "avoids the gradient backpropagation through CLIP’s encoders,\n",
      "leading to reduced computational requirements in terms of\n",
      "both training memory and time. Tip-Adapter [223] adopts\n",
      "the same design with CLIP-Adapter. Different from CLIP-\n",
      "Adapter, the weights of the adapter are obtained in a training-\n",
      "free manner from a query-key cache model [224], [225]\n",
      "constructed from few-shot supervisions in a non-parametric\n",
      "manner. As a result, Tip-Adapter exhibits great efficiency\n",
      "compared to CLIP-Adapter’s SGD training process.\n",
      "D. PEFT for Diffusion Models\n",
      "Diffusion models [226], [227] are a class of generative\n",
      "models that learn to generate data by transforming random\n",
      "noise into a structured output by a progressive denoising\n",
      "process. During training, diffusion models learn to reverse\n",
      "the noise added to training data using a denoising network,\n",
      "while in inference, they start from noise, using a denois-\n",
      "ing network to iteratively create data that mirrors the same\n",
      "distribution as the training examples. Diffusion models have\n",
      "various applications [228], [229], [230], [231], [232], while the\n",
      "most notable is stable diffusion [233], which bridges the gap\n",
      "between text and image with its robust capability to generate\n",
      "coherent and contextually relevant images directly from textual\n",
      "descriptions. Numerous studies leverage PEFT techniques to\n",
      "adapt a pre-trained diffusion model for downstream tasks, in-\n",
      "cluding accelerating sampling speed [234], [235], text-to-video\n",
      "adaptation [236], [237], text-to-3D adaptation [238], etc. This\n",
      "section mainly focuses on two scenarios: integrating additional\n",
      "input modalities beyond mere text-based conditioning, and\n",
      "customizing content generation based on pre-trained diffusion\n",
      "model.\n",
      "1) Additional Input Control: To incorporate additional in-\n",
      "put modalities (e.g., layout, keypoints) while retaining the\n",
      "16\n",
      "extensive knowledge in the pre-trained model, GLIGEN\n",
      "introduces a novel approach, which maintains the original\n",
      "model’s weights intact and integrates new, trainable gated\n",
      "Transformer layers [239] that take in the new grounding\n",
      "input. The resulting model can not only accurately repre-\n",
      "sent the grounding conditions but also produce high-quality\n",
      "images. Remarkably, the model can also generalize well to\n",
      "unseen objects during inference. ControlNet [240] fine-tunes\n",
      "a trainable copy of the encoding layers from Stable Diffusion\n",
      "while locking its pre-trained parameter weights. The fixed\n",
      "original model and the trainable copy are bridged through zero\n",
      "convolution layers. These layers, starting with zero-initialized\n",
      "weights, are designed to progressively adapt during training,\n",
      "ensuring that harmful noise does not affect the pre-trained\n",
      "features of Stable Diffusion at the beginning of training. This\n",
      "refined model is capable of conditioning on a variety of inputs\n",
      "such as Canny edges, Hough lines, user scribbles, human key\n",
      "points, segmentation maps, shape normals, depths, etc. Con-\n",
      "cept Sliders [241] introduces a plug-and-play LoRA adaptors\n",
      "to allow precise editing of concepts (e.g., age, smiling) within\n",
      "a diffusion model. T2I-Adapter [242] introduces a lightweight\n",
      "adapter model designed to align external control signals with\n",
      "the internal knowledge of text-to-image diffusion models. This\n",
      "adapter enables precise manipulation through structural control\n",
      "(e.g., sketch, depth map, semantic segmentation map, and\n",
      "keypose), color control (e.g., hue and color distribution), and\n",
      "integrating various controls by composing multiple adapters.\n",
      "2) Customized Generation: The effectiveness of text-to-\n",
      "image diffusion models is limited by the user’s ability to\n",
      "articulate the desired target through text descriptions. For\n",
      "instance, it is difficult to describe the precise features of an\n",
      "innovative toy car which is not encountered during large-scale\n",
      "model training. Consequently, the objective of customized\n",
      "generation is to enable the model to grasp new concepts from a\n",
      "minimal set of user-supplied images. Textual Inversion [243]\n",
      "addresses this by finding a new pseudo-word S˚ (similar to\n",
      "soft prompt discussed in Section III-A2) that represents new,\n",
      "specific concepts in the textual embedding space of pre-trained\n",
      "text-to-image diffusion models. The pseudo-word S˚ is opti-\n",
      "mized via the original optimization goal in diffusion models\n",
      "given a small image set (typically 3-5 images) depicting the\n",
      "concept, and the pre-trained model is left untouched. During\n",
      "inference, S˚ can be treated like any other word and composed\n",
      "with other textual queries (e.g., ”a photo of S˚ on the beach”).\n",
      "Custom Diffusion [244] tackles a more challenging setting:\n",
      "compositional fine-tuning of multiple concepts. It fine-tunes\n",
      "only the Wk, Wv mapping from text to latent features in\n",
      "attention layers, which yields superior performance in multi-\n",
      "concept learning scenarios. Additionally, during fine-tuning,\n",
      "Custom Diffusion prevents model forgetting by introducing\n",
      "a small set of real images with captions akin to the target,\n",
      "alongside employing augmentation for faster convergence and\n",
      "improved results. IP-Adapter [245] identifies limitations in\n",
      "current approaches (e.g., ControlNet and T2I-Adapter) which\n",
      "project condition signals into the cross-attention modules.\n",
      "When handling image conditions aiming at controlling content,\n",
      "these methods are unable to generate images faithful to the\n",
      "prompted image. The issue stems from that merging image\n",
      "features and text features within cross-attention layers loses\n",
      "image-specific information, leading to only coarse-grained\n",
      "controllable generation such as image style rather than image\n",
      "content. To overcome this, IP-Adapter introduces a novel\n",
      "decoupled cross-attention mechanism to distinguish between\n",
      "text and image features. IP-Adapter adds an additional cross-\n",
      "attention layer exclusively for image features in each cross-\n",
      "attention layer, and only the parameters of the new cross-\n",
      "attention layers are trained.\n",
      "VI. SYSTEM DESIGN CHALLENGE FOR PEFT\n",
      "A. System design for PEFT\n",
      "In this section, we begin by providing a concise overview\n",
      "of cloud-based PEFT systems and analyzing the design chal-\n",
      "lenges. These include the efficient handling of numerous task-\n",
      "specific queries via centralized PEFT query servicing, the\n",
      "resolution of privacy and data transmission issues through\n",
      "distributed PEFT training, and the complexities associated\n",
      "with concurrent multi-PEFT training processes. Centralized\n",
      "systems are required to process a substantial volume of queries\n",
      "with minimal latency and maximal throughput. Distributed\n",
      "training frameworks must address privacy concerns and the\n",
      "computational inefficiencies that arise from data exchanges\n",
      "between users and cloud services. Furthermore, multi-PEFT\n",
      "training necessitates the optimization of memory utilization,\n",
      "the management of simultaneous model training, and the\n",
      "formulation of system architectures capable of supporting\n",
      "multi-tenant workloads effectively. These challenges under-\n",
      "score the imperative for innovative approaches to improve\n",
      "scalability, safeguard privacy, and optimize resource allocation\n",
      "in PEFT system architectures. Following this, we present the\n",
      "corresponding metrics employed for evaluating the system\n",
      "performance. Furthermore, we delve into three prospective\n",
      "utilization scenarios to illustrate the challenges in system\n",
      "design.\n",
      "1) Centralized PEFT Query Serving: Cloud providers have\n",
      "recently introduced a range of LLM services aimed at pro-\n",
      "viding user applications through application programming\n",
      "interfaces (APIs) [246], [247]. These APIs facilitate the seam-\n",
      "less integration of many machine-learning functionalities into\n",
      "applications. When receiving one query for one specific down-\n",
      "stream task through API, the cloud-based server processes the\n",
      "query with one featured LLM model. Under this scenario, the\n",
      "importance of PEFT becomes apparent. Cloud providers store\n",
      "only a single copy of the LLM and multiple PEFT modules\n",
      "featuring different downstream tasks. This setup allows the\n",
      "LLM to maintain various branches of PEFT modules, each\n",
      "linked to specific API queries, i.e., PEFT queries.\n",
      "Centralized PEFT query serving solutions address scenarios\n",
      "where multiple PEFT queries arrive in quick succession. A\n",
      "case study of one state-of-the-art system for this purpose\n",
      "is discussed in Section VI-B. Figure 10 (b) illustrates the\n",
      "computation pattern for multi-query PEFT inference, wherein\n",
      "packed PEFT queries are scheduled and executed according\n",
      "to their deadlines and current system conditions.\n",
      "2) Distributed PEFT Training: In most cases, personal-\n",
      "ized tasks are not fully supported with pre-trained models,\n",
      "17\n",
      "LLMs\n",
      "Edge \n",
      "Device \n",
      "Personal data\n",
      "Cloud\n",
      "Trainable \n",
      "Modules\n",
      "🔥\n",
      "Frozen Large Models\n",
      "Scheduler\n",
      "Request Pool\n",
      "Query\n",
      "Response\n",
      "Execution\n",
      "Engine\n",
      "Serving System\n",
      "I like\n",
      "I enjoy\n",
      "LLM\n",
      "programming\n",
      "(a)\n",
      "(b)\n",
      "Fig. 10: (a) Distributed-based system computation pattern; (b)\n",
      "centralized PEFT Query inference.\n",
      "consequently, extra fine-tuning is required to be executed\n",
      "with the methodologies mentioned in the previous sections.\n",
      "However, significant concerns arise when considering the\n",
      "transfer of datasets to cloud providers, given the issues related\n",
      "to data privacy, copyright, proprietary information, and the\n",
      "complexities and inefficiencies involved in data transmission.\n",
      "Section VI-C gives two approaches that address this concern.\n",
      "3) Multi-PEFT Training: Different from multiple-PEFT\n",
      "serving, tuning with multiple customized PEFTs always in-\n",
      "volves different backbone LLMs. Therefore, simultaneously\n",
      "tuning multiple PEFTs can pose considerable challenges.\n",
      "Challenges like how to manage memory gradient and model\n",
      "weights storage, and how to design an efficient kernel for\n",
      "batching PEFT training remain unsolved. PEFTs will be cat-\n",
      "egorized based on their PEFT algorithms and backbone LLM\n",
      "models. The design challenge involves how to consolidate\n",
      "multiple PEFTs with the same LLM backbone and multiple\n",
      "different LLM backbones simultaneously. We present case\n",
      "studies related to this topic in Section VI-D.\n",
      "4) Evaluation Metrics: For the proposed evaluation met-\n",
      "rics, without loss of generality, we adopt large language\n",
      "models as the basis for our metric definitions.\n",
      "To evaluate the system performance of PEFT serving sys-\n",
      "tems, we propose a set of evaluation metrics:\n",
      "‚ System throughput: Considering PEFT queries as inter\n",
      "and intra tasks, we use tokens per second to measure the\n",
      "system throughput.\n",
      "‚ Memory footprint: Run-time memory consumption dur-\n",
      "ing query serving, the memory utilization comes from\n",
      "both model parameters and KV-cache as mentioned in\n",
      "Section IV-A.\n",
      "‚ Accuracy performance: Real-world queries normally\n",
      "have different context lengths, and performance with\n",
      "variation length serves as a performance benchmark.\n",
      "‚ Quality of services: Queries are associated with latency\n",
      "requirements and deadline missing rates are considered\n",
      "as another benchmark.\n",
      "To assess the efficacy of PEFT training systems, we also\n",
      "establish a set of evaluative metrics:\n",
      "‚ Accuracy performance: Performance of the fine-tuned\n",
      "model over the downstream tasks.\n",
      "‚ Compute cost: The compute cost during forward and\n",
      "backward propagation operations on cloud servers and\n",
      "edge devices.\n",
      "‚ Communication cost: Refers to the volume of data\n",
      "involved during the transfer of intermediate data between\n",
      "the edge device and the cloud.\n",
      "B. Centralized PEFT Serving Frameworks\n",
      "The PEFT algorithm is notable for its ability to distin-\n",
      "guish between modifiable and immutable weights within a\n",
      "model. This characteristic inspires developers to amalgamate\n",
      "diverse LLMs with distinct PEFT techniques into collective\n",
      "units. PetS, as introduced in [248], advocates for a com-\n",
      "prehensive approach to managing multiple PEFT tasks by\n",
      "suggesting a unified serving framework. The framework’s\n",
      "core advancement lies in the translation of varying PEFT\n",
      "tasks into integrated computation kernels to enhance efficiency.\n",
      "Moreover, PetS pioneers an orchestrated batching approach\n",
      "and a scheduling methodology, aiming to augment system\n",
      "throughput and leverage task parallelism respectively.\n",
      "As depicted in Figure 11, the PetS framework begins\n",
      "with users registering PEFT tasks through a standardized\n",
      "Application Programming Interface (API). Upon registration,\n",
      "developers are expected to provide the Pre-Trained Model Tag\n",
      "(e.g., LLaMA), PEFT parameters in a compressed format,\n",
      "and the specific PEFT algorithms (e.g., LoRA, Adapter, Bitfit,\n",
      "etc.). These tasks are then endowed with unique identifiers,\n",
      "and the inference engine takes charge of query processing.\n",
      "PetS bifurcates the primary computational workload (e.g.,\n",
      "linear layer computations) into three distinct computational\n",
      "operations: (1) Dense Matrix-Vector Multiplication (MVM)\n",
      "leveraging universally accessible, pre-trained weights. (2) Bias\n",
      "vector addition (Vadd), using either common or task-exclusive\n",
      "biases. (3) A combination of Sparse/dense MVM operations\n",
      "employing task-specific PET parameters. A unified pre-trained\n",
      "weight matrix W is employed across PetS, facilitating the\n",
      "batching of initial operations, Xt ˆ W. However, subsequent\n",
      "task-specific computations involving PET parameters, despite\n",
      "being relatively minimal in complexity, are processed individ-\n",
      "ually.\n",
      "Considering the Adapter and Bitfit tasks as an illustration,\n",
      "both aim at the MLP component of LLMs. The Adapter\n",
      "task integrates additional weight segments, whereas Bitfit\n",
      "adjusts bias elements. The Adapter operation is modeled as\n",
      "Y “ Xin1 ˆ pW ` Wadq ` b0, where Xin1 represents the\n",
      "input for the Adapter task, W and Wad are the original\n",
      "and adapter-specific PEFT weights respectively, and b0 is the\n",
      "initial bias. The Bitfit operation, on the other hand, is defined\n",
      "as Y\n",
      "“ Xin2 ˆ W ` b1, with b1 symbolizing the Bitfit-\n",
      "adjustable bias. These operations are further synthesized as\n",
      "tY1, Y2u “ tXin1, Xin2u ˆ W ` tXin1 ˆ Wad, 0u ` tb0, b1u,\n",
      "delineating that the tXin1, Xin2u ˆ W part is amenable to\n",
      "batching through MVM, while the tb0, b1u segment pertains\n",
      "to the Vadd operation.\n",
      "For tasks like Diff-Pruning III-B, is a little bit different\n",
      "than Bitfit and Adapter. For Diff-Pruning, the computation\n",
      "concerning the shared weight and ‘difference’ are conducted\n",
      "separately. Then the results are added up, namely\n",
      "Xt ˆ pW ` δtq “ Xt ˆ W ` Xt ˆ δt\n",
      ", here, the W denotes the backbone model weights while δt\n",
      "denotes the pruned weights which can be represented as Sparse\n",
      "MVM.\n",
      "18\n",
      "PET Serving\n",
      "PET Inference Pipeline\n",
      "Pre-train Model \n",
      "ID\n",
      "Shadow \n",
      "Parameters\n",
      "PET Type\n",
      "Pre-train Model \n",
      "ID\n",
      "Shadow \n",
      "Parameters\n",
      "PET Type\n",
      "Pre-trained \n",
      "Model Tag\n",
      "PET Parameters\n",
      "PET Type\n",
      "PET Parameters\n",
      "Shared Model\n",
      "Parameters\n",
      "Register Tasks\n",
      "u\n",
      "Task Register\n",
      "PET Manager\n",
      "Task Manager\n",
      "Parameter Repository\n",
      "v\n",
      "w\n",
      "<Task_id> \n",
      "<Input Data>\n",
      "…\n",
      "Query 0:\n",
      "Query 1:\n",
      "Input Queries\n",
      "x\n",
      "Performance \n",
      "Model\n",
      "Batch Scheduler\n",
      "Scheduling \n",
      "Policy\n",
      "Engine\n",
      "PET Task\n",
      "Scheduler\n",
      "PET Operator\n",
      "Library\n",
      "y\n",
      "Input \n",
      "Analyzing\n",
      "Input \n",
      "Reformatting\n",
      "User Inputs\n",
      "<Task_id> \n",
      "<Input Data>\n",
      "Fig. 11: PetS system overview: (1) Tasks register; (2) Task\n",
      "manager (3) Task schedule; (4) Task serving. (Image is taken\n",
      "from PetS [248])\n",
      "Task 0\n",
      "Task 4\n",
      "Step 1: Intra-Task Batching\n",
      "Task 1\n",
      "Task 2\n",
      "Task 3\n",
      "Mini\n",
      "Batch\n",
      "𝛽−Model\n",
      "𝛼−Model\n",
      "PET-OPs Profiling\n",
      "Batch 1\n",
      "Batch 0\n",
      "B=2, S=34\n",
      "Step 2: Inter-Task Batching\n",
      "Batch 2\n",
      "Macro\n",
      "Batch\n",
      "Shared-OPs\n",
      "Profiling\n",
      "B=4, S=34\n",
      "Task 0\n",
      "Task 1\n",
      "Task 3\n",
      "Task 2\n",
      "Task 4\n",
      "Fig. 12: Coordinated Batching (CB) Strategy\n",
      "The other challenge PetS proposed is how to schedule dif-\n",
      "ferent PEFT requests to achieve high performance. PetS sched-\n",
      "uler achieves high parallelism through a two-level scheduling\n",
      "policy: Coordinated Batching (CB) and Macro-batch Stream-\n",
      "ing (MS) as Figure 12 depicts. Through CB, the input queries\n",
      "will first be clustered based on their input length and then\n",
      "grouped based on their shared operator. This is to make sure\n",
      "the same sequence length of queries will be executed without\n",
      "wasting padding. MS strategy will take the grouped queries\n",
      "after coordinated batching and the theoretical latency for\n",
      "different operators as well as the system modeling parameters\n",
      "to generate the best execution order.\n",
      "The other example design is DLoRA [249], which intro-\n",
      "duces a system that improves the efficiency of serving low-\n",
      "rank adaptation (LoRA) models for large language models\n",
      "(LLMs) by dynamically managing the merging and unmerging\n",
      "of LoRA adapters and the migration of requests across worker\n",
      "replicas. This dynamic orchestration addresses the challenges\n",
      "of high memory footprints, low GPU utilization, and load\n",
      "imbalance caused by variable input and output lengths in\n",
      "traditional LLM serving systems. dLoRA’s novel approaches,\n",
      "including a credit-based batching algorithm and a request-\n",
      "adapter co-migration algorithm, significantly enhance through-\n",
      "put.\n",
      "C. Distributed PEFT Training Frameworks\n",
      "We already know that fine-tuning LLM for downstream\n",
      "tasks is challenging for two reasons: dual privacy concerns\n",
      "between cloud server and data owner, and issues with com-\n",
      "putational resources and efficiency. Firstly, the privacy of\n",
      "both parties is at risk: the weights of large models are often\n",
      "proprietary and not made public. Sharing data with model\n",
      "owners for fine-tuning can lead to data privacy concerns while\n",
      "providing model weights to data proprietors could compromise\n",
      "the ownership of proprietary models. Secondly, even if down-\n",
      "stream users have access to pre-trained weights, the stringent\n",
      "hardware requirements make transfer learning impractical for\n",
      "most end users.\n",
      "To resolve these two issues, DLoRA [250] presents a\n",
      "distributed PEFT framework. During the PEFT process, the\n",
      "backbone LLM is executed in the cloud servers while the\n",
      "PEFT modules are trained entirely within the user devices.\n",
      "DLoRA scheme is depicted in Figure 10(a).\n",
      "Similarly,\n",
      "Offsite-Tuning\n",
      "[251]\n",
      "presents\n",
      "a\n",
      "privacy-\n",
      "preserving and efficient transfer learning framework that\n",
      "enables foundational models to adapt to downstream tasks\n",
      "without the need to access the complete model weights. The\n",
      "key insight of Offsite-Tuning is the cloud provider sends an\n",
      "adapter and an emulator to the data proprietor. Then, with the\n",
      "assistance of the emulator, the data proprietor fine-tunes the\n",
      "adapter. The fine-tuned adapter is then sent back to the cloud\n",
      "side, which integrates it into the complete model, creating a\n",
      "fine-tuned foundational model for downstream users. Offsite-\n",
      "Tuning safeguards the privacy of data proprietors since they\n",
      "do not need to share their training data directly. It also\n",
      "protects the foundational model owners, as the complete model\n",
      "weights are not shared, and the emulator provided is lossy,\n",
      "with significantly degraded performance. Compared to existing\n",
      "fine-tuning methods that require access to the full model\n",
      "weights, Offsite-Tuning is more resource-efficient because it\n",
      "allows for fine-tuning through a compressed emulator without\n",
      "needing the complete model.\n",
      "D. Parallel PEFT Training Frameworks\n",
      "Unlike the PEFT query serving system, which aims to\n",
      "accommodate flexible multi-PEFT algorithms, Punica [252]\n",
      "focuses solely on facilitating multiple-LoRA blocks for various\n",
      "tasks. Designing multiple PEFT training systems presents key\n",
      "challenges in two main aspects:\n",
      "‚ Efficient concurrent execution of multiple PEFT models\n",
      "with the same LLM backbone.\n",
      "‚ Designing an efficient system for multi-tenant serving\n",
      "with different LLM backbones.\n",
      "a) Efficient kernel design: Punica addresses the first chal-\n",
      "lenge by using existing matrix multiplication for the backbone\n",
      "computation and introducing a new CUDA kernel, Segmented\n",
      "Gather Matrix-Vector Multiplication (SGMV), for adding the\n",
      "PEFT add-ons to the backbone computation in a batched\n",
      "manner. This kernel parallelizes the feature-weight multipli-\n",
      "cation for different requests in the batch and groups requests\n",
      "corresponding to the same PEFT model to increase operational\n",
      "intensity and use GPU Tensor Cores for acceleration.\n",
      "19\n",
      "The second challenge is beyond the computational cost,\n",
      "designing an efficient system architecture that can effectively\n",
      "serve multi-tenant PEFT model workloads on the smallest set\n",
      "of GPUs possible while occupying the least amount of GPU\n",
      "resources is another significant challenge. Punica addresses\n",
      "this by scheduling user requests to active GPUs that already\n",
      "serve or train PEFT models, thereby improving GPU utiliza-\n",
      "tion. For older requests, Punica periodically migrates them to\n",
      "consolidate workloads, thus freeing up GPU resources for new\n",
      "requests.\n",
      "b) Multi-Tenant PEFT design: Designing an efficient\n",
      "system for the multi-tenant PEFT model serving in the Punica\n",
      "framework focuses on addressing several key challenges to\n",
      "maximize hardware utilization and minimize resource con-\n",
      "sumption. The system aims to consolidate multi-tenant LoRA\n",
      "serving workloads onto the smallest set of GPUs possible. This\n",
      "consolidation is achieved through strategic scheduling of user\n",
      "requests to active GPUs that are already serving or training\n",
      "LoRA models, thereby improving GPU utilization. For older\n",
      "requests, Punica periodically migrates them to consolidate\n",
      "workloads further, thus freeing up GPU resources for new\n",
      "requests. It incorporates on-demand loading of LoRA model\n",
      "weights, which introduces only millisecond-level latency. This\n",
      "feature provides Punica with the flexibility to dynamically\n",
      "consolidate user requests to a small set of GPUs, without being\n",
      "constrained by the specific LoRA models already running on\n",
      "those GPUs. Besides that, Punica identifies that the decode\n",
      "stage is a predominant factor in the cost of model serving,\n",
      "Punica’s design primarily focuses on optimizing decode stage\n",
      "performance. Other aspects of model serving leverage straight-\n",
      "forward techniques, such as on-demand loading of LoRA\n",
      "model weights, to efficiently manage resource utilization.\n",
      "VII. CONCLUSION AND FUTURE DIRECTIONS\n",
      "In the current era dominated by large models and large\n",
      "datasets, PEFT stands out as a highly attractive method for\n",
      "efficiently adapting models to downstream tasks. This tech-\n",
      "nique gains its appeal by addressing the significant challenges\n",
      "posed by traditional full-model fine-tuning, which often places\n",
      "substantial computational and data demands. This survey of-\n",
      "fers a comprehensive examination of the most recent advance-\n",
      "ments in PEFT, including algorithmic design, computational\n",
      "efficiency, application scenarios, and system implementation\n",
      "for PEFT. It offers a comprehensive taxonomy and explanation\n",
      "that serves as an excellent guidance and knowledge base,\n",
      "which enables readers of various levels and disciplines to\n",
      "swiftly grasp the core concepts of PEFT.\n",
      "For further research on PEFT, we propose a series of pos-\n",
      "sible directions from both algorithm and system perspectives,\n",
      "hoping to inspire more researchers to engage in further studies\n",
      "in these areas.\n",
      "A. Simplify hyperparameter tuning\n",
      "The effectiveness of PEFT is often sensitive to its hyperpa-\n",
      "rameters, such as the bottleneck dimension of the adapter, the\n",
      "rank of LoRA, and the arrangement of various additive PEFT\n",
      "layers. Manually tuning these hyperparameters will cost lots\n",
      "of effort. Therefore, future efforts could focus on developing\n",
      "methods that are less dependent on manual tuning of these\n",
      "parameters, or automatically find the optimal configuration\n",
      "settings. Several studies [82], [83], [84], [98], [99], [100] have\n",
      "started to address this issue, but there’s a need for more simple\n",
      "and efficient solutions optimizing these hyperparameters.\n",
      "B. Establish a unified benchmark\n",
      "Despite the existence of libraries like HuggingFace’s\n",
      "PEFT [253] and AdapterHub [254], a comprehensive bench-\n",
      "mark for PEFT is still lacking. This gap hinders the ability\n",
      "to fairly compare the performance and efficiency of different\n",
      "PEFT approaches. A well-accepted, up-to-date benchmark\n",
      "akin to MMDetection [255] for object detection would enable\n",
      "researchers to validate their methods against a standard set\n",
      "of tasks and metrics, fostering innovation and collaboration\n",
      "within the community.\n",
      "C. Enhance training efficiency\n",
      "The presumed parameter efficiency of PEFT is not always\n",
      "consistent with computational and memory savings during\n",
      "training. Given that trainable parameters are intertwined within\n",
      "the pre-trained model’s architecture, computing and storing\n",
      "activations and gradients for the full model often become\n",
      "necessary during fine-tuning. This oversight calls for a rethink-\n",
      "ing of what constitutes efficiency. As outlined in Section IV,\n",
      "potential solutions lie in the integration of model compres-\n",
      "sion techniques such as pruning and quantization, alongside\n",
      "innovations specifically designed to optimize memory during\n",
      "PEFT tuning [256]. Further research into enhancing the com-\n",
      "putational efficiency of PEFT methodologies is imperative.\n",
      "D. Explore scaling laws\n",
      "The design and effectiveness of PEFT methods originally\n",
      "developed for smaller Transformer models do not necessarily\n",
      "scale with larger models. As the size of foundation models\n",
      "increases, identifying and adapting PEFT strategies that remain\n",
      "effective is crucial. This investigation will aid in customizing\n",
      "PEFT methodologies to suit the evolving landscape of large\n",
      "model architectures.\n",
      "E. Serve more models and tasks\n",
      "The rise of large foundation models across various domains\n",
      "presents new opportunities for PEFT. Designing PEFT meth-\n",
      "ods tailored to the unique characteristics of models, such as\n",
      "Sora [257], Mamba [258], and LVM [259], can unlock new\n",
      "application scenarios and opportunities.\n",
      "F. Enhancing data privacy\n",
      "Trusting centralized systems to serve or fine-tune personal-\n",
      "ized PEFT modules is yet another issue for system developers.\n",
      "Multiple types of inversion attacks [260], [261] have been pro-\n",
      "posed to reconstruct user’s data by hijacking the intermediate\n",
      "results. One perspective of future trust-worthy LLM system\n",
      "design involves developing an encryption protocol for both\n",
      "personal data and intermediate training and inference results.\n",
      "20\n",
      "G. PEFT with model compression\n",
      "Model compression is one of the most effective ways to\n",
      "make LLM executable on resource-limited devices. Yet, the\n",
      "impact of model compression techniques on the performance\n",
      "of PEFT algorithms running on hardware remains another\n",
      "systemic challenge. Common compression techniques such\n",
      "as quantization and pruning necessitate dedicated hardware\n",
      "platforms to expedite the process, and building such hardware\n",
      "platforms for compressed models is yet another direction for\n",
      "future research.\n"
     ]
    }
   ],
   "source": [
    "len(tokenizer.encode(chunks[113].page_content))\n",
    "print(chunks[113].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e4f9ebf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average chunk length (in tokens): 2466.4210526315787\n",
      "Max chunk length (in tokens): 77516\n",
      "Min chunk length (in tokens): 111\n",
      "Chunk with max length: 111\n"
     ]
    }
   ],
   "source": [
    "lengths = [len(tokenizer.encode(chunk.page_content)) for chunk in chunks]\n",
    "idx = [str(i) for i in range(len(lengths))]\n",
    "lengths_map = dict(zip(lengths, idx))\n",
    "print(f\"Average chunk length (in tokens): {sum(lengths) / len(lengths)}\")\n",
    "print(f\"Max chunk length (in tokens): {max(lengths)}\")\n",
    "print(f\"Min chunk length (in tokens): {min(lengths)}\")\n",
    "print(f\"Chunk with max length: {lengths_map[max(lengths)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c07cbc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from endpoints.tei import LocalTEIEmbeddings\n",
    "\n",
    "hf_embeddings = LocalTEIEmbeddings(\"http://127.0.0.1:8080\")\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"trainwise_data\",\n",
    "    embedding_function=hf_embeddings,\n",
    "    persist_directory=\"./chroma_db\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "103b8d32",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mvector_store\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m=\u001b[49m\u001b[43mall_docs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43muuids\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/trainwise/lib/python3.13/site-packages/langchain_core/vectorstores/base.py:257\u001b[39m, in \u001b[36mVectorStore.add_documents\u001b[39m\u001b[34m(self, documents, **kwargs)\u001b[39m\n\u001b[32m    255\u001b[39m     texts = [doc.page_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[32m    256\u001b[39m     metadatas = [doc.metadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    258\u001b[39m msg = (\n\u001b[32m    259\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m`add_documents` and `add_texts` has not been implemented \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    260\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mfor \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    261\u001b[39m )\n\u001b[32m    262\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/trainwise/lib/python3.13/site-packages/langchain_chroma/vectorstores.py:620\u001b[39m, in \u001b[36mChroma.add_texts\u001b[39m\u001b[34m(self, texts, metadatas, ids, **kwargs)\u001b[39m\n\u001b[32m    618\u001b[39m texts = \u001b[38;5;28mlist\u001b[39m(texts)\n\u001b[32m    619\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m     embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_embedding_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    621\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m metadatas:\n\u001b[32m    622\u001b[39m     \u001b[38;5;66;03m# fill metadatas with empty dicts if somebody\u001b[39;00m\n\u001b[32m    623\u001b[39m     \u001b[38;5;66;03m# did not specify metadata for all texts\u001b[39;00m\n\u001b[32m    624\u001b[39m     length_diff = \u001b[38;5;28mlen\u001b[39m(texts) - \u001b[38;5;28mlen\u001b[39m(metadatas)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/TrainWise/endpoints/tei.py:12\u001b[39m, in \u001b[36mLocalTEIEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34membed_documents\u001b[39m(\u001b[38;5;28mself\u001b[39m, texts):\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     res = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mendpoint_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m     res.raise_for_status()\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [d[\u001b[33m\"\u001b[39m\u001b[33membedding\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m res.json()[\u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m]]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/trainwise/lib/python3.13/site-packages/requests/api.py:115\u001b[39m, in \u001b[36mpost\u001b[39m\u001b[34m(url, data, json, **kwargs)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(url, data=\u001b[38;5;28;01mNone\u001b[39;00m, json=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m    104\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[32m    105\u001b[39m \n\u001b[32m    106\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    112\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/trainwise/lib/python3.13/site-packages/requests/api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/trainwise/lib/python3.13/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/trainwise/lib/python3.13/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/trainwise/lib/python3.13/site-packages/requests/adapters.py:644\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    641\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    659\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/trainwise/lib/python3.13/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/trainwise/lib/python3.13/site-packages/urllib3/connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    536\u001b[39m     \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/trainwise/lib/python3.13/site-packages/urllib3/connection.py:565\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    562\u001b[39m _shutdown = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.sock, \u001b[33m\"\u001b[39m\u001b[33mshutdown\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m565\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    568\u001b[39m     assert_header_parsing(httplib_response.msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.8/lib/python3.13/http/client.py:1430\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1428\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1429\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1430\u001b[39m         \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1431\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[32m   1432\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.8/lib/python3.13/http/client.py:331\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n\u001b[32m    333\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.8/lib/python3.13/http/client.py:292\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n\u001b[32m    294\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[33m\"\u001b[39m\u001b[33mstatus line\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.8/lib/python3.13/socket.py:719\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    717\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mcannot read from timed out object\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m719\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    721\u001b[39m     \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "vector_store.add_documents(documents=all_docs, ids=uuids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trainwise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
