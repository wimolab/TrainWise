{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Fine-Tuning Your First Large Language Model (LLM) with PyTorch and Hugging Face", "description": "A Blog post by Daniel Voigt Godoy on Hugging Face", "url": "https://huggingface.co/blog/dvgodoy/fine-tuning-llm-hugging-face", "source": "hf", "id": "a293f0c6-c0f0-40e9-9b92-217fa52de88b"}, "page_content": "[CLS]Fine-Tuning Your First Large Language Model (LLM) with PyTorch and Hugging Face\n\n> This blog post contains **\"Chapter 0: TL;DR\"** of my latest book [**_A Hands-On Guide to Fine-Tuning Large Language Models with PyTorch and Hugging Face_**](https://pytorchstepbystep.com/llms).\n\n## Spoilers\n\nIn this blog post, we'll get right to it and fine-tune a small language model, Microsoft's Phi-3 Mini 4K Instruct, to translate English into Yoda-speak. You can think of this initial chapter as a **recipe** you can just follow. It's a \" _shoot first, ask questions later_\" kind of post.\n\nYou'll learn how to:\n\n- Load a **quantized model** using `BitsAndBytes`\n- Configure **low-rank adapters (LoRA)** using Hugging Face's `peft`\n- Load and **format** a dataset\n- Fine-tune the model using the **supervised fine-tuning trainer** (`SFTTrainer`) from Hugging Face's `trl`\n- Use the fine-tuned model to **generate a sentence**\n\n## Jupyter Notebook\n\nThe Jupyter notebook corresponding to [this post](https://github.com/dvgodoy/FineTuningLLMs/blob/main/Chapter0.ipynb) is part of the official **_Fine-Tuning LLMs_** repository on GitHub. You can also run it directly in [**Google Colab**](https://colab.research.google.com/github/dvgodoy/FineTuningLLMs/blob/main/Chapter0.ipynb)\n\n### Setup\n\nIf you're running it on Colab, you'll need to `pip install` a few libraries: `datasets`, `bitsandbytes`, and `trl`.\n\nFor better reproducibility during training, however, use the pinned versions instead:\n\n```python\n#!pip install datasets bitsandbytes trl\n!pip install transformers==4.55.2 peft==0.17.0 accelerate==1.10.0 trl==0.21.0 bitsandbytes==0.47.0 datasets==4.0.0 huggingface-hub==0.34.4 safetensors==0.6.2 pandas==2.2.2 matplotlib==3.10.0 numpy==2.0.2\n```\n\n### Imports\n\nFor the sake of organization, all libraries needed throughout the code used are imported at its very start. For this post, we'll need the following imports:\n\n```python\nimport os\nimport torch\nfrom datasets import load_dataset\nfrom peft import get_peft_model, LoraConfig, prepare_model_for_kbit_training\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom trl import SFTConfig, SFTTrainer\n```\n\n## Loading a Quantized Base Model\n\nWe start by loading a quantized model, so it takes up less space in the GPU's RAM. A quantized model replaces the original weights with approximate values that are represented by fewer bits. The simplest and most straightforward way to quantize a model is to turn its weights from 32-bit floating-point (FP32) numbers into 4-bit floating-point numbers (NF4). This simple yet powerful change already **reduces the model's memory footprint** by roughly a factor of eight.\n\nWe can use an instance of `BitsAndBytesConfig` as the `quantization_config` argument while loading a model using the `from_pretrained()` method. To keep it flexible, so you can try it out with any other model of your choice, we're using Hugging Face's\n`AutoModelForCausalLM`. The repo you choose to use determines the model being loaded.\n\nWithout further ado, here's our quantized model being loaded:\n\n```python\nbnb_config = BitsAndBytesConfig(\n   load_in_4bit=True,\n   bnb_4bit_quant_type=\"nf4\",\n   bnb_4bit_use_double_quant=True,\n   bnb_4bit_compute_dtype=torch.float32\n)\nrepo_id ='microsoft/Phi-3-mini-4k-instruct'\nmodel = AutoModelForCausalLM.from_pretrained(\n   repo_id, device_map=\"cuda:0\", quantization_config=bnb_config\n)\n```\n\n> _\"The Phi-3-Mini-4K-Instruct is a 3.8B parameters, lightweight, state-of-the-art open model trained with the Phi-3 datasets that includes both synthetic data and the filtered publicly available websites data with a focus on high-quality and reasoning dense properties. The model belongs to the Phi-3 family with the Mini version in two variants 4K and 128K which is the context length (in tokens) that it can support.\"_\n>\n> Source: [Hugging Face Hub](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct)\n\nOnce the model is loaded, you can see how much space it occupies in memory using the `get_memory_footprint()` method.\n\n```python\nprint(model", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Fine-Tuning Your First Large Language Model (LLM) with PyTorch and Hugging Face", "description": "A Blog post by Daniel Voigt Godoy on Hugging Face", "url": "https://huggingface.co/blog/dvgodoy/fine-tuning-llm-hugging-face", "source": "hf", "id": "758180e9-ce8f-411c-8c71-d9bfcee7e78e"}, "page_content": ", device_map=\"cuda:0\", quantization_config=bnb_config\n)\n```\n\n> _\"The Phi-3-Mini-4K-Instruct is a 3.8B parameters, lightweight, state-of-the-art open model trained with the Phi-3 datasets that includes both synthetic data and the filtered publicly available websites data with a focus on high-quality and reasoning dense properties. The model belongs to the Phi-3 family with the Mini version in two variants 4K and 128K which is the context length (in tokens) that it can support.\"_\n>\n> Source: [Hugging Face Hub](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct)\n\nOnce the model is loaded, you can see how much space it occupies in memory using the `get_memory_footprint()` method.\n\n```python\nprint(model.get_memory_footprint()/1e6)\n```\n\n```\n2206.347264\n```\n\nEven though it's been quantized, the model still takes up a bit more than 2 gigabytes of RAM. The **quantization** procedure focuses on the **linear layers within the Transformer decoder blocks** (also referred to as \"layers\" in some cases):\n\n```python\nmodel\n```\n\n```\nPhi3ForCausalLM(\n  (model): Phi3Model(\n    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n    (embed_dropout): Dropout(p=0.0, inplace=False)\n    (layers): ModuleList(\n      (0-31): 32 x Phi3DecoderLayer(\n        (self_attn): Phi3Attention(\n          (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)        <1>\n          (qkv_proj): Linear4bit(in_features=3072, out_features=9216, bias=False)      <1>\n          (rotary_emb): Phi3RotaryEmbedding()\n        )\n        (mlp): Phi3MLP(\n          (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False) <1>\n          (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)     <1>\n          (activation_fn): SiLU()\n        )\n        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n      )\n    )\n    (norm): Phi3RMSNorm((3072,), eps=1e-05)\n  )\n  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n)\n```\n\n```\n<1> Quantized layers\n```\n\nA **quantized model** can be used directly for inference, but it **cannot be trained any further**. Those pesky `Linear4bit` layers take up much less space, which is the whole point of quantization; however, we cannot update them.\n\nWe need to add something else to our mix, a sprinkle of adapters.\n\n## Setting Up Low-Rank Adapters (LoRA)\n\nLow-rank adapters can be attached to each and every one of the quantized layers. The **adapters** are mostly **regular `Linear` layers** that can be easily updated as usual. The clever trick in this case is that these adapters are significantly **smaller** than the layers that have been quantized.\n\nSince the **quantized layers are frozen** (they cannot be updated), setting up **LoRA adapters** on a quantized model drastically **reduces the total number of trainable parameters** to just 1% (or less) of its original size.\n\nWe can set up LoRA adapters in three easy steps:\n\n- Call `prepare_model_for_kbit_training()` to _improve numerical stability_ during training.\n- Create an instance of `LoraConfig`.\n- Apply the configuration to the quantized base model using the `get_peft_model()` method.\n\nLet's try it out with our model:\n\n```python\nmodel = prepare_model_for_kbit_training(model)\n\nconfig = LoraConfig(\n    # the rank of the adapter, the lower the fewer parameters you'll need to train\n    r=8,\n    lora_alpha=16, # multiplier, usually 2*r\n    bias=\"none\",\n    lora_dropout=0.05,\n    task_type=\"CAUSAL_LM\",\n    # Newer models, such as Phi-3 at time of writing, may require\n    # manually setting target modules\n    target_modules=['o_proj', 'qkv_proj', 'gate_", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Fine-Tuning Your First Large Language Model (LLM) with PyTorch and Hugging Face", "description": "A Blog post by Daniel Voigt Godoy on Hugging Face", "url": "https://huggingface.co/blog/dvgodoy/fine-tuning-llm-hugging-face", "source": "hf", "id": "54c84abb-bb9f-4e00-bb84-73f29aa4a597"}, "page_content": " to _improve numerical stability_ during training.\n- Create an instance of `LoraConfig`.\n- Apply the configuration to the quantized base model using the `get_peft_model()` method.\n\nLet's try it out with our model:\n\n```python\nmodel = prepare_model_for_kbit_training(model)\n\nconfig = LoraConfig(\n    # the rank of the adapter, the lower the fewer parameters you'll need to train\n    r=8,\n    lora_alpha=16, # multiplier, usually 2*r\n    bias=\"none\",\n    lora_dropout=0.05,\n    task_type=\"CAUSAL_LM\",\n    # Newer models, such as Phi-3 at time of writing, may require\n    # manually setting target modules\n    target_modules=['o_proj', 'qkv_proj', 'gate_up_proj', 'down_proj'],\n)\nmodel = get_peft_model(model, config)\nmodel\n```\n\n```\nPeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): Phi3ForCausalLM(\n      (model): Phi3Model(\n        (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n        (embed_dropout): Dropout(p=0.0, inplace=False)\n        (layers): ModuleList(\n          (0-31): 32 x Phi3DecoderLayer(\n            (self_attn): Phi3Attention(\n              (o_proj): lora.Linear4bit(                      <1>\n                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n                (lora_dropout): ModuleDict((default): Dropout(p=0.05, inplace=False))\n                (lora_A): ModuleDict(\n                    (default): Linear(in_features=3072, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=3072, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (qkv_proj): lora.Linear4bit(...)                <1>\n              (rotary_emb): Phi3RotaryEmbedding()\n            )\n            (mlp): Phi3MLP(\n              (gate_up_proj): lora.Linear4bit(...)            <1>\n              (down_proj): lora.Linear4bit(...)               <1>\n              (activation_fn): SiLU()\n            )\n            (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n            (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n            (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n            (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n          )\n        )\n        (norm): Phi3RMSNorm((3072,), eps=1e-05)\n      )\n      (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n    )\n  )\n)\n```\n\n```\n<1> LoRA adapters\n```\n\nThe output of the other three LoRA layers (`qkv_proj`, `gate_up_proj`, and `down_proj`) was suppressed to shorten the output.\n\n> Did you get the following error?\n>\n>\n> ``ValueError: Please specify `target_modules` in `peft_config` ``\n>\n> Most likely, you don't need to specify the `target_modules` if you're using one of the well-known models. The `peft` library takes care of it by _automatically choosing the appropriate targets_. However, there may be a gap between the time a popular model is released and the time the library gets updated. So, if you get the error above, look for the quantized layers in your model and list their names in the `target_modules` argument.\n\nThe quantized layers (`Linear4bit`) have turned into `lora.Linear4bit` modules where the quantized layer itself became the `base_layer` with some regular `Linear` layers (`lora_A` and `lora_B`) added to the mix.\n\nThese extra layers would make the model only slightly larger. However, **the model preparation function** (`prepare_model_for_kbit_training()`) turned **every non-quantized layer to full precision (FP32)**, thus resulting in a 30% larger model:\n\n```python\nprint(model.get_memory_footprint()/1e6)\n```\n\n```\n2651.080704\n```\n\nSince most parameters are frozen, only a tiny fraction of the total number of parameters are currently train", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Fine-Tuning Your First Large Language Model (LLM) with PyTorch and Hugging Face", "description": "A Blog post by Daniel Voigt Godoy on Hugging Face", "url": "https://huggingface.co/blog/dvgodoy/fine-tuning-llm-hugging-face", "source": "hf", "id": "26a3d8aa-c862-493f-9755-0ece23ba51d8"}, "page_content": " error above, look for the quantized layers in your model and list their names in the `target_modules` argument.\n\nThe quantized layers (`Linear4bit`) have turned into `lora.Linear4bit` modules where the quantized layer itself became the `base_layer` with some regular `Linear` layers (`lora_A` and `lora_B`) added to the mix.\n\nThese extra layers would make the model only slightly larger. However, **the model preparation function** (`prepare_model_for_kbit_training()`) turned **every non-quantized layer to full precision (FP32)**, thus resulting in a 30% larger model:\n\n```python\nprint(model.get_memory_footprint()/1e6)\n```\n\n```\n2651.080704\n```\n\nSince most parameters are frozen, only a tiny fraction of the total number of parameters are currently trainable, thanks to LoRA!\n\n```python\ntrain_p, tot_p = model.get_nb_trainable_parameters()\nprint(f'Trainable parameters:      {train_p/1e6:.2f}M')\nprint(f'Total parameters:          {tot_p/1e6:.2f}M')\nprint(f'% of trainable parameters: {100*train_p/tot_p:.2f}%')\n```\n\n```\nTrainable parameters:      12.58M\nTotal parameters:          3833.66M\n% of trainable parameters: 0.33%\n```\n\nThe model is ready to be fine-tuned, but we are still missing one key component: our dataset.\n\n## Formatting Your Dataset\n\n> _\"Like Yoda, speak, you must. Hrmmm.\"_\n>\n> Master Yoda\n\nThe dataset [`yoda_sentences`](https://huggingface.co/datasets/dvgodoy/yoda_sentences) consists of 720 sentences translated from English to Yoda-speak. The dataset is hosted on the Hugging Face Hub and we can easily load it using the `load_dataset()` method from the Hugging Face `datasets` library:\n\n```python\ndataset = load_dataset(\"dvgodoy/yoda_sentences\", split=\"train\")\ndataset\n```\n\n```\nDataset({\nfeatures: ['sentence', 'translation', 'translation_extra'],\nnum_rows: 720\n})\n```\n\nThe dataset has three columns:\n\n- original English sentence (`sentence`)\n- basic translation to Yoda-speak (`translation`)\n- enhanced translation including typical `Yesss` and `Hrrmm` interjections (`translation_extra`)\n\n```python\ndataset[0]\n```\n\n```\n{'sentence': 'The birch canoe slid on the smooth planks.',\n'translation': 'On the smooth planks, the birch canoe slid.',\n'translation_extra': 'On the smooth planks, the birch canoe slid. Yes, hrrrm.'}\n```\n\nThe `SFTTrainer` we'll be using to fine-tune the model can automatically handle datasets in **conversational** format.\n\n```\n{\"messages\":[\\\n  {\"role\": \"system\", \"content\": \"<general directives>\"},\\\n  {\"role\": \"user\", \"content\": \"<prompt text>\"},\\\n  {\"role\": \"assistant\", \"content\": \"<ideal generated text>\"}\\\n]}\n```\n\n* * *\n\n**IMPORTANT UPDATE**: unfortunately, in more recent versions of the `trl` library, the \"instruction\" format is not properly supported anymore, thus leading to the chat template not being applied to the dataset. In order to avoid this issue, we can convert the dataset to the \"conversational\" format.\n\n* * *\n\nSo, we'll convert the dataset to the conversational format using the `format_dataset()` function below:\n\n```python\n# Adapted from trl.extras.dataset_formatting.instructions_formatting_function\n# Converts dataset from prompt/completion format (not supported anymore)\n# to the conversational format\ndef format_dataset(examples):\n    if isinstance(examples[\"prompt\"], list):\n        output_texts = []\n        for i in range(len(examples[\"prompt\"])):\n            converted_sample = [\\\n                {\"role\": \"user\", \"content\": examples[\"prompt\"][i]},\\\n                {\"role\": \"assistant\", \"content\": examples[\"completion\"][i]},\\\n            ]\n            output_texts.append(converted_sample)\n        return {'messages': output_texts}\n    else:\n        converted_sample = [\\\n            {\"role\": \"user\", \"content\": examples[\"prompt\"]},\\\n            {\"role\": \"assistant\", \"content\": examples[\"completion\"]},\\\n        ]\n        return {'messages': converted_sample}\n```\n\n```python\ndataset = dataset.rename_column(\"sentence\", \"prompt\")\ndataset = dataset.rename_column(\"translation_extra\", \"completion\")\ndataset = dataset.map(format_dataset)\ndataset = dataset.remove_columns(['prompt', 'completion', 'translation'])\nmessages = dataset[0]['messages']\nmessages\n```\n\n```\n[{'role': 'user',\\\n  'content': 'The birch canoe slid on the smooth planks.'", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Fine-Tuning Your First Large Language Model (LLM) with PyTorch and Hugging Face", "description": "A Blog post by Daniel Voigt Godoy on Hugging Face", "url": "https://huggingface.co/blog/dvgodoy/fine-tuning-llm-hugging-face", "source": "hf", "id": "175018a5-6aac-4c2e-ae24-c43c6217e084"}, "page_content": "[\"completion\"][i]},\\\n            ]\n            output_texts.append(converted_sample)\n        return {'messages': output_texts}\n    else:\n        converted_sample = [\\\n            {\"role\": \"user\", \"content\": examples[\"prompt\"]},\\\n            {\"role\": \"assistant\", \"content\": examples[\"completion\"]},\\\n        ]\n        return {'messages': converted_sample}\n```\n\n```python\ndataset = dataset.rename_column(\"sentence\", \"prompt\")\ndataset = dataset.rename_column(\"translation_extra\", \"completion\")\ndataset = dataset.map(format_dataset)\ndataset = dataset.remove_columns(['prompt', 'completion', 'translation'])\nmessages = dataset[0]['messages']\nmessages\n```\n\n```\n[{'role': 'user',\\\n  'content': 'The birch canoe slid on the smooth planks.'},\\\n {'role': 'assistant',\\\n  'content': 'On the smooth planks, the birch canoe slid. Yes, hrrrm.'}]\n```\n\n### Tokenizer\n\nBefore moving into the actual training, we still need to **load the tokenizer that corresponds to our model**. The tokenizer is an important part of this process, determining how to convert text into tokens in the same way used to train the model.\n\nFor instruction/chat models, the tokenizer also contains its corresponding **chat template** that specifies:\n\n- Which **special tokens** should be used, and where they should be placed.\n- Where the system directives, user prompt, and model response should be placed.\n- What is the **generation prompt**, that is, the special token that triggers the model's response (more on that in the \"Querying the Model\" section)\n\n* * *\n\n**IMPORTANT UPDATE**: due to changes in the default collator used by the `SFTTrainer` class while building the dataset, the EOS token (which is, in Phi-3, the same as the PAD token) was masked in the labels too thus leading to the model not being able to properly stop token generation.\n\nIn order to address this change, we can assign the UNK token to the PAD token, so the EOS token becomes unique and therefore not masked as part of the labels.\n\n* * *\n\n```python\ntokenizer = AutoTokenizer.from_pretrained(repo_id)\ntokenizer.pad_token = tokenizer.unk_token\ntokenizer.pad_token_id = tokenizer.unk_token_id\ntokenizer.chat_template\n```\n\n```\n\"{% for message in messages %}\n    {% if message['role'] ##'system' %}\n      {{'<|system|>\\n' + message['content'] + '<|end|>\\n'}}\n    {% elif message['role'] ## 'user' %}\n      {{'<|user|>\\n' + message['content'] + '<|end|>\\n'}}\n    {% elif message['role'] ## 'assistant' %}\n      {{'<|assistant|>\\n' + message['content'] + '<|end|>\\n'}}\n    {% endif %}\n{% endfor %}\n{% if add_generation_prompt %}\n  {{ '<|assistant|>\\n' }}{% else %}{{ eos_token }}\n{% endif %}\"\n```\n\nNever mind the seemingly overcomplicated template (I have added line breaks and indentation to it so it's easier to read). It simply organizes the messages into a coherent block with the appropriate tags, as shown below (`tokenize=False` ensures we get readable text back instead of a numeric sequence of token IDs):\n\n```python\nprint(tokenizer.apply_chat_template(messages, tokenize=False))\n```\n\n```\n<|user|>\nThe birch canoe slid on the smooth planks.<|end|>\n<|assistant|>\nOn the smooth planks, the birch canoe slid. Yes, hrrrm.<|end|>\n<|endoftext|>\n```\n\nNotice that each interaction is wrapped in either `<|user|>` or `<|assistant|>` tokens at the beginning and `<|end|>` at the end. Moreover, the `<|endoftext|>` token indicates the end of the whole block.\n\nDifferent models will have different templates and tokens to indicate the beginning and end of sentences and blocks.\n\nWe're now ready to tackle the actual fine-tuning!\n\n## Fine-Tuning with SFTTrainer\n\n**Fine-tuning a model**, whether large or otherwise, follows exactly **the same training procedure as training a model from scratch**. We could write our own training loop in pure PyTorch, or we could use Hugging Face's `Trainer` to fine-tune our model.\n\nIt is much easier, however, to use `SFTTrainer` instead (which uses `Trainer` underneath, by the way), since it takes care of most of the nitty-gritty details for us, as long as we provide it with the following four arguments:\n\n- a model\n- a tokenizer\n- a dataset\n- a configuration object\n\nWe've already got the first three elements; let's work on the last one.\n\n### SFTConfig\n\nThere are many parameters that we can set in the configuration object. We have divided them into four groups:\n\n- **Memory usage** optimization parameters related to **gradient accumulation and checkpointing**", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Fine-Tuning Your First Large Language Model (LLM) with PyTorch and Hugging Face", "description": "A Blog post by Daniel Voigt Godoy on Hugging Face", "url": "https://huggingface.co/blog/dvgodoy/fine-tuning-llm-hugging-face", "source": "hf", "id": "655adef1-8b80-4f36-86fe-6ffded595539"}, "page_content": " a model**, whether large or otherwise, follows exactly **the same training procedure as training a model from scratch**. We could write our own training loop in pure PyTorch, or we could use Hugging Face's `Trainer` to fine-tune our model.\n\nIt is much easier, however, to use `SFTTrainer` instead (which uses `Trainer` underneath, by the way), since it takes care of most of the nitty-gritty details for us, as long as we provide it with the following four arguments:\n\n- a model\n- a tokenizer\n- a dataset\n- a configuration object\n\nWe've already got the first three elements; let's work on the last one.\n\n### SFTConfig\n\nThere are many parameters that we can set in the configuration object. We have divided them into four groups:\n\n- **Memory usage** optimization parameters related to **gradient accumulation and checkpointing**\n- **Dataset**-related arguments, such as the `max_seq_length` required by your data, and whether you are packing or not the sequences\n- Typical **training parameters** such as the `learning_rate` and the `num_train_epochs`\n- **Environment and logging** parameters such as `output_dir` (this will be the name of the model if you choose to push it to the Hugging Face Hub once it's trained), `logging_dir`, and `logging_steps`.\n\nWhile the _learning rate_ is a very important parameter (as a starting point, you can try the learning rate used to train the base model in the first place), it's actually the **maximum sequence length** that's more likely to cause **out-of-memory issues**.\n\nMake sure to always pick the shortest possible `max_seq_length` that makes sense for your use case. In ours, the sentences\u2014both in English and Yoda-speak\u2014are quite short, and a sequence of 64 tokens is more than enough to cover the prompt, the completion, and the added special tokens.\n\n> Flash attention (which, unfortunately, isn't supported in Colab), allows for more flexibility in working with longer sequences, avoiding the potential issue of OOM errors.\n\n* * *\n\n**IMPORTANT UPDATE**: The release of `trl` version 0.20 brought several changes to the `SFTConfig`:\n\n- packing is performed differently than it was, unless `packing_strategy='wrapped'` is set;\n- the `max_seq_length` argument was renamed to `max_length`;\n- the `bf16` defaults to `True` but, at the time of this update (Aug/2025), it didn't check if the BF16 type was actually available or not, so it's included in the configuration now.\n\n* * *\n\n```python\nsft_config = SFTConfig(\n    ## GROUP 1: Memory usage\n    # These arguments will squeeze the most out of your GPU's RAM\n    # Checkpointing\n    gradient_checkpointing=True,    # this saves a LOT of memory\n    # Set this to avoid exceptions in newer versions of PyTorch\n    gradient_checkpointing_kwargs={'use_reentrant': False},\n    # Gradient Accumulation / Batch size\n    # Actual batch (for updating) is same (1x) as micro-batch size\n    gradient_accumulation_steps=1,\n    # The initial (micro) batch size to start off with\n    per_device_train_batch_size=16,\n    # If batch size would cause OOM, halves its size until it works\n    auto_find_batch_size=True,\n\n    ## GROUP 2: Dataset-related\n    max_length=64, # renamed in v0.20\n    # Dataset\n    # packing a dataset means no padding is needed\n    packing=True,\n    packing_strategy='wrapped', # added to approximate original packing behavior\n\n    ## GROUP 3: These are typical training parameters\n    num_train_epochs=10,\n    learning_rate=3e-4,\n    # Optimizer\n    # 8-bit Adam optimizer - doesn't help much if you're using LoRA!\n    optim='paged_adamw_8bit',\n\n    ## GROUP 4: Logging parameters\n    logging_steps=10,\n    logging_dir='./logs',\n    output_dir='./phi3-mini-yoda-adapter',\n    report_to='none'.\n\n    # ensures bf16 (the new default) is only used when it is actually available\n    bf16=torch.cuda.is_bf16_supported(including_emulation=False)\n)\n```\n\n### SFTTrainer\n\n> _\"It is training time!\"_\n>\n> The Hulk\n\n* * *\n\n**IMPORTANT UPDATE**: The current version of trl (0.21) has a known issue where training fails if the LoRA configuration has already been applied to the model, as the trainer freezes the whole model, including the adapters.\n\nHowever, it works as expected when the configuration is passed as the `peft_config` argument to the trainer, since it is applied after freezing the existing layers.\n\nIf the model already contains the adapters, as in our case, training still works, but we need to use the underlying original model instead (`model.base_model.", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Fine-Tuning Your First Large Language Model (LLM) with PyTorch and Hugging Face", "description": "A Blog post by Daniel Voigt Godoy on Hugging Face", "url": "https://huggingface.co/blog/dvgodoy/fine-tuning-llm-hugging-face", "source": "hf", "id": "c6df6429-658c-4da8-833d-38bee693e51a"}, "page_content": " bf16 (the new default) is only used when it is actually available\n    bf16=torch.cuda.is_bf16_supported(including_emulation=False)\n)\n```\n\n### SFTTrainer\n\n> _\"It is training time!\"_\n>\n> The Hulk\n\n* * *\n\n**IMPORTANT UPDATE**: The current version of trl (0.21) has a known issue where training fails if the LoRA configuration has already been applied to the model, as the trainer freezes the whole model, including the adapters.\n\nHowever, it works as expected when the configuration is passed as the `peft_config` argument to the trainer, since it is applied after freezing the existing layers.\n\nIf the model already contains the adapters, as in our case, training still works, but we need to use the underlying original model instead (`model.base_model.model`) to ensure the `save_model()` method functions correctly.\n\n* * *\n\nWe can now finally create an instance of the supervised fine-tuning trainer:\n\n```python\ntrainer = SFTTrainer(\n    model=model.base_model.model, # the underlying Phi-3 model\n    peft_config=config,  # added to fix issue in TRL>=0.20\n    processing_class=tokenizer,\n    args=sft_config,\n    train_dataset=dataset,\n)\n```\n\nThe `SFTTrainer` had already preprocessed our dataset, so we can take a look inside and see how each mini-batch was assembled:\n\n```python\ndl = trainer.get_train_dataloader()\nbatch = next(iter(dl))\n```\n\nLet's check the labels; after all, we didn't provide any, did we?\n\n```python\nbatch['input_ids'][0], batch['labels'][0]\n```\n\n```\n(tensor([ 1746, 29892,   278, 10435,  3147,   698,   287, 29889,  32007, 32000, 32000,\\\n  32010, 10987,   278,  3252,   262,  1058,   380,  1772,   278,  282,   799,   29880,\\\n  18873,  1265, 29889, 32007, 32001, 11644,   380,  1772,   278,  282,   799,   29880,\\\n  18873,  1265, 29892,  1284,   278,  3252,   262, 29892,   366,  1818, 29889,   3869,\\\n  29892,   298, 21478,  1758, 29889, 32007, 32000, 32000, 32010,   315,   329,    278,\\\n  13793,   393,  7868, 29879,   278], device='cuda:0'),\n tensor([ 1746, 29892,   278, 10435,  3147,   698,   287, 29889,  32007, 32000, 32000,\\\n  32010, 10987,   278,  3252,   262,  1058,   380,  1772,   278,  282,   799,   29880,\\\n  18873,  1265, 29889, 32007, 32001, 11644,   380,  1772,   278,  282,   799,   29880,\\\n  18873,  1265, 29892,  1284,   278,  3252,   262, 29892,   366,  1818, 29889,   3869,\\\n  29892,   298, 21478,  1758, 29889, 32007, 32000, 32000, 32010,   315,   329,    278,\\\n  13793,   393,  7868, 29879,   278], device='cuda:0'))\n```\n\nThe **labels were added automatically**, and they're **exactly the same as the inputs**. Thus, this is a case of **self-supervised fine-tuning**.\n\nThe shifting of the labels will be handled automatically as well; there's no need to be concerned about it.\n\n> Although this is a 3.8 billion-parameter model, the configuration above allows us to squeeze training, using a mini-batch of eight, into an old setup with a consumer-grade GPU such as a GTX 1060 with only 6 GB RAM. True story!\n>\n>\n> It takes about 35 minutes to complete the training process.\n\nNext, we call the `train()` method and wait:\n\n```python\ntrainer.train()\n```\n\n| Step | Training Loss |\n| --- | --- |\n| 10 | 2.990700 |\n| 20 | 1.789500 |\n| 30 | 1.581700 |\n| 40 | 1.458300 |\n| 50 | 1.362300 |\n| 100 | 0.607900 |\n| 150 | 0.353600 |\n| 200 | 0.277500 |\n| 220 | 0.252400 |\n\n## Querying the Model\n\nNow, our model should be able to produce a Yoda-like sentence as a response to any short sentence we give it.\n\nSo, the model", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Fine-Tuning Your First Large Language Model (LLM) with PyTorch and Hugging Face", "description": "A Blog post by Daniel Voigt Godoy on Hugging Face", "url": "https://huggingface.co/blog/dvgodoy/fine-tuning-llm-hugging-face", "source": "hf", "id": "bd828637-da6f-44e8-9da4-76f0094db021"}, "page_content": "TX 1060 with only 6 GB RAM. True story!\n>\n>\n> It takes about 35 minutes to complete the training process.\n\nNext, we call the `train()` method and wait:\n\n```python\ntrainer.train()\n```\n\n| Step | Training Loss |\n| --- | --- |\n| 10 | 2.990700 |\n| 20 | 1.789500 |\n| 30 | 1.581700 |\n| 40 | 1.458300 |\n| 50 | 1.362300 |\n| 100 | 0.607900 |\n| 150 | 0.353600 |\n| 200 | 0.277500 |\n| 220 | 0.252400 |\n\n## Querying the Model\n\nNow, our model should be able to produce a Yoda-like sentence as a response to any short sentence we give it.\n\nSo, the model requires its inputs to be properly formatted. We need to build a list of \"messages\"\u2014ours, from the `user`, in this case\u2014and prompt the model to answer by indicating it's its turn to write.\n\nThis is the purpose of the `add_generation_prompt` argument: it adds `<|assistant|>` to the end of the conversation, so the model can predict the next word\u2014and continue doing so until it predicts an `<|endoftext|>` token.\n\nThe helper function below assembles a message (in the conversational format) and **applies the chat template** to it, **appending the generation prompt** to its end.\n\n```python\ndef gen_prompt(tokenizer, sentence):\n    converted_sample = [{\"role\": \"user\", \"content\": sentence}]\n    prompt = tokenizer.apply_chat_template(\n        converted_sample, tokenize=False, add_generation_prompt=True\n    )\n    return prompt\n```\n\nLet's try generating a prompt for an example sentence:\n\n```python\nsentence = 'The Force is strong in you!'\nprompt = gen_prompt(tokenizer, sentence)\nprint(prompt)\n```\n\n```\n<|user|>\nThe Force is strong in you!<|end|>\n<|assistant|>\n```\n\nThe prompt seems about right; let's use it to generate a completion. The helper function below does the following:\n\n- It **tokenizes the prompt** into a tensor of token IDs (`add_special_tokens` is set to `False` because the tokens were already added by the chat template).\n- It sets the model to **evaluation mode**.\n- It calls the model's `generate()` method to **produce the output** (generated token IDs).\n  - If the model was trained using mixed-precision, we wrap the generation in the `autocast()` context manager, which automatically handles conversion between data types.\n- It **decodes the generated token IDs** back into readable text.\n\n```python\ndef generate(model, tokenizer, prompt, max_new_tokens=64, skip_special_tokens=False):\n    tokenized_input = tokenizer(\n        prompt, add_special_tokens=False, return_tensors=\"pt\"\n    ).to(model.device)\n\n    model.eval()\n    # if it was trained using mixed precision, uses autocast context\n    ctx = torch.autocast(device_type=model.device.type, dtype=model.dtype) \\\n          if model.dtype in [torch.float16, torch.bfloat16] else nullcontext()\n    with ctx:\n        gen_output = model.generate(**tokenized_input,\n                                    eos_token_id=tokenizer.eos_token_id,\n                                    max_new_tokens=max_new_tokens)\n\n    output = tokenizer.batch_decode(gen_output, skip_special_tokens=skip_special_tokens)\n    return output[0]\n```\n\nNow, we can finally try out our model and see if it's indeed capable of generating Yoda-speak.\n\n```python\nprint(generate(model, tokenizer, prompt))\n```\n\n```\n<|user|> The Force is strong in you!<|end|><|assistant|> Strong in you, the Force is. Yes, hrrmmm.<|end|>\n```\n\nAwesome! It works! Like Yoda, the model speaks. Hrrrmm.\n\n**Congratulations, you've fine-tuned your first LLM!**\n\nNow, you've got a small adapter that can be loaded into an instance of the Phi-3 Mini 4K Instruct model to turn it into a Yoda translator! How cool is that?\n\n## Saving the Adapter\n\nOnce the training is completed, you can save the adapter (and the tokenizer) to disk by calling the trainer's `save_model()` method. It will save everything to the specified folder:\n\n```python\ntrainer.save_model('local-phi3-mini-yoda-adapter')\n```\n\nThe files that were saved include:\n\n- the adapter configuration (`adapter_config.json`) and weights (`adapter_model.safetensors`)\u2014the adapter itself is just 50 MB in size\n- the training arguments (`training_args.bin`)\n- the tokenizer (`tokenizer.json` and `tokenizer.model`), its configuration (`tokenizer_config", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Fine-Tuning Your First Large Language Model (LLM) with PyTorch and Hugging Face", "description": "A Blog post by Daniel Voigt Godoy on Hugging Face", "url": "https://huggingface.co/blog/dvgodoy/fine-tuning-llm-hugging-face", "source": "hf", "id": "151b4a25-d52c-4c4e-a53c-acb8cd49c0ab"}, "page_content": "\nNow, you've got a small adapter that can be loaded into an instance of the Phi-3 Mini 4K Instruct model to turn it into a Yoda translator! How cool is that?\n\n## Saving the Adapter\n\nOnce the training is completed, you can save the adapter (and the tokenizer) to disk by calling the trainer's `save_model()` method. It will save everything to the specified folder:\n\n```python\ntrainer.save_model('local-phi3-mini-yoda-adapter')\n```\n\nThe files that were saved include:\n\n- the adapter configuration (`adapter_config.json`) and weights (`adapter_model.safetensors`)\u2014the adapter itself is just 50 MB in size\n- the training arguments (`training_args.bin`)\n- the tokenizer (`tokenizer.json` and `tokenizer.model`), its configuration (`tokenizer_config.json`), and its special tokens (`added_tokens.json` and `speciak_tokens_map.json`)\n- a README file\n\nIf you'd like to share your adapter with everyone, you can also push it to the Hugging Face Hub. First, log in using a token that has permission to write:\n\n```python\nfrom huggingface_hub import login\nlogin()\n```\n\nThe code above will ask you to enter an access token:\n\n[![image/png](https://cdn-uploads.huggingface.co/production/uploads/63407b3179f2908105f7b595/lFrSZoVfn7XubBEAGrZtG.png)](https://cdn-uploads.huggingface.co/production/uploads/63407b3179f2908105f7b595/lFrSZoVfn7XubBEAGrZtG.png)\n\nA successful login should look like this (pay attention to the permissions):\n\n[![image/png](https://cdn-uploads.huggingface.co/production/uploads/63407b3179f2908105f7b595/0c6K-V0CgvZEE82CyDE8L.png)](https://cdn-uploads.huggingface.co/production/uploads/63407b3179f2908105f7b595/0c6K-V0CgvZEE82CyDE8L.png)\n\nThen, you can use the trainer's `push_to_hub()` method to upload everything to your account in the Hub. The model will be named after the `output_dir` argument of the training arguments:\n\n```python\ntrainer.push_to_hub()\n```\n\nThere you go! Our model is out there in the world, and anyone can use it to translate English into Yoda speak.\n\nThat's a wrap!\n\n**Did you like this post? You can learn much more about fine-tuning in my latest book: [_A Hands-On Guide to Fine-Tuning Large Language Models with PyTorch and Hugging Face_](https://pytorchstepbystep.com/llms).**\n\n[![](https://cdn-uploads.huggingface.co/production/uploads/63407b3179f2908105f7b595/dbXdN-6ACXvcBCsqdWS2C.png)](https://pytorchstepbystep.com/llms)\n\n## Subscribe Follow Connect\n\n- [dvgodoy.com](https://dvgodoy.com/)\n- [X](https://x.com/dvgodoy)\n- [GitHub](https://github.com/dvgodoy)\n- [LinkedIn](https://www.linkedin.com/in/dvgodoy)[SEP]", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Training and Finetuning Embedding Models with Sentence Transformers v3", "description": "Hugging Face Blog post titled  Training and Finetuning Embedding Models with Sentence Transformers v3", "url": "https://huggingface.co/blog/train-sentence-transformers", "source": "hf", "id": "9751c145-6a9a-4d9b-b3e0-73389fa92cac"}, "page_content": "[CLS]Training and Finetuning Embedding Models with Sentence Transformers v3\n\n[Sentence Transformers](https://sbert.net/) is a Python library for using and training embedding models for a wide range of applications, such as retrieval augmented generation, semantic search, semantic textual similarity, paraphrase mining, and more. Its v3.0 update is the largest since the project's inception, introducing a new training approach. In this blogpost, I'll show you how to use it to finetune Sentence Transformer models to improve their performance on specific tasks. You can also use this method to train new Sentence Transformer models from scratch.\n\nFinetuning Sentence Transformers now involves several components, including datasets, loss functions, training arguments, evaluators, and the new trainer itself. I'll go through each of these components in detail and provide examples of how to use them to train effective models.\n\n## Table of Contents\n\n- [Why Finetune?](https://huggingface.co/blog/train-sentence-transformers#why-finetune)\n- [Training Components](https://huggingface.co/blog/train-sentence-transformers#training-components)\n- [Dataset](https://huggingface.co/blog/train-sentence-transformers#dataset)\n  - [Data on Hugging Face Hub](https://huggingface.co/blog/train-sentence-transformers#data-on-hugging-face-hub)\n  - [Local Data (CSV, JSON, Parquet, Arrow, SQL)](https://huggingface.co/blog/train-sentence-transformers#local-data-csv-json-parquet-arrow-sql)\n  - [Local Data that requires pre-processing](https://huggingface.co/blog/train-sentence-transformers#local-data-that-requires-pre-processing)\n  - [Dataset Format](https://huggingface.co/blog/train-sentence-transformers#dataset-format)\n- [Loss Function](https://huggingface.co/blog/train-sentence-transformers#loss-function)\n- [Training Arguments](https://huggingface.co/blog/train-sentence-transformers#training-arguments)\n- [Evaluator](https://huggingface.co/blog/train-sentence-transformers#evaluator)\n  - [EmbeddingSimilarityEvaluator with STSb](https://huggingface.co/blog/train-sentence-transformers#embeddingsimilarityevaluator-with-stsb)\n  - [TripletEvaluator with AllNLI](https://huggingface.co/blog/train-sentence-transformers#tripletevaluator-with-allnli)\n- [Trainer](https://huggingface.co/blog/train-sentence-transformers#trainer)\n  - [Callbacks](https://huggingface.co/blog/train-sentence-transformers#callbacks)\n- [Multi-Dataset Training](https://huggingface.co/blog/train-sentence-transformers#multi-dataset-training)\n- [Deprecation](https://huggingface.co/blog/train-sentence-transformers#deprecation)\n- [Additional Resources](https://huggingface.co/blog/train-sentence-transformers#additional-resources)\n  - [Training Examples](https://huggingface.co/blog/train-sentence-transformers#training-examples)\n  - [Documentation](https://huggingface.co/blog/train-sentence-transformers#documentation)\n\n## Why Finetune?\n\nFinetuning Sentence Transformer models can significantly enhance their performance on specific tasks. This is because each task requires a unique notion of similarity. Let's consider a couple of news article headlines as an example:\n\n- \"Apple launches the new iPad\"\n- \"NVIDIA is gearing up for the next GPU generation\"\n\nDepending on the use case, we might want similar or dissimilar embeddings for these texts. For instance, a classification model for news articles could treat these texts as similar since they both belong to the Technology category. On the other hand, a semantic textual similarity or retrieval model should consider them dissimilar due to their distinct meanings.\n\n## Training Components\n\nTraining Sentence Transformer models involves the following components:\n\n1. [**Dataset**](https://huggingface.co/blog/train-sentence-transformers#dataset): The data used for training and evaluation.\n2. [**Loss Function**](https://huggingface.co/blog/train-sentence-transformers#loss-function): A function that quantifies the model's performance and guides the optimization process.\n3. [**Training Arguments**](https://huggingface.co/blog/train-sentence-transformers#training-arguments) (optional): Parameters that influence training performance and tracking/debugging.\n4. [**Evaluator**](https://huggingface.co/blog/train-sentence-transformers#evaluator) (optional): A tool for evaluating the model before, during, or after training.\n5. [**Trainer**](https://huggingface.co/blog/train-sentence-transformers#trainer): Brings", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Training and Finetuning Embedding Models with Sentence Transformers v3", "description": "Hugging Face Blog post titled  Training and Finetuning Embedding Models with Sentence Transformers v3", "url": "https://huggingface.co/blog/train-sentence-transformers", "source": "hf", "id": "4240908a-1f73-44ba-b619-88183a60c113"}, "page_content": "aset**](https://huggingface.co/blog/train-sentence-transformers#dataset): The data used for training and evaluation.\n2. [**Loss Function**](https://huggingface.co/blog/train-sentence-transformers#loss-function): A function that quantifies the model's performance and guides the optimization process.\n3. [**Training Arguments**](https://huggingface.co/blog/train-sentence-transformers#training-arguments) (optional): Parameters that influence training performance and tracking/debugging.\n4. [**Evaluator**](https://huggingface.co/blog/train-sentence-transformers#evaluator) (optional): A tool for evaluating the model before, during, or after training.\n5. [**Trainer**](https://huggingface.co/blog/train-sentence-transformers#trainer): Brings together the model, dataset, loss function, and other components for training.\n\nNow, let's dive into each of these components in more detail.\n\n## Dataset\n\nThe [`SentenceTransformerTrainer`](https://sbert.net/docs/package_reference/sentence_transformer/SentenceTransformer.html#sentence_transformers.SentenceTransformer) uses [`datasets.Dataset`](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset) or [`datasets.DatasetDict`](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.DatasetDict) instances for training and evaluation. You can load data from the Hugging Face Datasets Hub or use local data in various formats such as CSV, JSON, Parquet, Arrow, or SQL.\n\nNote: Many Hugging Face datasets that work out of the box with Sentence Transformers have been tagged with `sentence-transformers`, allowing you to easily find them by browsing to [https://huggingface.co/datasets?other=sentence-transformers](https://huggingface.co/datasets?other=sentence-transformers). We strongly recommend that you browse these datasets to find training datasets that might be useful for your tasks.\n\n### Data on Hugging Face Hub\n\nTo load data from datasets in the Hugging Face Hub, use the [`load_dataset`](https://huggingface.co/docs/datasets/main/en/package_reference/loading_methods#datasets.load_dataset) function:\n\n```python\nfrom datasets import load_dataset\n\ntrain_dataset = load_dataset(\"sentence-transformers/all-nli\", \"pair-class\", split=\"train\")\neval_dataset = load_dataset(\"sentence-transformers/all-nli\", \"pair-class\", split=\"dev\")\n\nprint(train_dataset)\n\"\"\"\nDataset({\n    features: ['premise', 'hypothesis', 'label'],\n    num_rows: 942069\n})\n\"\"\"\n```\n\nSome datasets, like [`sentence-transformers/all-nli`](https://huggingface.co/datasets/sentence-transformers/all-nli), have multiple subsets with different data formats. You need to specify the subset name along with the dataset name.\n\n### Local Data (CSV, JSON, Parquet, Arrow, SQL)\n\nIf you have local data in common file formats, you can easily load it using [`load_dataset`](https://huggingface.co/docs/datasets/main/en/package_reference/loading_methods#datasets.load_dataset) too:\n\n```python\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"csv\", data_files=\"my_file.csv\")\n# or\ndataset = load_dataset(\"json\", data_files=\"my_file.json\")\n```\n\n### Local Data that requires pre-processing\n\nIf your local data requires pre-processing, you can use [`datasets.Dataset.from_dict`](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.from_dict) to initialize your dataset with a dictionary of lists:\n\n```python\nfrom datasets import Dataset\n\nanchors = []\npositives = []\n# Open a file, perform preprocessing, filtering, cleaning, etc.\n# and append to the lists\n\ndataset = Dataset.from_dict({\n    \"anchor\": anchors,\n    \"positive\": positives,\n})\n```\n\nEach key in the dictionary becomes a column in the resulting dataset.\n\n### Dataset Format\n\nIt's crucial to ensure that your dataset format matches your chosen [loss function](https://huggingface.co/blog/train-sentence-transformers#loss-function). This involves checking two things:\n\n1. If your loss function requires a _Label_ (as indicated in the [Loss Overview](https://sbert.net/docs/sentence_transformer/loss_overview.html) table), your dataset must have a column named **\"label\"** or **\"score\"**.\n2. All columns other than **\"label\"** or **\"score\"** are considered _Inputs_ (as indicated in the [Loss Overview](https://sbert.net", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Training and Finetuning Embedding Models with Sentence Transformers v3", "description": "Hugging Face Blog post titled  Training and Finetuning Embedding Models with Sentence Transformers v3", "url": "https://huggingface.co/blog/train-sentence-transformers", "source": "hf", "id": "97caf57e-f1c8-41d7-8736-7436f6602c48"}, "page_content": " to the lists\n\ndataset = Dataset.from_dict({\n    \"anchor\": anchors,\n    \"positive\": positives,\n})\n```\n\nEach key in the dictionary becomes a column in the resulting dataset.\n\n### Dataset Format\n\nIt's crucial to ensure that your dataset format matches your chosen [loss function](https://huggingface.co/blog/train-sentence-transformers#loss-function). This involves checking two things:\n\n1. If your loss function requires a _Label_ (as indicated in the [Loss Overview](https://sbert.net/docs/sentence_transformer/loss_overview.html) table), your dataset must have a column named **\"label\"** or **\"score\"**.\n2. All columns other than **\"label\"** or **\"score\"** are considered _Inputs_ (as indicated in the [Loss Overview](https://sbert.net/docs/sentence_transformer/loss_overview.html) table). The number of these columns must match the number of valid inputs for your chosen loss function. The names of the columns don't matter, **only their order matters**.\n\nFor example, if your loss function accepts `(anchor, positive, negative) triplets`, then your first, second, and third dataset columns correspond with `anchor`, `positive`, and `negative`, respectively. This means that your first and second column must contain texts that should embed closely, and that your first and third column must contain texts that should embed far apart. That is why depending on your loss function, your dataset column order matters.\n\nConsider a dataset with columns `[\"text1\", \"text2\", \"label\"]`, where the `\"label\"` column contains floating point similarity scores. This dataset can be used with `CoSENTLoss`, `AnglELoss`, and `CosineSimilarityLoss` because:\n\n1. The dataset has a \"label\" column, which is required by these loss functions.\n2. The dataset has 2 non-label columns, matching the number of inputs required by these loss functions.\n\nIf the columns in your dataset are not ordered correctly, use [`Dataset.select_columns`](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.select_columns) to reorder them. Additionally, remove any extraneous columns (e.g., `sample_id`, `metadata`, `source`, `type`) using [`Dataset.remove_columns`](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.remove_columns), as they will be treated as inputs otherwise.\n\n## Loss Function\n\nLoss functions measure how well a model performs on a given batch of data and guide the optimization process. The choice of loss function depends on your available data and target task. Refer to the [Loss Overview](https://sbert.net/docs/sentence_transformer/loss_overview.html) for a comprehensive list of options.\n\nMost loss functions can be initialized with just the `SentenceTransformer``model` that you're training:\n\n```python\nfrom datasets import load_dataset\nfrom sentence_transformers import SentenceTransformer\nfrom sentence_transformers.losses import CoSENTLoss\n\n# Load a model to train/finetune\nmodel = SentenceTransformer(\"FacebookAI/xlm-roberta-base\")\n\n# Initialize the CoSENTLoss\n# This loss requires pairs of text and a floating point similarity score as a label\nloss = CoSENTLoss(model)\n\n# Load an example training dataset that works with our loss function:\ntrain_dataset = load_dataset(\"sentence-transformers/all-nli\", \"pair-score\", split=\"train\")\n\"\"\"\nDataset({\n    features: ['sentence1','sentence2', 'label'],\n    num_rows: 942069\n})\n\"\"\"\n```\n\n## Training Arguments\n\nThe [`SentenceTransformersTrainingArguments`](https://sbert.net/docs/package_reference/sentence_transformer/training_args.html#sentencetransformertrainingarguments) class allows you to specify parameters that influence training performance and tracking/debugging. While optional, experimenting with these arguments can help improve training efficiency and provide insights into the training process.\n\nIn the Sentence Transformers documentation, I've outlined some of the most useful training arguments. I would recommend reading it in [Training Overview > Training Arguments](https://sbert.net/docs/sentence_transformer/training_overview.html#training-arguments).\n\nHere's an example of how to initialize [`SentenceTransformersTrainingArguments`](https://sbert.net/docs/package_reference/sentence_transformer/training_args.html#sentencetransformertrainingarguments):\n\n```python\nfrom sentence_transformers.training_args import SentenceTransformerTrainingArguments\n\nargs = SentenceTransformerTrainingArguments(\n    # Required parameter:\n    output_dir=\"models/mpnet-base-all-nli-triplet\",\n    # Optional training parameters:\n    num_train_epochs=1,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    warmup_ratio=0.1,\n    fp16=True,  # Set to", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Training and Finetuning Embedding Models with Sentence Transformers v3", "description": "Hugging Face Blog post titled  Training and Finetuning Embedding Models with Sentence Transformers v3", "url": "https://huggingface.co/blog/train-sentence-transformers", "source": "hf", "id": "0ada8313-dd3f-4d4a-bff6-4661fd7a3e23"}, "page_content": ".net/docs/sentence_transformer/training_overview.html#training-arguments).\n\nHere's an example of how to initialize [`SentenceTransformersTrainingArguments`](https://sbert.net/docs/package_reference/sentence_transformer/training_args.html#sentencetransformertrainingarguments):\n\n```python\nfrom sentence_transformers.training_args import SentenceTransformerTrainingArguments\n\nargs = SentenceTransformerTrainingArguments(\n    # Required parameter:\n    output_dir=\"models/mpnet-base-all-nli-triplet\",\n    # Optional training parameters:\n    num_train_epochs=1,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    warmup_ratio=0.1,\n    fp16=True,  # Set to False if your GPU can't handle FP16\n    bf16=False,  # Set to True if your GPU supports BF16\n    batch_sampler=BatchSamplers.NO_DUPLICATES,  # Losses using \"in-batch negatives\" benefit from no duplicates\n    # Optional tracking/debugging parameters:\n    eval_strategy=\"steps\",\n    eval_steps=100,\n    save_strategy=\"steps\",\n    save_steps=100,\n    save_total_limit=2,\n    logging_steps=100,\n    run_name=\"mpnet-base-all-nli-triplet\",  # Used in W&B if `wandb` is installed\n)\n```\n\nNote that `eval_strategy` was introduced in `transformers` version `4.41.0`. Prior versions should use `evaluation_strategy` instead.\n\n## Evaluator\n\nYou can provide the [`SentenceTransformerTrainer`](https://sbert.net/docs/package_reference/sentence_transformer/SentenceTransformer.html#sentence_transformers.SentenceTransformer) with an `eval_dataset` to get the evaluation loss during training, but it may be useful to get more concrete metrics during training, too. For this, you can use evaluators to assess the model's performance with useful metrics before, during, or after training. You can both an `eval_dataset` and an evaluator, one or the other, or neither. They evaluate based on the `eval_strategy` and `eval_steps` [Training Arguments](https://huggingface.co/blog/train-sentence-transformers#training-arguments).\n\nHere are the implemented Evaluators that come with Sentence Transformers:\n\n| Evaluator | Required Data |\n| --- | --- |\n| [`BinaryClassificationEvaluator`](https://sbert.net/docs/package_reference/sentence_transformer/evaluation.html#binaryclassificationevaluator) | Pairs with class labels |\n| [`EmbeddingSimilarityEvaluator`](https://sbert.net/docs/package_reference/sentence_transformer/evaluation.html#embeddingsimilarityevaluator) | Pairs with similarity scores |\n| [`InformationRetrievalEvaluator`](https://sbert.net/docs/package_reference/sentence_transformer/evaluation.html#informationretrievalevaluator) | Queries (qid => question), Corpus (cid => document), and relevant documents (qid => set\\[cid\\]) |\n| [`MSEEvaluator`](https://sbert.net/docs/package_reference/sentence_transformer/evaluation.html#mseevaluator) | Source sentences to embed with a teacher model and target sentences to embed with the student model. Can be the same texts. |\n| [`ParaphraseMiningEvaluator`](https://sbert.net/docs/package_reference/sentence_transformer/evaluation.html#paraphraseminingevaluator) | Mapping of IDs to sentences & pairs with IDs of duplicate sentences. |\n| [`RerankingEvaluator`](https://sbert.net/docs/package_reference/sentence_transformer/evaluation.html#rerankingevaluator) | List of {'query': '..', 'positive': \\[...\\], 'negative': \\[...\\]} dictionaries. |\n| [`TranslationEvaluator`](https://sbert.net/docs/package_reference/sentence_transformer/evaluation.html#translationevaluator) | Pairs of sentences in two separate languages. |\n| [`TripletEvaluator`](https://sbert.net/docs/package_reference/sentence_transformer/evaluation.html#tripletevaluator) | (anchor, positive, negative) pairs. |\n\nAdditionally, you can use [`SequentialEvaluator`](https://sbert.net/docs/package_reference/sentence_transformer/evaluation.html#sequentialevaluator) to combine multiple evaluators into one, which can then be passed to the [`SentenceTransformerTrainer`](https://sbert.net/docs/package_reference/sentence_transformer/SentenceTransformer.html#sentence_transformers.SentenceTransformer).\n\nIf you don't have the necessary evaluation data but still want to track the model's performance on common benchmarks, you can", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Training and Finetuning Embedding Models with Sentence Transformers v3", "description": "Hugging Face Blog post titled  Training and Finetuning Embedding Models with Sentence Transformers v3", "url": "https://huggingface.co/blog/train-sentence-transformers", "source": "hf", "id": "ee2af045-6c0c-4353-872d-24ca85b01e59"}, "page_content": "evaluation.html#translationevaluator) | Pairs of sentences in two separate languages. |\n| [`TripletEvaluator`](https://sbert.net/docs/package_reference/sentence_transformer/evaluation.html#tripletevaluator) | (anchor, positive, negative) pairs. |\n\nAdditionally, you can use [`SequentialEvaluator`](https://sbert.net/docs/package_reference/sentence_transformer/evaluation.html#sequentialevaluator) to combine multiple evaluators into one, which can then be passed to the [`SentenceTransformerTrainer`](https://sbert.net/docs/package_reference/sentence_transformer/SentenceTransformer.html#sentence_transformers.SentenceTransformer).\n\nIf you don't have the necessary evaluation data but still want to track the model's performance on common benchmarks, you can use these evaluators with data from Hugging Face:\n\n### EmbeddingSimilarityEvaluator with STSb\n\nThe STS Benchmark (a.k.a. STSb) is a commonly used benchmarking dataset to measure the model's understanding of semantic textual similarity of short texts like \"A man is feeding a mouse to a snake.\".\n\nFeel free to browse the [sentence-transformers/stsb](https://huggingface.co/datasets/sentence-transformers/stsb) dataset on Hugging Face.\n\n```python\nfrom datasets import load_dataset\nfrom sentence_transformers.evaluation import EmbeddingSimilarityEvaluator, SimilarityFunction\n\n# Load the STSB dataset\neval_dataset = load_dataset(\"sentence-transformers/stsb\", split=\"validation\")\n\n# Initialize the evaluator\ndev_evaluator = EmbeddingSimilarityEvaluator(\n    sentences1=eval_dataset[\"sentence1\"],\n    sentences2=eval_dataset[\"sentence2\"],\n    scores=eval_dataset[\"score\"],\n    main_similarity=SimilarityFunction.COSINE,\n    name=\"sts-dev\",\n)\n# Run evaluation manually:\n# print(dev_evaluator(model))\n\n# Later, you can provide this evaluator to the trainer to get results during training\n```\n\n### TripletEvaluator with AllNLI\n\nAllNLI is a concatenation of the [SNLI](https://huggingface.co/datasets/stanfordnlp/snli) and [MultiNLI](https://huggingface.co/datasets/nyu-mll/multi_nli) datasets, both of which are datasets for Natural Language Inference. This task is traditionally for determining whether two texts are an entailment, contradiction, or neither. It has since been adopted for training embedding models, as the entailing and contradictory sentences make for useful `(anchor, positive, negative)` triplets: a common format for training embedding models.\n\nIn this snippet, it is used to evaluate how frequently the model considers the anchor text and the entailing text to be more similar than the anchor text and the contradictory text. An example text is \"An older man is drinking orange juice at a restaurant.\".\n\nFeel free to browse the [sentence-transformers/all-nli](https://huggingface.co/datasets/sentence-transformers/all-nli) dataset on Hugging Face.\n\n```python\nfrom datasets import load_dataset\nfrom sentence_transformers.evaluation import TripletEvaluator, SimilarityFunction\n\n# Load triplets from the AllNLI dataset\nmax_samples = 1000\neval_dataset = load_dataset(\"sentence-transformers/all-nli\", \"triplet\", split=f\"dev[:{max_samples}]\")\n\n# Initialize the evaluator\ndev_evaluator = TripletEvaluator(\n    anchors=eval_dataset[\"anchor\"],\n    positives=eval_dataset[\"positive\"],\n    negatives=eval_dataset[\"negative\"],\n    main_distance_function=SimilarityFunction.COSINE,\n    name=f\"all-nli-{max_samples}-dev\",\n)\n# Run evaluation manually:\n# print(dev_evaluator(model))\n\n# Later, you can provide this evaluator to the trainer to get results during training\n```\n\n## Trainer\n\nThe [`SentenceTransformerTrainer`](https://sbert.net/docs/package_reference/sentence_transformer/SentenceTransformer.html#sentence_transformers.SentenceTransformer) brings together the model, dataset, loss function, and other components for training:\n\n```python\nfrom datasets import load_dataset\nfrom sentence_transformers import (\n    SentenceTransformer,\n    SentenceTransformerTrainer,\n    SentenceTransformerTrainingArguments,\n    SentenceTransformerModelCardData,\n)\nfrom sentence_transformers.losses import MultipleNegativesRankingLoss\nfrom sentence_transformers.training_args import BatchSamplers\nfrom sentence_transformers.evaluation import TripletEvaluator\n\n# 1. Load a model to finetune with 2. (Optional) model card data\nmodel = SentenceTransformer(\n    \"microsoft/mpnet-base\",\n    model_card_data=SentenceTransformerModelCardData(\n        language=\"en\",\n        license=\"apache-", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Training and Finetuning Embedding Models with Sentence Transformers v3", "description": "Hugging Face Blog post titled  Training and Finetuning Embedding Models with Sentence Transformers v3", "url": "https://huggingface.co/blog/train-sentence-transformers", "source": "hf", "id": "3def1580-3218-478a-a468-19d93d3001de"}, "page_content": "former.html#sentence_transformers.SentenceTransformer) brings together the model, dataset, loss function, and other components for training:\n\n```python\nfrom datasets import load_dataset\nfrom sentence_transformers import (\n    SentenceTransformer,\n    SentenceTransformerTrainer,\n    SentenceTransformerTrainingArguments,\n    SentenceTransformerModelCardData,\n)\nfrom sentence_transformers.losses import MultipleNegativesRankingLoss\nfrom sentence_transformers.training_args import BatchSamplers\nfrom sentence_transformers.evaluation import TripletEvaluator\n\n# 1. Load a model to finetune with 2. (Optional) model card data\nmodel = SentenceTransformer(\n    \"microsoft/mpnet-base\",\n    model_card_data=SentenceTransformerModelCardData(\n        language=\"en\",\n        license=\"apache-2.0\",\n        model_name=\"MPNet base trained on AllNLI triplets\",\n    )\n)\n\n# 3. Load a dataset to finetune on\ndataset = load_dataset(\"sentence-transformers/all-nli\", \"triplet\")\ntrain_dataset = dataset[\"train\"].select(range(100_000))\neval_dataset = dataset[\"dev\"]\ntest_dataset = dataset[\"test\"]\n\n# 4. Define a loss function\nloss = MultipleNegativesRankingLoss(model)\n\n# 5. (Optional) Specify training arguments\nargs = SentenceTransformerTrainingArguments(\n    # Required parameter:\n    output_dir=\"models/mpnet-base-all-nli-triplet\",\n    # Optional training parameters:\n    num_train_epochs=1,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    warmup_ratio=0.1,\n    fp16=True,  # Set to False if GPU can't handle FP16\n    bf16=False,  # Set to True if GPU supports BF16\n    batch_sampler=BatchSamplers.NO_DUPLICATES,  # MultipleNegativesRankingLoss benefits from no duplicates\n    # Optional tracking/debugging parameters:\n    eval_strategy=\"steps\",\n    eval_steps=100,\n    save_strategy=\"steps\",\n    save_steps=100,\n    save_total_limit=2,\n    logging_steps=100,\n    run_name=\"mpnet-base-all-nli-triplet\",  # Used in W&B if `wandb` is installed\n)\n\n# 6. (Optional) Create an evaluator & evaluate the base model\ndev_evaluator = TripletEvaluator(\n    anchors=eval_dataset[\"anchor\"],\n    positives=eval_dataset[\"positive\"],\n    negatives=eval_dataset[\"negative\"],\n    name=\"all-nli-dev\",\n)\ndev_evaluator(model)\n\n# 7. Create a trainer & train\ntrainer = SentenceTransformerTrainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    loss=loss,\n    evaluator=dev_evaluator,\n)\ntrainer.train()\n\n# (Optional) Evaluate the trained model on the test set, after training completes\ntest_evaluator = TripletEvaluator(\n    anchors=test_dataset[\"anchor\"],\n    positives=test_dataset[\"positive\"],\n    negatives=test_dataset[\"negative\"],\n    name=\"all-nli-test\",\n)\ntest_evaluator(model)\n\n# 8. Save the trained model\nmodel.save_pretrained(\"models/mpnet-base-all-nli-triplet/final\")\n\n# 9. (Optional) Push it to the Hugging Face Hub\nmodel.push_to_hub(\"mpnet-base-all-nli-triplet\")\n```\n\nIn this example I'm finetuning from [`microsoft/mpnet-base`](https://huggingface.co/microsoft/mpnet-base), a base model that is not yet a Sentence Transformer model. This requires more training data than finetuning an existing Sentence Transformer model, like [`all-mpnet-base-v2`](https://huggingface.co/sentence-transformers/all-mpnet-base-v2).\n\nAfter running this script, the [tomaarsen/mpnet-base-all-nli-triplet](https://huggingface.co/tomaarsen/mpnet-base-all-nli-triplet) model was uploaded for me. The triplet accuracy using cosine similarity, i.e. what percentage of the time `cosine_similarity(anchor, positive) > cosine_similarity(anchor, negative)` is 90.04% for the development set and 91.5% for the testing set! For reference, the [`microsoft/mpnet-base`](https://huggingface.co/microsoft/mpnet-base) model scored only 68.32% on the dev set before training.\n\nAll of this information is stored in the automatically generated model card, including the base model, language", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Training and Finetuning Embedding Models with Sentence Transformers v3", "description": "Hugging Face Blog post titled  Training and Finetuning Embedding Models with Sentence Transformers v3", "url": "https://huggingface.co/blog/train-sentence-transformers", "source": "hf", "id": "20516574-6700-4e2d-b096-86bc1167aed1"}, "page_content": "co/sentence-transformers/all-mpnet-base-v2).\n\nAfter running this script, the [tomaarsen/mpnet-base-all-nli-triplet](https://huggingface.co/tomaarsen/mpnet-base-all-nli-triplet) model was uploaded for me. The triplet accuracy using cosine similarity, i.e. what percentage of the time `cosine_similarity(anchor, positive) > cosine_similarity(anchor, negative)` is 90.04% for the development set and 91.5% for the testing set! For reference, the [`microsoft/mpnet-base`](https://huggingface.co/microsoft/mpnet-base) model scored only 68.32% on the dev set before training.\n\nAll of this information is stored in the automatically generated model card, including the base model, language, license, evaluation results, training & evaluation dataset info, hyperparameters, training logs, and more. Without any effort, your uploaded models should contain all the information that your potential users would need to determine whether your model is suitable for them.\n\n### Callbacks\n\nThe Sentence Transformers trainer supports various [`transformers.TrainerCallback`](https://huggingface.co/docs/transformers/main_classes/callback#transformers.TrainerCallback) subclasses, including:\n\n- [`WandbCallback`](https://huggingface.co/docs/transformers/en/main_classes/callback#transformers.integrations.WandbCallback) for logging training metrics to W&B if `wandb` is installed\n- [`TensorBoardCallback`](https://huggingface.co/docs/transformers/en/main_classes/callback#transformers.integrations.TensorBoardCallback) for logging training metrics to TensorBoard if `tensorboard` is accessible\n- [`CodeCarbonCallback`](https://huggingface.co/docs/transformers/en/main_classes/callback#transformers.integrations.CodeCarbonCallback) for tracking carbon emissions during training if `codecarbon` is installed\n\nThese are automatically used without you having to specify anything, as long as the required dependency is installed.\n\nRefer to the [Transformers Callbacks documentation](https://huggingface.co/docs/transformers/en/main_classes/callback) for more information on these callbacks and how to create your own.\n\n## Multi-Dataset Training\n\nTop-performing models are often trained using multiple datasets simultaneously. The [`SentenceTransformerTrainer`](https://sbert.net/docs/package_reference/sentence_transformer/SentenceTransformer.html#sentence_transformers.SentenceTransformer) simplifies this process by allowing you to train with multiple datasets without converting them to the same format. You can even apply different loss functions to each dataset. Here are the steps for multi-dataset training:\n\n1. Use a dictionary of [`datasets.Dataset`](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset) instances (or a [`datasets.DatasetDict`](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.DatasetDict)) as the `train_dataset` and `eval_dataset`.\n2. (Optional) Use a dictionary of loss functions mapping dataset names to losses if you want to use different losses for different datasets.\n\nEach training/evaluation batch will contain samples from only one of the datasets. The order in which batches are sampled from the multiple datasets is determined by the [`MultiDatasetBatchSamplers`](https://sbert.net/docs/package_reference/sentence_transformer/training_args.html#sentence_transformers.training_args.MultiDatasetBatchSamplers) enum, which can be passed to the [`SentenceTransformersTrainingArguments`](https://sbert.net/docs/package_reference/sentence_transformer/training_args.html#sentencetransformertrainingarguments) via `multi_dataset_batch_sampler`. The valid options are:\n\n- `MultiDatasetBatchSamplers.ROUND_ROBIN`: Samples from each dataset in a round-robin fashion until one is exhausted. This strategy may not use all samples from each dataset, but it ensures equal sampling from each dataset.\n- `MultiDatasetBatchSamplers.PROPORTIONAL` (default): Samples from each dataset proportionally to its size. This strategy ensures that all samples from each dataset are used, and larger datasets are sampled from more frequently.\n\nMulti-task training has proven to be highly effective. For instance, [Huang et al. 2024](https://arxiv.org/pdf/2405.06932) employed [`MultipleNegativesRankingLoss`](https://sbert.net/docs/package_reference/sentence_transformer/losses.html#multiplenegativesrankingloss), [`CoSENTLoss`](https://sbert.net/docs/package_reference/sentence_transformer/losses.html#cosentloss), and a variation of [`MultipleNegativesRankingLoss`](https://sbert.net/docs/package_reference/sentence_transformer/losses.html#multiplenegativesrankingloss", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Training and Finetuning Embedding Models with Sentence Transformers v3", "description": "Hugging Face Blog post titled  Training and Finetuning Embedding Models with Sentence Transformers v3", "url": "https://huggingface.co/blog/train-sentence-transformers", "source": "hf", "id": "cf715bc0-0813-4fe3-81cc-6e925eb40058"}, "page_content": "Samplers.PROPORTIONAL` (default): Samples from each dataset proportionally to its size. This strategy ensures that all samples from each dataset are used, and larger datasets are sampled from more frequently.\n\nMulti-task training has proven to be highly effective. For instance, [Huang et al. 2024](https://arxiv.org/pdf/2405.06932) employed [`MultipleNegativesRankingLoss`](https://sbert.net/docs/package_reference/sentence_transformer/losses.html#multiplenegativesrankingloss), [`CoSENTLoss`](https://sbert.net/docs/package_reference/sentence_transformer/losses.html#cosentloss), and a variation of [`MultipleNegativesRankingLoss`](https://sbert.net/docs/package_reference/sentence_transformer/losses.html#multiplenegativesrankingloss) without in-batch negatives and only hard negatives to achieve state-of-the-art performance on Chinese. They also applied [`MatryoshkaLoss`](https://sbert.net/docs/package_reference/sentence_transformer/losses.html#matryoshkaloss) to enable the model to produce [Matryoshka Embeddings](https://huggingface.co/blog/matryoshka).\n\nHere's an example of multi-dataset training:\n\n```python\nfrom datasets import load_dataset\nfrom sentence_transformers import SentenceTransformer, SentenceTransformerTrainer\nfrom sentence_transformers.losses import CoSENTLoss, MultipleNegativesRankingLoss, SoftmaxLoss\n\n# 1. Load a model to finetune\nmodel = SentenceTransformer(\"bert-base-uncased\")\n\n# 2. Loadseveral Datasets to train with\n# (anchor, positive)\nall_nli_pair_train = load_dataset(\"sentence-transformers/all-nli\", \"pair\", split=\"train[:10000]\")\n# (premise, hypothesis) + label\nall_nli_pair_class_train = load_dataset(\"sentence-transformers/all-nli\", \"pair-class\", split=\"train[:10000]\")\n# (sentence1, sentence2) + score\nall_nli_pair_score_train = load_dataset(\"sentence-transformers/all-nli\", \"pair-score\", split=\"train[:10000]\")\n# (anchor, positive, negative)\nall_nli_triplet_train = load_dataset(\"sentence-transformers/all-nli\", \"triplet\", split=\"train[:10000]\")\n# (sentence1, sentence2) + score\nstsb_pair_score_train = load_dataset(\"sentence-transformers/stsb\", split=\"train[:10000]\")\n# (anchor, positive)\nquora_pair_train = load_dataset(\"sentence-transformers/quora-duplicates\", \"pair\", split=\"train[:10000]\")\n# (query, answer)\nnatural_questions_train = load_dataset(\"sentence-transformers/natural-questions\", split=\"train[:10000]\")\n\n# Combine all datasets into a dictionary with dataset names to datasets\ntrain_dataset = {\n    \"all-nli-pair\": all_nli_pair_train,\n    \"all-nli-pair-class\": all_nli_pair_class_train,\n    \"all-nli-pair-score\": all_nli_pair_score_train,\n    \"all-nli-triplet\": all_nli_triplet_train,\n    \"stsb\": stsb_pair_score_train,\n    \"quora\": quora_pair_train,\n    \"natural-questions\": natural_questions_train,\n}\n\n# 3. Load several Datasets to evaluate with\n# (anchor, positive, negative)\nall_nli_triplet_dev = load_dataset(\"sentence-transformers/all-nli\", \"triplet\", split=\"dev\")\n# (sentence1, sentence2, score)\nstsb_pair_score_dev = load_dataset(\"sentence-transformers/stsb\", split=\"validation\")\n# (anchor, positive)\nquora_pair_dev = load_dataset(\"sentence-transformers/quora-duplicates\", \"pair\", split=\"train[10000:11000]\")\n# (query, answer)\nnatural_questions_dev = load_dataset(\"sentence-transformers/natural-questions\", split=\"train[10000:11000]\")\n\n# Use a dictionary for the evaluation dataset too, or just use one dataset or none at all\neval_dataset = {\n    \"all-nli-triplet\": all_nli_triplet_dev,\n    \"stsb\": stsb_pair_score_dev,\n    \"quora\": quora_pair_dev,\n    \"natural-questions\": natural_questions_dev,\n}\n\n# 4. Load several loss functions to train with\n# (anchor, positive), (anchor, positive, negative)\nmnrl_loss = MultipleNegativesRankingLoss(model)\n# (sentence_A, sentence_B) + class\nsoftmax_loss = SoftmaxLoss(model)\n# (sentence_A, sentence_B) + score\ncosent_loss = CoS", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Training and Finetuning Embedding Models with Sentence Transformers v3", "description": "Hugging Face Blog post titled  Training and Finetuning Embedding Models with Sentence Transformers v3", "url": "https://huggingface.co/blog/train-sentence-transformers", "source": "hf", "id": "5626a29b-1dc6-4f90-8c04-f11123b869ec"}, "page_content": "_dataset(\"sentence-transformers/natural-questions\", split=\"train[10000:11000]\")\n\n# Use a dictionary for the evaluation dataset too, or just use one dataset or none at all\neval_dataset = {\n    \"all-nli-triplet\": all_nli_triplet_dev,\n    \"stsb\": stsb_pair_score_dev,\n    \"quora\": quora_pair_dev,\n    \"natural-questions\": natural_questions_dev,\n}\n\n# 4. Load several loss functions to train with\n# (anchor, positive), (anchor, positive, negative)\nmnrl_loss = MultipleNegativesRankingLoss(model)\n# (sentence_A, sentence_B) + class\nsoftmax_loss = SoftmaxLoss(model)\n# (sentence_A, sentence_B) + score\ncosent_loss = CoSENTLoss(model)\n\n# Create a mapping with dataset names to loss functions, so the trainer knows which loss to apply where\n# Note: You can also just use one loss if all your training/evaluation datasets use the same loss\nlosses = {\n    \"all-nli-pair\": mnrl_loss,\n    \"all-nli-pair-class\": softmax_loss,\n    \"all-nli-pair-score\": cosent_loss,\n    \"all-nli-triplet\": mnrl_loss,\n    \"stsb\": cosent_loss,\n    \"quora\": mnrl_loss,\n    \"natural-questions\": mnrl_loss,\n}\n\n# 5. Define a simple trainer, although it's recommended to use one with args & evaluators\ntrainer = SentenceTransformerTrainer(\n    model=model,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    loss=losses,\n)\ntrainer.train()\n\n# 6. Save the trained model and optionally push it to the Hugging Face Hub\nmodel.save_pretrained(\"bert-base-all-nli-stsb-quora-nq\")\nmodel.push_to_hub(\"bert-base-all-nli-stsb-quora-nq\")\n```\n\n## Deprecation\n\nPrior to the Sentence Transformer v3 release, all models would be trained using the [`SentenceTransformer.fit`](https://sbert.net/docs/package_reference/sentence_transformer/SentenceTransformer.html#sentence_transformers.SentenceTransformer.fit) method. Rather than deprecating this method, starting from v3.0, this method will use the [`SentenceTransformerTrainer`](https://sbert.net/docs/package_reference/sentence_transformer/trainer.html#sentence_transformers.trainer.SentenceTransformerTrainer) behind the scenes. This means that your old training code should still work, and should even be upgraded with the new features such as multi-gpu training, loss logging, etc. That said, the new training approach is much more powerful, so it is **recommended** to write new training scripts using the new approach.\n\n## Additional Resources\n\n### Training Examples\n\nThe following pages contain training examples with explanations as well as links to code. We recommend that you browse through these to familiarize yourself with the training loop:\n\n- [Semantic Textual Similarity](https://sbert.net/examples/training/sts/README.html)\n- [Natural Language Inference](https://sbert.net/examples/training/nli/README.html)\n- [Paraphrases](https://sbert.net/examples/training/paraphrases/README.html)\n- [Quora Duplicate Questions](https://sbert.net/examples/training/quora_duplicate_questions/README.html)\n- [Matryoshka Embeddings](https://sbert.net/examples/training/matryoshka/README.html)\n- [Adaptive Layer Models](https://sbert.net/examples/training/adaptive_layer/README.html)\n- [Multilingual Models](https://sbert.net/examples/training/multilingual/README.html)\n- [Model Distillation](https://sbert.net/examples/training/distillation/README.html)\n- [Augmented Sentence Transformers](https://sbert.net/examples/training/data_augmentation/README.html)\n\n### Documentation\n\nAdditionally, the following pages may be useful to learn more about Sentence Transformers:\n\n- [Installation](https://sbert.net/docs/installation.html)\n- [Quickstart](https://sbert.net/docs/quickstart.html)\n- [Usage](https://sbert.net/docs/sentence_transformer/usage/usage.html)\n- [Pretrained Models](https://sbert.net/docs/sentence_transformer/pretrained_models.html)\n- [Training Overview](https://sbert.net/docs/sentence_transformer/training_overview.html) (This blogpost is a distillation of the Training Overiew documentation)\n- [Dataset Overview](https://sbert.net/docs/sentence_transformer/dataset_overview.", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Training and Finetuning Embedding Models with Sentence Transformers v3", "description": "Hugging Face Blog post titled  Training and Finetuning Embedding Models with Sentence Transformers v3", "url": "https://huggingface.co/blog/train-sentence-transformers", "source": "hf", "id": "6331060c-032f-47c2-af44-1b8a4448873c"}, "page_content": "net/examples/training/data_augmentation/README.html)\n\n### Documentation\n\nAdditionally, the following pages may be useful to learn more about Sentence Transformers:\n\n- [Installation](https://sbert.net/docs/installation.html)\n- [Quickstart](https://sbert.net/docs/quickstart.html)\n- [Usage](https://sbert.net/docs/sentence_transformer/usage/usage.html)\n- [Pretrained Models](https://sbert.net/docs/sentence_transformer/pretrained_models.html)\n- [Training Overview](https://sbert.net/docs/sentence_transformer/training_overview.html) (This blogpost is a distillation of the Training Overiew documentation)\n- [Dataset Overview](https://sbert.net/docs/sentence_transformer/dataset_overview.html)\n- [Loss Overview](https://sbert.net/docs/sentence_transformer/loss_overview.html)\n- [API Reference](https://sbert.net/docs/package_reference/sentence_transformer/index.html)\n\nAnd lastly, here are some advanced pages that might interest you:\n\n- [Hyperparameter Optimization](https://sbert.net/examples/training/hpo/README.html)\n- [Distributed Training](https://sbert.net/docs/sentence_transformer/training/distributed.html)\n\nMore Articles from our Blog\n\n[![](https://huggingface.co/blog/assets/embeddinggemma/thumbnail.png)\\\\\n\\\\\nopen-sourcecommunitynlp\\\\\n\\\\\n**Welcome EmbeddingGemma, Google's new efficient embedding model**\\\\\n\\\\\n-![](https://cdn-avatars.huggingface.co/v1/production/uploads/6317233cc92fd6fee317e030/cJHSvvimr1kqgQfHOjO5n.png)\\\\\n-![](https://cdn-avatars.huggingface.co/v1/production/uploads/61b253b7ac5ecaae3d1efe0c/hwiQ0uvz3t-L5a-NtBIO6.png)\\\\\n-![](https://cdn-avatars.huggingface.co/v1/production/uploads/60f0608166e5701b80ed3f02/BHso-wSWpR9b8b8CKvodC.jpeg)\\\\\n-![](https://cdn-avatars.huggingface.co/v1/production/uploads/608aabf24955d2bfc3cd99c6/-YxmtpzEmf3NKOTktODRP.jpeg)\\\\\n- +2\\\\\n\\\\\n261\\\\\n\\\\\nSeptember 4, 2025](https://huggingface.co/blog/embeddinggemma)\n\n[![](https://huggingface.co/blog/assets/train-sentence-transformers/st-hf-thumbnail.png)\\\\\n\\\\\nnlpguidecommunity\\\\\n\\\\\n**Training and Finetuning Sparse Embedding Models with Sentence Transformers v5**\\\\\n\\\\\n-![](https://cdn-avatars.huggingface.co/v1/production/uploads/6317233cc92fd6fee317e030/cJHSvvimr1kqgQfHOjO5n.png)\\\\\n-![](https://cdn-avatars.huggingface.co/v1/production/uploads/663ce8bfc6827e9ba288eed4/0xrGyvLOq8DtP4TQsesnS.jpeg)\\\\\n\\\\\n130\\\\\n\\\\\nJuly 1, 2025](https://huggingface.co/blog/train-sparse-encoder)[SEP]", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Fine-tuning LLMs to 1.58bit: extreme quantization made easy", "description": "Hugging Face Blog post titled  Fine-tuning LLMs to 1.58bit: extreme quantization made easy", "url": "https://huggingface.co/blog/1_58_llm_extreme_quantization", "source": "hf", "id": "d7a47474-572e-4dff-9ea9-70bc74bd5e1e"}, "page_content": "[CLS]Fine-tuning LLMs to 1.58bit: extreme quantization made easy\n\nAs Large Language Models (LLMs) grow in size and complexity, finding ways to reduce their computational and energy costs has become a critical challenge. One popular solution is quantization, where the precision of parameters is reduced from the standard 16-bit floating-point (FP16) or 32-bit floating-point (FP32) to lower-bit formats like 8-bit or 4-bit. While this approach significantly cuts down on memory usage and speeds up computation, it often comes at the expense of accuracy. Reducing the precision too much can cause models to lose crucial information, resulting in degraded performance.\n\n[BitNet](https://arxiv.org/abs/2402.17764) is a special transformers architecture that represents each parameter with only three values: `(-1, 0, 1)`, offering a extreme quantization of just 1.58 ( log2(3) log\\_2(3) log2\u200b(3) ) bits per parameter. However, it requires to train a model from scratch. While the results are impressive, not everybody has the budget to pre-train an LLM. To overcome this limitation, we explored a few tricks that allow fine-tuning an existing model to 1.58 bits! Keep reading to find out how!\n\n## Table of Contents\n\n- [TL;DR](https://huggingface.co/blog/1_58_llm_extreme_quantization#tldr)\n- [What is BitNet in More Depth?](https://huggingface.co/blog/1_58_llm_extreme_quantization#what-is-bitnet-in-more-depth)\n- [Pre-training Results in 1.58b](https://huggingface.co/blog/1_58_llm_extreme_quantization#pre-training-results-in-158b)\n- [Fine-tuning in 1.58b](https://huggingface.co/blog/1_58_llm_extreme_quantization#fine-tuning-in-158bit)\n- [Kernels used & Benchmarks](https://huggingface.co/blog/1_58_llm_extreme_quantization#kernels-used--benchmarks)\n- [Conclusion](https://huggingface.co/blog/1_58_llm_extreme_quantization#conclusion)\n- [Acknowledgements](https://huggingface.co/blog/1_58_llm_extreme_quantization#acknowledgements)\n- [Additional Resources](https://huggingface.co/blog/1_58_llm_extreme_quantization#additional-resources)\n\n## TL;DR\n\n[BitNet](https://arxiv.org/abs/2402.17764) is an architecture introduced by Microsoft Research that uses extreme quantization, representing each parameter with only three values: -1, 0, and 1. This results in a model that uses just 1.58 bits per parameter, significantly reducing computational and memory requirements.\n\nThis architecture uses INT8 addition calculations when performing matrix multiplication, in contrast to LLaMA LLM's FP16 addition and multiplication operations.\n\n![The new computation paradigm of BitNet b1.58](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/matmulfree.png)The new computation paradigm of BitNet b1.58 (source: BitNet paper https://arxiv.org/abs/2402.17764)\n\nThis results in a theoretically reduced energy consumption, with BitNet b1.58 saving 71.4 times the arithmetic operations energy for matrix multiplication compared to the Llama baseline.\n\n![Energy consumption of BitNet b1.58 compared to LLaMA](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/energy_consumption.png)Energy consumption of BitNet b1.58 compared to LLama (source: BitNet paper https://arxiv.org/abs/2402.17764)\n\nWe have successfully fine-tuned a [Llama3 8B model](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct) using the BitNet architecture, achieving strong performance on downstream tasks. The 8B models we developed are released under the [HF1BitLLM](https://huggingface.co/HF1BitLLM) organization. Two of these models were fine-tuned on 10B tokens with different training setup, while the third was fine-tuned on 100B tokens. Notably, our models surpass the Llama 1 7B model in MMLU benchmarks.\n\n### How to Use with Transformers\n\nTo integrate the BitNet architecture into Transformers, we introduced a new quantization method called \"bitnet\" ( [PR](https://github.com/huggingface/transformers/pull/33410)). This method involves replacing the standard Linear layers with specialized BitLinear layers that are compatible with the BitNet architecture, with appropriate dynamic quantization of activations, weight unpacking, and matrix multiplication.\n\nLoading and testing the model in Transformers is incredibly straightforward, there are zero changes to the API:\n\n```python\nmodel", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Fine-tuning LLMs to 1.58bit: extreme quantization made easy", "description": "Hugging Face Blog post titled  Fine-tuning LLMs to 1.58bit: extreme quantization made easy", "url": "https://huggingface.co/blog/1_58_llm_extreme_quantization", "source": "hf", "id": "6ef3be74-0d2f-4048-8728-d169f7809e43"}, "page_content": " we developed are released under the [HF1BitLLM](https://huggingface.co/HF1BitLLM) organization. Two of these models were fine-tuned on 10B tokens with different training setup, while the third was fine-tuned on 100B tokens. Notably, our models surpass the Llama 1 7B model in MMLU benchmarks.\n\n### How to Use with Transformers\n\nTo integrate the BitNet architecture into Transformers, we introduced a new quantization method called \"bitnet\" ( [PR](https://github.com/huggingface/transformers/pull/33410)). This method involves replacing the standard Linear layers with specialized BitLinear layers that are compatible with the BitNet architecture, with appropriate dynamic quantization of activations, weight unpacking, and matrix multiplication.\n\nLoading and testing the model in Transformers is incredibly straightforward, there are zero changes to the API:\n\n```python\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"HF1BitLLM/Llama3-8B-1.58-100B-tokens\",\n    device_map=\"cuda\",\n    torch_dtype=torch.bfloat16\n)\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n\ninput_text = \"Daniel went back to the garden. Mary travelled to the kitchen. Sandra journeyed to the kitchen. Sandra went to the hallway. John went to the bedroom. Mary went back to the garden. Where is Mary?\\nAnswer:\"\n\ninput_ids = tokenizer.encode(input_text, return_tensors=\"pt\").cuda()\noutput = model.generate(input_ids, max_new_tokens=10)\ngenerated_text = tokenizer.decode(output[0], skip_special_tokens=True)\nprint(generated_text)\n```\n\nWith this code, everything is managed seamlessly behind the scenes, so there's no need to worry about additional complexities, you just need to install the latest version of transformers.\n\nFor a quick test of the model, check out this [notebook](https://colab.research.google.com/drive/1ovmQUOtnYIdvcBkwEE4MzVL1HKfFHdNT?usp=sharing)\n\n## What is BitNet In More Depth?\n\n[BitNet](https://arxiv.org/abs/2402.17764) replaces traditional Linear layers in Multi-Head Attention and Feed-Forward Networks with specialized layers called BitLinear that use ternary precision (or even binary, in the initial version). The BitLinear layers we use in this project quantize the weights using ternary precision (with values of -1, 0, and 1), and we quantize the activations to 8-bit precision. We use a different implementation of BitLinear for training than we do for inference, as we'll see in the next section.\n\nThe main obstacle to training in ternary precision is that the weight values are discretized (via the `round()` function) and thus non-differentiable. BitLinear solves this with a nice trick: [STE (Straight Through Estimator)](https://arxiv.org/abs/1903.05662). The STE allows gradients to flow through the non-differentiable rounding operation by approximating its gradient as 1 (treating `round()` as equivalent to the identity function). Another way to view it is that, instead of stopping the gradient at the rounding step, the STE lets the gradient pass through as if the rounding never occurred, enabling weight updates using standard gradient-based optimization techniques.\n\n![The architecture of BitNet with BitLinear layers](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/bitlinear.png)The architecture of BitNet with BitLinear layers (source: BitNet paper https://arxiv.org/pdf/2310.11453)\n\n### Training\n\nWe train in full precision, but quantize the weights into ternary values as we go, using symmetric per tensor quantization. First, we compute the average of the absolute values of the weight matrix and use this as a scale. We then divide the weights by the scale, round the values, constrain them between -1 and 1, and finally rescale them to continue in full precision.\n\nscalew=11nm\u2211ij\u2223Wij\u2223 scale\\_w = \\\\frac{1}{\\\\frac{1}{nm} \\\\sum\\_{ij} \\|W\\_{ij}\\|} scalew\u200b=nm1\u200b\u2211ij\u200b\u2223Wij\u200b\u22231\u200b\n\nWq=clamp\\[\u22121,1\\](round(W\u2217scale)) W\\_q = \\\\text{clamp}\\_{\\[-1,1\\]}(\\\\text{round}(W\\*scale)) Wq\u200b=clamp\\[\u22121,1\\]\u200b(round(W\u2217scale))\n\nWdequantized=Wq\u2217scalew W\\_{dequantized} = W\\_q\\*scale\\_w Wdequantized\u200b=Wq\u200b\u2217scalew\u200b\n\nActivations are then quantized to a specified bit-width (8-bit, in our case) using absmax per token quantization (for a comprehensive introduction to quantization methods check out this [post](https://mlabonne.github.io/blog/posts/Introduction_to_Weight_", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Fine-tuning LLMs to 1.58bit: extreme quantization made easy", "description": "Hugging Face Blog post titled  Fine-tuning LLMs to 1.58bit: extreme quantization made easy", "url": "https://huggingface.co/blog/1_58_llm_extreme_quantization", "source": "hf", "id": "3480e5c6-ab7f-489c-a482-ed0c486aeb20"}, "page_content": "sum\\_{ij} \\|W\\_{ij}\\|} scalew\u200b=nm1\u200b\u2211ij\u200b\u2223Wij\u200b\u22231\u200b\n\nWq=clamp\\[\u22121,1\\](round(W\u2217scale)) W\\_q = \\\\text{clamp}\\_{\\[-1,1\\]}(\\\\text{round}(W\\*scale)) Wq\u200b=clamp\\[\u22121,1\\]\u200b(round(W\u2217scale))\n\nWdequantized=Wq\u2217scalew W\\_{dequantized} = W\\_q\\*scale\\_w Wdequantized\u200b=Wq\u200b\u2217scalew\u200b\n\nActivations are then quantized to a specified bit-width (8-bit, in our case) using absmax per token quantization (for a comprehensive introduction to quantization methods check out this [post](https://mlabonne.github.io/blog/posts/Introduction_to_Weight_Quantization.html)). This involves scaling the activations into the range `[\u2212128, 127]` for an 8-bit bit-width. The quantization formula is:\n\nscalex=127\u2223X\u2223max,dim=\u22121 scale\\_x = \\\\frac{127}{\\|X\\|\\_{\\\\text{max}, \\\\, \\\\text{dim}=-1}} scalex\u200b=\u2223X\u2223max,dim=\u22121\u200b127\u200b\n\nXq=clamp\\[\u2212128,127\\](round(X\u2217scale)) X\\_q = \\\\text{clamp}\\_{\\[-128,127\\]}(\\\\text{round}(X\\*scale)) Xq\u200b=clamp\\[\u2212128,127\\]\u200b(round(X\u2217scale))\n\nXdequantized=Xq\u2217scalex X\\_{dequantized} = X\\_q \\* scale\\_x Xdequantized\u200b=Xq\u200b\u2217scalex\u200b\n\nTo make the formulas clearer, here are examples of weight and activation quantization using a 3x3 matrix:\n\n* * *\n\nExample 1: Weight Matrix Quantization\n\nLet the weight matrix ( W ) be:\n\nW=\\[0.8\u22120.51.2\u22121.50.4\u22120.91.3\u22120.70.2\\] W =\n\\\\begin{bmatrix}\n0.8 & -0.5 & 1.2 \\\\\\\n -1.5 & 0.4 & -0.9 \\\\\\\n1.3 & -0.7 & 0.2\n\\\\end{bmatrix} W=\u200b0.8\u22121.51.3\u200b\u22120.50.4\u22120.7\u200b1.2\u22120.90.2\u200b\u200b\n\n**Step 1: Compute the Scale for Weights**\n\nUsing the formula:\n\nscalew=11nm\u2211ij\u2223Wij\u2223 scale\\_w = \\\\frac{1}{\\\\frac{1}{nm} \\\\sum\\_{ij} \\|W\\_{ij}\\|} scalew\u200b=nm1\u200b\u2211ij\u200b\u2223Wij\u200b\u22231\u200b\n\nwe calculate the average absolute value of ( W ):\n\n1nm\u2211ij\u2223Wij\u2223=19(0.8+0.5+1.2+1.5+0.4+0.9+1.3+0.7+0.2)=19(7.5)=0.8333 \\\\frac{1}{nm} \\\\sum\\_{ij} \\|W\\_{ij}\\| = \\\\frac{1}{9}(0.8 + 0.5 + 1.2 + 1.5 + 0.4 + 0.9 + 1.3 + 0.7 + 0.2) = \\\\frac{1}{9}(7.5) = 0.8333 nm1\u200b\u2211ij\u200b\u2223Wij\u200b\u2223=91\u200b(0.8+0.5+1.2+1.5+0.4+0.9+1.3+0.7+0.2)=91\u200b(7.5)=0.8333\n\nNow, the scale factor is:\n\nscalew=10.8333\u22481.2 scale\\_w = \\\\frac{1}{0.8333} \\\\approx 1.2 scalew\u200b=0.83331\u200b\u22481.2\n\n**Step 2: Quantize the Weight Matrix**\n\nUsing the formula:\n\nWq=clamp\\[\u22121,1\\](round(W\u00d7scalew)) W\\_q = \\\\text{clamp}\\_{\\[-1, 1\\]}(\\\\text{round}(W \\\\times scale\\_w)) Wq\u200b=clamp\\[\u22121,1\\]\u200b(round(W\u00d7scalew\u200b))\n\nWe first scale the weights by scalew\u22481.2 scale\\_w \\\\approx 1.2 scalew\u200b\u22481.2:\n\nW\u00d7scalew=\\[0.8\u00d71.2\u22120.5\u00d71.21.2\u00d71.2\u22121.5\u00d71.20.4\u00d71.2\u22120.9\u00d71.21.3\u00d71.2\u22120.7\u00d71.20.2\u00d71.2\\]=\\[0.96\u22120.61.44\u22121.80.48\u22121.081.56\u22120.840.24\\] W \\\\times scale\\_w =\n\\\\begin{bmatrix}\n0.8 \\\\times 1.2 & -0.5 \\\\times 1.2 & 1.2 \\\\times 1.2", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Fine-tuning LLMs to 1.58bit: extreme quantization made easy", "description": "Hugging Face Blog post titled  Fine-tuning LLMs to 1.58bit: extreme quantization made easy", "url": "https://huggingface.co/blog/1_58_llm_extreme_quantization", "source": "hf", "id": "f170069b-6dc0-42d2-ad33-47b9eab2fbb5"}, "page_content": "\\_w)) Wq\u200b=clamp\\[\u22121,1\\]\u200b(round(W\u00d7scalew\u200b))\n\nWe first scale the weights by scalew\u22481.2 scale\\_w \\\\approx 1.2 scalew\u200b\u22481.2:\n\nW\u00d7scalew=\\[0.8\u00d71.2\u22120.5\u00d71.21.2\u00d71.2\u22121.5\u00d71.20.4\u00d71.2\u22120.9\u00d71.21.3\u00d71.2\u22120.7\u00d71.20.2\u00d71.2\\]=\\[0.96\u22120.61.44\u22121.80.48\u22121.081.56\u22120.840.24\\] W \\\\times scale\\_w =\n\\\\begin{bmatrix}\n0.8 \\\\times 1.2 & -0.5 \\\\times 1.2 & 1.2 \\\\times 1.2 \\\\\\\n -1.5 \\\\times 1.2 & 0.4 \\\\times 1.2 & -0.9 \\\\times 1.2 \\\\\\\n1.3 \\\\times 1.2 & -0.7 \\\\times 1.2 & 0.2 \\\\times 1.2\n\\\\end{bmatrix}\n=\n\\\\begin{bmatrix}\n0.96 & -0.6 & 1.44 \\\\\\\n -1.8 & 0.48 & -1.08 \\\\\\\n1.56 & -0.84 & 0.24\n\\\\end{bmatrix} W\u00d7scalew\u200b=\u200b0.8\u00d71.2\u22121.5\u00d71.21.3\u00d71.2\u200b\u22120.5\u00d71.20.4\u00d71.2\u22120.7\u00d71.2\u200b1.2\u00d71.2\u22120.9\u00d71.20.2\u00d71.2\u200b\u200b=\u200b0.96\u22121.81.56\u200b\u22120.60.48\u22120.84\u200b1.44\u22121.080.24\u200b\u200b\n\nNext, we round the values and clamp them to the range \\[\u22121,1\\] \\[-1, 1\\] \\[\u22121,1\\]:\n\nWq=\\[1\u221211\u221210\u221211\u221210\\] W\\_q =\n\\\\begin{bmatrix}\n1 & -1 & 1 \\\\\\\n -1 & 0 & -1 \\\\\\\n1 & -1 & 0\n\\\\end{bmatrix} Wq\u200b=\u200b1\u221211\u200b\u221210\u22121\u200b1\u221210\u200b\u200b\n\n**Step 3: Dequantize the Weights**\n\nFinally, we dequantize the weights using:\n\nWdequantized=Wq\u00d7scalew W\\_{dequantized} = W\\_q \\\\times scale\\_w Wdequantized\u200b=Wq\u200b\u00d7scalew\u200b\n\nSubstituting scale\\_w, we get:\n\nWdequantized=\\[1\u00d71.2\u22121\u00d71.21\u00d71.2\u22121\u00d71.20\u00d71.2\u22121\u00d71.21\u00d71.2\u22121\u00d71.20\u00d71.2\\]=\\[1.2\u22121.21.2\u22121.20\u22121.21.2\u22121.20\\] W\\_{dequantized} =\n\\\\begin{bmatrix}\n1 \\\\times 1.2 & -1 \\\\times 1.2 & 1 \\\\times 1.2 \\\\\\\n -1 \\\\times 1.2 & 0 \\\\times 1.2 & -1 \\\\times 1.2 \\\\\\\n1 \\\\times 1.2 & -1 \\\\times 1.2 & 0 \\\\times 1.2\n\\\\end{bmatrix}\n=\n\\\\begin{bmatrix}\n1.2 & -1.2 & 1.2 \\\\\\\n -1.2 & 0 & -1.2 \\\\\\\n1.2 & -1.2 & 0\n\\\\end{bmatrix} Wdequantized\u200b=\u200b1\u00d71.2\u22121\u00d71.21\u00d71.2\u200b\u22121\u00d71.20\u00d71.2\u22121\u00d71.2\u200b1\u00d71.2\u22121\u00d71.20\u00d71.2\u200b\u200b=\u200b1.2\u22121.21.2\u200b\u22121.20\u22121.2\u200b1.2\u22121.20\u200b\u200b\n\nExample 2: Activation Matrix Quantization\n\nLet the activation matrix ( X ) be:\n\nX=\\[1.0\u22120.60.7\u22120.90.4\u22121.20.8\u22120.50.3\\] X =\n\\\\begin{bmatrix}\n1.0 & -0.6 & 0.7 \\\\\\\n -0.9 & 0.4 & -1.2 \\\\\\\n0.8 & -0.5 & 0.3\n\\\\end{bmatrix} X=\u200b1.0\u22120.90.8\u200b\u22120.60.4\u22120.5\u200b0.7\u22121.20.3\u200b\u200b\n\n**Step 1: Compute the Scale for Activations**\n\nFor each row (or channel), compute the maximum absolute value:\n\n- **Row 1**: Maximum absolute value = 1.0\n- **Row 2**: Maximum absolute value = 1.2\n- **Row 3**: Maximum absolute value = 0.8\n\nCompute the scale factors for each row:\n\nscale=\\[1271.01271.21270.8\\]=\\[127105.83158.75\\] \\\\text{scale} = \\\\begin{bmatrix}\n\\\\frac{127", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Fine-tuning LLMs to 1.58bit: extreme quantization made easy", "description": "Hugging Face Blog post titled  Fine-tuning LLMs to 1.58bit: extreme quantization made easy", "url": "https://huggingface.co/blog/1_58_llm_extreme_quantization", "source": "hf", "id": "89852c94-3529-43b1-b9e1-176ae5af8272"}, "page_content": "0.6 & 0.7 \\\\\\\n -0.9 & 0.4 & -1.2 \\\\\\\n0.8 & -0.5 & 0.3\n\\\\end{bmatrix} X=\u200b1.0\u22120.90.8\u200b\u22120.60.4\u22120.5\u200b0.7\u22121.20.3\u200b\u200b\n\n**Step 1: Compute the Scale for Activations**\n\nFor each row (or channel), compute the maximum absolute value:\n\n- **Row 1**: Maximum absolute value = 1.0\n- **Row 2**: Maximum absolute value = 1.2\n- **Row 3**: Maximum absolute value = 0.8\n\nCompute the scale factors for each row:\n\nscale=\\[1271.01271.21270.8\\]=\\[127105.83158.75\\] \\\\text{scale} = \\\\begin{bmatrix}\n\\\\frac{127}{1.0} \\\\\\\n\\\\frac{127}{1.2} \\\\\\\n\\\\frac{127}{0.8}\n\\\\end{bmatrix}\n=\n\\\\begin{bmatrix}\n127 \\\\\\\n105.83 \\\\\\\n158.75\n\\\\end{bmatrix} scale=\u200b1.0127\u200b1.2127\u200b0.8127\u200b\u200b\u200b=\u200b127105.83158.75\u200b\u200b\n\n**Step 2: Quantize the Activation Matrix**\n\nUsing the formula:\n\nXq=clamp\\[\u2212128,127\\](round(X\u00d7scale)) X\\_q = \\\\text{clamp}\\_{\\[-128,127\\]}(\\\\text{round}(X \\\\times \\\\text{scale})) Xq\u200b=clamp\\[\u2212128,127\\]\u200b(round(X\u00d7scale))\n\nScale the activations:\n\nX\u00d7scale=\\[1.0\u00d7127\u22120.6\u00d71270.7\u00d7127\u22120.9\u00d7105.830.4\u00d7105.83\u22121.2\u00d7105.830.8\u00d7158.75\u22120.5\u00d7158.750.3\u00d7158.75\\]=\\[127\u221276.288.9\u221295.242.3\u2212127127\u221279.447.6\\] X \\\\times \\\\text{scale} =\n\\\\begin{bmatrix}\n1.0 \\\\times 127 & -0.6 \\\\times 127 & 0.7 \\\\times 127 \\\\\\\n -0.9 \\\\times 105.83 & 0.4 \\\\times 105.83 & -1.2 \\\\times 105.83 \\\\\\\n0.8 \\\\times 158.75 & -0.5 \\\\times 158.75 & 0.3 \\\\times 158.75\n\\\\end{bmatrix}\n=\n\\\\begin{bmatrix}\n127 & -76.2 & 88.9 \\\\\\\n -95.2 & 42.3 & -127 \\\\\\\n127 & -79.4 & 47.6\n\\\\end{bmatrix} X\u00d7scale=\u200b1.0\u00d7127\u22120.9\u00d7105.830.8\u00d7158.75\u200b\u22120.6\u00d71270.4\u00d7105.83\u22120.5\u00d7158.75\u200b0.7\u00d7127\u22121.2\u00d7105.830.3\u00d7158.75\u200b\u200b=\u200b127\u221295.2127\u200b\u221276.242.3\u221279.4\u200b88.9\u221212747.6\u200b\u200b\n\nRound the values and clamp them to the range \\[\u2212128,127\\]\\[-128, 127\\] \\[\u2212128,127\\]:\n\nXq=\\[127\u22127689\u22129542\u2212127127\u22127948\\] X\\_q =\n\\\\begin{bmatrix}\n127 & -76 & 89 \\\\\\\n -95 & 42 & -127 \\\\\\\n127 & -79 & 48\n\\\\end{bmatrix} Xq\u200b=\u200b127\u221295127\u200b\u22127642\u221279\u200b89\u221212748\u200b\u200b\n\n**Step 3: Dequantize the Activations**\n\nFinally, dequantize the activations using:\n\nXdequantized=Xq\u00d71scale X\\_{dequantized} = X\\_q \\\\times \\\\frac{1}{\\\\text{scale}} Xdequantized\u200b=Xq\u200b\u00d7scale1\u200b\n\nSubstituting the scales:\n\nXdequantized=\\[127\u00d71127\u221276\u00d7112789\u00d71127\u221295\u00d71105.8342\u00d71105.83\u2212127\u00d71105.83127\u00d71158.75\u221279\u00d71158.7548\u00d71158.75\\]=\\[1.0\u22120.60.7\u22120.90.4\u22121.20.8\u22120.50.3\\] X\\_{dequantized} =\n\\\\begin{bmatrix}\n127 \\\\times \\\\frac{1}{127} & -76 \\\\times \\\\frac{1}{127} & 89 \\\\times \\\\frac{1}{127} \\\\\\\n -95 \\\\times \\\\frac{1}{105.83} & 42 \\\\times \\\\frac{1}{105.83} & -127 \\\\times \\\\frac{1}{105.83} \\\\\\\n127 \\\\times \\\\frac{1}{158.75} & -79 \\\\times \\\\frac{1}{158.75} & 48 \\\\times \\\\frac{1}{158.75}\n\\\\end{bmatrix}\n=\n\\\\begin{bmatrix}\n1.0 & -0.6 & 0.7 \\\\\\\n -0.9 & 0.4 & -1.2 \\\\\\\n0.8 &", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Fine-tuning LLMs to 1.58bit: extreme quantization made easy", "description": "Hugging Face Blog post titled  Fine-tuning LLMs to 1.58bit: extreme quantization made easy", "url": "https://huggingface.co/blog/1_58_llm_extreme_quantization", "source": "hf", "id": "f232112c-4b20-4fd4-ba55-c135aee48bb4"}, "page_content": ".90.4\u22121.20.8\u22120.50.3\\] X\\_{dequantized} =\n\\\\begin{bmatrix}\n127 \\\\times \\\\frac{1}{127} & -76 \\\\times \\\\frac{1}{127} & 89 \\\\times \\\\frac{1}{127} \\\\\\\n -95 \\\\times \\\\frac{1}{105.83} & 42 \\\\times \\\\frac{1}{105.83} & -127 \\\\times \\\\frac{1}{105.83} \\\\\\\n127 \\\\times \\\\frac{1}{158.75} & -79 \\\\times \\\\frac{1}{158.75} & 48 \\\\times \\\\frac{1}{158.75}\n\\\\end{bmatrix}\n=\n\\\\begin{bmatrix}\n1.0 & -0.6 & 0.7 \\\\\\\n -0.9 & 0.4 & -1.2 \\\\\\\n0.8 & -0.5 & 0.3\n\\\\end{bmatrix} Xdequantized\u200b=\u200b127\u00d71271\u200b\u221295\u00d7105.831\u200b127\u00d7158.751\u200b\u200b\u221276\u00d71271\u200b42\u00d7105.831\u200b\u221279\u00d7158.751\u200b\u200b89\u00d71271\u200b\u2212127\u00d7105.831\u200b48\u00d7158.751\u200b\u200b\u200b=\u200b1.0\u22120.90.8\u200b\u22120.60.4\u22120.5\u200b0.7\u22121.20.3\u200b\u200b\n\n* * *\n\nWe apply Layer Normalization (LN) before quantizing the activations to maintain the variance of the output:\n\nLN(x)=x\u2212E(x)Var(x)+\u03f5 \\\\text{LN}(x) = \\\\frac{x - E(x)}{\\\\sqrt{\\\\text{Var}(x) + \\\\epsilon}} LN(x)=Var(x)+\u03f5\u200bx\u2212E(x)\u200b\n\nwhere \u03f5 is a small number to prevent overflow.\n\nThe `round()` function is not differentiable, as mentioned before. We use `detach()` as a trick to implement a differentiable straight-through estimator in the backward pass:\n\n```python\n# Adapted from https://github.com/microsoft/unilm/blob/master/bitnet/The-Era-of-1-bit-LLMs__Training_Tips_Code_FAQ.pdf\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndef activation_quant(x):\n    scale = 127.0 / x.abs().max(dim=-1, keepdim=True).values.clamp_(min=1e-5)\n    y = (x * scale).round().clamp_(-128, 127) / scale\n    return y\n\ndef weight_quant(w):\n    scale = 1.0 / w.abs().mean().clamp_(min=1e-5)\n    u = (w * scale).round().clamp_(-1, 1) / scale\n    return u\n\nclass BitLinear(nn.Linear):\n    \"\"\"\n    Only for training\n    \"\"\"\n    def forward(self, x):\n        w = self.weight\n        x_norm = LN(x)\n\n        # A trick for implementing Straight\u2212Through\u2212Estimator (STE) using detach()\n        x_quant = x_norm + (activation_quant(x_norm) - x_norm).detach()\n        w_quant = w + (weight_quant(w) - w).detach()\n\n        # Perform quantized linear transformation\n        y = F.linear(x_quant, w_quant)\n        return y\n```\n\n### Inference\n\nDuring inference, we simply quantize the weights to ternary values without rescaling. We apply the same approach to activations using 8-bit precision, then perform a matrix multiplication with an efficient kernel, followed by dividing by both the weight and activation scales. This should significantly improve inference speed, particularly with optimized hardware. You can see that the rescaling process differs during training, as matrix multiplications are kept in fp16/bf16/fp32 for proper training.\n\n```python\n# Adapted from https://github.com/microsoft/unilm/blob/master/bitnet/The-Era-of-1-bit-LLMs__Training_Tips_Code_FAQ.pdf\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndef activation_quant_inference(x):\n    x = LN(x)\n    scale = 127.0 / x.abs().max(dim=-1, keepdim=True).values.clamp_(min=1e-5)\n    y = (x * scale).round().clamp_(-128, 127)\n    return y, scale\n\nclass BitLinear(nn.Linear):\n    \"\"\"\n    Only for training\n    \"\"\"\n    def forward(self, x):\n        w = self.weight # weights here are already quantized to (-1, 0, 1)\n        w_scale = self.w_scale\n        x_quant, x_scale = activation_quant_inference(x)\n        y = efficient_kernel(x_quant, w) / w_scale / x_scale\n        return y\n```\n\n## Pre-training Results in 1.58b\n\nBefore attempting fine-tuning, we first tried to reproduce the", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Fine-tuning LLMs to 1.58bit: extreme quantization made easy", "description": "Hugging Face Blog post titled  Fine-tuning LLMs to 1.58bit: extreme quantization made easy", "url": "https://huggingface.co/blog/1_58_llm_extreme_quantization", "source": "hf", "id": "b87efab0-6549-4be0-aa37-7690332f5787"}, "page_content": "(x)\n    scale = 127.0 / x.abs().max(dim=-1, keepdim=True).values.clamp_(min=1e-5)\n    y = (x * scale).round().clamp_(-128, 127)\n    return y, scale\n\nclass BitLinear(nn.Linear):\n    \"\"\"\n    Only for training\n    \"\"\"\n    def forward(self, x):\n        w = self.weight # weights here are already quantized to (-1, 0, 1)\n        w_scale = self.w_scale\n        x_quant, x_scale = activation_quant_inference(x)\n        y = efficient_kernel(x_quant, w) / w_scale / x_scale\n        return y\n```\n\n## Pre-training Results in 1.58b\n\nBefore attempting fine-tuning, we first tried to reproduce the results of the BitNet paper with pre-training. We started with a small dataset, [tinystories](https://huggingface.co/datasets/roneneldan/TinyStories), and a [Llama3 8B model](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct). We confirmed that adding a normalization function, like the paper does, improves performance. For example, after 2000 steps of training, we had a perplexity on the validation set equal to 6.3 without normalization, and 5.9 with normalization. Training was stable in both cases.\n\n![Pre-training plots without (blue) & with (green) layer normalisation](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/pre-training.png)Pre-training plots without (blue) & with (orange) layer normalisation\n\nWhile this approach looks very interesting for pre-training, only a few institutions can afford doing it at the necessary scale. However, there is already a wide range of strong pretrained models, and it would be extremely useful if they could be converted to 1.58bit after pre-training. Other groups had reported that fine-tuning results were not as strong as those achieved with pre-training, so we set out on an investigation to see if we could make 1.58 fine-tuning work.\n\n## Fine-tuning in 1.58bit\n\nWhen we began fine-tuning from the pre-trained Llama3 8B weights, the model performed slightly better but not as well as we expected.\n\n> **Note:** All our experiments were conducted using [Nanotron](https://github.com/huggingface/nanotron). If you're interested in trying 1.58bit pre-training or fine-tuning, you can check out this [PR](https://github.com/huggingface/nanotron/pull/180).\n\n![Fine-tuning plot compared to pre-training plot](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/finetuning_basic.png)Fine-tuning plot compared to pre-training plot\n\nTo understand why, we tried to inspect both the weight distributions of the randomly initialized model and the pre-trained model to identify potential issues.\n\n![Random weights distribution (2 merged stds)](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/poids_al%C3%A9atoires.png)Random weights distribution (2 merged stds)![Pre-trained Llama3 weights distribution](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/poids_llama3.png)Pre-trained Llama3 weights distribution\n\nAnd the scale values for the two distributions are, respectively :\n\n![Random weights scales distribution](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/scales_random.png)Random weights scales distribution![Pre-trained Llama3 weights distribution](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/scales_llama3.png)Pre-trained Llama3 weights distribution\n\nThe initial random weight distribution is a mix of two normal distributions:\n\n- One with a standard deviation (std) of 0.025 0.025 0.025\n- Another with a std of 0.0252\u22c5num\\_hidden\\_layers=0.00325 \\\\frac{0.025}{\\\\sqrt{2 \\\\cdot \\\\text{num\\\\\\_hidden\\\\\\_layers}}} = 0.00325 2\u22c5num\\_hidden\\_layers\u200b0.025\u200b=0.00325\n\nThis results from using different stds for column linear and row linear weights in `nanotron`. In the quantized version, all matrices have only 2 weight scales (50.25 and 402), which are the inverse of the mean absolute value of the weights for each matrix: `scale = 1.0 / w", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Fine-tuning LLMs to 1.58bit: extreme quantization made easy", "description": "Hugging Face Blog post titled  Fine-tuning LLMs to 1.58bit: extreme quantization made easy", "url": "https://huggingface.co/blog/1_58_llm_extreme_quantization", "source": "hf", "id": "866ff9b4-9066-4f6b-acbb-a90d20b6ba83"}, "page_content": "1.58llm_extreme_quantization/scales_llama3.png)Pre-trained Llama3 weights distribution\n\nThe initial random weight distribution is a mix of two normal distributions:\n\n- One with a standard deviation (std) of 0.025 0.025 0.025\n- Another with a std of 0.0252\u22c5num\\_hidden\\_layers=0.00325 \\\\frac{0.025}{\\\\sqrt{2 \\\\cdot \\\\text{num\\\\\\_hidden\\\\\\_layers}}} = 0.00325 2\u22c5num\\_hidden\\_layers\u200b0.025\u200b=0.00325\n\nThis results from using different stds for column linear and row linear weights in `nanotron`. In the quantized version, all matrices have only 2 weight scales (50.25 and 402), which are the inverse of the mean absolute value of the weights for each matrix: `scale = 1.0 / w.abs().mean().clamp_(min=1e-5)`\n\n- For scale=50.25\\\\text{scale} = 50.25 scale=50.25, w.abs().mean()=0.0199 w.abs().mean() = 0.0199 w.abs().mean()=0.0199, leading to std=0.025\\\\text{std} = 0.025 std=0.025 which matches our first standard deviation. The formula used to derive the std is based on the expectation of the half-normal distribution of \u2223w\u2223 \\|w\\| \u2223w\u2223:\n\nE(\u2223w\u2223)=std(w)\u22c52\u03c0 \\\\mathbb{E}(\\|w\\|) = \\\\text{std}(w) \\\\cdot \\\\sqrt{\\\\frac{2}{\\\\pi}} E(\u2223w\u2223)=std(w)\u22c5\u03c02\u200b\u200b\n- For scale=402 \\\\text{scale} = 402 scale=402, w.abs().mean()=0.0025 w.abs().mean() = 0.0025 w.abs().mean()=0.0025, leading to std=0.00325\\\\text{std} = 0.00325 std=0.00325\n\nOn the other hand, the pretrained weight's distribution looks like a normal distribution with an std=0.013 \\\\text{std} = 0.013 std=0.013\n\nClearly, the pretrained model starts with more information (scales), while the randomly initialized model starts with practically no information and adds to it over time. Our conclusion was that starting with random weights gives the model minimal initial information, enabling a gradual learning process, while during fine-tuning, the introduction of BitLinear layers overwhelms the model into losing all its prior information.\n\nTo improve the fine-tuning results, we tried different techniques. For example, instead of using per-tensor quantization, we tried per-row and per-column quantization to keep more information from the Llama 3 weights. We also tried to change the way the scale is computed: instead of just taking the mean absolute value of the weights as a scale, we take the mean absolute value of the outliers as a scale (an outlier value is a value that exceeds k\\*mean\\_absolute\\_value, where k is a constant we tried to vary in our experiments), but we didn\u2019t notice big improvements.\n\n```python\ndef scale_outliers(tensor, threshold_factor=1):\n    mean_absolute_value = torch.mean(torch.abs(tensor))\n    threshold = threshold_factor * mean_absolute_value\n    outliers = tensor[torch.abs(tensor) > threshold]\n    mean_outlier_value = torch.mean(torch.abs(outliers))\n    return mean_outlier_value\n\ndef weight_quant_scaling(w):\n    scale = 1.0 / scale_outliers(w).clamp_(min=1e-5)\n    quantized_weights = (w * scale).round().clamp_(-1, 1) / scale\n    return quantized_weights\n```\n\nWe observed that both the random weights and the Llama 3 weights resulted in losses starting at approximately the same value of 13. This suggests that the Llama 3 model loses all of its prior information when quantization is introduced. To further investigate how much information the model loses during this process, we experimented with per-group quantization.\n\nAs a sanity check, we first set the group size to 1, which essentially means no quantization. In this scenario, the loss started at 1.45, same as we see during normal fine-tuning. However, when we increased the group size to 2, the loss jumped to around 11. This indicates that even with a minimal group size of 2, the model still loses nearly all of its information.\n\nTo address this issue, we considered the possibility of introducing quantization gradually rather than applying it abruptly to the weights and activations for each tensor. To achieve this, we implemented a lambda value to control the process :\n\n```python\nlambda_ =?\nx_quant = x + lambda_ * (activation_quant(x) - x).detach()\nw_quant = w + lambda_ * (weight_quant(w) - w).detach()\n```\n\nWhen `lambda` is set to 0, there is essentially no quantization occurring, while at `lambda=1`, full quantization is applied.\n\nWe initially tested some discrete `lambda` values, such as 0.25, 0.5, 0.75, and 1. However, this approach", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Fine-tuning LLMs to 1.58bit: extreme quantization made easy", "description": "Hugging Face Blog post titled  Fine-tuning LLMs to 1.58bit: extreme quantization made easy", "url": "https://huggingface.co/blog/1_58_llm_extreme_quantization", "source": "hf", "id": "95a6c09e-9822-4119-8e88-28886dabeb3f"}, "page_content": " increased the group size to 2, the loss jumped to around 11. This indicates that even with a minimal group size of 2, the model still loses nearly all of its information.\n\nTo address this issue, we considered the possibility of introducing quantization gradually rather than applying it abruptly to the weights and activations for each tensor. To achieve this, we implemented a lambda value to control the process :\n\n```python\nlambda_ =?\nx_quant = x + lambda_ * (activation_quant(x) - x).detach()\nw_quant = w + lambda_ * (weight_quant(w) - w).detach()\n```\n\nWhen `lambda` is set to 0, there is essentially no quantization occurring, while at `lambda=1`, full quantization is applied.\n\nWe initially tested some discrete `lambda` values, such as 0.25, 0.5, 0.75, and 1. However, this approach did not lead to any significant improvement in results, mainly because `lambda=0.25` is already high enough for the loss to start very high.\n\n![Fine-tuning plot with lambda = 0.25->0.5->0.75->1](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/lambda_0.25.png)Fine-tuning plot with lambda = 0.25->0.5->0.75->1\n\nAs a result, we decided to experiment with a `lambda` value that adjusts dynamically based on the training step.\n\n```python\nlambda_ = training_step / total_training_steps\n```\n\nUsing this dynamic `lambda` value led to better loss convergence, but the perplexity (ppl) results during inference, when `lambda` was set to 1, were still far from satisfactory. We realized this was likely because the model hadn't been trained long enough with `lambda=1`. To address this, we adjusted our `lambda` value to improve the training process.\n\n```python\nlambda_ = min(2 * training_step / total_training_steps, 1)\n```\n\nWith this configuration, after 2000 steps we have :\n\n![Fine-tuning plot with lambda = min(2*training_step/total_training_steps, 1)](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/lambda_training_step.png)Fine-tuning plot with lambda = min(2\\*training\\_step/total\\_training\\_steps, 1)\n\nOur fine-tuning method shows better convergence overall. You can observe a slight increase in the loss curve around 1,000 steps, which corresponds to when we begin approaching `lambda=1`, or full quantization. However, immediately after this point, the loss starts to converge again, leading to an improved perplexity of approximately 4.\n\nDespite this progress, when we tested the quantized model on the WikiText dataset (instead of the tinystories one we used for fine-tuning), it showed a very high perplexity. This suggests that fine-tuning the model in low-bit mode on a specific dataset causes it to lose much of its general knowledge. This issue might arise because the minimal representations we aim for with ternary weights can vary significantly from one dataset to another. To address this problem, we scaled our training process to include the larger [FineWeb-edu](https://huggingface.co/datasets/HuggingFaceFW/fineweb) dataset. We maintained a `lambda` value of:\n\n```python\nlambda_ = min(training_step/1000, 1)\n```\n\nWe chose this `lambda` value because it seemed to be a good starting point for warming up the model. We then trained the model using a learning rate of 1e-4 for 5,000 steps on the FineWeb-edu dataset. The training involved a batch size (BS) of 2 million, totaling 10 billion tokens.\n\nFinding the right learning rate and the right decay was challenging; it seems to be a crucial factor in the model's performance.\n\n![Fine-tuning plot with warmup quantization on Fineweb-edu](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/fineweb-edu.png)Fine-tuning plot with warmup quantization on Fineweb-edu\n\nAfter the fine-tuning process on Fineweb-Edu, the perplexity on the WikiText dataset reached 12.2, which is quite impressive given that we only used 10 billion tokens. The other evaluation metrics also show strong performance considering the limited amount of data (see results).\n\nWe also tried to smooth out the sharp increase when lambda approaches 1. To do this, we considered using lambda schedulers that grow exponentially at first, then level off as they get closer to 1.\n\n```python\ndef scheduler(step, total_steps, k):\n    normalized_step = step / total_steps\n    return 1 - (1 - normalized_step)**k\n```\n\nfor different k values, with a number of total warmup steps of 1, we have plots like the following :\n\n![Exponential scheduler for different k values](https://huggingface.co/datasets/huggingface/documentation-images/resolve/", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Fine-tuning LLMs to 1.58bit: extreme quantization made easy", "description": "Hugging Face Blog post titled  Fine-tuning LLMs to 1.58bit: extreme quantization made easy", "url": "https://huggingface.co/blog/1_58_llm_extreme_quantization", "source": "hf", "id": "0cfae6db-41bb-4f66-89ad-51f8172ee053"}, "page_content": "uning process on Fineweb-Edu, the perplexity on the WikiText dataset reached 12.2, which is quite impressive given that we only used 10 billion tokens. The other evaluation metrics also show strong performance considering the limited amount of data (see results).\n\nWe also tried to smooth out the sharp increase when lambda approaches 1. To do this, we considered using lambda schedulers that grow exponentially at first, then level off as they get closer to 1.\n\n```python\ndef scheduler(step, total_steps, k):\n    normalized_step = step / total_steps\n    return 1 - (1 - normalized_step)**k\n```\n\nfor different k values, with a number of total warmup steps of 1, we have plots like the following :\n\n![Exponential scheduler for different k values](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/exp_scheduler.png)Exponential scheduler for different k values\n\nWe ran 4 experiments using the best-performing learning rate of 1e-4, testing values of k in \\[4, 6, 8, 10\\].\n\n![Fine-tuning plots with exponential scheduler](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/exp_scheduler_results.png)Fine-tuning plots with exponential scheduler\n\nThe smoothing worked well, as there's no spike like with the linear scheduler. However, the perplexity isn't great, staying around ~15, and the performance on downstream tasks is not better.\n\nWe also noticed the spike at the beginning, which the model struggled to recover from. With lambda = 0, there's essentially no quantization, so the loss starts low, around ~2. But right after the first step, there's a spike, similar to what happened with the linear scheduler (as seen in the blue plot above). So, we tried a different scheduler\u2014a sigmoid one\u2014that starts slowly, rises sharply to 1, and then levels off as it approaches 1.\n\n```python\ndef sigmoid_scheduler(step, total_steps, k):\n    # Sigmoid-like curve: slow start, fast middle, slow end\n    normalized_step = step / total_steps\n    return 1 / (1 + np.exp(-k * (normalized_step - 0.5)))\n```\n\nFor different k values we have the following curves :\n\n![Sigmoid scheduler for different k values](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/sig_scheduler.png)Sigmoid scheduler for different k values\n\nWe ran 5 experiments this time with k in \\[15, 20, 25, 40, 100\\] :\n\n![Finetuning plots with sigmoid scheduler](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/sig_scheduler_exps.png)Fine-tuning plots with sigmoid scheduler\n\nThe sharp increase in lambda caused instability around the 500th step and didn\u2019t fix the first divergence issue. However, for k=100 k = 100 k=100, we observed some improvement in downstream tasks (see the results table), although perplexity remained around ~13.5. Despite this, it didn\u2019t show a clear performance boost over a linear scheduler.\n\nAdditionally, we experimented with training models from scratch using random weights and various learning rates. This allowed us to compare the effectiveness of our fine-tuning approach against traditional pre-training methods.\n\n![Different Pre-training plots with different learning rates](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/exp-randoms.png)Different Pre-training plots with different learning rates\n\nNone of the models trained from random weights performed better than our fine-tuned model. The best perplexity we achieved with those models was 26, which falls short compared to the results from our fine-tuning approach.\n\n### Scaling to 100B Tokens!\n\nWe scaled our experiments to 100 billion tokens to see if we could match the performance of Llama 3 8B. We conducted longer training runs, starting from our best-performing checkpoint from the shorter runs with the linear scheduler, and continued fine-tuning for 45,000 steps. We experimented with different learning rates, and while the model performed closely to the Llama 3 model in some metrics, on average, it still lagged behind.\n\nHere are some examples of the metrics we evaluated at various checkpoints during the training :\n\n![Metrics evaluations during the training for different lrs](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/metrics_100B.png)Metrics evaluations during the training for different lrs\n\nand the average score looks like :\n\n![Average evaluation during the training for different lrs](https://huggingface.co/datasets/huggingface/", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Fine-tuning LLMs to 1.58bit: extreme quantization made easy", "description": "Hugging Face Blog post titled  Fine-tuning LLMs to 1.58bit: extreme quantization made easy", "url": "https://huggingface.co/blog/1_58_llm_extreme_quantization", "source": "hf", "id": "4687d1e0-8f0e-46ea-8ca8-0548bcc3608b"}, "page_content": "ama 3 8B. We conducted longer training runs, starting from our best-performing checkpoint from the shorter runs with the linear scheduler, and continued fine-tuning for 45,000 steps. We experimented with different learning rates, and while the model performed closely to the Llama 3 model in some metrics, on average, it still lagged behind.\n\nHere are some examples of the metrics we evaluated at various checkpoints during the training :\n\n![Metrics evaluations during the training for different lrs](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/metrics_100B.png)Metrics evaluations during the training for different lrs\n\nand the average score looks like :\n\n![Average evaluation during the training for different lrs](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/metric_avg.png)Average evaluation during the training for different lrs\n\n### Experiments on Smaller Models\n\nIn our initial experiments with smaller models like SmolLM, we observed that the warmup quantization technique didn\u2019t yield as much improvement as it did with larger models. This suggests that the effectiveness of warmup quantization could be more closely related to model size and complexity.\n\nFor example, here are the loss curves for the [SmolLM 135M](https://huggingface.co/HuggingFaceTB/SmolLM-135M) model, comparing warmup quantization with full quantization from the start. Interestingly, the curves closely align, and the resulting perplexities aren't significantly different.\n\n![Smoll LLm fine-tuning experiment with & without warmup quantization](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/smol_llm_exp.png)Smoll LLm fine-tuning experiment with & without warmup quantization\n\n### Results & Comparison\n\nBitNet is effective in delivering strong performance compared to baseline methods, especially at lower bit levels. According to the paper, BitNet achieves scores that are on par with 8-bit models but with significantly lower inference costs. In the case of 4-bit models, methods that only quantize weights outperform those that quantize both weights and activations, as activations are harder to quantify. However, BitNet, which uses 1.58-bit weights, surpasses both weight-only and weight-and-activation quantization methods.\n\nThe table below presents the results for various metrics after the 10B fine-tuning process of Llama3 8B. These results are compared against those from other model architectures to provide a comprehensive overview of performance (All evaluations were conducted using [Lighteval](https://github.com/huggingface/lighteval) on the [Nanotron](https://github.com/huggingface/nanotron) format model)\n\n![Metrics comparison with Llama models](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/metrics_comparison_updated.png)Metrics comparison with Llama models : Linear means Linear lambda scheduler, and Sigmoid means Sigmoid lambda scheduler (in our case k = 100)\n\nAfter fine-tuning on just 10 billion tokens using ternary weights, the model demonstrates impressive performance, especially when compared to other models that underwent much more extensive training. For instance, it outperforms the Bitnet 7B model, which was trained on a significantly larger dataset of 100 billion tokens. Additionally, it performs better than the FBI LLM (Fully Binarized LLM), a model that was distilled on an even more massive 1.26 trillion tokens. This highlights the model's efficiency and effectiveness despite the relatively smaller scale of its fine-tuning process.\n\nFor the 100B tokens experiments, the best performing checkpoint we had is the following :\n\n![Metrics comparison with Llama models for the model trained on 100B tokens](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/metrics_100B_table.png)Metrics comparison with Llama models for the model trained on 100B tokens\n\nTo replicate these results, you can check out this [PR](https://github.com/huggingface/nanotron/pull/174) to convert models to nanotron format, unpack the weights (check the function [unpack\\_weights](https://gist.github.com/MekkCyber/78c1532e8767e8da0588b778faf61866)), and use lighteval\n\nNote that even though the models are fine-tuned from an Instruct-tuned model, they still need to be fine-tuned using an Instruct dataset as well. These can be considered base models.\n\n## Custom Kernels & Benchmarks\n\nTo benefit from the BitNet low-precision weights, we pack them into an `int8` tensor (this makes the number of parameters go from 8B to 2.8B!). During inference, these weights must be unpacked before performing matrix multiplication. We implemented custom kernels in Cuda and Triton to handle the on-the-fly unpacking during the matrix multiplication process. For the", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Fine-tuning LLMs to 1.58bit: extreme quantization made easy", "description": "Hugging Face Blog post titled  Fine-tuning LLMs to 1.58bit: extreme quantization made easy", "url": "https://huggingface.co/blog/1_58_llm_extreme_quantization", "source": "hf", "id": "c6742266-612d-4e9c-8e9d-8b9ad8e455dc"}, "page_content": "pull/174) to convert models to nanotron format, unpack the weights (check the function [unpack\\_weights](https://gist.github.com/MekkCyber/78c1532e8767e8da0588b778faf61866)), and use lighteval\n\nNote that even though the models are fine-tuned from an Instruct-tuned model, they still need to be fine-tuned using an Instruct dataset as well. These can be considered base models.\n\n## Custom Kernels & Benchmarks\n\nTo benefit from the BitNet low-precision weights, we pack them into an `int8` tensor (this makes the number of parameters go from 8B to 2.8B!). During inference, these weights must be unpacked before performing matrix multiplication. We implemented custom kernels in Cuda and Triton to handle the on-the-fly unpacking during the matrix multiplication process. For the matrix multiplication itself, we employed the cached tiled matrix multiplication technique. To fully grasp this approach, let's first review some Cuda programming fundamentals.\n\n### Basic GPU Concepts: Threads, Blocks, and Shared Memory\n\nBefore diving into cached tiled matrix multiplication, it's important to understand some basic GPU concepts:\n\n- **Threads and Blocks**: GPUs execute thousands of threads simultaneously. These threads are grouped into blocks, and each block runs independently. The grid is made up of these blocks, and it represents the entire problem space. For example, in matrix multiplication, each thread might be responsible for computing a single element of the output matrix.\n- **Shared Memory**: Each block has access to a limited amount of shared memory, which is much faster than global memory (the main memory on the GPU). However, shared memory is limited in size and shared among all threads within a block. Using shared memory effectively is key to improving performance in GPU programs.\n\n### Challenges in Matrix Multiplication\n\nA simple implementation of matrix multiplication on a GPU might involve each thread computing a single element of the result matrix by directly reading the necessary elements from global memory. However, this approach can be inefficient for the following reasons:\n\n- **Memory Bandwidth**: Accessing global memory is relatively slow compared to the speed at which the GPU cores can perform computations. If each thread reads matrix elements directly from global memory, the memory access times can become a bottleneck.\n- **Redundant Data Access**: In matrix multiplication, many elements of the input matrices are used multiple times. If each thread fetches the required data from global memory independently, the same data might be loaded into the GPU multiple times, leading to inefficiency. For example, if each thread is used to compute a single element in the output matrix, the thread responsible for calculating the element at position (i, j) will need to load the i-th row of matrix A and the j-th column of matrix B from global memory. However, other threads, such as the one computing the element at position (i+1, j), cannot reuse this data and will have to reload the same j-th column from global memory again.\n\n### The Idea of Tiling\n\nTiling is a technique used to address these challenges, and it was mainly used in FlashAttention to improve the kernel's efficiency. The basic idea is to divide the matrices into smaller sub-matrices, called tiles, which can fit into the shared memory of the GPU. Instead of computing the entire output matrix in one go, the computation is broken down into smaller pieces that are processed tile by tile.\n\nIn the context of matrix multiplication, this means dividing matrices A and B into blocks (tiles), loading these tiles into shared memory, and then performing the multiplication on these smaller blocks. This approach allows the threads to reuse data stored in the fast shared memory, reducing the need to access global memory repeatedly.\n\nHere\u2019s how it works:\n\n- **Loading Tiles into Shared Memory**: Each block of threads cooperatively loads a tile of matrix A and a corresponding tile of matrix B from global memory into shared memory. This operation is done once per tile, and then the tile is reused multiple times by the threads in the block.\n- **Computing Partial Products**: Once the tiles are loaded into shared memory, each thread computes a partial product. Since all threads in a block are working on the same tiles in shared memory, they can efficiently reuse the data without additional global memory accesses.\n- **Accumulating Results**: After computing the partial products for one tile, the threads load the next tiles from matrices A and B into shared memory and repeat the process. The results are accumulated in a register (or local memory), and once all tiles have been processed, the final value for the output matrix element is written back to global memory.\n\n![Tiled Matrix multiplication illustration](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/illustration_tiling.png)Tiled Matrix multiplication illustration (source https://cnugteren.github.io/tutorial/pages/page4.html)\n\n**Practical Considerations**\n\nWhen implementing cached tiled matrix multiplication, several factors are considered:\n\n- **Tile Size**: The size of the tiles should be chosen to balance the trade-off between the amount of data that can fit into shared memory and the number of global memory accesses.\n- **Memory Coalescing**: the global memory accesses are coalesced, which means that adjacent threads access adjacent memory locations.\n- **Occupancy**: The number of threads per block and the number of", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Fine-tuning LLMs to 1.58bit: extreme quantization made easy", "description": "Hugging Face Blog post titled  Fine-tuning LLMs to 1.58bit: extreme quantization made easy", "url": "https://huggingface.co/blog/1_58_llm_extreme_quantization", "source": "hf", "id": "6b7bde0f-c35c-465a-9ace-ba959bd07f4d"}, "page_content": " element is written back to global memory.\n\n![Tiled Matrix multiplication illustration](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/illustration_tiling.png)Tiled Matrix multiplication illustration (source https://cnugteren.github.io/tutorial/pages/page4.html)\n\n**Practical Considerations**\n\nWhen implementing cached tiled matrix multiplication, several factors are considered:\n\n- **Tile Size**: The size of the tiles should be chosen to balance the trade-off between the amount of data that can fit into shared memory and the number of global memory accesses.\n- **Memory Coalescing**: the global memory accesses are coalesced, which means that adjacent threads access adjacent memory locations.\n- **Occupancy**: The number of threads per block and the number of blocks in the grid should be chosen to ensure high occupancy, which means having as many active warps (a warp is a set of 32 threads) as possible on the GPU to hide memory latency.\n\n### Triton Kernel\n\nHere is the kernel in triton we benchmarked :\n\n```python\n@triton.autotune(\n    configs=get_cuda_autotune_config(),\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef matmul_kernel(\n        a_ptr, b_ptr, c_ptr,\n        M, N, K,\n        stride_am, stride_ak,\n        stride_bk, stride_bn,\n        stride_cm, stride_cn,\n        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n        GROUP_SIZE_M: tl.constexpr,\n):\n\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + ((pid % num_pid_in_group) % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.int32)\n\n    for i in range(4) :\n        b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n        for j in range(0, tl.cdiv(K // 4, BLOCK_SIZE_K) ):\n            k = i * tl.cdiv(K // 4, BLOCK_SIZE_K) + j\n\n            # BLOCK_SIZE_K must be a divisor of K / 4\n            a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0)\n            b_uint8 = tl.load(b_ptrs, mask=offs_k[:, None] < K // 4 - j * BLOCK_SIZE_K, other=0)\n            mask = 3<<(2*i)\n            b = ((b_uint8 & mask) >> (2*i))\n\n            # We accumulate the tiles along the K dimension.\n            tensor_full = tl.full((1,), 1, dtype=tl.int8)\n\n            accumulator += tl.dot(a, (b.to(tl.int8) - tensor_full), out_dtype=tl.int32)\n\n            a_ptrs += BLOCK_SIZE_K * stride_ak\n            b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    c = accumulator\n\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Fine-tuning LLMs to 1.58bit: extreme quantization made easy", "description": "Hugging Face Blog post titled  Fine-tuning LLMs to 1.58bit: extreme quantization made easy", "url": "https://huggingface.co/blog/1_58_llm_extreme_quantization", "source": "hf", "id": "7c171ab6-a510-4097-bc58-9dbbdd8c7518"}, "page_content": " other=0)\n            mask = 3<<(2*i)\n            b = ((b_uint8 & mask) >> (2*i))\n\n            # We accumulate the tiles along the K dimension.\n            tensor_full = tl.full((1,), 1, dtype=tl.int8)\n\n            accumulator += tl.dot(a, (b.to(tl.int8) - tensor_full), out_dtype=tl.int32)\n\n            a_ptrs += BLOCK_SIZE_K * stride_ak\n            b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    c = accumulator\n\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, c, mask=c_mask)\n\ndef matmul(a, b):\n    assert a.shape[1] == b.shape[0] * 4, \"Incompatible dimensions, the weight matrix need to be packed\"\n    assert a.is_contiguous(), \"Matrix A must be contiguous\"\n    M, K = a.shape\n    _, N = b.shape\n    c = torch.empty((M, N), device=a.device, dtype=torch.float16)\n    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']), )\n    matmul_kernel[grid](\n        a, b, c,\n        M, N, K,\n        a.stride(0), a.stride(1),\n        b.stride(0), b.stride(1),\n        c.stride(0), c.stride(1),\n    )\n    return c\n```\n\n### Code Breakdown\n\n1. **Determining Tile Positions**\n\nThe kernel first determines which tile (block) of the output matrix each thread block is responsible for:\n\n- `pid` is the unique identifier for each thread block, obtained using `tl.program_id(axis=0)`.\n- The grid is divided into groups of thread blocks (`GROUP_SIZE_M`). Each group processes a portion of the output matrix.\n- `pid_m` and `pid_n` are the coordinates of the tile in the M and N dimensions, respectively.\n- Offsets (`offs_am`, `offs_bn`, `offs_k`) are calculated to determine which elements of matrices A and B each thread in the block will work on\n\n2. **Loading and Computing Tiles**\n\nThe kernel uses a loop to iterate over the K dimension in chunks of `BLOCK_SIZE_K`. For each chunk:\n\n- **Load Tiles**: tiles from matrices A and B are loaded from global memory.\n- **Unpacking Matrix B**: The kernel assumes that matrix B is packed with `int8` values, meaning each element actually represents four smaller values packed into one byte. The unpacking happens within the loop:\n  - `b_uint8` is loaded from global memory as packed `int8`.\n  - Each packed value is unpacked to obtain the actual weight values used for computation.\n- **Dot Product**: The kernel computes the dot product of the loaded tiles from A and B, accumulating the results in the `accumulator`. The `accumulator` stores the partial results for the tile of the output matrix C.\n\n3. **Storing Results**\n\nAfter all tiles along the K dimension have been processed, the final results stored in the `accumulator` are converted to `float16` and written back to the corresponding tile of matrix C in global memory. The writing process respects memory boundaries using a mask to ensure that only valid elements are written.\n\nFor a more detailed explanation of the code, checkout this [PR](https://github.com/linkedin/Liger-Kernel/pull/195/files)\n\n### Benchmark\n\nWe benchmarked our kernel against the method of unpacking the weights using `@torch.compile` followed by performing the matmul in BF16 precision, and found that both approaches achieved approximately the same performance. To ensure accurate benchmarking, we performed the matmul operation over 2000 iterations and averaged the time taken during the last 1000 iterations, to eliminate any inefficiencies related to initial loading or compilation. Below is a graph showing the benchmark results. We also tested various matrix sizes, with the x-axis representing the number of multiplications on a log scale, and the y-axis showing the average time in ms.\n\n![Triton kernel compared to torch.compile](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/without_bitblas.png)Triton kernel compared to torch.compile\n\nWe also tried using BitBlas, which is a software library designed to perform matrix operations with mixed precision.", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Fine-tuning LLMs to 1.58bit: extreme quantization made easy", "description": "Hugging Face Blog post titled  Fine-tuning LLMs to 1.58bit: extreme quantization made easy", "url": "https://huggingface.co/blog/1_58_llm_extreme_quantization", "source": "hf", "id": "9501a527-3ec5-4cdd-aa09-5d04a02a2c4f"}, "page_content": ".compile` followed by performing the matmul in BF16 precision, and found that both approaches achieved approximately the same performance. To ensure accurate benchmarking, we performed the matmul operation over 2000 iterations and averaged the time taken during the last 1000 iterations, to eliminate any inefficiencies related to initial loading or compilation. Below is a graph showing the benchmark results. We also tested various matrix sizes, with the x-axis representing the number of multiplications on a log scale, and the y-axis showing the average time in ms.\n\n![Triton kernel compared to torch.compile](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/without_bitblas.png)Triton kernel compared to torch.compile\n\nWe also tried using BitBlas, which is a software library designed to perform matrix operations with mixed precision. It helps optimize these operations by allowing calculations to be done in lower precision formats like INT8, INT4, or even INT2, instead of the traditional FP32 or FP16 formats.\n\nThe benchmark results are promising, as BitBlas outperforms both our custom kernel and Torch's `matmul` function in low precision, as shown in the graph.\n\n![Bitblas benchmark](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/with_bitblas.png)Bitblas benchmark\n\nHowever, during model loading, BitBlas needs to compile kernels tailored to the shape of the weight matrix and store them in a local database, which can increase the initial loading time.\n\n## Conclusion\n\nIn conclusion, as LLMs continue to expand, reducing their computational demands through quantization is essential. This blog has explored the approach of 1.58-bit quantization, which uses ternary weights. While pre-training models in 1.58 bits is resource-intensive, we\u2019ve demonstrated that, with some tricks, it\u2019s possible to fine-tune existing models to this precision level, achieving efficient performance without sacrificing accuracy. By optimizing inference speed through specialized kernels, BitNet opens new possibilities for making LLMs more practical and scalable.\n\n## Acknowledgements\n\nWe would like to express our sincere gratitude to Leandro von Werra, Thomas Wolf, and Marc Sun for their invaluable assistance and insights throughout this project. We also extend our thanks to Omar Sanseviero and Pedro Cuenca for their contributions in refining this blog post, helping to communicate our findings clearly and effectively to the AI community.\nFurthermore, we want to acknowledge the GeneralAI team for their pioneering work on the BitNet project. Their research has been foundational to our efforts, and we are particularly grateful for the clear and precise figures provided in their paper.\n\n## Additional Resources\n\n1. H. Wang et al., _BitNet: Scaling 1-bit Transformers for Large Language Models_. [arxiv paper](https://arxiv.org/pdf/2310.11453)\n2. S. Ma et al., _The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits_. [arxiv paper](https://arxiv.org/pdf/2402.17764)\n3. S. Ma et al., _The Era of 1-bit LLMs: Training Tips, Code and FAQ_. [link](https://github.com/microsoft/unilm/blob/master/bitnet/The-Era-of-1-bit-LLMs__Training_Tips_Code_FAQ.pdf)\n4. RJ. Honicky, _Are All Large Language Models Really in 1.58 Bits?_. [blogpost](https://learning-exhaust.hashnode.dev/are-all-large-language-models-really-in-158-bits)\n5. L. Mao, _CUDA Matrix Multiplication Optimization_. [blogpost](https://leimao.github.io/article/CUDA-Matrix-Multiplication-Optimization/)\n6. _Tutorial: OpenCL SGEMM tuning for Kepler_. [link](https://cnugteren.github.io/tutorial/pages/page4.html)\n7. _CUDAMODE_. [github](https://github.com/cuda-mode), [youtube](https://www.youtube.com/channel/UCJgIbYl6C5no72a0NUAPcTA)\n8. Wen-mei W. Hwu, David B. Kirk, Izzat El Hajj, _Programming Massively Parallel Processors : A Hands-on Approach_\n\nMore Articles from our Blog\n\n[![](https://huggingface.co/blog/assets/rteb/thumbnail.png)\\\\\n\\\\\nnlpevaluationretrieval\\\\\n\\\\\n**Introducing RTEB: A New Standard for Retrieval Evaluation**\\\\\n\\\\\n-![](https://cdn-avatars.huggingface.co/v1/production/uploads/61f33092a92c9a858b654991/jFRUSeZ6DnI27dlCAQRHq.jpeg)\\\\\n-![](https://cdn-avatars.huggingface.co/v1/production/uploads/5ff5943752c26e9bc240bada/Exyzf3C_gJ2KdsL4K5_", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Fine-tuning LLMs to 1.58bit: extreme quantization made easy", "description": "Hugging Face Blog post titled  Fine-tuning LLMs to 1.58bit: extreme quantization made easy", "url": "https://huggingface.co/blog/1_58_llm_extreme_quantization", "source": "hf", "id": "26728540-9e5d-4883-a012-ad6567af8662"}, "page_content": "zzat El Hajj, _Programming Massively Parallel Processors : A Hands-on Approach_\n\nMore Articles from our Blog\n\n[![](https://huggingface.co/blog/assets/rteb/thumbnail.png)\\\\\n\\\\\nnlpevaluationretrieval\\\\\n\\\\\n**Introducing RTEB: A New Standard for Retrieval Evaluation**\\\\\n\\\\\n-![](https://cdn-avatars.huggingface.co/v1/production/uploads/61f33092a92c9a858b654991/jFRUSeZ6DnI27dlCAQRHq.jpeg)\\\\\n-![](https://cdn-avatars.huggingface.co/v1/production/uploads/5ff5943752c26e9bc240bada/Exyzf3C_gJ2KdsL4K5_cq.png)\\\\\n-![](https://huggingface.co/avatars/7a4067accdd1005f78c3c4adad3ee0a5.svg)\\\\\n-![](https://cdn-avatars.huggingface.co/v1/production/uploads/64cc0e80a257a3212c0c4b24/wqs6WZN8-3OQthcnQXgN7.png)\\\\\n- +2\\\\\n\\\\\n128\\\\\n\\\\\nOctober 1, 2025](https://huggingface.co/blog/rteb)\n\n[![](https://huggingface.co/blog/assets/mmbert/thumbnail.png)\\\\\n\\\\\nllmnlpcommunity\\\\\n\\\\\n**mmBERT: ModernBERT goes Multilingual**\\\\\n\\\\\n-![](https://huggingface.co/avatars/ce271d1686f6e5ee1f5b2d429cb60de6.svg)\\\\\n-![](https://cdn-avatars.huggingface.co/v1/production/uploads/6362d9712691058b19de1ba4/c9QrA2oE6lcs_46ShaTY1.jpeg)\\\\\n-![](https://huggingface.co/avatars/f73424b8fc72073cb0916e2d2cf9ca56.svg)\\\\\n-![](https://cdn-avatars.huggingface.co/v1/production/uploads/1671479459688-63a0c07a3c8841cfe2cd1e70.jpeg)\\\\\n- +2\\\\\n\\\\\n128\\\\\n\\\\\nSeptember 9, 2025](https://huggingface.co/blog/mmbert)[SEP]", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Fine-tune Llama 3.1 Ultra-Efficiently with Unsloth", "description": "A Blog post by Maxime Labonne on Hugging Face", "url": "https://huggingface.co/blog/mlabonne/sft-llama3", "source": "hf", "id": "3d08714a-8df2-4f69-a847-0c0bca5ce0e5"}, "page_content": "[CLS]Fine-tune Llama 3.1 Ultra-Efficiently with Unsloth\n\n_A beginner's guide to state-of-the-art supervised fine-tuning_\n\n[![](https://i.imgur.com/jUDo6ID.jpeg)](https://i.imgur.com/jUDo6ID.jpeg)\n\nThe recent release of Llama 3.1 offers models with an incredible level of performance, closing the gap between closed-source and open-weight models. Instead of using frozen, general-purpose LLMs like GPT-4o and Claude 3.5, you can fine-tune Llama 3.1 for your specific use cases to achieve better performance and customizability at a lower cost.\n\n[![](https://i.imgur.com/u0rJPa6.png)](https://i.imgur.com/u0rJPa6.png)\n\nIn this article, we will provide a comprehensive overview of supervised fine-tuning. We will compare it to prompt engineering to understand when it makes sense to use it, detail the main techniques with their pros and cons, and introduce major concepts, such as LoRA hyperparameters, storage formats, and chat templates. Finally, we will implement it in practice by fine-tuning Llama 3.1 8B in Google Colab with state-of-the-art optimization using Unsloth.\n\nAll the code used in this article is available on [Google Colab](https://colab.research.google.com/drive/164cg_O7SV7G8kZr_JXqLd6VC7pd86-1Z#scrollTo=PoPKQjga6obN) and in the [LLM Course](https://github.com/mlabonne/llm-course). Special thanks to Daniel Han for answering my questions.\n\n## \ud83d\udd27 Supervised Fine-Tuning\n\n[![](https://i.imgur.com/0akg8cN.png)](https://i.imgur.com/0akg8cN.png)\n\nSupervised Fine-Tuning (SFT) is a method to **improve and customize** pre-trained LLMs. It involves retraining base models on a smaller dataset of instructions and answers. The main goal is to transform a basic model that predicts text into an assistant that can follow instructions and answer questions. SFT can also enhance the model's overall performance, add new knowledge, or adapt it to specific tasks and domains. Fine-tuned models can then go through an optional preference alignment stage (see [my article about DPO](https://mlabonne.github.io/blog/posts/Fine_tune_Mistral_7b_with_DPO.html)) to remove unwanted responses, modify their style, and more.\n\nThe following figure shows an instruction sample. It includes a system prompt to steer the model, a user prompt to provide a task, and the output the model is expected to generate. You can find a list of high-quality open-source instruction datasets in the [\ud83d\udcbe LLM Datasets](https://github.com/mlabonne/llm-datasets) GitHub repo.\n\n[![](https://i.imgur.com/RqlJEtH.png)](https://i.imgur.com/RqlJEtH.png)\n\nBefore considering SFT, I recommend trying prompt engineering techniques like **few-shot prompting** or **retrieval augmented generation** (RAG). In practice, these methods can solve many problems without the need for fine-tuning, using either closed-source or open-weight models (e.g., Llama 3.1 Instruct). If this approach doesn't meet your objectives (in terms of quality, cost, latency, etc.), then SFT becomes a viable option when instruction data is available. Note that SFT also offers benefits like additional control and customizability to create personalized LLMs.\n\nHowever, SFT has limitations. It works best when leveraging knowledge already present in the base model. Learning completely new information like an unknown language can be challenging and lead to more frequent hallucinations. For new domains unknown to the base model, it is recommended to continuously pre-train it on a raw dataset first.\n\nOn the opposite end of the spectrum, instruct models (i.e., already fine-tuned models) can already be very close to your requirements. For example, a model might perform very well but state that it was trained by OpenAI or Meta instead of you. In this case, you might want to slightly steer the instruct model's behavior using preference alignment. By providing chosen and rejected samples for a small set of instructions (between 100 and 1000 samples), you can force the LLM to say that you trained it instead of OpenAI.\n\n## \u2696\ufe0f SFT Techniques\n\nThe three most popular SFT techniques are full fine-tuning, LoRA, and QLoRA.\n\n[![](https://i.imgur.com/P6sLsxl.png)](https://i.imgur.com/P6sLsxl.png)\n\n**Full fine-tuning** is the most straightforward SFT technique. It involves retraining all parameters of a pre-trained model on an instruction dataset. This method often provides the best results but requires significant computational resources (several high-end GPUs are required to fine-tune a 8B model). Because it modifies the entire model, it is also the most destructive method and can lead to the catastrophic forgetting of previous skills and knowledge.\n", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Fine-tune Llama 3.1 Ultra-Efficiently with Unsloth", "description": "A Blog post by Maxime Labonne on Hugging Face", "url": "https://huggingface.co/blog/mlabonne/sft-llama3", "source": "hf", "id": "f0c4dd07-37b3-41c0-ab75-60a904daf065"}, "page_content": " samples for a small set of instructions (between 100 and 1000 samples), you can force the LLM to say that you trained it instead of OpenAI.\n\n## \u2696\ufe0f SFT Techniques\n\nThe three most popular SFT techniques are full fine-tuning, LoRA, and QLoRA.\n\n[![](https://i.imgur.com/P6sLsxl.png)](https://i.imgur.com/P6sLsxl.png)\n\n**Full fine-tuning** is the most straightforward SFT technique. It involves retraining all parameters of a pre-trained model on an instruction dataset. This method often provides the best results but requires significant computational resources (several high-end GPUs are required to fine-tune a 8B model). Because it modifies the entire model, it is also the most destructive method and can lead to the catastrophic forgetting of previous skills and knowledge.\n\n**Low-Rank Adaptation (LoRA)** is a popular parameter-efficient fine-tuning technique. Instead of retraining the entire model, it freezes the weights and introduces small adapters (low-rank matrices) at each targeted layer. This allows LoRA to train a number of parameters that is drastically lower than full fine-tuning (less than 1%), reducing both memory usage and training time. This method is non-destructive since the original parameters are frozen, and adapters can then be switched or combined at will.\n\n**QLoRA (Quantization-aware Low-Rank Adaptation)** is an extension of LoRA that offers even greater memory savings. It provides up to 33% additional memory reduction compared to standard LoRA, making it particularly useful when GPU memory is constrained. This increased efficiency comes at the cost of longer training times, with QLoRA typically taking about 39% more time to train than regular LoRA.\n\nWhile QLoRA requires more training time, its substantial memory savings can make it the only viable option in scenarios where GPU memory is limited. For this reason, this is the technique we will use in the next section to fine-tune a Llama 3.1 8B model on Google Colab.\n\n## \ud83e\udd99 Fine-Tune Llama 3.1 8B\n\nTo efficiently fine-tune a [Llama 3.1 8B](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B) model, we'll use the [Unsloth](https://github.com/unslothai/unsloth) library by Daniel and Michael Han. Thanks to its custom kernels, Unsloth provides 2x faster training and 60% memory use compared to other options, making it ideal in a constrained environment like Colab. Unfortunately, Unsloth only supports single-GPU settings at the moment. For multi-GPU settings, I recommend popular alternatives like [TRL](https://huggingface.co/docs/trl/en/index) and [Axolotl](https://github.com/OpenAccess-AI-Collective/axolotl) (both also include Unsloth as a backend).\n\nIn this example, we will QLoRA fine-tune it on the [mlabonne/FineTome-100k](https://huggingface.co/datasets/mlabonne/FineTome-100k) dataset. It's a subset of [arcee-ai/The-Tome](https://huggingface.co/datasets/arcee-ai/The-Tome) (without [arcee-ai/qwen2-72b-magpie-en](https://huggingface.co/datasets/arcee-ai/qwen2-72b-magpie-en)) that I re-filtered using [HuggingFaceFW/fineweb-edu-classifier](https://huggingface.co/HuggingFaceFW/fineweb-edu-classifier). Note that this classifier wasn't designed for instruction data quality evaluation, but we can use it as a rough proxy. The resulting FineTome is an ultra-high quality dataset that includes conversations, reasoning problems, function calling, and more.\n\nLet's start by installing all the required libraries.\n\n```python\n!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes\n```\n\nOnce installed, we can import them as follows.\n\n```python\nimport torch\nfrom trl import SFTTrainer\nfrom datasets import load_dataset\nfrom transformers import TrainingArguments, TextStreamer\nfrom unsloth.chat_templates import get_chat_template\nfrom unsloth import FastLanguageModel, is_bfloat16_supported\n```\n\nLet's now load the model. Since we want to use QLoRA, I chose the pre-quantized [unsloth/Meta-Llama-3.1-8B-bnb-4bit](https://huggingface.co/unsloth/Meta-Llama-3.1-8B-bnb-4bit). This 4-bit precision version of [meta-llama/Meta-Llama-3.1-8B](https://huggingface.co/blog/mlabonne/meta-llama/", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Fine-tune Llama 3.1 Ultra-Efficiently with Unsloth", "description": "A Blog post by Maxime Labonne on Hugging Face", "url": "https://huggingface.co/blog/mlabonne/sft-llama3", "source": "hf", "id": "99b42050-ba8c-442f-8d5a-5c2dd511ddf5"}, "page_content": ", we can import them as follows.\n\n```python\nimport torch\nfrom trl import SFTTrainer\nfrom datasets import load_dataset\nfrom transformers import TrainingArguments, TextStreamer\nfrom unsloth.chat_templates import get_chat_template\nfrom unsloth import FastLanguageModel, is_bfloat16_supported\n```\n\nLet's now load the model. Since we want to use QLoRA, I chose the pre-quantized [unsloth/Meta-Llama-3.1-8B-bnb-4bit](https://huggingface.co/unsloth/Meta-Llama-3.1-8B-bnb-4bit). This 4-bit precision version of [meta-llama/Meta-Llama-3.1-8B](https://huggingface.co/blog/mlabonne/meta-llama/Meta-Llama-3.1-8B) is significantly smaller (5.4 GB) and faster to download compared to the original 16-bit precision model (16 GB). We load in NF4 format using the bitsandbytes library.\n\nWhen loading the model, we must specify a maximum sequence length, which restricts its context window. Llama 3.1 supports up to 128k context length, but we will set it to 2,048 in this example since it consumes more compute and VRAM. Finally, the `dtype` parameter automatically detects if your GPU supports the [BF16 format](https://mlabonne.github.io/blog/posts/Introduction_to_Weight_Quantization.html#background-on-floating-point-representation) for more stability during training (this feature is restricted to Ampere and more recent GPUs).\n\n```python\nmax_seq_length = 2048\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",\n    max_seq_length=max_seq_length,\n    load_in_4bit=True,\n    dtype=None,\n)\n```\n\nNow that our model is loaded in 4-bit precision, we want to prepare it for parameter-efficient fine-tuning with LoRA adapters. LoRA has three important parameters:\n\n- **Rank** (r), which determines LoRA matrix size. Rank typically starts at 8 but can go up to 256. Higher ranks can store more information but increase the computational and memory cost of LoRA. We set it to 16 here.\n- **Alpha** (\u03b1), a scaling factor for updates. Alpha directly impacts the adapters' contribution and is often set to 1x or 2x the rank value.\n- **Target modules**: LoRA can be applied to various model components, including attention mechanisms (Q, K, V matrices), output projections, feed-forward blocks, and linear output layers. While initially focused on attention mechanisms, extending LoRA to other components has shown benefits. However, adapting more modules increases the number of trainable parameters and memory needs.\n\nHere, we set r=16, \u03b1=16, and target every linear module to maximize quality. We don't use dropout and biases for faster training.\n\nIn addition, we will use [Rank-Stabilized LoRA](https://arxiv.org/abs/2312.03732) (rsLoRA), which modifies the scaling factor of LoRA adapters to be proportional to 1/\u221ar instead of 1/r. This stabilizes learning (especially for higher adapter ranks) and allows for improved fine-tuning performance as rank increases. Gradient checkpointing is handled by Unsloth to offload input and output embeddings to disk and save VRAM.\n\n```python\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=16,\n    lora_alpha=16,\n    lora_dropout=0,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"up_proj\", \"down_proj\", \"o_proj\", \"gate_proj\"],\n    use_rslora=True,\n    use_gradient_checkpointing=\"unsloth\"\n)\n```\n\nWith this LoRA configuration, we'll only train 42 million out of 8 billion parameters (0.5196%). This shows how much more efficient LoRA is compared to full fine-tuning.\n\nLet's now load and prepare our dataset. Instruction datasets are stored in a **particular format**: it can be Alpaca, ShareGPT, OpenAI, etc. First, we want to parse this format to retrieve our instructions and answers. Our [mlabonne/FineTome-100k](https://huggingface.co/datasets/mlabonne/FineTome-100k) dataset uses the ShareGPT format with a unique \"conversations\" column containing messages in JSONL. Unlike simpler formats like Alpaca, ShareGPT is ideal for storing multi-turn conversations, which is closer to how users interact with LLMs.\n\nOnce our instruction-answer pairs are parsed, we want to reformat them to follow a **chat template**. Chat templates are a way to structure conversations between users and models. They typically include special tokens to identify the beginning and the end of a message, who's speaking, etc. Base models don't have chat templates so we can choose any: ChatML, Llama3, Mistral", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Fine-tune Llama 3.1 Ultra-Efficiently with Unsloth", "description": "A Blog post by Maxime Labonne on Hugging Face", "url": "https://huggingface.co/blog/mlabonne/sft-llama3", "source": "hf", "id": "172d385d-4b2f-45b1-bad3-4c17181124f4"}, "page_content": " can be Alpaca, ShareGPT, OpenAI, etc. First, we want to parse this format to retrieve our instructions and answers. Our [mlabonne/FineTome-100k](https://huggingface.co/datasets/mlabonne/FineTome-100k) dataset uses the ShareGPT format with a unique \"conversations\" column containing messages in JSONL. Unlike simpler formats like Alpaca, ShareGPT is ideal for storing multi-turn conversations, which is closer to how users interact with LLMs.\n\nOnce our instruction-answer pairs are parsed, we want to reformat them to follow a **chat template**. Chat templates are a way to structure conversations between users and models. They typically include special tokens to identify the beginning and the end of a message, who's speaking, etc. Base models don't have chat templates so we can choose any: ChatML, Llama3, Mistral, etc. In the open-source community, the ChatML template (originally from OpenAI) is a popular option. It simply adds two special tokens (`<|im_start|>` and `<|im_end|>`) to indicate who's speaking.\n\nIf we apply this template to the previous instruction sample, here's what we get:\n\n```\n<|im_start|>system\nYou are a helpful assistant, who always provide explanation. Think like you are answering to a five year old.<|im_end|>\n<|im_start|>user\nRemove the spaces from the following sentence: It prevents users to suspect that there are some hidden products installed on theirs device.\n<|im_end|>\n<|im_start|>assistant\nItpreventsuserstosuspectthattherearesomehiddenproductsinstalledontheirsdevice.<|im_end|>\n```\n\nIn the following code block, we parse our ShareGPT dataset with the `mapping` parameter and include the ChatML template. We then load and process the entire dataset to apply the chat template to every conversation.\n\n```python\ntokenizer = get_chat_template(\n    tokenizer,\n    mapping={\"role\": \"from\", \"content\": \"value\", \"user\": \"human\", \"assistant\": \"gpt\"},\n    chat_template=\"chatml\",\n)\n\ndef apply_template(examples):\n    messages = examples[\"conversations\"]\n    text = [tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=False) for message in messages]\n    return {\"text\": text}\n\ndataset = load_dataset(\"mlabonne/FineTome-100k\", split=\"train\")\ndataset = dataset.map(apply_template, batched=True)\n```\n\nWe're now ready to specify the training parameters for our run. I want to briefly introduce the most important hyperparameters:\n\n- **Learning rate**: It controls how strongly the model updates its parameters. Too low, and training will be slow and may get stuck in local minima. Too high, and training may become unstable or diverge, which degrades performance.\n- **LR scheduler**: It adjusts the learning rate (LR) during training, starting with a higher LR for rapid initial progress and then decreasing it in later stages. Linear and cosine schedulers are the two most common options.\n- **Batch size**: Number of samples processed before the weights are updated. Larger batch sizes generally lead to more stable gradient estimates and can improve training speed, but they also require more memory. Gradient accumulation allows for effectively larger batch sizes by accumulating gradients over multiple forward/backward passes before updating the model.\n- **Num epochs**: The number of complete passes through the training dataset. More epochs allow the model to see the data more times, potentially leading to better performance. However, too many epochs can cause overfitting.\n- **Optimizer**: Algorithm used to adjust the parameters of a model to minimize the loss function. In practice, AdamW 8-bit is strongly recommended: it performs as well as the 32-bit version while using less GPU memory. The paged version of AdamW is only interesting in distributed settings.\n- **Weight decay**: A regularization technique that adds a penalty for large weights to the loss function. It helps prevent overfitting by encouraging the model to learn simpler, more generalizable features. However, too much weight decay can impede learning.\n- **Warmup steps**: A period at the beginning of training where the learning rate is gradually increased from a small value to the initial learning rate. Warmup can help stabilize early training, especially with large learning rates or batch sizes, by allowing the model to adjust to the data distribution before making large updates.\n- **Packing**: Batches have a pre-defined sequence length. Instead of assigning one batch per sample, we can combine multiple small samples in one batch, increasing efficiency.\n\nI trained the model on the entire dataset (100k samples) using an A100 GPU (40 GB of VRAM) on Google Colab. The training took 4 hours and 45 minutes. Of course, you can use smaller GPUs with less VRAM and a smaller batch size, but they're not nearly as fast. For example, it takes roughly 19 hours and 40 minutes on an L4 and a whopping 47 hours on a free T4.\n\nIn this case, I recommend only loading a subset of the dataset to speed up training. You can do it by modifying the previous code block, like `dataset = load_dataset(\"mlabonne/FineTome-100k\", split=\"train[:10000]\")` to only", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Fine-tune Llama 3.1 Ultra-Efficiently with Unsloth", "description": "A Blog post by Maxime Labonne on Hugging Face", "url": "https://huggingface.co/blog/mlabonne/sft-llama3", "source": "hf", "id": "070d7eb2-3788-4554-a806-2ba19a67d7b1"}, "page_content": " the data distribution before making large updates.\n- **Packing**: Batches have a pre-defined sequence length. Instead of assigning one batch per sample, we can combine multiple small samples in one batch, increasing efficiency.\n\nI trained the model on the entire dataset (100k samples) using an A100 GPU (40 GB of VRAM) on Google Colab. The training took 4 hours and 45 minutes. Of course, you can use smaller GPUs with less VRAM and a smaller batch size, but they're not nearly as fast. For example, it takes roughly 19 hours and 40 minutes on an L4 and a whopping 47 hours on a free T4.\n\nIn this case, I recommend only loading a subset of the dataset to speed up training. You can do it by modifying the previous code block, like `dataset = load_dataset(\"mlabonne/FineTome-100k\", split=\"train[:10000]\")` to only load 10k samples. Alternatively, you can use cheaper cloud GPU providers like Paperspace, RunPod, or Lambda Labs.\n\n```python\ntrainer=SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    dataset_num_proc=2,\n    packing=True,\n    args=TrainingArguments(\n        learning_rate=3e-4,\n        lr_scheduler_type=\"linear\",\n        per_device_train_batch_size=8,\n        gradient_accumulation_steps=2,\n        num_train_epochs=1,\n        fp16=not is_bfloat16_supported(),\n        bf16=is_bfloat16_supported(),\n        logging_steps=1,\n        optim=\"adamw_8bit\",\n        weight_decay=0.01,\n        warmup_steps=10,\n        output_dir=\"output\",\n        seed=0,\n    ),\n)\n\ntrainer.train()\n```\n\nNow that the model is trained, let's test it with a simple prompt. This is not a rigorous evaluation but just a quick check to detect potential issues. We use `FastLanguageModel.for_inference()` to get 2x faster inference.\n\n```python\nmodel = FastLanguageModel.for_inference(model)\n\nmessages = [\\\n    {\"from\": \"human\", \"value\": \"Is 9.11 larger than 9.9?\"},\\\n]\ninputs = tokenizer.apply_chat_template(\n    messages,\n    tokenize=True,\n    add_generation_prompt=True,\n    return_tensors=\"pt\",\n).to(\"cuda\")\n\ntext_streamer = TextStreamer(tokenizer)\n_ = model.generate(input_ids=inputs, streamer=text_streamer, max_new_tokens=128, use_cache=True)\n```\n\nThe model's response is \"9.9\", which is correct!\n\nLet's now save our trained model. If you remember the part about LoRA and QLoRA, what we trained is not the model itself but a set of adapters. There are three save methods in Unsloth: `lora` to only save the adapters, and `merged_16bit`/`merged_4bit` to merge the adapters with the model in 16-bit/ 4-bit precision.\n\nIn the following, we merge them in 16-bit precision to maximize the quality. We first save it locally in the \"model\" directory and then upload it to the Hugging Face Hub. You can find the trained model on [mlabonne/FineLlama-3.1-8B](https://huggingface.co/mlabonne/FineLlama-3.1-8B).\n\n```python\nmodel.save_pretrained_merged(\"model\", tokenizer, save_method=\"merged_16bit\")\nmodel.push_to_hub_merged(\"mlabonne/FineLlama-3.1-8B\", tokenizer, save_method=\"merged_16bit\")\n```\n\nUnsloth also allows you to directly convert your model into GGUF format. This is a quantization format created for llama.cpp and compatible with most inference engines, like [LM Studio](https://lmstudio.ai/), [Ollama](https://ollama.com/), and oobabooga's [text-generation-webui](https://github.com/oobabooga/text-generation-webui). Since you can specify different precisions (see [my article about GGUF and llama.cpp](https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html)), we'll loop over a list to quantize it in `q2_k`, `q3_k_m`, `q4_k_m`, `q5_k_m`, `q6_k`, `q8_0` and upload these quants on Hugging Face. The [mlabonne/FineLlama-3.1-8B-GGUF](https://huggingface.co/mlabonne/FineLlama-3.1-8B-GGUF) contains all our GGUFs.\n\n```python\nquant_", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Fine-tune Llama 3.1 Ultra-Efficiently with Unsloth", "description": "A Blog post by Maxime Labonne on Hugging Face", "url": "https://huggingface.co/blog/mlabonne/sft-llama3", "source": "hf", "id": "2115cc0b-f310-4a60-8f89-55d68ab4bbfe"}, "page_content": "webui](https://github.com/oobabooga/text-generation-webui). Since you can specify different precisions (see [my article about GGUF and llama.cpp](https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html)), we'll loop over a list to quantize it in `q2_k`, `q3_k_m`, `q4_k_m`, `q5_k_m`, `q6_k`, `q8_0` and upload these quants on Hugging Face. The [mlabonne/FineLlama-3.1-8B-GGUF](https://huggingface.co/mlabonne/FineLlama-3.1-8B-GGUF) contains all our GGUFs.\n\n```python\nquant_methods = [\"q2_k\", \"q3_k_m\", \"q4_k_m\", \"q5_k_m\", \"q6_k\", \"q8_0\"]\nfor quant in quant_methods:\n    model.push_to_hub_gguf(\"mlabonne/FineLlama-3.1-8B-GGUF\", tokenizer, quant)\n```\n\nCongratulations, we fine-tuned a model from scratch and uploaded quants you can now use in your favorite inference engine. Feel free to try the final model available on [mlabonne/FineLlama-3.1-8B-GGUF](https://huggingface.co/mlabonne/FineLlama-3.1-8B-GGUF). What to do now? Here are some ideas on how to use your model:\n\n- **Evaluate** it on the [Open LLM Leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard) (you can submit it for free) or using other evals like in [LLM AutoEval](https://github.com/mlabonne/llm-autoeval).\n- **Align** it with Direct Preference Optimization using a preference dataset like [mlabonne/orpo-dpo-mix-40k](https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k) to boost performance.\n- **Quantize** it in other formats like EXL2, AWQ, GPTQ, or HQQ for faster inference or lower precision using [AutoQuant](https://colab.research.google.com/drive/1b6nqC7UZVt8bx4MksX7s656GXPM-eWw4?usp=sharing).\n- **Deploy** it on a Hugging Face Space with [ZeroChat](https://colab.research.google.com/drive/1LcVUW5wsJTO2NGmozjji5CkC--646LgC) for models that have been sufficiently trained to follow a chat template (~20k samples).\n\n## Conclusion\n\nThis article provided a comprehensive overview of supervised fine-tuning and how to apply it in practice to a Llama 3.1 8B model. By leveraging QLoRA's efficient memory usage, we managed to fine-tune an 8B LLM on a super high-quality dataset with limited GPU resources. We also provided more efficient alternatives for bigger runs and suggestions for further steps, including evaluation, preference alignment, quantization, and deployment.\n\nI hope this guide was useful. If you're interested in learning more about LLMs, I recommend checking the [LLM Course](https://github.com/mlabonne/llm-course). If you enjoyed this article, follow me on X [@maximelabonne](https://x.com/maximelabonne) and on Hugging Face [@mlabonne](https://huggingface.co/mlabonne). Good luck fine-tuning models![SEP]", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Fine-tuning a large language model on Kaggle Notebooks (or even on your own computer) for solving real-world tasks", "description": "A Blog post by Luca Massaron on Hugging Face", "url": "https://huggingface.co/blog/lmassaron/fine-tuning-llms-on-kaggle-notebooks", "source": "hf", "id": "9b1495ae-e0a4-4fdc-9809-81a41e6b5e0c"}, "page_content": "[CLS]Fine-tuning a large language model on Kaggle Notebooks (or even on your own computer) for solving real-world tasks\n\n[![Image](https://miro.medium.com/v2/resize:fit:720/format:webp/1*kNzCq6jDkg01_6W8x12Hgw.jpeg)](https://miro.medium.com/v2/resize:fit:720/format:webp/1*kNzCq6jDkg01_6W8x12Hgw.jpeg)\n\ncredits: [DALL\u00b7E 3](https://openai.com/dall-e-3)\n\n## Exploring, with simple words and concepts, some theory and ideas on adapting LLMs to your needs\n\nThanks to their in-context learning, generative large language models (LLMs) are a feasible solution if you want a model to tackle your specific problem. In fact, we can provide the LLM with a few examples of the target task directly through the input prompt, which it wasn\u2019t explicitly trained on. However, this can prove dissatisfying because the LLM may need to learn the nuances of complex problems, and you cannot fit too many examples in a prompt. Also, you can host your own model on your own premises and have control of the data you provide to external sources. The solution is fine-tuning your local LLM because fine-tuning changes the behavior and increases the knowledge of an LLM model of your choice.\n\nFine-tuning requires more high-quality data, more computations, and some effort because you must prompt and code a solution. Still, it rewards you with LLMs that are less prone to hallucinate, can be hosted on your servers or even your computers, and are best suited to tasks you want the model to execute at its best. In these two short articles, I will present all the theory basics and tools to fine-tune a model for a specific problem in a Kaggle notebook, easily accessible by everyone. The theory part owes a lot to the writings by Sebastian Raschka in his community blog posts on lightning.ai, where he systematically explored the fine-tuning methods for language models.\n\nSince we\u2019ll be working with a Llama 2 model taken from Kaggle Models, you must visit the model\u2019s page on Kaggle ( [https://www.kaggle.com/models/metaresearch/llama-2](https://www.kaggle.com/models/metaresearch/llama-2)) and follow the instructions there to ask Meta for the access to their model (you can use this page: [https://ai.meta.com/resources/models-and-libraries/llama-downloads/).](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)\n\nFine-tuning for language models already has a history of working with generative models like GPT, based on decoder architectures, and embedding-centric models like BERT, which rely on encoder architectures (the E in BERT stands for encoder). This involves keeping frozen in terms of weight update a larger or lesser part of the language model and attaching a machine learning classifier (typically a logistic regression model, but it can be a support vector classifier, a random forest, or an XGBoost model) or some additional neural architecture to the end of the model. The more you keep unfrozen the original language model, the more parts of it, especially the embeddings, will adapt to your problem (and you will get better evaluation scores for your model), but that will require a lot of computation power if the model is large (and LLMs are incredibly huge in terms of weights and layers) and also a lot of data because you need a lot of evidence for correctly updating many parameters in a model. Suppose you have a few labeled examples of your task, which is extremely common for business applications and not many resources. In that case, the right solution is to keep most of the original model frozen and update the parameters of its classification terminal part.\n\nTherefore, there are increased limitations when it comes to large language models because you cannot easily have the computational power and volume of data sufficient to update its layers. Fortunately, various ingenious approaches for fine-tuning LLMs have been devised in recent years, ensuring excellent modeling results with minimal parameter training. These techniques are commonly known as parameter-efficient fine-tuning methods or PEFT. All PEFT methods involve prompt modification, adapter methods, and parameter updates:\n\n- Prompt modification involves altering the input prompt to attain the desired results. It can be achieved by hard changes when we directly change the prompt by trial and error or by soft changes when we rely on backpropagation to figure out how to enhance the embeddings of the existing prompt by learning an additional tensor of free embeddings. These methods intervene at the beginning of the architecture of LLMs.\n- Adapter methods involve adding inside the architecture of the LLM a few adaptable layers that are updated by backpropagation and modify the model\u2019s behavior. The methods intervene in the middle of the architecture of LLMs\n- Parameter updates may involve a specific part of the network weights or the network itself by a low-rank adaptation of the weights ( [https://arxiv.org/abs/2106.09685](https://arxiv.org/abs/2106.09685)), a method that \u201ccan reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by three times\u201d.\n\nIn particular, parameter updates by low-rank adaptation (LoRA) is a kind", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Fine-tuning a large language model on Kaggle Notebooks (or even on your own computer) for solving real-world tasks", "description": "A Blog post by Luca Massaron on Hugging Face", "url": "https://huggingface.co/blog/lmassaron/fine-tuning-llms-on-kaggle-notebooks", "source": "hf", "id": "bff03e5e-5da1-431e-bbc9-fbfdafefbeec"}, "page_content": " directly change the prompt by trial and error or by soft changes when we rely on backpropagation to figure out how to enhance the embeddings of the existing prompt by learning an additional tensor of free embeddings. These methods intervene at the beginning of the architecture of LLMs.\n- Adapter methods involve adding inside the architecture of the LLM a few adaptable layers that are updated by backpropagation and modify the model\u2019s behavior. The methods intervene in the middle of the architecture of LLMs\n- Parameter updates may involve a specific part of the network weights or the network itself by a low-rank adaptation of the weights ( [https://arxiv.org/abs/2106.09685](https://arxiv.org/abs/2106.09685)), a method that \u201ccan reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by three times\u201d.\n\nIn particular, parameter updates by low-rank adaptation (LoRA) is a kind of hacking the regular backpropagation updates by splitting the update matrix into two smaller matrices that, multiplied together, can give back the original update matrix. This is similar to matrix decomposition (such as SVD), where a reduction is obtained by allowing an inevitable loss in the contents of the original matrix. In our case, when training LLMs for specific tasks, a loss of its original complexity is actually permissible for the LLM to gain expertise on our task of interest.\n\nTherefore, if the update matrix dimension for a layer is 1,024 by 1,024, which equates to 1,048,576 numeric values, a decomposition into two matrices sized 1,024 by 16 and 16 by 1,024, which multiplied can return something similar to the original matrix, will decrease the numeric values to be handled to 32,768.\n\nThis matrix decomposition is left to the backpropagation of the neural network, and the hyperparameter r allows us to designate the rank of the low-rank matrices for adaptation. A smaller r corresponds to a more straightforward low-rank matrix, reducing the number of parameters for adaptation. Consequently, this can accelerate training and potentially lower computational demands. In LoRA, selecting a smaller value for r involves a trade-off between model complexity, adaptation capability, and the potential for underfitting or overfitting. Therefore, conducting experiments with various r values is crucial to strike the right balance between LoRA parameters.\n\nMoreover, after finishing fine-tuning, if we keep the low-rank matrices we used for the updates, which do not weigh much, we can reuse them by multiplication on the original LLM that we fine-tuned without any need to update the weights of the model itself directly. At this point, we can save memory and disk space by reducing the size of the LLM on which LoRA has been used. The answer is quantizing the original LLM, reducing its precision to 4-bit precision. It is just like compressing a file, and in the same way, the LLM is kept compressed (i.e., quantized) only to be expanded when it is necessary to compute the LoRA matrix reduction and update. In this way, you can tune large language models on a single GPU while preserving the performance of the LLM after fine-tuning. This approach is called QLoRA, based on the work by Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer (see [https://arxiv.org/abs/2305.14314).](https://arxiv.org/abs/2305.14314).) It is also available as an open-source project on GitHub.\n\nIn the upcoming second part of this article, I will offer references and insights into the practical aspects of working with LLMs for fine-tuning tasks, especially in resource-constrained environments like Kaggle Notebooks. I will also demonstrate how to effortlessly put these techniques into practice with just a few commands and minimal configuration settings.\n\n## Hands-on fine-tuning for financial sentiment analysis\n\n> You can find all the code in this section at this Kaggle Notebook: [Fine-tune Llama-2 for Sentiment Analysis](https://www.kaggle.com/code/lucamassaron/fine-tune-llama-2-for-sentiment-analysis)\n\nWe will deal with sentiment analysis of financial and economic information for this hands-on tutorial on fine-tuning a Llama 2 model on Kaggle Notebooks, showing how to handle such a task with limited and commonly available resources. Sentiment analysis on financial and economic information is highly relevant for businesses for several key reasons, ranging from market insights (gain valuable insights into market trends, investor confidence, and consumer behavior) to risk management (identifying potential reputational risks) to investment decisions (gauging the sentiment of stakeholders, investors, and the general public businesses can assess the potential success of various investment opportunities).\n\nBefore the technicalities of fine-tuning a large language model like Llama 2, we had to find the correct dataset to demonstrate the potentialities of fine-tuning.\n\nParticularly within finance and economic texts, annotated datasets are notably rare, with many exclusively reserved for proprietary purposes. In 2014, scholars from the Aalto University School of Business introduced a set of approximately 5,000 sentences to address the issue of insufficient training data (Malo, P., Sinha, A., Korhonen, P., Wallenius, J., & Takala, P., 2014, \u201cGood debt or bad debt: Detecting semantic orientations in economic texts.\u201d Journal of the Association for Information Science and Technology, 65\\[4\\], 782\u2013796 - [https://arxiv.org/abs/1307.", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Fine-tuning a large language model on Kaggle Notebooks (or even on your own computer) for solving real-world tasks", "description": "A Blog post by Luca Massaron on Hugging Face", "url": "https://huggingface.co/blog/lmassaron/fine-tuning-llms-on-kaggle-notebooks", "source": "hf", "id": "b7f27726-d229-4817-b310-a9955c0705d1"}, "page_content": " risks) to investment decisions (gauging the sentiment of stakeholders, investors, and the general public businesses can assess the potential success of various investment opportunities).\n\nBefore the technicalities of fine-tuning a large language model like Llama 2, we had to find the correct dataset to demonstrate the potentialities of fine-tuning.\n\nParticularly within finance and economic texts, annotated datasets are notably rare, with many exclusively reserved for proprietary purposes. In 2014, scholars from the Aalto University School of Business introduced a set of approximately 5,000 sentences to address the issue of insufficient training data (Malo, P., Sinha, A., Korhonen, P., Wallenius, J., & Takala, P., 2014, \u201cGood debt or bad debt: Detecting semantic orientations in economic texts.\u201d Journal of the Association for Information Science and Technology, 65\\[4\\], 782\u2013796 - [https://arxiv.org/abs/1307.5336](https://arxiv.org/abs/1307.5336)). This collection aimed to establish human-annotated benchmarks, serving as a standard for evaluating alternative modeling techniques. The involved annotators (16 people with adequate background knowledge of financial markets) were instructed to assess the sentences solely from an investor's perspective, evaluating whether the news potentially holds a positive, negative, or neutral impact on the stock price.\n\nThe FinancialPhraseBank dataset is a comprehensive collection that captures the sentiments of financial news headlines from the viewpoint of a retail investor. Comprising two key columns, \u201cSentiment\u201d and \u201cNews Headline,\u201d the dataset effectively classifies sentiments as negative, neutral, or positive. This structured dataset is a valuable resource for analyzing and understanding the complex dynamics of sentiment in financial news. It has been used in various studies and research initiatives since its inception in the paper published in the Journal of the Association for Information Science and Technology in 2014.\n\nThe data is available under the license [CC BY-NC-SA 3.0 DEED](https://creativecommons.org/licenses/by-nc-sa/3.0/), and it can be found complete with detailed descriptions and instructions at [https://huggingface.co/datasets/financial\\_phrasebank](https://huggingface.co/datasets/financial_phrasebank). There are also a couple of Kaggle Datasets mirrors, too. In our example, we sample from all the available data (4840 sentences from English language financial news categorized by sentiment) 900 examples for training and 900 for testing. The examples in the training and testing sets are balanced and have the same number of examples of positive, neutral, and negative samples. We also use a sample of about one hundred examples, mainly of remaining positive and neutral examples (not so many negative examples were left) for evaluation purposes during training (we just use evaluation for monitoring; no decision is taken based on such a sample).\n\nWithout much ado, we just point out to the Kaggle notebook where all the cells are commented on step by step, showing how to structure the analysis:\n\nIn this article, we will illustrate instead the logical steps of fine-tuning. From a larger perspective, as in any machine learning project, you:\n\n1. retrieve data\n2. arrange data for training, validation, and testing\n3. instantiate your model\n4. evaluate your model as it is\n5. fine-tune (train) your model\n6. evaluate your model\n\nWhen dealing with LLMs, however, it makes sense to evaluate the model, inducted just by hard prompting engineering, in order to establish a benchmark that can make sense to your work (if your LLM is already skillful enough in achieving the desired task, you actually do not need to perform any further fine-tuning).\n\nLet\u2019s now delve into the practicalities of instantiating and fine-tuning your model.\n\nFirst of all, the used packages are:\n\n- PyTorch 2.1.2 (previously 2.0.0)\n- transformers 4.36.2 (previously 4.31)\n- datasets 2.16.1\n- accelerate 0.26.1 (previously 0.23.0)\n- bitsandbytes 0.42.0 (previously 0.41.1)\n\nAs for _trl_, I picked a commit from GitHub published on Jan 22, 2024, and for _peft_, I retrieved another commit published on the same date (so both packages are as fresh as possible)\n\nThen, you need to define what LLM you are going to tune.\n\n```\nmodel_name = \"../input/llama-2/pytorch/7b-hf/1\"\n```\n\nHow choice fell on Llama 2 7b-hf, the 7B pre-trained model from Meta, converted for the Hugging Face Transformers format. Llama 2 constitutes a series of preexisting and optimized generative text models, varying in size from 7 billion to 70 billion parameters. Employing an enhanced transformer architecture, Llama 2 operates as an auto-regressive language model. Its fine-tuned iterations involve both supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF), ensuring conformity with human standards for helpfulness and safety. Apart from being an already well-performing LLM, the choice for this model resides on the fact that it is the most nimble of the Llama family and, thus, the most suitable to demonstrate how even the smaller LLMs are good choices for fine-tuning for specialistic tasks.\n\nOur next step is defining the BitsAndBytes configuration", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Fine-tuning a large language model on Kaggle Notebooks (or even on your own computer) for solving real-world tasks", "description": "A Blog post by Luca Massaron on Hugging Face", "url": "https://huggingface.co/blog/lmassaron/fine-tuning-llms-on-kaggle-notebooks", "source": "hf", "id": "b332d6f3-fb6a-4ac7-bb00-704cfa89050e"}, "page_content": "-hf/1\"\n```\n\nHow choice fell on Llama 2 7b-hf, the 7B pre-trained model from Meta, converted for the Hugging Face Transformers format. Llama 2 constitutes a series of preexisting and optimized generative text models, varying in size from 7 billion to 70 billion parameters. Employing an enhanced transformer architecture, Llama 2 operates as an auto-regressive language model. Its fine-tuned iterations involve both supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF), ensuring conformity with human standards for helpfulness and safety. Apart from being an already well-performing LLM, the choice for this model resides on the fact that it is the most nimble of the Llama family and, thus, the most suitable to demonstrate how even the smaller LLMs are good choices for fine-tuning for specialistic tasks.\n\nOur next step is defining the BitsAndBytes configuration.\n\n```\ncompute_dtype = getattr(torch, \"float16\")\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=False,\n)\n```\n\nBitsandbytes is a Python package developed by Tim Dettmers, which acts as a lightweight wrapper around CUDA custom functions, particularly 8-bit optimizers, matrix multiplication (LLM.int8()), and quantization functions. It allows running models stored in 4-bit precision: while 4-bit bitsandbytes stores weights in 4-bits, the computation still happens in 16 or 32-bit, and here any combination can be chosen (float16, bfloat16, float32, and so on). The idea behind Bitsandbytes has been formalized in the paper by Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer (see [https://arxiv.org/abs/2305.14314)](https://arxiv.org/abs/2305.14314).).\n\nYou can actually think of it as a compressor of the LLM that can allow us to safely store it both on disk and memory of a standard computer or server: the neural network is stored to 4-bit precision (normalized float 4 which has the better performances), potentially saving a lot from the typical 32-bit precision. Additionally, to increase the compression, one can opt for bnb\\_4bit\\_use\\_double\\_quant (but we don\u2019t in our example), which implements a secondary quantization following the initial one, resulting in a supplementary reduction of 0.4 bits per parameter. However, when computing on the network, computations are executed according to the bnb\\_4bit\\_compute\\_dtype defined by us, which is 16-bit precision, a suitable numeric precision comprising both fast and exact computations. This decompression phase may take more time, according to the reductions previously obtained.\n\nAs a next step, once initialized the Bitsandbytes compression is to load our model using HuggingFace (HF) AutoModelForCausalLM and its tokenizer:\n\n```\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=\"auto\",\n    quantization_config=bnb_config,\n)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\ntokenizer = AutoTokenizer.from_pretrained(model_name,\n                                          trust_remote_code=True,\n                                         )\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n```\n\nHere, apart from the quantization\\_config (the Bitsandbytes compression) and the device\\_ap set to \u201cauto\u201d so you can leverage whatever you have on your system (CPU or GPUs), we have to notice as specifics for this model, the pretraining\\_tp parameter necessarily set to one (a value stated by HF documentation necessary to ensure exact reproducibility of the pretraining results) and the use\\_cache set to False (whether or not the model should return the last key/values attentions, not necessary for Llama). On the hand of the tokenizer, the pad token is equated to the eos token ( the end-of-sequence token used to indicate the end of a sequence of tokens), and the padding side is set to be the right one, commonly indicated as the right side to use when working with Llama models.\n\nAfter instantiating the model, we have to prepare the training phase, which requires implementing a LoRA strategy based on a reduced number of parameters to update to adapt the original LLM to our task (see the previous article for more details).\n\n```\npeft_config = LoraConfig(\n    lora_alpha=16,\n    lora_dropout=0.1,\n    r=64,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=\"all-linear\",\n)\n```\n\nThe LoRA config specifies the parameters for PEFT. Following are the explained parameters that we use:\n\n- r: The rank of the LoRA update matrices. The reduction coefficient represents a trade-off: the lower it is, the less memory is consumed, but with increased approximation during updates.\n- lora\\_alpha: The learning rate for the LoRA update matrices", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Fine-tuning a large language model on Kaggle Notebooks (or even on your own computer) for solving real-world tasks", "description": "A Blog post by Luca Massaron on Hugging Face", "url": "https://huggingface.co/blog/lmassaron/fine-tuning-llms-on-kaggle-notebooks", "source": "hf", "id": "64feea19-c64d-4a99-a763-eccecbb89b27"}, "page_content": "\nAfter instantiating the model, we have to prepare the training phase, which requires implementing a LoRA strategy based on a reduced number of parameters to update to adapt the original LLM to our task (see the previous article for more details).\n\n```\npeft_config = LoraConfig(\n    lora_alpha=16,\n    lora_dropout=0.1,\n    r=64,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=\"all-linear\",\n)\n```\n\nThe LoRA config specifies the parameters for PEFT. Following are the explained parameters that we use:\n\n- r: The rank of the LoRA update matrices. The reduction coefficient represents a trade-off: the lower it is, the less memory is consumed, but with increased approximation during updates.\n- lora\\_alpha: The learning rate for the LoRA update matrices. As a rule of thumb, remember that it should be the double of the r value.\n- lora\\_dropout: The dropout probability for the LoRA update matrices.\n- bias: The type of bias to use. The possible values are none, additive, and learned. We go for none because the option removes biases from the LoRA model, which can reduce the model size by up to 20%.\n- task\\_type: The type of task that the model is being trained for. The possible values are CAUSAL\\_LM and MASKED\\_LM. Many say it doesn\u2019t make a difference, but CAUSAL\\_LM is the right choice for our purpose.\n\nFinally, we have to explain about the final adding the _parameter target\\_modules=\u201dall-linear\u201d_ to _LoraConfig_. The LoraConfig object contains a target\\_modules parameter to be expressed as a list or an array. In some examples you find online, the target modules commonly are \\[\u201cquery\\_key\\_value\u201d\\]; somewhere else, they are something else (in our case, the linear layers, expressed by \u201call-linear\u201d string value), but always referring to the Transformer architecture. The choice of what layers to fine-tune actually depends on what you want to achieve (and what works better with your problem). As stated in the LoRA paper ( [https://arxiv.org/abs/2106.09685](https://arxiv.org/abs/2106.09685)), Hu, Edward J., et al. \u201cLora: Low-rank adaptation of large language models.\u201d _arXiv preprint arXiv:2106.09685_ (2021), \u201c\\_we can apply LoRA to any subset of weight matrices in a neural network to reduce the number of trainable parameters\\_\u201d and that \u201c\\_we limit our study to only adapting the attention weights for downstream tasks and freeze the MLP modules \u2026 both for simplicity and parameter-efficiency\\_\u201d. Finally, the paper states that \u201c\\_we leave the empirical investigation of adapting the MLP layers, LayerNorm layers, and biases to a future work\\_\u201d implying that you can actually fine-tune whatever layers you want based on the results you obtain and your \u201cparameter budget\u201d (more layers you fine-tune, more computations and memory are required). This is stated even more clearly in section 7.1 of the paper, \u201c\\_WHICH WEIGHT MATRICES IN TRANSFORMER SHOULD WE APPLY LORA TO?\\_\u201d, where the choices of the author of the paper are justified by their parameter \u201cbudget\u201d, but you are not limited to just that, you have to look for the best performance overall given your architecture and problem.\n\nThe default LoRA settings in _peft_ adhere to the original LoRA paper, incorporating trainable weights into each attention block's query and value layers. This is what I did in the first implementation of the fine-tuning. However, in the QLoRA paper ( [https://huggingface.co/papers/2305.14314](https://huggingface.co/papers/2305.14314)), research revealed that introducing trainable weights to all linear layers in a transformer model enhances performance to match that of full-finetuning. Given that the selection of modules may differ based on the architecture, and you would have to search manually in the architecture of the model of your choice for such linear layers, they have introduced a user-friendly shorthand: simply specify _target\\_modules=\u2019all-linear,\u2019_ and let the left package take care of the rest.\n\nAfter defining LoRA settings, separately, we have to go for the training parameters:\n\n```\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=3,\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    optim=\"paged_adamw_32bit\",\n    save_steps=0,\n    logging_steps=25,\n    learning_rate=2e-4,\n    weight_decay=0.001,\n    fp16=True,\n    bf16=False,\n    max_grad_norm=0.3,\n    max_steps=-1,\n    warmup_ratio=0.03,\n    group_by_length=True,\n    lr_scheduler_type=\"cosine\",\n    report_to=\"tensorboard\",\n    evaluation_strategy=\"epoch\"\n)\n```\n\nThe training\\_arguments object specifies the parameters for training the model. The following are some of the most important", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Fine-tuning a large language model on Kaggle Notebooks (or even on your own computer) for solving real-world tasks", "description": "A Blog post by Luca Massaron on Hugging Face", "url": "https://huggingface.co/blog/lmassaron/fine-tuning-llms-on-kaggle-notebooks", "source": "hf", "id": "a736b0fc-3357-433c-ad11-56ffe42bc1a7"}, "page_content": "num_train_epochs=3,\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    optim=\"paged_adamw_32bit\",\n    save_steps=0,\n    logging_steps=25,\n    learning_rate=2e-4,\n    weight_decay=0.001,\n    fp16=True,\n    bf16=False,\n    max_grad_norm=0.3,\n    max_steps=-1,\n    warmup_ratio=0.03,\n    group_by_length=True,\n    lr_scheduler_type=\"cosine\",\n    report_to=\"tensorboard\",\n    evaluation_strategy=\"epoch\"\n)\n```\n\nThe training\\_arguments object specifies the parameters for training the model. The following are some of the most important parameters:\n\n- output\\_dir: The directory where the training logs and checkpoints will be saved.\n- num\\_train\\_epochs: The number of epochs to train the model for.\n- per\\_device\\_train\\_batch\\_size: The number of samples in each batch on each device.\n- gradient\\_accumulation\\_steps: The number of batches accumulating gradients before updating the model parameters.\n- optim: The optimizer to use for training the model. Our choice is for\n\nthe paged\\_adamw\\_32bit optimizer, a variant of the AdamW optimizer designed to be more efficient on 32-bit GPUs. It does this by breaking the model parameters into smaller pages and optimizing each page separately. This can reduce the memory usage of the optimizer and improve its performance on 32-bit GPUs.\n- save\\_steps: The number of steps after which to save a checkpoint.\n- logging\\_steps: The number of steps after which to log the training metrics.\n- learning\\_rate: The learning rate for the optimizer.\n- weight\\_decay: The weight decay parameter for the optimizer.\n- fp16: Whether to use 16-bit floating-point precision. Training on GPU with fp16 set to True, as we do, can reduce memory usage by up to half, improve training speed by up to 2x, and reduce training costs by up to half. However, it can also reduce the accuracy of the trained model and make the training process more difficult.\n- bf16: Whether to use BFloat16 precision (not for our GPU).\n- max\\_grad\\_norm: The maximum gradient norm. The maximum gradient norm is a hyperparameter used to control the magnitude of the gradient updates during training. It is relevant in training because it can help to prevent the model from becoming unstable and overfitting to the training data by taking too strong updates.\n- max\\_steps: The maximum number of steps to train the model for.\n- warmup\\_ratio: The proportion of the training steps to use for warming up the learning rate, i.e., the proportion of the training steps to gradually increase the learning rate from 0 to its final value. It is relevant in training because the warm-up can help improve the model's stability and performance.\n- group\\_by\\_length: Whether to group the training samples by length to minimize padding applied and be more efficient.\n- lr\\_scheduler\\_type: The type of learning rate scheduler to use. Our choice is the cosine scheduler, which gradually increases the learning rate at the beginning of training, thus helping the model learn the basic features of the data quickly. Then, it gradually decreases the learning rate towards the end of the training, which helps the model converge to a better solution.\n- report\\_to: The tools to report the training metrics to. Our choice is to use TensorBoard.\n- evaluation\\_strategy: The strategy for evaluating the model during training. By deciding on \u201cepoch\u201d, we have an evaluation of every epoch on the eval dataset, which can help us figure out if training and eval measures are diverging or not.\n\nFinally, we can define the training itself, which is entrusted to the SFTTrainer from the trl package. The trl is a library by HuggingFace providing a set of tools to train transformer language models with Reinforcement Learning and other methods, from the Supervised Fine-tuning step (SFT), Reward Modeling step (RM) to the Proximal Policy Optimization step (PPO).\n\nThe trl library now also simplifies the process of setting up a model and tokenizer for conversational AI tasks with the help of the `setup_chat_format()` function. This function performs the following tasks:\n\n1. Introduces special tokens, such as `<s>` and `<e>`, signifying the beginning and end of a conversation to the tokenizer.\n2. Adjusts the model\u2019s embedding layer to accommodate these newly added tokens.\n3. Defines the chat template of the tokenizer, responsible for formatting input data into a conversation-like structure. The default template is chatml, which was inspired by OpenAI.\n4. Additionally, users have the option to specify the `resize_to_multiple_of` parameter, enabling them to resize the embedding layer to a multiple of the provided argument (e.g., 64).\n\nHere is an example of how to use this function:\n\n```\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n# Load model and tokenizer\nmodel = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\")\ntokenizer = AutoTokenizer.from_", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Fine-tuning a large language model on Kaggle Notebooks (or even on your own computer) for solving real-world tasks", "description": "A Blog post by Luca Massaron on Hugging Face", "url": "https://huggingface.co/blog/lmassaron/fine-tuning-llms-on-kaggle-notebooks", "source": "hf", "id": "9e78ad9c-ad82-4460-af0d-45337a6bb693"}, "page_content": " special tokens, such as `<s>` and `<e>`, signifying the beginning and end of a conversation to the tokenizer.\n2. Adjusts the model\u2019s embedding layer to accommodate these newly added tokens.\n3. Defines the chat template of the tokenizer, responsible for formatting input data into a conversation-like structure. The default template is chatml, which was inspired by OpenAI.\n4. Additionally, users have the option to specify the `resize_to_multiple_of` parameter, enabling them to resize the embedding layer to a multiple of the provided argument (e.g., 64).\n\nHere is an example of how to use this function:\n\n```\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n# Load model and tokenizer\nmodel = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\")\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n# Set up the chat format with default 'chatml' format\nmodel, tokenizer = setup_chat_format(model, tokenizer)\n```\n\nAdding special tokens to a language model during fine-tuning is crucial, especially when training chat models. These tokens are pivotal in delineating the various roles within a conversation, such as the user, assistant, and system. By inserting these tokens strategically, the model gains an understanding of the structural components and the sequential flow inherent in a conversation.\n\nIn other words, the setup provided by _set\\_chat\\_format_ assists the model in recognizing the nuances of conversational dynamics. The model becomes attuned to transitions between different speakers and comprehends the contextual cues associated with each role. This enhanced awareness is essential for the model to generate coherent, contextually appropriate responses within the context of a chat environment.\n\nInstead, as for as training, the trl package provides the SFTTrainer, a class for Supervised fine-tuning (or SFT for short). SFT is a technique commonly used in machine learning, particularly in the context of deep learning, to adapt a pre-trained model to a specific task or dataset.\n\nHere's how it typically works:\n\n- Pre-training: Initially, a neural network model is trained on a large dataset for a general task, such as image classification on a dataset like ImageNet. During this pre-training phase, the model learns to recognize high-level features and patterns from the data. In our case, we are leveraging a LLM such as Llama 2.\n\n- Fine-tuning: After pre-training, the model can be further trained or fine-tuned on a smaller, task-specific dataset. This fine-tuning process involves updating the parameters of the pre-trained model using the new dataset. However, instead of starting the training from scratch, the model starts with the weights learned during pre-training. This allows the model to quickly adapt to the new task or dataset by adjusting its parameters to better fit the new data.\n\n- Supervision: The fine-tuning process is supervised, meaning that the model is provided with labeled examples (input-output pairs) from the task-specific dataset. This supervision guides the learning process and helps the model improve its performance on the specific task.\n\n\nSupervised fine-tuning is particularly useful when you have a small dataset available for your target task, as it leverages the knowledge encoded in the pre-trained model while still adapting to the specifics of the new task. This approach often leads to faster convergence and better performance compared to training a model from scratch, especially when the pre-trained model has been trained on a large and diverse dataset.\n\nHere is our setting up of the SFTTrainer:\n\n```\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=train_data,\n    eval_dataset=eval_data,\n    peft_config=peft_config,\n    dataset_text_field=\"text\",\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing=False,\n    max_seq_length=1024,\n)\n```\n\nThe SFTTrainer object is initialized with the following arguments:\n\n- model: The model to be trained.\n- train\\_dataset: The training dataset.\n- eval\\_dataset: The evaluation dataset.\n- peft\\_config: The PEFT configuration.\n- dataset\\_text\\_field: The name of the text field in the dataset (we used the HuggingFace Dataset implementation).\n- tokenizer: The tokenizer to use.\n- args: The training arguments we previously set.\n- packing: Whether to pack the training samples.\n- max\\_seq\\_length: The maximum sequence length.\n\nThis basically completes our fine-tuning work because all that is left to do is the training itself and then save the updated model to disk:\n\n```\ntrainer.train()\ntrainer.model.save_pretrained(\"trained-model\")\n```\n\nHowever, we cannot say everythin is completed if we cannot re-use or share our fine-tuned model.\nHow do you save your fine-tuned model and publish or re-use it?\n\nA few more commands will do the magic, though they require quite a lot of free CPU and GPU memory and that means, if we keep on operating on the same Kaggle notebook, we need to do some cleaning.\n\nThings start after we have saved our fine-tuned QLoRA weights to disk:\n\n```\ntrainer.save_", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Fine-tuning a large language model on Kaggle Notebooks (or even on your own computer) for solving real-world tasks", "description": "A Blog post by Luca Massaron on Hugging Face", "url": "https://huggingface.co/blog/lmassaron/fine-tuning-llms-on-kaggle-notebooks", "source": "hf", "id": "d43785f3-40db-4b54-ae76-71b636e74993"}, "page_content": " packing: Whether to pack the training samples.\n- max\\_seq\\_length: The maximum sequence length.\n\nThis basically completes our fine-tuning work because all that is left to do is the training itself and then save the updated model to disk:\n\n```\ntrainer.train()\ntrainer.model.save_pretrained(\"trained-model\")\n```\n\nHowever, we cannot say everythin is completed if we cannot re-use or share our fine-tuned model.\nHow do you save your fine-tuned model and publish or re-use it?\n\nA few more commands will do the magic, though they require quite a lot of free CPU and GPU memory and that means, if we keep on operating on the same Kaggle notebook, we need to do some cleaning.\n\nThings start after we have saved our fine-tuned QLoRA weights to disk:\n\n```\ntrainer.save_model()\ntokenizer.save_pretrained(output_dir)\n```\n\nThe point here is that we are just saving QLora weights, which are a modifier (by matrix multiplication) of our original model (in our example, a LLama 2 7B). In fact, when working with QLoRA, we exclusively train adapters instead of the entire model. So, when you save the model during training, you only preserve the adapter weights, not the entire model.\n\nIf you want to save the entire model for easier use with Text Generation Inference, you can merge the adapter weights into the model weights using the merge\\_and\\_unload method. Then, you can save the model using the save\\_pretrained method. This will create a default model that\u2019s ready for inference tasks. A simple command can achieve merging, but first, we have to clean up our memory:\n\n```\nimport gc\ndel [model, tokenizer, peft_config, trainer, train_data, eval_data, bnb_config, training_arguments]\ndel [df, X_train, X_eval]\ndel [TrainingArguments, SFTTrainer, LoraConfig, BitsAndBytesConfig]\nfor _ in range(100):\n    torch.cuda.empty_cache()\n    gc.collect()\n```\n\nAfter deleting the models and data we won\u2019t use anymore, we garbage collect the memory with gc.collect() and clean the GPU memory cache by torch.cuda.empty\\_cache().\n\nThen, we can proceed to merge the weights and use the merged model for our testing purposes.\n\n```\nfrom peft import AutoPeftModelForCausalLM\nfinetuned_model = \"./trained_weigths/\"\ncompute_dtype = getattr(torch, \"float16\")\ntokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/llama-2/pytorch/7b-hf/1\")\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n     finetuned_model,\n     torch_dtype=compute_dtype,\n     return_dict=False,\n     low_cpu_mem_usage=True,\n     device_map=device,\n)\nmerged_model = model.merge_and_unload()\nmerged_model.save_pretrained(\"./merged_model\",safe_serialization=True, max_shard_size=\"2GB\")\ntokenizer.save_pretrained(\"./merged_model\")\n```\n\nSeveral tasks are performed in the above code snippet to the QLoRA weights, the original model, and its associated tokenizer. Firstly, relevant modules are imported, including the AutoPeftModelForCausalLM from the peft package, and it relies on existing components like torch and AutoTokenizer from the transformers library.\n\nPaths and configurations are then defined, such as the directory containing the fine-tuned model weights (finetuned\\_model), and the data type for computations is set to float16 (compute\\_dtype). The tokenizer is loaded from the LLama 2 model location. Subsequently, the model is loaded using specified configurations, possibly including optimizations for memory usage. Following loading, the model undergoes a merging and unloading process to consolidate the QLoRA and original weights together. This operation takes time and quite a lot of memory. If errors happen here, it is because you don\u2019t have enough available memory (typically: NotImplementedError: Cannot copy out of meta tensor; no data!). Just recheck the situation with your memory (both CPU and GPU; a nvidia-smi command may help), clean better, collect memory garbage, and retry.\n\nThe merged model is finally saved to a designated directory, ensuring safe serialization and limiting shard size to 2GB. Furthermore, the tokenizer is saved alongside the merged model, facilitating future use.\n\nThat\u2019s all. The model is now stored in a new directory, ready to be loaded and used for any task you need. As a last step, we just need to test the model on our test set.\n\nA classification report highlights:\n\n```\n Accuracy: 0.851\n Accuracy for label 0: 0.913\n Accuracy for label 1: 0.863\n Accuracy for label 2: 0.777\n\n Classification Report:\n               precision    recall  f1-score   support\n\n            0       0.95      0.91      0.93       300\n            1       0.74      0.86      0.80       300\n            ", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Fine-tuning a large language model on Kaggle Notebooks (or even on your own computer) for solving real-world tasks", "description": "A Blog post by Luca Massaron on Hugging Face", "url": "https://huggingface.co/blog/lmassaron/fine-tuning-llms-on-kaggle-notebooks", "source": "hf", "id": "5f927d90-95c7-4e27-bb83-9494f65ed721"}, "page_content": "), clean better, collect memory garbage, and retry.\n\nThe merged model is finally saved to a designated directory, ensuring safe serialization and limiting shard size to 2GB. Furthermore, the tokenizer is saved alongside the merged model, facilitating future use.\n\nThat\u2019s all. The model is now stored in a new directory, ready to be loaded and used for any task you need. As a last step, we just need to test the model on our test set.\n\nA classification report highlights:\n\n```\n Accuracy: 0.851\n Accuracy for label 0: 0.913\n Accuracy for label 1: 0.863\n Accuracy for label 2: 0.777\n\n Classification Report:\n               precision    recall  f1-score   support\n\n            0       0.95      0.91      0.93       300\n            1       0.74      0.86      0.80       300\n            2       0.88      0.78      0.82       300\n\n     accuracy                           0.85       900\n    macro avg       0.86      0.85      0.85       900\n weighted avg       0.86      0.85      0.85       900\n\n Confusion Matrix:\n [[274  24   2]\\\n  [ 11 259  30]\\\n  [  2  65 233]]\n```\n\nWhich is definitely a strong improvement over a simpler baseline on the very same problem (exactly with the same training and testing data) that returns a 0.623 as overall accuracy (see: [LSTM Baseline for Sentiment Analysis](https://www.kaggle.com/code/lucamassaron/lstm-baseline-for-sentiment-analysis)).\n\n## Reprising hands-on fine-tuning for financial sentiment analysis with Mistral 7B Instruct v0.2 and Phi-2\n\nAfter fine-tuning LLama 7B on a dataset for financial sentiment analysis on consumer-grade, easily accessible, and free GPUs ( [https://www.kaggle.com/code/lucamassaron/fine-tune-llama-2-for-sentiment-analysis/](https://www.kaggle.com/code/lucamassaron/fine-tune-llama-2-for-sentiment-analysis/)) you can re-use the very same code to fine-tune also most recently appeared large language models such as :\n\n- Mistral\u2019s Mistral 7B Instruct v0.2\n\n[https://www.kaggle.com/code/lucamassaron/fine-tune-mistral-v0-2-for-sentiment-analysis](https://www.kaggle.com/code/lucamassaron/fine-tune-mistral-v0-2-for-sentiment-analysis)\n- Microsoft\u2019s Phi-2\n\n[https://www.kaggle.com/code/lucamassaron/fine-tune-phi-2-for-sentiment-analysis](https://www.kaggle.com/code/lucamassaron/fine-tune-phi-2-for-sentiment-analysis)\n\nIn this article, I will present the exciting characteristics of these new large language models and how to modify the starting LLama fine-tuning to adapt to each of them.\n\n**The** new **models**\n\nMistral 7B Instruct v0.2 builds upon the foundation of its predecessor, Mistral 7B Instruct v0.1, introducing refined instruct-finetuning techniques that elevate its capabilities. Everything starts from the Mistral 7B developed by Mistral, a Paris-based AI startup founded by former Google\u2019s DeepMind and Meta employees, aiming to compete with OpenAI in constructing, training, and applying large language models and generative AI.\n\nSuch a 7.3B parameter model, Mistral 7B, stands out among its counterparts, consistently surpassing Llama 2 13B on all benchmarks and matching Llama 1 34B performance on numerous tasks. It even rivals CodeLlama 7B\u2019s proficiency in code-related areas while maintaining its excellence in English-based tasks (but it can egregiously handle all European languages).\n\nTo achieve this remarkable level of performance, Mistral 7B employs two innovative techniques: Grouped-query attention (GQA) for accelerated inference and Sliding Window Attention (SWA) for efficiently handling lengthy sequences at a lower cost.\n\nGQA streamlines the inference process by grouping and processing relevant query terms in parallel, reducing computational time and enhancing overall speed. SWA, on the other hand, tackles the challenge of operating on lengthy sequences by dividing them into smaller windows and applying attention mechanisms to each window independently, resulting in more efficient processing and reduced memory consumption.\n\nThe Mistral 7B Instruct model is designed to be fine-tuned for specific tasks, such as instruction following, creative text generation, and question answering, thus proving how flexible Mistral 7B is to be fine-tuned. As a caveat, it has no built-in moderation mechanism to filter out inappropriate or harmful content.\n\nPhi-2 is instead a small language model (LLM) developed by Microsoft Research. It has only 2.7 billion parameters, significantly smaller than other LLMs. Its training has been based on a similar corpus than Phi-1 and Phi-1.5, focusing on \u201ctextbook-quality\u201d data, including subsets of Python codes from The Stack v1.2", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Fine-tuning a large language model on Kaggle Notebooks (or even on your own computer) for solving real-world tasks", "description": "A Blog post by Luca Massaron on Hugging Face", "url": "https://huggingface.co/blog/lmassaron/fine-tuning-llms-on-kaggle-notebooks", "source": "hf", "id": "abe345b8-0d08-4d21-90db-50b3ece6cd10"}, "page_content": " computational time and enhancing overall speed. SWA, on the other hand, tackles the challenge of operating on lengthy sequences by dividing them into smaller windows and applying attention mechanisms to each window independently, resulting in more efficient processing and reduced memory consumption.\n\nThe Mistral 7B Instruct model is designed to be fine-tuned for specific tasks, such as instruction following, creative text generation, and question answering, thus proving how flexible Mistral 7B is to be fine-tuned. As a caveat, it has no built-in moderation mechanism to filter out inappropriate or harmful content.\n\nPhi-2 is instead a small language model (LLM) developed by Microsoft Research. It has only 2.7 billion parameters, significantly smaller than other LLMs. Its training has been based on a similar corpus than Phi-1 and Phi-1.5, focusing on \u201ctextbook-quality\u201d data, including subsets of Python codes from The Stack v1.2, Q&A content from StackOverflow, competition code from code\\_contests, and synthetic Python textbooks and exercises generated by gpt-3.5-turbo-0301. Also Phi-2 has not undergone fine-tuning through reinforcement learning from human feedback, hence there is no filtering of any kind.\n\n**Code adjustments**\n\nSetting Mistral 7B Instruct to work is a breeze (no pun intended \ud83d\ude04). All you must consider is that to utilize instruction fine-tuning, you need to enclose your prompt between \\[INST\\] and \\[/INST\\] markers. That\u2019s all! After the fine-tuning process, the results return a top performance for all the classes in terms of detected sentiment on our test set:\n\n```\nAccuracy: 0.868\nAccuracy for label negative: 0.977\nAccuracy for label neutral: 0.743\nAccuracy for label positive: 0.883\n```\n\nPhi-2 requires more work because it has less stringent requirements for instructions and displays a very peculiar behavior. It tends to deal with the question as a quiz and to return even unrequited elements from the texts it has used for its original learning. Therefore, after evaluating the sentiment of a text, it eruditely starts a discussion about the Mughal empire. The most efficient way to obtain answers from the network is to limit the response tokens to at least 3 to allow extra spaces and answer letters to appear before the prediction (something that can\u2019t be avoided) and to structure the prompt as:\n\nThe sentiment of the following phrase: \u2018\u2026\u2019\n\nSolution: The correct option is...\n\nAnother essential fact about Phi-2 is that you need to declare the target modules you want to adjust when setting the parameters for the LoRA (Low-Rank Attention) module, a parameter reduction technique used to compress the attention matrices in a transformer model. Here, we found it is necessary to specify \u201cWqkv\u201d and \u201cout\\_proj explicitly\u201d. \u201cWqkv\u201d and \u201cout\\_proj\u201d are modules in the Transformer architecture used for attention and output projection.\n\nWqkv is a 3-layer feed-forward network that generates the attention mechanism's query, key, and value vectors. These vectors are then used to compute the attention scores, which are used to determine the relevance of each word in the input sequence to each word in the output sequence.\n\nout\\_proj is a linear layer used to project the decoder output into the vocabulary space. The layer is responsible for converting the decoder\u2019s hidden state into a probability distribution over the vocabulary, which is then used to select the next token to generate.\n\nIn the context of the Phi-2 model, these modules are used to fine-tune the model for instruction following tasks. The model can learn to understand better and respond to instructions by fine-tuning these modules.\n\nBy doing so, the results are relatively less performing than Mistral Instruct but better than LLama and with a much smaller model:\n\n```\nAccuracy: 0.856\nAccuracy for label negative: 0.973\nAccuracy for label neutral: 0.743\nAccuracy for label positive: 0.850\n```\n\n## Conclusions\n\nThis completes our tour of the step for fine-tuning an LLM such as Meta\u2019s LLama 2 (and Mistral and Phi2) in Kaggle Notebooks (it can work on consumer hardware, too). As for many machine learning problems, after grasping the technicalities for running the learning, everything boils down to a good understanding of the problem, proper data preparation, and some experimentation to adapt your tools to the problem (and vice versa if necessary).\n\nCode references:\n\n- [Fine-tune Llama-2 for Sentiment Analysis](https://www.kaggle.com/code/lucamassaron/fine-tune-llama-2-for-sentiment-analysis)\n- [Fine-tune Mistral v0.2 for Sentiment Analysis](https://www.kaggle.com/code/lucamassaron/fine-tune-mistral-v0-2-for-sentiment-analysis)\n- [Fine-tune Phi 2 for Sentiment Analysis](https://www.kaggle.com/code/lucamassaron/fine-tune-phi-2-for-sentiment-analysis)\n- [LSTM Baseline for Sentiment Analysis](https://www.kaggle.com/code/lucamassaron/lstm-baseline-for-sentiment-analysis)).[SEP]", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Fine-tuning a large language model on Kaggle Notebooks (or even on your own computer) for solving real-world tasks", "description": "A Blog post by Luca Massaron on Hugging Face", "url": "https://huggingface.co/blog/lmassaron/fine-tuning-llms-on-kaggle-notebooks", "source": "hf", "id": "a24c6fb5-c9de-4950-852f-1eec84bbb7b1"}, "page_content": "ama-2 for Sentiment Analysis](https://www.kaggle.com/code/lucamassaron/fine-tune-llama-2-for-sentiment-analysis)\n- [Fine-tune Mistral v0.2 for Sentiment Analysis](https://www.kaggle.com/code/lucamassaron/fine-tune-mistral-v0-2-for-sentiment-analysis)\n- [Fine-tune Phi 2 for Sentiment Analysis](https://www.kaggle.com/code/lucamassaron/fine-tune-phi-2-for-sentiment-analysis)\n- [LSTM Baseline for Sentiment Analysis](https://www.kaggle.com/code/lucamassaron/lstm-baseline-for-sentiment-analysis)).[SEP]", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "LoRA", "description": "Hugging Face Official Documentation of LoRA", "url": "https://huggingface.co/docs/peft/main/en/conceptual_guides/lora", "source": "hf", "id": "cda0438d-5520-476e-a218-7aaab8de8ac9"}, "page_content": "[CLS]# LoRA\n\nThis conceptual guide gives a brief overview of [LoRA](https://arxiv.org/abs/2106.09685), a technique that accelerates\nthe fine-tuning of large models while consuming less memory.\n\nTo make fine-tuning more efficient, LoRA\u2019s approach is to represent the weight updates with two smaller\nmatrices (called **update matrices**) through low-rank decomposition. These new matrices can be trained to adapt to the\nnew data while keeping the overall number of changes low. The original weight matrix remains frozen and doesn\u2019t receive\nany further adjustments. To produce the final results, both the original and the adapted weights are combined.\n\nThis approach has a number of advantages:\n\n- LoRA makes fine-tuning more efficient by drastically reducing the number of trainable parameters.\n- The original pre-trained weights are kept frozen, which means you can have multiple lightweight and portable LoRA models for various downstream tasks built on top of them.\n- LoRA is orthogonal to many other parameter-efficient methods and can be combined with many of them.\n- Performance of models fine-tuned using LoRA is comparable to the performance of fully fine-tuned models.\n- LoRA does not add any inference latency because adapter weights can be merged with the base model.\n\nIn principle, LoRA can be applied to any subset of weight matrices in a neural network to reduce the number of trainable\nparameters. However, for simplicity and further parameter efficiency, in Transformer models LoRA is typically applied to\nattention blocks only. The resulting number of trainable parameters in a LoRA model depends on the size of the low-rank\nupdate matrices, which is determined mainly by the rank `r` and the shape of the original weight matrix.\n\n## Merge LoRA weights into the base model\n\nWhile LoRA is significantly smaller and faster to train, you may encounter latency issues during inference due to separately loading the base model and the LoRA model. To eliminate latency, use the [merge\\_and\\_unload()](https://huggingface.co/docs/peft/main/en/package_reference/lora#peft.LoraModel.merge_and_unload) function to merge the adapter weights with the base model which allows you to effectively use the newly merged model as a standalone model.\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/lora_diagram.png)\n\nThis works because during training, the smaller weight matrices ( _A_ and _B_ in the diagram above) are separate. But once training is complete, the weights can actually be merged into a new weight matrix that is identical.\n\n## Utils for LoRA\n\nUse [merge\\_adapter()](https://huggingface.co/docs/peft/main/en/package_reference/tuners#peft.tuners.tuners_utils.BaseTuner.merge_adapter) to merge the LoRa layers into the base model while retaining the PeftModel.\nThis will help in later unmerging, deleting, loading different adapters and so on.\n\nUse [unmerge\\_adapter()](https://huggingface.co/docs/peft/main/en/package_reference/tuners#peft.tuners.tuners_utils.BaseTuner.unmerge_adapter) to unmerge the LoRa layers from the base model while retaining the PeftModel.\nThis will help in later merging, deleting, loading different adapters and so on.\n\nUse [unload()](https://huggingface.co/docs/peft/main/en/package_reference/lora#peft.LoraModel.unload) to get back the base model without the merging of the active lora modules.\nThis will help when you want to get back the pretrained base model in some applications when you want to reset the model to its original state.\nFor example, in Stable Diffusion WebUi, when the user wants to infer with base model post trying out LoRAs.\n\nUse [delete\\_adapter()](https://huggingface.co/docs/peft/main/en/package_reference/lora#peft.LoraModel.delete_adapter) to delete an existing adapter.\n\nUse [add\\_weighted\\_adapter()](https://huggingface.co/docs/peft/main/en/package_reference/lora#peft.LoraModel.add_weighted_adapter) to combine multiple LoRAs into a new adapter based on the user provided weighing scheme.\n\n## Common LoRA parameters in PEFT\n\nAs with other methods supported by PEFT, to fine-tune a model using LoRA, you need to:\n\n1. Instantiate a base model.\n2. Create a configuration (`LoraConfig`) where you define LoRA-specific parameters.\n3. Wrap the base model with `get_peft_model()` to get a trainable `PeftModel`.\n4. Train the `PeftModel` as you normally would train the base model.\n\n`LoraConfig` allows you to control how LoRA is applied to the base model through the following parameters:\n\n- `r`: the rank of the update matrices, expressed in `int`. Lower rank results in smaller update matrices with fewer trainable parameters.\n- `target_modules`: The modules (for", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "LoRA", "description": "Hugging Face Official Documentation of LoRA", "url": "https://huggingface.co/docs/peft/main/en/conceptual_guides/lora", "source": "hf", "id": "b4b3e1b3-e106-48f0-878b-b1b8814cc00d"}, "page_content": "_adapter) to combine multiple LoRAs into a new adapter based on the user provided weighing scheme.\n\n## Common LoRA parameters in PEFT\n\nAs with other methods supported by PEFT, to fine-tune a model using LoRA, you need to:\n\n1. Instantiate a base model.\n2. Create a configuration (`LoraConfig`) where you define LoRA-specific parameters.\n3. Wrap the base model with `get_peft_model()` to get a trainable `PeftModel`.\n4. Train the `PeftModel` as you normally would train the base model.\n\n`LoraConfig` allows you to control how LoRA is applied to the base model through the following parameters:\n\n- `r`: the rank of the update matrices, expressed in `int`. Lower rank results in smaller update matrices with fewer trainable parameters.\n- `target_modules`: The modules (for example, attention blocks) to apply the LoRA update matrices.\n- `lora_alpha`: LoRA scaling factor.\n- `bias`: Specifies if the `bias` parameters should be trained. Can be `'none'`, `'all'` or `'lora_only'`.\n- `use_rslora`: When set to True, uses [Rank-Stabilized LoRA](https://doi.org/10.48550/arXiv.2312.03732) which sets the adapter scaling factor to `lora_alpha/math.sqrt(r)`, since it was proven to work better. Otherwise, it will use the original default value of `lora_alpha/r`.\n- `modules_to_save`: List of modules apart from LoRA layers to be set as trainable and saved in the final checkpoint. These typically include model\u2019s custom head that is randomly initialized for the fine-tuning task.\n- `layers_to_transform`: List of layers to be transformed by LoRA. If not specified, all layers in `target_modules` are transformed.\n- `layers_pattern`: Pattern to match layer names in `target_modules`, if `layers_to_transform` is specified. By default `PeftModel` will look at common layer pattern (`layers`, `h`, `blocks`, etc.), use it for exotic and custom models.\n- `rank_pattern`: The mapping from layer names or regexp expression to ranks which are different from the default rank specified by `r`.\n- `alpha_pattern`: The mapping from layer names or regexp expression to alphas which are different from the default alpha specified by `lora_alpha`.\n\n## LoRA examples\n\nFor an example of LoRA method application to various downstream tasks, please refer to the following guides:\n\n- [Image classification using LoRA](https://huggingface.co/docs/peft/main/en/task_guides/image_classification_lora)\n- [Semantic segmentation](https://huggingface.co/docs/peft/main/en/task_guides/semantic_segmentation_lora)\n\nWhile the original paper focuses on language models, the technique can be applied to any dense layers in deep learning\nmodels. As such, you can leverage this technique with diffusion models. See [Dreambooth fine-tuning with LoRA](https://huggingface.co/docs/peft/main/en/task_guides/task_guides/dreambooth_lora) task guide for an example.\n\n## Initialization options\n\nThe initialization of LoRA weights is controlled by the parameter `init_lora_weights` of the `LoraConfig`. By default, PEFT initializes LoRA weights the same way as the [reference implementation](https://github.com/microsoft/LoRA), i.e. using Kaiming-uniform for weight A and initializing weight B as zeros, resulting in an identity transform.\n\nIt is also possible to pass `init_lora_weights=\"gaussian\"`. As the name suggests, this results in initializing weight A with a Gaussian distribution (weight B is still zeros). This corresponds to the way that [diffusers](https://huggingface.co/docs/diffusers/index) initializes LoRA weights.\n\nWhen quantizing the base model, e.g. for QLoRA training, consider using the [LoftQ initialization](https://arxiv.org/abs/2310.08659), which has been shown to improve the performance with quantization. The idea is that the LoRA weights are initialized such that the quantization error is minimized. To use this option, _do not_ quantize the base model. Instead, proceed as follows:\n\nCopied\n\n```\nfrom peft import LoftQConfig, LoraConfig, get_peft_model\n\nbase_model = AutoModelForCausalLM.from_pretrained(...)  # don't quantize here\nloftq_config = LoftQConfig(loftq_bits=4,...)           # set 4bit quantization\nlora_config = LoraConfig(..., init_lora_weights=\"loftq\", loftq_config=loftq_config)\npeft_model = get_peft_model(base_model, lora_config)\n```\n\nThere is also an option to set `initialize_lora_weights=False`. When choosing this option, the LoRA weights are initialized such that they do _not_ result in an", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "LoRA", "description": "Hugging Face Official Documentation of LoRA", "url": "https://huggingface.co/docs/peft/main/en/conceptual_guides/lora", "source": "hf", "id": "30257a46-2b93-47c0-9c25-c2356f40edf8"}, "page_content": ". To use this option, _do not_ quantize the base model. Instead, proceed as follows:\n\nCopied\n\n```\nfrom peft import LoftQConfig, LoraConfig, get_peft_model\n\nbase_model = AutoModelForCausalLM.from_pretrained(...)  # don't quantize here\nloftq_config = LoftQConfig(loftq_bits=4,...)           # set 4bit quantization\nlora_config = LoraConfig(..., init_lora_weights=\"loftq\", loftq_config=loftq_config)\npeft_model = get_peft_model(base_model, lora_config)\n```\n\nThere is also an option to set `initialize_lora_weights=False`. When choosing this option, the LoRA weights are initialized such that they do _not_ result in an identity transform. This is useful for debugging and testing purposes and should not be used otherwise.\n\nFinally, the LoRA architecture scales each adapter during every forward pass by a fixed scalar, which is set at initialization, and depends on the rank `r`. Although the original LoRA method uses the scalar function `lora_alpha/r`, the research [Rank-Stabilized LoRA](https://doi.org/10.48550/arXiv.2312.03732) proves that instead using `lora_alpha/math.sqrt(r)`, stabilizes the adapters and unlocks the increased performance potential from higher ranks. Set `use_rslora=True` to use the rank-stabilized scaling `lora_alpha/math.sqrt(r)`.\n\n[\ud83e\udd17 PEFT\u2192](https://huggingface.co/docs/peft/main/en/index)[SEP]", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "PEFT", "description": "Hugging Face Official Documentation of PEFT", "url": "https://huggingface.co/docs/peft/main/en/index", "source": "hf", "id": "0be20a4b-8fff-4de5-92c5-a1b5c114601f"}, "page_content": "[CLS]# PEFT\n\n\ud83e\udd17 PEFT (Parameter-Efficient Fine-Tuning) is a library for efficiently adapting large pretrained models to various downstream applications without fine-tuning all of a model\u2019s parameters because it is prohibitively costly. PEFT methods only fine-tune a small number of (extra) model parameters - significantly decreasing computational and storage costs - while yielding performance comparable to a fully fine-tuned model. This makes it more accessible to train and store large language models (LLMs) on consumer hardware.\n\nPEFT is integrated with the Transformers, Diffusers, and Accelerate libraries to provide a faster and easier way to load, train, and use large models for inference.\n\n[Quicktour\\\\\n\\\\\nStart here if you're new to \ud83e\udd17 PEFT to get an overview of the library's main features, and how to train a model with a PEFT method.](https://huggingface.co/docs/peft/main/en/quicktour) [How-to guides\\\\\n\\\\\nPractical guides demonstrating how to apply various PEFT methods across different types of tasks like image classification, causal language modeling, automatic speech recognition, and more. Learn how to use \ud83e\udd17 PEFT with the DeepSpeed and Fully Sharded Data Parallel scripts.](https://huggingface.co/docs/peft/main/en/task_guides/prompt_based_methods) [Conceptual guides\\\\\n\\\\\nGet a better theoretical understanding of how LoRA and various soft prompting methods help reduce the number of trainable parameters to make training more efficient.](https://huggingface.co/docs/peft/main/en/conceptual_guides/adapter) [Reference\\\\\n\\\\\nTechnical descriptions of how \ud83e\udd17 PEFT classes and methods work.](https://huggingface.co/docs/peft/main/en/package_reference/config)\n\nPEFT methods\n\n[Update on GitHub](https://github.com/huggingface/peft/blob/main/docs/source/index.md)\n\n[Quicktour\u2192](https://huggingface.co/docs/peft/main/en/quicktour)[SEP]", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Quicktour", "description": "Hugging Face Official Documentation of Quicktour", "url": "https://huggingface.co/docs/peft/main/en/quicktour", "source": "hf", "id": "18ba7ddc-4eeb-42df-9993-022de600475d"}, "page_content": "[CLS]# Quicktour\n\nPEFT offers parameter-efficient methods for finetuning large pretrained models. The traditional paradigm is to finetune all of a model\u2019s parameters for each downstream task, but this is becoming exceedingly costly and impractical because of the enormous number of parameters in models today. Instead, it is more efficient to train a smaller number of prompt parameters or use a reparametrization method like low-rank adaptation (LoRA) to reduce the number of trainable parameters.\n\nThis quicktour will show you PEFT\u2019s main features and how you can train or run inference on large models that would typically be inaccessible on consumer devices.\n\n## Train\n\nEach PEFT method is defined by a [PeftConfig](https://huggingface.co/docs/peft/main/en/package_reference/config#peft.PeftConfig) class that stores all the important parameters for building a [PeftModel](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.PeftModel). For example, to train with LoRA, load and create a [LoraConfig](https://huggingface.co/docs/peft/main/en/package_reference/lora#peft.LoraConfig) class and specify the following parameters:\n\n- `task_type`: the task to train for (sequence-to-sequence language modeling in this case)\n- `inference_mode`: whether you\u2019re using the model for inference or not\n- `r`: the dimension of the low-rank matrices\n- `lora_alpha`: the scaling factor for the low-rank matrices\n- `lora_dropout`: the dropout probability of the LoRA layers\n\nCopied\n\n```\nfrom peft import LoraConfig, TaskType\n\npeft_config = LoraConfig(task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1)\n```\n\n> See the [LoraConfig](https://huggingface.co/docs/peft/main/en/package_reference/lora#peft.LoraConfig) reference for more details about other parameters you can adjust, such as the modules to target or the bias type.\n\nOnce the [LoraConfig](https://huggingface.co/docs/peft/main/en/package_reference/lora#peft.LoraConfig) is setup, create a [PeftModel](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.PeftModel) with the [get\\_peft\\_model()](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.get_peft_model) function. It takes a base model - which you can load from the Transformers library - and the [LoraConfig](https://huggingface.co/docs/peft/main/en/package_reference/lora#peft.LoraConfig) containing the parameters for how to configure a model for training with LoRA.\n\nLoad the base model you want to finetune.\n\nCopied\n\n```\nfrom transformers import AutoModelForSeq2SeqLM\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"bigscience/mt0-large\")\n```\n\nWrap the base model and `peft_config` with the [get\\_peft\\_model()](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.get_peft_model) function to create a [PeftModel](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.PeftModel). To get a sense of the number of trainable parameters in your model, use the `print_trainable_parameters` method.\n\nCopied\n\n```\nfrom peft import get_peft_model\n\nmodel = get_peft_model(model, peft_config)\nmodel.print_trainable_parameters()\n\"output: trainable params: 2359296 || all params: 1231940608 || trainable%: 0.19151053100118282\"\n```\n\nOut of [bigscience/mt0-large\u2019s](https://huggingface.co/bigscience/mt0-large) 1.2B parameters, you\u2019re only training 0.19% of them!\n\nThat is it \ud83c\udf89! Now you can train the model with the Transformers [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer), Accelerate, or any custom PyTorch training loop.\n\nFor example, to train with the [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) class, setup a [TrainingArguments](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments) class with some training", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Quicktour", "description": "Hugging Face Official Documentation of Quicktour", "url": "https://huggingface.co/docs/peft/main/en/quicktour", "source": "hf", "id": "8740ce08-963f-422e-b43d-280a8b3c2fe6"}, "page_content": " [bigscience/mt0-large\u2019s](https://huggingface.co/bigscience/mt0-large) 1.2B parameters, you\u2019re only training 0.19% of them!\n\nThat is it \ud83c\udf89! Now you can train the model with the Transformers [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer), Accelerate, or any custom PyTorch training loop.\n\nFor example, to train with the [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) class, setup a [TrainingArguments](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments) class with some training hyperparameters.\n\nCopied\n\n```\ntraining_args = TrainingArguments(\n    output_dir=\"your-name/bigscience/mt0-large-lora\",\n    learning_rate=1e-3,\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=32,\n    num_train_epochs=2,\n    weight_decay=0.01,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n)\n```\n\nPass the model, training arguments, dataset, tokenizer, and any other necessary component to the [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer), and call [train](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.train) to start training.\n\nCopied\n\n```\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"test\"],\n    processing_class=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\n\ntrainer.train()\n```\n\n### Save model\n\nAfter your model is finished training, you can save your model to a directory using the [save\\_pretrained](https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained) function.\n\nCopied\n\n```\nmodel.save_pretrained(\"output_dir\")\n```\n\nYou can also save your model to the Hub (make sure you\u2019re logged in to your Hugging Face account first) with the [push\\_to\\_hub](https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.push_to_hub) function.\n\nCopied\n\n```\nfrom huggingface_hub import notebook_login\n\nnotebook_login()\nmodel.push_to_hub(\"your-name/bigscience/mt0-large-lora\")\n```\n\nBoth methods only save the extra PEFT weights that were trained, meaning it is super efficient to store, transfer, and load. For example, this [facebook/opt-350m](https://huggingface.co/ybelkada/opt-350m-lora) model trained with LoRA only contains two files: `adapter_config.json` and `adapter_model.safetensors`. The `adapter_model.safetensors` file is just 6.3MB!\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/PEFT-hub-screenshot.png)The adapter weights for a opt-350m model stored on the Hub are only ~6MB compared to the full size of the model weights, which can be ~700MB.\n\n## Inference\n\n> Take a look at the [AutoPeftModel](https://huggingface.co/docs/peft/main/en/package_reference/auto_class) API reference for a complete list of available `AutoPeftModel` classes.\n\nEasily load any PEFT-trained model for inference with the [AutoPeftModel](https://huggingface.co/docs/peft/main/en/package_reference/auto_class#peft.AutoPeftModel) class and the [from\\_pretrained](https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) method:\n\nCopied\n\n```\nfrom peft import AutoPeftModelForCausalLM\nfrom transformers import AutoTokenizer\nimport torch\n\nmodel = AutoPeftModelForCausalLM.from_pretrained(\"ybelkada/opt-350m-lora\")\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n\nmodel = model.to(\"cuda\")\nmodel.", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Quicktour", "description": "Hugging Face Official Documentation of Quicktour", "url": "https://huggingface.co/docs/peft/main/en/quicktour", "source": "hf", "id": "f5d587ef-aaea-4444-b5b9-7a61809456d1"}, "page_content": "\n\nEasily load any PEFT-trained model for inference with the [AutoPeftModel](https://huggingface.co/docs/peft/main/en/package_reference/auto_class#peft.AutoPeftModel) class and the [from\\_pretrained](https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) method:\n\nCopied\n\n```\nfrom peft import AutoPeftModelForCausalLM\nfrom transformers import AutoTokenizer\nimport torch\n\nmodel = AutoPeftModelForCausalLM.from_pretrained(\"ybelkada/opt-350m-lora\")\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n\nmodel = model.to(\"cuda\")\nmodel.eval()\ninputs = tokenizer(\"Preheat the oven to 350 degrees and place the cookie dough\", return_tensors=\"pt\")\n\noutputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=50)\nprint(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0])\n\n\"Preheat the oven to 350 degrees and place the cookie dough in the center of the oven. In a large bowl, combine the flour, baking powder, baking soda, salt, and cinnamon. In a separate bowl, combine the egg yolks, sugar, and vanilla.\"\n```\n\nFor other tasks that aren\u2019t explicitly supported with an `AutoPeftModelFor` class - such as automatic speech recognition - you can still use the base [AutoPeftModel](https://huggingface.co/docs/peft/main/en/package_reference/auto_class#peft.AutoPeftModel) class to load a model for the task.\n\nCopied\n\n```\nfrom peft import AutoPeftModel\n\nmodel = AutoPeftModel.from_pretrained(\"smangrul/openai-whisper-large-v2-LORA-colab\")\n```\n\n## Next steps\n\nNow that you\u2019ve seen how to train a model with one of the PEFT methods, we encourage you to try out some of the other methods like prompt tuning. The steps are very similar to the ones shown in the quicktour:\n\n1. prepare a [PeftConfig](https://huggingface.co/docs/peft/main/en/package_reference/config#peft.PeftConfig) for a PEFT method\n2. use the [get\\_peft\\_model()](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.get_peft_model) method to create a [PeftModel](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.PeftModel) from the configuration and base model\n\nThen you can train it however you like! To load a PEFT model for inference, you can use the [AutoPeftModel](https://huggingface.co/docs/peft/main/en/package_reference/auto_class#peft.AutoPeftModel) class.\n\nFeel free to also take a look at the task guides if you\u2019re interested in training a model with another PEFT method for a specific task such as semantic segmentation, multilingual automatic speech recognition, DreamBooth, token classification, and more.\n\n[Update on GitHub](https://github.com/huggingface/peft/blob/main/docs/source/quicktour.md)\n\n[\u2190\ud83e\udd17 PEFT](https://huggingface.co/docs/peft/main/en/index) [Installation\u2192](https://huggingface.co/docs/peft/main/en/install)[SEP]", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Installation", "description": "Hugging Face Official Documentation of Installation", "url": "https://huggingface.co/docs/peft/main/en/install", "source": "hf", "id": "537ff51b-425e-4255-a3fd-9986a8925b63"}, "page_content": "[CLS]# Installation\n\nBefore you start, you will need to setup your environment, install the appropriate packages, and configure \ud83e\udd17 PEFT. \ud83e\udd17 PEFT is tested on **Python 3.9+**.\n\n\ud83e\udd17 PEFT is available on PyPI, as well as GitHub:\n\n## PyPI\n\nTo install \ud83e\udd17 PEFT from PyPI:\n\nCopied\n\n```\npip install peft\n```\n\n## Source\n\nNew features that haven\u2019t been released yet are added every day, which also means there may be some bugs. To try them out, install from the GitHub repository:\n\nCopied\n\n```\npip install git+https://github.com/huggingface/peft\n```\n\nIf you\u2019re working on contributing to the library or wish to play with the source code and see live\nresults as you run the code, an editable version can be installed from a locally-cloned version of the\nrepository:\n\nCopied\n\n```\ngit clone https://github.com/huggingface/peft\ncd peft\npip install -e.[test]\n```\n\n[Update on GitHub](https://github.com/huggingface/peft/blob/main/docs/source/install.md)\n\n[\u2190Quicktour](https://huggingface.co/docs/peft/main/en/quicktour) [Configurations and models\u2192](https://huggingface.co/docs/peft/main/en/tutorial/peft_model_config)[SEP]", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "PEFT configurations and models", "description": "Hugging Face Official Documentation of PEFT configurations and models", "url": "https://huggingface.co/docs/peft/main/en/tutorial/peft_model_config", "source": "hf", "id": "db439e0f-1ce4-4587-8ef7-9ee369b2a9f8"}, "page_content": "[CLS]# PEFT configurations and models\n\nThe sheer size of today\u2019s large pretrained models - which commonly have billions of parameters - presents a significant training challenge because they require more storage space and more computational power to crunch all those calculations. You\u2019ll need access to powerful GPUs or TPUs to train these large pretrained models which is expensive, not widely accessible to everyone, not environmentally friendly, and not very practical. PEFT methods address many of these challenges. There are several types of PEFT methods (soft prompting, matrix decomposition, adapters), but they all focus on the same thing, reduce the number of trainable parameters. This makes it more accessible to train and store large models on consumer hardware.\n\nThe PEFT library is designed to help you quickly train large models on free or low-cost GPUs, and in this tutorial, you\u2019ll learn how to setup a configuration to apply a PEFT method to a pretrained base model for training. Once the PEFT configuration is setup, you can use any training framework you like (Transformer\u2019s [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) class, [Accelerate](https://hf.co/docs/accelerate), a custom PyTorch training loop).\n\n## PEFT configurations\n\n> Learn more about the parameters you can configure for each PEFT method in their respective API reference page.\n\nA configuration stores important parameters that specify how a particular PEFT method should be applied.\n\nFor example, take a look at the following [`LoraConfig`](https://huggingface.co/ybelkada/opt-350m-lora/blob/main/adapter_config.json) for applying LoRA and [`PromptEncoderConfig`](https://huggingface.co/smangrul/roberta-large-peft-p-tuning/blob/main/adapter_config.json) for applying p-tuning (these configuration files are already JSON-serialized). Whenever you load a PEFT adapter, it is a good idea to check whether it has an associated adapter\\_config.json file which is required.\n\nLoraConfig\n\nPromptEncoderConfig\n\nCopied\n\n```\n{\n  \"base_model_name_or_path\": \"facebook/opt-350m\", #base model to apply LoRA to\n  \"bias\": \"none\",\n  \"fan_in_fan_out\": false,\n  \"inference_mode\": true,\n  \"init_lora_weights\": true,\n  \"layers_pattern\": null,\n  \"layers_to_transform\": null,\n  \"lora_alpha\": 32,\n  \"lora_dropout\": 0.05,\n  \"modules_to_save\": null,\n  \"peft_type\": \"LORA\", #PEFT method type\n  \"r\": 16,\n  \"revision\": null,\n  \"target_modules\": [\\\n    \"q_proj\", #model modules to apply LoRA to (query and value projection layers)\\\n    \"v_proj\"\\\n  ],\n  \"task_type\": \"CAUSAL_LM\" #type of task to train model on\n}\n```\n\nYou can create your own configuration for training by initializing a [LoraConfig](https://huggingface.co/docs/peft/main/en/package_reference/lora#peft.LoraConfig).\n\nCopied\n\n```\nfrom peft import LoraConfig, TaskType\n\nlora_config = LoraConfig(\n    r=16,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    task_type=TaskType.CAUSAL_LM,\n    lora_alpha=32,\n    lora_dropout=0.05\n)\n```\n\n## PEFT models\n\nWith a PEFT configuration in hand, you can now apply it to any pretrained model to create a [PeftModel](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.PeftModel). Choose from any of the state-of-the-art models from the [Transformers](https://hf.co/docs/transformers) library, a custom model, and even new and unsupported transformer architectures.\n\nFor this tutorial, load a base [facebook/opt-350m](https://huggingface.co/facebook/opt-350m) model to finetune.\n\nCopied\n\n```\nfrom transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\")\n```\n\nUse the [get\\_peft\\_model()](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.get_peft_model) function to create a [PeftModel](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.PeftModel) from the base facebook/opt-350m model and the `lora_config` you created earlier.\n\nCopied\n\n```\nfrom peft import get_peft_model\n\nlora_model = get_pe", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "PEFT configurations and models", "description": "Hugging Face Official Documentation of PEFT configurations and models", "url": "https://huggingface.co/docs/peft/main/en/tutorial/peft_model_config", "source": "hf", "id": "5667e9dc-3139-49ed-92b3-acb2958e94ba"}, "page_content": "/facebook/opt-350m) model to finetune.\n\nCopied\n\n```\nfrom transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\")\n```\n\nUse the [get\\_peft\\_model()](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.get_peft_model) function to create a [PeftModel](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.PeftModel) from the base facebook/opt-350m model and the `lora_config` you created earlier.\n\nCopied\n\n```\nfrom peft import get_peft_model\n\nlora_model = get_peft_model(model, lora_config)\nlora_model.print_trainable_parameters()\n\"trainable params: 1,572,864 || all params: 332,769,280 || trainable%: 0.472659014678278\"\n```\n\n> When calling [get\\_peft\\_model()](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.get_peft_model), the base model will be modified _in-place_. That means, when calling [get\\_peft\\_model()](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.get_peft_model) on a model that was already modified in the same way before, this model will be further mutated. Therefore, if you would like to modify your PEFT configuration after having called `get_peft_model()` before, you would first have to unload the model with [unload()](https://huggingface.co/docs/peft/main/en/package_reference/tuners#peft.tuners.tuners_utils.BaseTuner.unload) and then call `get_peft_model()` with your new configuration. Alternatively, you can re-initialize the model to ensure a fresh, unmodified state before applying a new PEFT configuration.\n\nNow you can train the [PeftModel](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.PeftModel) with your preferred training framework! After training, you can save your model locally with [save\\_pretrained()](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.PeftModel.save_pretrained) or upload it to the Hub with the [push\\_to\\_hub](https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.push_to_hub) method.\n\nCopied\n\n```\n# save locally\nlora_model.save_pretrained(\"your-name/opt-350m-lora\")\n\n# push to Hub\nlora_model.push_to_hub(\"your-name/opt-350m-lora\")\n```\n\nTo load a [PeftModel](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.PeftModel) for inference, you\u2019ll need to provide the [PeftConfig](https://huggingface.co/docs/peft/main/en/package_reference/config#peft.PeftConfig) used to create it and the base model it was trained from.\n\nCopied\n\n```\nfrom peft import PeftModel, PeftConfig\n\nconfig = PeftConfig.from_pretrained(\"ybelkada/opt-350m-lora\")\nmodel = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path)\nlora_model = PeftModel.from_pretrained(model, \"ybelkada/opt-350m-lora\")\n```\n\n> By default, the [PeftModel](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.PeftModel) is set for inference, but if you\u2019d like to train the adapter some more you can set `is_trainable=True`.\n>\n> Copied\n>\n> ```\n> lora_model = PeftModel.from_pretrained(model, \"ybelkada/opt-350m-lora\", is_trainable=True)\n> ```\n\nThe [PeftModel.from\\_pretrained()](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.PeftModel.from_pretrained) method is the most flexible way to load a [PeftModel](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.PeftModel) because it doesn\u2019t matter what model framework was used", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "PEFT configurations and models", "description": "Hugging Face Official Documentation of PEFT configurations and models", "url": "https://huggingface.co/docs/peft/main/en/tutorial/peft_model_config", "source": "hf", "id": "3cb378b4-53b8-40d4-a444-32536f3e78f5"}, "page_content": ".PeftModel) is set for inference, but if you\u2019d like to train the adapter some more you can set `is_trainable=True`.\n>\n> Copied\n>\n> ```\n> lora_model = PeftModel.from_pretrained(model, \"ybelkada/opt-350m-lora\", is_trainable=True)\n> ```\n\nThe [PeftModel.from\\_pretrained()](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.PeftModel.from_pretrained) method is the most flexible way to load a [PeftModel](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.PeftModel) because it doesn\u2019t matter what model framework was used (Transformers, timm, a generic PyTorch model). Other classes, like [AutoPeftModel](https://huggingface.co/docs/peft/main/en/package_reference/auto_class#peft.AutoPeftModel), are just a convenient wrapper around the base [PeftModel](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.PeftModel), and makes it easier to load PEFT models directly from the Hub or locally where the PEFT weights are stored.\n\nCopied\n\n```\nfrom peft import AutoPeftModelForCausalLM\n\nlora_model = AutoPeftModelForCausalLM.from_pretrained(\"ybelkada/opt-350m-lora\")\n```\n\nTake a look at the [AutoPeftModel](https://huggingface.co/docs/peft/main/en/tutorial/package_reference/auto_class) API reference to learn more about the [AutoPeftModel](https://huggingface.co/docs/peft/main/en/package_reference/auto_class#peft.AutoPeftModel) classes.\n\n## Next steps\n\nWith the appropriate [PeftConfig](https://huggingface.co/docs/peft/main/en/package_reference/config#peft.PeftConfig), you can apply it to any pretrained model to create a [PeftModel](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.PeftModel) and train large powerful models faster on freely available GPUs! To learn more about PEFT configurations and models, the following guide may be helpful:\n\n- Learn how to configure a PEFT method for models that aren\u2019t from Transformers in the [Working with custom models](https://huggingface.co/docs/peft/main/en/developer_guides/custom_models) guide.\n\n[Update on GitHub](https://github.com/huggingface/peft/blob/main/docs/source/tutorial/peft_model_config.md)\n\n[\u2190Installation](https://huggingface.co/docs/peft/main/en/install) [Integrations\u2192](https://huggingface.co/docs/peft/main/en/tutorial/peft_integrations)[SEP]", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "PEFT integrations", "description": "Hugging Face Official Documentation of PEFT integrations", "url": "https://huggingface.co/docs/peft/main/en/tutorial/peft_integrations", "source": "hf", "id": "6917c91c-1a07-4274-9d5c-4e0cb54106dd"}, "page_content": "[CLS]# PEFT integrations\n\nPEFT\u2019s practical benefits extends to other Hugging Face libraries like [Diffusers](https://hf.co/docs/diffusers) and [Transformers](https://hf.co/docs/transformers). One of the main benefits of PEFT is that an adapter file generated by a PEFT method is a lot smaller than the original model, which makes it super easy to manage and use multiple adapters. You can use one pretrained base model for multiple tasks by simply loading a new adapter finetuned for the task you\u2019re solving. Or you can combine multiple adapters with a text-to-image diffusion model to create new effects.\n\nThis tutorial will show you how PEFT can help you manage adapters in Diffusers and Transformers.\n\n## Diffusers\n\nDiffusers is a generative AI library for creating images and videos from text or images with diffusion models. LoRA is an especially popular training method for diffusion models because you can very quickly train and share diffusion models to generate images in new styles. To make it easier to use and try multiple LoRA models, Diffusers uses the PEFT library to help manage different adapters for inference.\n\nFor example, load a base model and then load the [artificialguybr/3DRedmond-V1](https://huggingface.co/artificialguybr/3DRedmond-V1) adapter for inference with the [`load_lora_weights`](https://huggingface.co/docs/diffusers/v0.24.0/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_weights) method. The `adapter_name` argument in the loading method is enabled by PEFT and allows you to set a name for the adapter so it is easier to reference.\n\nCopied\n\n```\nimport torch\nfrom diffusers import DiffusionPipeline\n\npipeline = DiffusionPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16\n).to(\"cuda\")\npipeline.load_lora_weights(\n    \"peft-internal-testing/artificialguybr__3DRedmond-V1\",\n    weight_name=\"3DRedmond-3DRenderStyle-3DRenderAF.safetensors\",\n    adapter_name=\"3d\"\n)\nimage = pipeline(\"sushi rolls shaped like kawaii cat faces\").images[0]\nimage\n```\n\n![](https://huggingface.co/datasets/ybelkada/documentation-images/resolve/main/test-lora-diffusers.png)\n\nNow let\u2019s try another cool LoRA model, [ostris/super-cereal-sdxl-lora](https://huggingface.co/ostris/super-cereal-sdxl-lora). All you need to do is load and name this new adapter with `adapter_name`, and use the [`set_adapters`](https://huggingface.co/docs/diffusers/api/loaders/unet#diffusers.loaders.UNet2DConditionLoadersMixin.set_adapters) method to set it as the currently active adapter.\n\nCopied\n\n```\npipeline.load_lora_weights(\n    \"ostris/super-cereal-sdxl-lora\",\n    weight_name=\"cereal_box_sdxl_v1.safetensors\",\n    adapter_name=\"cereal\"\n)\npipeline.set_adapters(\"cereal\")\nimage = pipeline(\"sushi rolls shaped like kawaii cat faces\").images[0]\nimage\n```\n\n![](https://huggingface.co/datasets/ybelkada/documentation-images/resolve/main/test-lora-diffusers-2.png)\n\nFinally, you can call the [`disable_lora`](https://huggingface.co/docs/diffusers/api/loaders/unet#diffusers.loaders.UNet2DConditionLoadersMixin.disable_lora) method to restore the base model.\n\nCopied\n\n```\npipeline.disable_lora()\n```\n\nLearn more about how PEFT supports Diffusers in the [Inference with PEFT](https://huggingface.co/docs/diffusers/tutorials/using_peft_for_inference) tutorial.\n\n## Transformers\n\n\ud83e\udd17 [Transformers](https://hf.co/docs/transformers) is a collection of pretrained models for all types of tasks in all modalities. You can load these models for training or inference. Many of the models are large language models (LLMs), so it makes sense to integrate PEFT with Transformers to manage and train adapters.\n\nLoad a base pretrained model to train.\n\nCopied\n\n```\nfrom transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\")\n```\n\nNext, add an adapter configuration to specify how to adapt the model parameters. Call the [add\\_adapter()](https://huggingface.co/docs/peft/main/en/package_reference/peft_model", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "PEFT integrations", "description": "Hugging Face Official Documentation of PEFT integrations", "url": "https://huggingface.co/docs/peft/main/en/tutorial/peft_integrations", "source": "hf", "id": "49dfc611-c6ea-41fe-93bb-6ffb6c56d43f"}, "page_content": "ials/using_peft_for_inference) tutorial.\n\n## Transformers\n\n\ud83e\udd17 [Transformers](https://hf.co/docs/transformers) is a collection of pretrained models for all types of tasks in all modalities. You can load these models for training or inference. Many of the models are large language models (LLMs), so it makes sense to integrate PEFT with Transformers to manage and train adapters.\n\nLoad a base pretrained model to train.\n\nCopied\n\n```\nfrom transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\")\n```\n\nNext, add an adapter configuration to specify how to adapt the model parameters. Call the [add\\_adapter()](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.PeftModel.add_adapter) method to add the configuration to the base model.\n\nCopied\n\n```\nfrom peft import LoraConfig\n\npeft_config = LoraConfig(\n    lora_alpha=16,\n    lora_dropout=0.1,\n    r=64,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\nmodel.add_adapter(peft_config)\n```\n\nNow you can train the model with Transformer\u2019s [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) class or whichever training framework you prefer.\n\nTo use the newly trained model for inference, the [AutoModel](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoModel) class uses PEFT on the backend to load the adapter weights and configuration file into a base pretrained model.\n\nCopied\n\n```\nfrom transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\"peft-internal-testing/opt-350m-lora\")\n```\n\nAlternatively, you can use transformers [Pipelines](https://huggingface.co/docs/transformers/en/main_classes/pipelines) to load the model for conveniently running inference:\n\nCopied\n\n```\nfrom transformers import pipeline\n\nmodel = pipeline(\"text-generation\", \"peft-internal-testing/opt-350m-lora\")\nprint(model(\"Hello World\"))\n```\n\nIf you\u2019re interested in comparing or using more than one adapter, you can call the [add\\_adapter()](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.PeftModel.add_adapter) method to add the adapter configuration to the base model. The only requirement is the adapter type must be the same (you can\u2019t mix a LoRA and LoHa adapter).\n\nCopied\n\n```\nfrom transformers import AutoModelForCausalLM\nfrom peft import LoraConfig\n\nmodel = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\")\nmodel.add_adapter(lora_config_1, adapter_name=\"adapter_1\")\n```\n\nCall [add\\_adapter()](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.PeftModel.add_adapter) again to attach a new adapter to the base model.\n\nCopied\n\n```\nmodel.add_adapter(lora_config_2, adapter_name=\"adapter_2\")\n```\n\nThen you can use [set\\_adapter()](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.PeftModel.set_adapter) to set the currently active adapter.\n\nCopied\n\n```\nmodel.set_adapter(\"adapter_1\")\noutput = model.generate(**inputs)\nprint(tokenizer.decode(output_disabled[0], skip_special_tokens=True))\n```\n\nTo disable the adapter, call the [disable\\_adapters](https://github.com/huggingface/transformers/blob/4e3490f79b40248c53ee54365a9662611e880892/src/transformers/integrations/peft.py#L313) method.\n\nCopied\n\n```\nmodel.disable_adapters()\n```\n\nThe [enable\\_adapters](https://github.com/huggingface/transformers/blob/4e3490f79b40248c53ee54365a9662611e880892/src/transformers/integrations/peft.py#L336) can be used to enable the adapters again.\n\nIf you\u2019re curious, check out the [Load and train adapters with PEFT](https://huggingface.co/docs/transformers/main/peft) tutorial to learn more.\n\n[Update on GitHub](https://github.com/huggingface/peft/blob/main/docs/source/tutorial/peft_integrations.md)\n\n[", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "PEFT integrations", "description": "Hugging Face Official Documentation of PEFT integrations", "url": "https://huggingface.co/docs/peft/main/en/tutorial/peft_integrations", "source": "hf", "id": "8697fba2-96a1-4b60-9728-4f173e8a6f22"}, "page_content": "880892/src/transformers/integrations/peft.py#L313) method.\n\nCopied\n\n```\nmodel.disable_adapters()\n```\n\nThe [enable\\_adapters](https://github.com/huggingface/transformers/blob/4e3490f79b40248c53ee54365a9662611e880892/src/transformers/integrations/peft.py#L336) can be used to enable the adapters again.\n\nIf you\u2019re curious, check out the [Load and train adapters with PEFT](https://huggingface.co/docs/transformers/main/peft) tutorial to learn more.\n\n[Update on GitHub](https://github.com/huggingface/peft/blob/main/docs/source/tutorial/peft_integrations.md)\n\n[\u2190Configurations and models](https://huggingface.co/docs/peft/main/en/tutorial/peft_model_config) [Prompt-based methods\u2192](https://huggingface.co/docs/peft/main/en/task_guides/prompt_based_methods)[SEP]", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Prompt-based methods", "description": "Hugging Face Official Documentation of Prompt-based methods", "url": "https://huggingface.co/docs/peft/main/en/task_guides/prompt_based_methods", "source": "hf", "id": "55a7ca57-98d5-4b03-9bad-22eda3c99b81"}, "page_content": "[CLS]# Prompt-based methods\n\nA prompt can describe a task or provide an example of a task you want the model to learn. Instead of manually creating these prompts, soft prompting methods add learnable parameters to the input embeddings that can be optimized for a specific task while keeping the pretrained model\u2019s parameters frozen. This makes it both faster and easier to finetune large language models (LLMs) for new downstream tasks.\n\nThe PEFT library supports several types of prompting methods (p-tuning, prefix tuning, prompt tuning) and you can learn more about how these methods work conceptually in the [Soft prompts](https://huggingface.co/docs/peft/main/en/conceptual_guides/prompting) guide. If you\u2019re interested in applying these methods to other tasks and use cases, take a look at our [notebook collection](https://huggingface.co/spaces/PEFT/soft-prompting)!\n\nThis guide will show you how to train a causal language model - with a soft prompting method - to _generate a classification_ for whether a tweet is a complaint or not.\n\n> Some familiarity with the general process of training a causal language model would be really helpful and allow you to focus on the soft prompting methods. If you\u2019re new, we recommend taking a look at the [Causal language modeling](https://huggingface.co/docs/transformers/tasks/language_modeling) guide first from the Transformers documentation. When you\u2019re ready, come back and see how easy it is to drop PEFT in to your training!\n\nBefore you begin, make sure you have all the necessary libraries installed.\n\nCopied\n\n```\npip install -q peft transformers datasets\n```\n\n## Dataset\n\nFor this guide, you\u2019ll use the `twitter_complaints` subset of the [RAFT](https://huggingface.co/datasets/ought/raft) dataset. The `twitter_complaints` subset contains tweets labeled as `complaint` and `no complaint` and you can check out the [dataset viewer](https://huggingface.co/datasets/ought/raft/viewer/twitter_complaints) for a better idea of what the data looks like.\n\nUse the [load\\_dataset](https://huggingface.co/docs/datasets/main/en/package_reference/loading_methods#datasets.load_dataset) function to load the dataset and create a new `text_label` column so it is easier to understand what the `Label` values, `1` and `2` mean.\n\nCopied\n\n```\nfrom datasets import load_dataset\n\nds = load_dataset(\n    \"parquet\",\n    data_files={\n        \"train\": \"hf://datasets/ought/raft@refs/convert/parquet/twitter_complaints/train/0000.parquet\",\n        \"test\": \"hf://datasets/ought/raft@refs/convert/parquet/twitter_complaints/test/0000.parquet\"\n    }\n)\n\nclasses = [k.replace(\"_\", \" \") for k in ds[\"train\"].features[\"Label\"].names]\nds = ds.map(\n    lambda x: {\"text_label\": [classes[label] for label in x[\"Label\"]]},\n    batched=True,\n    num_proc=1,\n)\nds[\"train\"][0]\n{\"Tweet text\": \"@HMRCcustomers No this is my first job\", \"ID\": 0, \"Label\": 2, \"text_label\": \"no complaint\"}\n```\n\nLoad a tokenizer, define the padding token to use, and determine the maximum length of the tokenized label.\n\nCopied\n\n```\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloomz-560m\")\nif tokenizer.pad_token_id is None:\n    tokenizer.pad_token_id = tokenizer.eos_token_id\ntarget_max_length = max([len(tokenizer(class_label)[\"input_ids\"]) for class_label in classes])\nprint(target_max_length)\n```\n\nCreate a preprocessing function that tokenizes the tweet text and labels, pad the inputs and labels in each batch, create an attention mask, and truncate sequences to the `max_length`. Then convert the `input_ids`, `attention_mask`, and `labels` to PyTorch tensors.\n\nCopied\n\n```\nimport torch\n\nmax_length = 64\n\ndef preprocess_function(examples, text_column=\"Tweet text\", label_column=\"text_label\"):\n    batch_size = len(examples[text_column])\n    inputs = [f\"{text_column} : {x} Label : \" for x in examples[text_column]]\n    targets = [str(x) for x in examples[label_column]]\n    model_inputs = tokenizer(inputs)\n    labels = tokenizer(targets)\n    classes = [k.replace(\"_\", \" \") for k in ds[\"train\"].features[\"Label\"].names]\n    for i in range(batch_size):\n        sample_input_ids = model_inputs[\"input_ids\"][i]\n        label_input_ids = labels[\"input_ids\"][i]\n        model", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Prompt-based methods", "description": "Hugging Face Official Documentation of Prompt-based methods", "url": "https://huggingface.co/docs/peft/main/en/task_guides/prompt_based_methods", "source": "hf", "id": "9244c49b-6cd8-4b1f-8bf1-78f496874195"}, "page_content": "\n\nCopied\n\n```\nimport torch\n\nmax_length = 64\n\ndef preprocess_function(examples, text_column=\"Tweet text\", label_column=\"text_label\"):\n    batch_size = len(examples[text_column])\n    inputs = [f\"{text_column} : {x} Label : \" for x in examples[text_column]]\n    targets = [str(x) for x in examples[label_column]]\n    model_inputs = tokenizer(inputs)\n    labels = tokenizer(targets)\n    classes = [k.replace(\"_\", \" \") for k in ds[\"train\"].features[\"Label\"].names]\n    for i in range(batch_size):\n        sample_input_ids = model_inputs[\"input_ids\"][i]\n        label_input_ids = labels[\"input_ids\"][i]\n        model_inputs[\"input_ids\"][i] = [tokenizer.pad_token_id] * (\n            max_length - len(sample_input_ids)\n        ) + sample_input_ids\n        model_inputs[\"attention_mask\"][i] = [0] * (max_length - len(sample_input_ids)) + model_inputs[\\\n            \"attention_mask\"\\\n        ][i]\n        labels[\"input_ids\"][i] = [-100] * (max_length - len(label_input_ids)) + label_input_ids\n        model_inputs[\"input_ids\"][i] = torch.tensor(model_inputs[\"input_ids\"][i][:max_length])\n        model_inputs[\"attention_mask\"][i] = torch.tensor(model_inputs[\"attention_mask\"][i][:max_length])\n        labels[\"input_ids\"][i] = torch.tensor(labels[\"input_ids\"][i][:max_length])\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n```\n\nApply the preprocessing function to the entire dataset with the [map](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.map) function, and remove the unprocessed columns because the model won\u2019t need them.\n\nCopied\n\n```\nprocessed_ds = ds.map(\n    preprocess_function,\n    batched=True,\n    num_proc=1,\n    remove_columns=ds[\"train\"].column_names,\n    load_from_cache_file=False,\n    desc=\"Running tokenizer on dataset\",\n)\n```\n\nFinally, create a training and evaluation [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader). You can set `pin_memory=True` to speed up the data transfer to the GPU during training if the samples in your dataset are on a CPU.\n\nCopied\n\n```\nfrom torch.utils.data import DataLoader\nfrom transformers import default_data_collator\n\ntrain_ds = processed_ds[\"train\"]\neval_ds = processed_ds[\"test\"]\n\nbatch_size = 16\n\ntrain_dataloader = DataLoader(train_ds, shuffle=True, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)\neval_dataloader = DataLoader(eval_ds, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)\n```\n\n## Model\n\nNow let\u2019s load a pretrained model to use as the base model for the soft prompt method. This guide uses the [bigscience/bloomz-560m](https://huggingface.co/bigscience/bloomz-560m) model, but you can use any causal language model you want.\n\nCopied\n\n```\nfrom transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\"bigscience/bloomz-560m\")\n```\n\n### PEFT configuration and model\n\nFor any PEFT method, you\u2019ll need to create a configuration which contains all the parameters that specify how the PEFT method should be applied. Once the configuration is setup, pass it to the [get\\_peft\\_model()](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.get_peft_model) function along with the base model to create a trainable [PeftModel](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.PeftModel).\n\n> Call the [print\\_trainable\\_parameters()](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.PeftModel.print_trainable_parameters) method to compare the number of trainable parameters of [PeftModel](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.PeftModel) versus the number of parameters in the base model!\n\np-t", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Prompt-based methods", "description": "Hugging Face Official Documentation of Prompt-based methods", "url": "https://huggingface.co/docs/peft/main/en/task_guides/prompt_based_methods", "source": "hf", "id": "cad023f8-7d95-47ce-96f2-2f7c0d9583c3"}, "page_content": ".co/docs/peft/main/en/package_reference/peft_model#peft.get_peft_model) function along with the base model to create a trainable [PeftModel](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.PeftModel).\n\n> Call the [print\\_trainable\\_parameters()](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.PeftModel.print_trainable_parameters) method to compare the number of trainable parameters of [PeftModel](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.PeftModel) versus the number of parameters in the base model!\n\np-tuning\n\nprefix tuning\n\nprompt tuning\n\n[P-tuning](https://huggingface.co/docs/peft/main/en/conceptual_guides/prompting#p-tuning) adds a trainable embedding tensor where the prompt tokens can be added anywhere in the input sequence. Create a [PromptEncoderConfig](https://huggingface.co/docs/peft/main/en/package_reference/p_tuning#peft.PromptEncoderConfig) with the task type, the number of virtual tokens to add and learn, and the hidden size of the encoder for learning the prompt parameters.\n\nCopied\n\n```\nfrom peft import PromptEncoderConfig, get_peft_model\n\npeft_config = PromptEncoderConfig(task_type=\"CAUSAL_LM\", num_virtual_tokens=20, encoder_hidden_size=128)\nmodel = get_peft_model(model, peft_config)\nmodel.print_trainable_parameters()\n\"trainable params: 300,288 || all params: 559,514,880 || trainable%: 0.05366935013417338\"\n```\n\n### Training\n\nSet up an optimizer and learning rate scheduler.\n\nCopied\n\n```\nfrom transformers import get_linear_schedule_with_warmup\n\nlr = 3e-2\nnum_epochs = 50\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=lr)\nlr_scheduler = get_linear_schedule_with_warmup(\n    optimizer=optimizer,\n    num_warmup_steps=0,\n    num_training_steps=(len(train_dataloader) * num_epochs),\n)\n```\n\nMove the model to the GPU and create a training loop that reports the loss and perplexity for each epoch.\n\nCopied\n\n```\nfrom tqdm import tqdm\n\ndevice = \"cuda\"\nmodel = model.to(device)\n\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0\n    for step, batch in enumerate(tqdm(train_dataloader)):\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs.loss\n        total_loss += loss.detach().float()\n        loss.backward()\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n\n    model.eval()\n    eval_loss = 0\n    eval_preds = []\n    for step, batch in enumerate(tqdm(eval_dataloader)):\n        batch = {k: v.to(device) for k, v in batch.items()}\n        with torch.no_grad():\n            outputs = model(**batch)\n        loss = outputs.loss\n        eval_loss += loss.detach().float()\n        eval_preds.extend(\n            tokenizer.batch_decode(torch.argmax(outputs.logits, -1).detach().cpu().numpy(), skip_special_tokens=True)\n        )\n\n    eval_epoch_loss = eval_loss / len(eval_dataloader)\n    eval_ppl = torch.exp(eval_epoch_loss)\n    train_epoch_loss = total_loss / len(train_dataloader)\n    train_ppl = torch.exp(train_epoch_loss)\n    print(f\"{epoch=}: {train_ppl=} {train_epoch_loss=} {eval_ppl=} {eval_epoch_loss=}\")\n```\n\n## Share your model\n\nOnce training is complete, you can upload your model to the Hub with the [push\\_to\\_hub](https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.push_to_hub) method. You\u2019ll need to login to your Hugging Face account first and enter your token when prompted.\n\nCopied\n\n```\nfrom huggingface_hub import notebook_login\n\naccount = <your-hf-account-name>\npeft_model_id = f\"{account", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Prompt-based methods", "description": "Hugging Face Official Documentation of Prompt-based methods", "url": "https://huggingface.co/docs/peft/main/en/task_guides/prompt_based_methods", "source": "hf", "id": "e4852ec0-aca0-44b1-8860-94ecaf0c2df5"}, "page_content": "oader)\n    train_ppl = torch.exp(train_epoch_loss)\n    print(f\"{epoch=}: {train_ppl=} {train_epoch_loss=} {eval_ppl=} {eval_epoch_loss=}\")\n```\n\n## Share your model\n\nOnce training is complete, you can upload your model to the Hub with the [push\\_to\\_hub](https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.push_to_hub) method. You\u2019ll need to login to your Hugging Face account first and enter your token when prompted.\n\nCopied\n\n```\nfrom huggingface_hub import notebook_login\n\naccount = <your-hf-account-name>\npeft_model_id = f\"{account}/bloomz-560-m-peft-method\"\nmodel.push_to_hub(peft_model_id)\n```\n\nIf you check the model file size in the repository, you\u2019ll see that it is a lot smaller than a full sized model!\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/PEFT-hub-screenshot.png)For example, the adapter weights for a opt-350m model stored on the Hub are only ~6MB compared to the full model size which can be ~700MB.\n\n## Inference\n\nLet\u2019s load the model for inference and test it out on a tweet!\n\nCopied\n\n```\nfrom peft import AutoPeftModelForCausalLM\n\nmodel = AutoPeftModelForCausalLM.from_pretrained(\"peft_model_id\").to(\"cuda\")\ntokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloomz-560m\")\n\ni = 15\ninputs = tokenizer(f'{text_column} : {ds[\"test\"][i][\"Tweet text\"]} Label : ', return_tensors=\"pt\")\nprint(ds[\"test\"][i][\"Tweet text\"])\n\"@NYTsupport i have complained a dozen times &amp; yet my papers are still thrown FAR from my door. Why is this so hard to resolve?\"\n```\n\nCall the [generate](https://huggingface.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationMixin.generate) method to generate the predicted classification label.\n\nCopied\n\n```\nwith torch.no_grad():\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    outputs = model.generate(input_ids=inputs[\"input_ids\"], max_new_tokens=10)\n    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True))\n\"['Tweet text : @NYTsupport i have complained a dozen times &amp; yet my papers are still thrown FAR from my door. Why is this so hard to resolve? Label : complaint']\"\n```\n\n[Update on GitHub](https://github.com/huggingface/peft/blob/main/docs/source/task_guides/prompt_based_methods.md)\n\n[\u2190Integrations](https://huggingface.co/docs/peft/main/en/tutorial/peft_integrations) [LoRA methods\u2192](https://huggingface.co/docs/peft/main/en/task_guides/lora_based_methods)[SEP]", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "LoRA methods", "description": "Hugging Face Official Documentation of LoRA methods", "url": "https://huggingface.co/docs/peft/main/en/task_guides/lora_based_methods", "source": "hf", "id": "ce081c8b-a514-4550-a33a-3bfab736f93f"}, "page_content": "[CLS]# LoRA methods\n\nA popular way to efficiently train large models is to insert (typically in the attention blocks) smaller trainable matrices that are a low-rank decomposition of the delta weight matrix to be learnt during finetuning. The pretrained model\u2019s original weight matrix is frozen and only the smaller matrices are updated during training. This reduces the number of trainable parameters, reducing memory usage and training time which can be very expensive for large models.\n\nThere are several different ways to express the weight matrix as a low-rank decomposition, but [Low-Rank Adaptation (LoRA)](https://huggingface.co/docs/peft/main/en/conceptual_guides/adapter#low-rank-adaptation-lora) is the most common method. The PEFT library supports several other LoRA variants, such as [Low-Rank Hadamard Product (LoHa)](https://huggingface.co/docs/peft/main/en/conceptual_guides/adapter#low-rank-hadamard-product-loha), [Low-Rank Kronecker Product (LoKr)](https://huggingface.co/docs/peft/main/en/conceptual_guides/adapter#low-rank-kronecker-product-lokr), and [Adaptive Low-Rank Adaptation (AdaLoRA)](https://huggingface.co/docs/peft/main/en/conceptual_guides/adapter#adaptive-low-rank-adaptation-adalora). You can learn more about how these methods work conceptually in the [Adapters](https://huggingface.co/docs/peft/main/en/conceptual_guides/adapter) guide. If you\u2019re interested in applying these methods to other tasks and use cases like semantic segmentation, token classification, take a look at our [notebook collection](https://huggingface.co/collections/PEFT/notebooks-6573b28b33e5a4bf5b157fc1)!\n\nAdditionally, PEFT supports the [X-LoRA](https://huggingface.co/docs/peft/main/en/conceptual_guides/adapter#mixture-of-lora-experts-x-lora) Mixture of LoRA Experts method.\n\nThis guide will show you how to quickly train an image classification model - with a low-rank decomposition method - to identify the class of food shown in an image.\n\n> Some familiarity with the general process of training an image classification model would be really helpful and allow you to focus on the low-rank decomposition methods. If you\u2019re new, we recommend taking a look at the [Image classification](https://huggingface.co/docs/transformers/tasks/image_classification) guide first from the Transformers documentation. When you\u2019re ready, come back and see how easy it is to drop PEFT in to your training!\n\nBefore you begin, make sure you have all the necessary libraries installed.\n\nCopied\n\n```\npip install -q peft transformers datasets\n```\n\n## Dataset\n\nIn this guide, you\u2019ll use the [Food-101](https://huggingface.co/datasets/food101) dataset which contains images of 101 food classes (take a look at the [dataset viewer](https://huggingface.co/datasets/food101/viewer/default/train) to get a better idea of what the dataset looks like).\n\nLoad the dataset with the [load\\_dataset](https://huggingface.co/docs/datasets/main/en/package_reference/loading_methods#datasets.load_dataset) function.\n\nCopied\n\n```\nfrom datasets import load_dataset\n\nds = load_dataset(\"food101\")\n```\n\nEach food class is labeled with an integer, so to make it easier to understand what these integers represent, you\u2019ll create a `label2id` and `id2label` dictionary to map the integer to its class label.\n\nCopied\n\n```\nlabels = ds[\"train\"].features[\"label\"].names\nlabel2id, id2label = dict(), dict()\nfor i, label in enumerate(labels):\n    label2id[label] = i\n    id2label[i] = label\n\nid2label[2]\n\"baklava\"\n```\n\nLoad an image processor to properly resize and normalize the pixel values of the training and evaluation images.\n\nCopied\n\n```\nfrom transformers import AutoImageProcessor\n\nimage_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n```\n\nYou can also use the image processor to prepare some transformation functions for data augmentation and pixel scaling.\n\nCopied\n\n```\nfrom torchvision.transforms import (\n    CenterCrop,\n    Compose,\n    Normalize,\n    RandomHorizontalFlip,\n    RandomResizedCrop,\n    Resize,\n    ToTensor,\n)\n\nnormalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\ntrain_transforms = Compose(\n    [\\\n        RandomResizedCrop(image_processor.size[\"height\"]),\\\n        RandomHorizontalFlip(),\\\n        ToTensor(),\\\n        normalize,\\\n", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "LoRA methods", "description": "Hugging Face Official Documentation of LoRA methods", "url": "https://huggingface.co/docs/peft/main/en/task_guides/lora_based_methods", "source": "hf", "id": "098b3dee-3664-4ea1-b6b4-e9bd076d0eeb"}, "page_content": "\n```\nfrom transformers import AutoImageProcessor\n\nimage_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n```\n\nYou can also use the image processor to prepare some transformation functions for data augmentation and pixel scaling.\n\nCopied\n\n```\nfrom torchvision.transforms import (\n    CenterCrop,\n    Compose,\n    Normalize,\n    RandomHorizontalFlip,\n    RandomResizedCrop,\n    Resize,\n    ToTensor,\n)\n\nnormalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\ntrain_transforms = Compose(\n    [\\\n        RandomResizedCrop(image_processor.size[\"height\"]),\\\n        RandomHorizontalFlip(),\\\n        ToTensor(),\\\n        normalize,\\\n    ]\n)\n\nval_transforms = Compose(\n    [\\\n        Resize(image_processor.size[\"height\"]),\\\n        CenterCrop(image_processor.size[\"height\"]),\\\n        ToTensor(),\\\n        normalize,\\\n    ]\n)\n\ndef preprocess_train(example_batch):\n    example_batch[\"pixel_values\"] = [train_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]]\n    return example_batch\n\ndef preprocess_val(example_batch):\n    example_batch[\"pixel_values\"] = [val_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]]\n    return example_batch\n```\n\nDefine the training and validation datasets, and use the [set\\_transform](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.set_transform) function to apply the transformations on-the-fly.\n\nCopied\n\n```\ntrain_ds = ds[\"train\"]\nval_ds = ds[\"validation\"]\n\ntrain_ds.set_transform(preprocess_train)\nval_ds.set_transform(preprocess_val)\n```\n\nFinally, you\u2019ll need a data collator to create a batch of training and evaluation data and convert the labels to `torch.tensor` objects.\n\nCopied\n\n```\nimport torch\n\ndef collate_fn(examples):\n    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n    labels = torch.tensor([example[\"label\"] for example in examples])\n    return {\"pixel_values\": pixel_values, \"labels\": labels}\n```\n\n## Model\n\nNow let\u2019s load a pretrained model to use as the base model. This guide uses the [google/vit-base-patch16-224-in21k](https://huggingface.co/google/vit-base-patch16-224-in21k) model, but you can use any image classification model you want. Pass the `label2id` and `id2label` dictionaries to the model so it knows how to map the integer labels to their class labels, and you can optionally pass the `ignore_mismatched_sizes=True` parameter if you\u2019re finetuning a checkpoint that has already been finetuned.\n\nCopied\n\n```\nfrom transformers import AutoModelForImageClassification, TrainingArguments, Trainer\n\nmodel = AutoModelForImageClassification.from_pretrained(\n    \"google/vit-base-patch16-224-in21k\",\n    label2id=label2id,\n    id2label=id2label,\n    ignore_mismatched_sizes=True,\n)\n```\n\n### PEFT configuration and model\n\nEvery PEFT method requires a configuration that holds all the parameters specifying how the PEFT method should be applied. Once the configuration is setup, pass it to the [get\\_peft\\_model()](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.get_peft_model) function along with the base model to create a trainable [PeftModel](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.PeftModel).\n\n> Call the [print\\_trainable\\_parameters()](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.PeftModel.print_trainable_parameters) method to compare the number of parameters of [PeftModel](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.PeftModel) versus the number of parameters in the base model!\n\nLoRA\n\nLoHa\n\nLoKr\n\nAdaLoRA\n\n[LoRA](https://huggingface.co/docs/peft/main/en/conceptual_guides/adapter#low-rank-adaptation-lora) decomposes the weight update matrix into _two_ smaller matrices. The size of these low-rank matrices is determined by its _rank_ or `r`. A higher rank means the model has more parameters to train, but it also means the", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "LoRA methods", "description": "Hugging Face Official Documentation of LoRA methods", "url": "https://huggingface.co/docs/peft/main/en/task_guides/lora_based_methods", "source": "hf", "id": "292e5d06-9b7d-4b0e-8f7e-c4f86c64efda"}, "page_content": "co/docs/peft/main/en/package_reference/peft_model#peft.PeftModel.print_trainable_parameters) method to compare the number of parameters of [PeftModel](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.PeftModel) versus the number of parameters in the base model!\n\nLoRA\n\nLoHa\n\nLoKr\n\nAdaLoRA\n\n[LoRA](https://huggingface.co/docs/peft/main/en/conceptual_guides/adapter#low-rank-adaptation-lora) decomposes the weight update matrix into _two_ smaller matrices. The size of these low-rank matrices is determined by its _rank_ or `r`. A higher rank means the model has more parameters to train, but it also means the model has more learning capacity. You\u2019ll also want to specify the `target_modules` which determine where the smaller matrices are inserted. For this guide, you\u2019ll target the _query_ and _value_ matrices of the attention blocks. Other important parameters to set are `lora_alpha` (scaling factor), `bias` (whether `none`, `all` or only the LoRA bias parameters should be trained), and `modules_to_save` (the modules apart from the LoRA layers to be trained and saved). All of these parameters - and more - are found in the [LoraConfig](https://huggingface.co/docs/peft/main/en/package_reference/lora#peft.LoraConfig).\n\nCopied\n\n```\nfrom peft import LoraConfig, get_peft_model\n\nconfig = LoraConfig(\n    r=16,\n    lora_alpha=16,\n    target_modules=[\"query\", \"value\"],\n    lora_dropout=0.1,\n    bias=\"none\",\n    modules_to_save=[\"classifier\"],\n)\nmodel = get_peft_model(model, config)\nmodel.print_trainable_parameters()\n\"trainable params: 667,493 || all params: 86,543,818 || trainable%: 0.7712775047664294\"\n```\n\n### Training\n\nFor training, let\u2019s use the [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) class from Transformers. The `Trainer` contains a PyTorch training loop, and when you\u2019re ready, call [train](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.train) to start training. To customize the training run, configure the training hyperparameters in the [TrainingArguments](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments) class. With LoRA-like methods, you can afford to use a higher batch size and learning rate.\n\n> AdaLoRA has an [update\\_and\\_allocate()](https://huggingface.co/docs/peft/main/en/package_reference/adalora#peft.AdaLoraModel.update_and_allocate) method that should be called at each training step to update the parameter budget and mask, otherwise the adaptation step is not performed. This requires writing a custom training loop or subclassing the [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) to incorporate this method. As an example, take a look at this [custom training loop](https://github.com/huggingface/peft/blob/912ad41e96e03652cabf47522cd876076f7a0c4f/examples/conditional_generation/peft_adalora_seq2seq.py#L120).\n\nCopied\n\n```\nfrom transformers import TrainingArguments, Trainer\n\naccount = \"stevhliu\"\npeft_model_id = f\"{account}/google/vit-base-patch16-224-in21k-lora\"\nbatch_size = 128\n\nargs = TrainingArguments(\n    peft_model_id,\n    remove_unused_columns=False,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=5e-3,\n    per_device_train_batch_size=batch_size,\n    gradient_accumulation_steps=4,\n    per_device_eval_batch_size=batch_size,\n    fp16=True,\n    num_train_epochs=5,\n    logging_steps=10,\n    load_best_model_at_end=True,\n    label_names=[\"labels\"],\n)\n```\n\nBegin training with [train](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.train).\n\nCopied\n\n```\ntrainer = Trainer(\n    model,\n    args,\n    train_dataset=train_ds,\n    eval_dataset=val", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "LoRA methods", "description": "Hugging Face Official Documentation of LoRA methods", "url": "https://huggingface.co/docs/peft/main/en/task_guides/lora_based_methods", "source": "hf", "id": "ba21626e-e758-4453-af5a-0aed6baffb45"}, "page_content": "och\",\n    save_strategy=\"epoch\",\n    learning_rate=5e-3,\n    per_device_train_batch_size=batch_size,\n    gradient_accumulation_steps=4,\n    per_device_eval_batch_size=batch_size,\n    fp16=True,\n    num_train_epochs=5,\n    logging_steps=10,\n    load_best_model_at_end=True,\n    label_names=[\"labels\"],\n)\n```\n\nBegin training with [train](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.train).\n\nCopied\n\n```\ntrainer = Trainer(\n    model,\n    args,\n    train_dataset=train_ds,\n    eval_dataset=val_ds,\n    processing_class=image_processor,\n    data_collator=collate_fn,\n)\ntrainer.train()\n```\n\n## Share your model\n\nOnce training is complete, you can upload your model to the Hub with the [push\\_to\\_hub](https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.push_to_hub) method. You\u2019ll need to login to your Hugging Face account first and enter your token when prompted.\n\nCopied\n\n```\nfrom huggingface_hub import notebook_login\n\nnotebook_login()\n```\n\nCall [push\\_to\\_hub](https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.push_to_hub) to save your model to your repositoy.\n\nCopied\n\n```\nmodel.push_to_hub(peft_model_id)\n```\n\n## Inference\n\nLet\u2019s load the model from the Hub and test it out on a food image.\n\nCopied\n\n```\nfrom peft import PeftConfig, PeftModel\nfrom transformers import AutoImageProcessor\nfrom PIL import Image\nimport requests\n\nconfig = PeftConfig.from_pretrained(\"stevhliu/vit-base-patch16-224-in21k-lora\")\nmodel = AutoModelForImageClassification.from_pretrained(\n    config.base_model_name_or_path,\n    label2id=label2id,\n    id2label=id2label,\n    ignore_mismatched_sizes=True,\n)\nmodel = PeftModel.from_pretrained(model, \"stevhliu/vit-base-patch16-224-in21k-lora\")\n\nurl = \"https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/beignets.jpeg\"\nimage = Image.open(requests.get(url, stream=True).raw)\nimage\n```\n\n![](https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/beignets.jpeg)\n\nConvert the image to RGB and return the underlying PyTorch tensors.\n\nCopied\n\n```\nencoding = image_processor(image.convert(\"RGB\"), return_tensors=\"pt\")\n```\n\nNow run the model and return the predicted class!\n\nCopied\n\n```\nwith torch.no_grad():\n    outputs = model(**encoding)\n    logits = outputs.logits\n\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n\"Predicted class: beignets\"\n```\n\n[Update on GitHub](https://github.com/huggingface/peft/blob/main/docs/source/task_guides/lora_based_methods.md)\n\n[\u2190Prompt-based methods](https://huggingface.co/docs/peft/main/en/task_guides/prompt_based_methods) [IA3\u2192](https://huggingface.co/docs/peft/main/en/task_guides/ia3)[SEP]", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "IA3", "description": "Hugging Face Official Documentation of IA3", "url": "https://huggingface.co/docs/peft/main/en/task_guides/ia3", "source": "hf", "id": "f938c25b-0dfb-49df-84de-62aa5ad7f058"}, "page_content": "[CLS]# IA3\n\n[IA3](https://huggingface.co/docs/peft/main/en/conceptual_guides/ia3) multiplies the model\u2019s activations (the keys and values in the self-attention and encoder-decoder attention blocks, and the intermediate activation of the position-wise feedforward network) by three learned vectors. This PEFT method introduces an even smaller number of trainable parameters than LoRA which introduces weight matrices instead of vectors. The original model\u2019s parameters are kept frozen and only these vectors are updated. As a result, it is faster, cheaper and more efficient to finetune for a new downstream task.\n\nThis guide will show you how to train a sequence-to-sequence model with IA3 to _generate a sentiment_ given some financial news.\n\n> Some familiarity with the general process of training a sequence-to-sequence would be really helpful and allow you to focus on how to apply IA3. If you\u2019re new, we recommend taking a look at the [Translation](https://huggingface.co/docs/transformers/tasks/translation) and [Summarization](https://huggingface.co/docs/transformers/tasks/summarization) guides first from the Transformers documentation. When you\u2019re ready, come back and see how easy it is to drop PEFT in to your training!\n\n## Dataset\n\nYou\u2019ll use the sentences\\_allagree subset of the [financial\\_phrasebank](https://huggingface.co/datasets/financial_phrasebank) dataset. This subset contains financial news with 100% annotator agreement on the sentiment label. Take a look at the [dataset viewer](https://huggingface.co/datasets/financial_phrasebank/viewer/sentences_allagree) for a better idea of the data and sentences you\u2019ll be working with.\n\nLoad the dataset with the [load\\_dataset](https://huggingface.co/docs/datasets/main/en/package_reference/loading_methods#datasets.load_dataset) function. This subset of the dataset only contains a train split, so use the `train_test_split` function to create a train and validation split. Create a new `text_label` column so it is easier to understand what the `label` values `0`, `1`, and `2` mean.\n\nCopied\n\n```\nfrom datasets import load_dataset\n\nds = load_dataset(\"financial_phrasebank\", \"sentences_allagree\")\nds = ds[\"train\"].train_test_split(test_size=0.1)\nds[\"validation\"] = ds[\"test\"]\ndel ds[\"test\"]\n\nclasses = ds[\"train\"].features[\"label\"].names\nds = ds.map(\n    lambda x: {\"text_label\": [classes[label] for label in x[\"label\"]]},\n    batched=True,\n    num_proc=1,\n)\n\nds[\"train\"][0]\n{'sentence': 'It will be operated by Nokia, and supported by its Nokia NetAct network and service management system.',\n 'label': 1,\n 'text_label': 'neutral'}\n```\n\nLoad a tokenizer and create a preprocessing function that:\n\n1. tokenizes the inputs, pads and truncates the sequence to the `max_length`\n2. apply the same tokenizer to the labels but with a shorter `max_length` that corresponds to the label\n3. mask the padding tokens\n\nCopied\n\n```\nfrom transformers import AutoTokenizer\n\ntext_column = \"sentence\"\nlabel_column = \"text_label\"\nmax_length = 128\n\ntokenizer = AutoTokenizer.from_pretrained(\"bigscience/mt0-large\")\n\ndef preprocess_function(examples):\n    inputs = examples[text_column]\n    targets = examples[label_column]\n    model_inputs = tokenizer(inputs, max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n    labels = tokenizer(targets, max_length=3, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n    labels = labels[\"input_ids\"]\n    labels[labels == tokenizer.pad_token_id] = -100\n    model_inputs[\"labels\"] = labels\n    return model_inputs\n```\n\nUse the [map](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.map) function to apply the preprocessing function to the entire dataset.\n\nCopied\n\n```\nprocessed_ds = ds.map(\n    preprocess_function,\n    batched=True,\n    num_proc=1,\n    remove_columns=ds[\"train\"].column_names,\n    load_from_cache_file=False,\n    desc=\"Running tokenizer on dataset\",\n)\n```\n\nCreate a training and evaluation [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader), and set `pin_memory=True` to speed up data transfer to the accelerator during training if your dataset samples are on a CPU.\n\nCopied\n\n```\nfrom torch.utils.data import DataLoader\nfrom transformers import", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "IA3", "description": "Hugging Face Official Documentation of IA3", "url": "https://huggingface.co/docs/peft/main/en/task_guides/ia3", "source": "hf", "id": "50bcd259-f3e4-472c-ad1b-22784281a085"}, "page_content": "/main/en/package_reference/main_classes#datasets.Dataset.map) function to apply the preprocessing function to the entire dataset.\n\nCopied\n\n```\nprocessed_ds = ds.map(\n    preprocess_function,\n    batched=True,\n    num_proc=1,\n    remove_columns=ds[\"train\"].column_names,\n    load_from_cache_file=False,\n    desc=\"Running tokenizer on dataset\",\n)\n```\n\nCreate a training and evaluation [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader), and set `pin_memory=True` to speed up data transfer to the accelerator during training if your dataset samples are on a CPU.\n\nCopied\n\n```\nfrom torch.utils.data import DataLoader\nfrom transformers import default_data_collator\n\ntrain_ds = processed_ds[\"train\"]\neval_ds = processed_ds[\"validation\"]\n\nbatch_size = 8\n\ntrain_dataloader = DataLoader(\n    train_ds, shuffle=True, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True\n)\neval_dataloader = DataLoader(eval_ds, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)\n```\n\n## Model\n\nNow you can load a pretrained model to use as the base model for IA3. This guide uses the [bigscience/mt0-large](https://huggingface.co/bigscience/mt0-large) model, but you can use any sequence-to-sequence model you like.\n\nCopied\n\n```\nfrom transformers import AutoModelForSeq2SeqLM\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"bigscience/mt0-large\")\n```\n\n### PEFT configuration and model\n\nAll PEFT methods need a configuration that contains and specifies all the parameters for how the PEFT method should be applied. Create an [IA3Config](https://huggingface.co/docs/peft/main/en/package_reference/ia3#peft.IA3Config) with the task type and set the inference mode to `False`. You can find additional parameters for this configuration in the [API reference](https://huggingface.co/docs/peft/main/en/package_reference/ia3#ia3config).\n\n> Call the [print\\_trainable\\_parameters()](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.PeftModel.print_trainable_parameters) method to compare the number of trainable parameters of [PeftModel](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.PeftModel) versus the number of parameters in the base model!\n\nOnce the configuration is setup, pass it to the [get\\_peft\\_model()](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.get_peft_model) function along with the base model to create a trainable [PeftModel](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.PeftModel).\n\nCopied\n\n```\nfrom peft import IA3Config, get_peft_model\n\npeft_config = IA3Config(task_type=\"SEQ_2_SEQ_LM\")\nmodel = get_peft_model(model, peft_config)\nmodel.print_trainable_parameters()\n\"trainable params: 282,624 || all params: 1,229,863,936 || trainable%: 0.022980103060766553\"\n```\n\n### Training\n\nSet up an optimizer and learning rate scheduler.\n\nCopied\n\n```\nimport torch\nfrom transformers import get_linear_schedule_with_warmup\n\nlr = 8e-3\nnum_epochs = 3\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=lr)\nlr_scheduler = get_linear_schedule_with_warmup(\n    optimizer=optimizer,\n    num_warmup_steps=0,\n    num_training_steps=(len(train_dataloader) * num_epochs),\n)\n```\n\nMove the model to the accelerator and create a training loop that reports the loss and perplexity for each epoch.\n\nCopied\n\n```\nfrom tqdm import tqdm\n\ndevice = torch.accelerator.current_accelerator().type if hasattr(torch, \"accelerator\") else \"cuda\"\nmodel = model.to(device)\n\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0\n    for step, batch in enumerate(tqdm(train_dataloader)):\n        batch = {k: v.to(device) for k, v in batch.items()", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "IA3", "description": "Hugging Face Official Documentation of IA3", "url": "https://huggingface.co/docs/peft/main/en/task_guides/ia3", "source": "hf", "id": "6fe519da-bb5e-4fe3-a356-ee8d13f63c40"}, "page_content": "up(\n    optimizer=optimizer,\n    num_warmup_steps=0,\n    num_training_steps=(len(train_dataloader) * num_epochs),\n)\n```\n\nMove the model to the accelerator and create a training loop that reports the loss and perplexity for each epoch.\n\nCopied\n\n```\nfrom tqdm import tqdm\n\ndevice = torch.accelerator.current_accelerator().type if hasattr(torch, \"accelerator\") else \"cuda\"\nmodel = model.to(device)\n\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0\n    for step, batch in enumerate(tqdm(train_dataloader)):\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs.loss\n        total_loss += loss.detach().float()\n        loss.backward()\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n\n    model.eval()\n    eval_loss = 0\n    eval_preds = []\n    for step, batch in enumerate(tqdm(eval_dataloader)):\n        batch = {k: v.to(device) for k, v in batch.items()}\n        with torch.no_grad():\n            outputs = model(**batch)\n        loss = outputs.loss\n        eval_loss += loss.detach().float()\n        eval_preds.extend(\n            tokenizer.batch_decode(torch.argmax(outputs.logits, -1).detach().cpu().numpy(), skip_special_tokens=True)\n        )\n\n    eval_epoch_loss = eval_loss / len(eval_dataloader)\n    eval_ppl = torch.exp(eval_epoch_loss)\n    train_epoch_loss = total_loss / len(train_dataloader)\n    train_ppl = torch.exp(train_epoch_loss)\n    print(f\"{epoch=}: {train_ppl=} {train_epoch_loss=} {eval_ppl=} {eval_epoch_loss=}\")\n```\n\n## Share your model\n\nAfter training is complete, you can upload your model to the Hub with the [push\\_to\\_hub](https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.push_to_hub) method. You\u2019ll need to login to your Hugging Face account first and enter your token when prompted.\n\nCopied\n\n```\nfrom huggingface_hub import notebook_login\n\naccount = <your-hf-account-name>\npeft_model_id = f\"{account}/mt0-large-ia3\"\nmodel.push_to_hub(peft_model_id)\n```\n\n## Inference\n\nTo load the model for inference, use the [from\\_pretrained()](https://huggingface.co/docs/peft/main/en/package_reference/auto_class#peft.AutoPeftModel.from_pretrained) method. Let\u2019s also load a sentence of financial news from the dataset to generate a sentiment for.\n\nCopied\n\n```\nfrom peft import AutoPeftModelForSeq2SeqLM\n\ndevice = torch.accelerator.current_accelerator().type if hasattr(torch, \"accelerator\") else \"cuda\"\n\nmodel = AutoPeftModelForSeq2SeqLM.from_pretrained(\"<your-hf-account-name>/mt0-large-ia3\").to(device)\ntokenizer = AutoTokenizer.from_pretrained(\"bigscience/mt0-large\")\n\ni = 15\ninputs = tokenizer(ds[\"validation\"][text_column][i], return_tensors=\"pt\")\nprint(ds[\"validation\"][text_column][i])\n\"The robust growth was the result of the inclusion of clothing chain Lindex in the Group in December 2007.\"\n```\n\nCall the [generate](https://huggingface.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationMixin.generate) method to generate the predicted sentiment label.\n\nCopied\n\n```\nwith torch.no_grad():\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    outputs = model.generate(input_ids=inputs[\"input_ids\"], max_new_tokens=10)\n    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True))\n['positive']\n```\n\n[Update on GitHub](https://github.com/huggingface/peft/blob/main/docs/source/task_guides/ia3.md)\n\n[\u2190LoRA methods](https://huggingface.co/docs/peft/main/en/task_guides/lora_based_methods) [Model merging\u2192](https://h", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "IA3", "description": "Hugging Face Official Documentation of IA3", "url": "https://huggingface.co/docs/peft/main/en/task_guides/ia3", "source": "hf", "id": "1deecfe0-6538-4510-8086-3479cc49521f"}, "page_content": ".GenerationMixin.generate) method to generate the predicted sentiment label.\n\nCopied\n\n```\nwith torch.no_grad():\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    outputs = model.generate(input_ids=inputs[\"input_ids\"], max_new_tokens=10)\n    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True))\n['positive']\n```\n\n[Update on GitHub](https://github.com/huggingface/peft/blob/main/docs/source/task_guides/ia3.md)\n\n[\u2190LoRA methods](https://huggingface.co/docs/peft/main/en/task_guides/lora_based_methods) [Model merging\u2192](https://huggingface.co/docs/peft/main/en/developer_guides/model_merging)[SEP]", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Model merging", "description": "Hugging Face Official Documentation of Model merging", "url": "https://huggingface.co/docs/peft/main/en/developer_guides/model_merging", "source": "hf", "id": "78018134-0d9e-421b-b527-72ad6800bc19"}, "page_content": "[CLS]# Model merging\n\nTraining a model for each task can be costly, take up storage space, and the models aren\u2019t able to learn new information to improve their performance. Multitask learning can overcome some of these limitations by training a model to learn several tasks, but it is expensive to train and designing a dataset for it is challenging. _Model merging_ offers a solution to these challenges by combining multiple pretrained models into one model, giving it the combined abilities of each individual model without any additional training.\n\nPEFT provides several methods for merging models like a linear or SVD combination. This guide focuses on two methods that are more efficient for merging LoRA adapters by eliminating redundant parameters:\n\n- [TIES](https://hf.co/papers/2306.01708) \\- TrIm, Elect, and Merge (TIES) is a three-step method for merging models. First, redundant parameters are trimmed, then conflicting signs are resolved into an aggregated vector, and finally the parameters whose signs are the same as the aggregate sign are averaged. This method takes into account that some values (redundant and sign disagreement) can degrade performance in the merged model.\n- [DARE](https://hf.co/papers/2311.03099) \\- Drop And REscale is a method that can be used to prepare for other model merging methods like TIES. It works by randomly dropping parameters according to a drop rate and rescaling the remaining parameters. This helps to reduce the number of redundant and potentially interfering parameters among multiple models.\n\nModels are merged with the [add\\_weighted\\_adapter()](https://huggingface.co/docs/peft/main/en/package_reference/lora#peft.LoraModel.add_weighted_adapter) method, and the specific model merging method is specified in the `combination_type` parameter.\n\n## Merge method\n\nWith TIES and DARE, merging is enabled by setting `combination_type` and `density` to a value of the weights to keep from the individual models. For example, let\u2019s merge three finetuned [TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T](https://huggingface.co/TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T) models: [tinyllama\\_lora\\_nobots](https://huggingface.co/smangrul/tinyllama_lora_norobots), [tinyllama\\_lora\\_sql](https://huggingface.co/smangrul/tinyllama_lora_sql), and [tinyllama\\_lora\\_adcopy](https://huggingface.co/smangrul/tinyllama_lora_adcopy).\n\n> When you\u2019re attempting to merge fully trained models with TIES, you should be aware of any special tokens each model may have added to the embedding layer which are not a part of the original checkpoint\u2019s vocabulary. This may cause an issue because each model may have added a special token to the same embedding position. If this is the case, you should use the [resize\\_token\\_embeddings](https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.resize_token_embeddings) method to avoid merging the special tokens at the same embedding index.\n>\n> This shouldn\u2019t be an issue if you\u2019re only merging LoRA adapters trained from the same base model.\n\nLoad a base model and can use the [load\\_adapter()](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.PeftModel.load_adapter) method to load and assign each adapter a name:\n\nCopied\n\n```\nfrom peft import PeftConfig, PeftModel\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\nconfig = PeftConfig.from_pretrained(\"smangrul/tinyllama_lora_norobots\")\nmodel = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, load_in_4bit=True, device_map=\"auto\").eval()\ntokenizer = AutoTokenizer.from_pretrained(\"smangrul/tinyllama_lora_norobots\")\n\nmodel.config.vocab_size = 32005\nmodel.resize_token_embeddings(32005)\n\nmodel = PeftModel.from_pretrained(model, \"smangrul/tinyllama_lora_norobots\", adapter_name=\"norobots\")\n_ = model.load_adapter(\"smangrul/tinyllama_lora_sql\", adapter_name=\"sql\")\n_ = model.load_adapter(\"smangrul/tinyllama_lora_adcopy\", adapter_name=\"adcopy\")\n```\n\nSet the adapters, weights, `adapter_name`, `combination_type`, and `density` with the [add\\_weighted\\_adapter()](https://huggingface.co/docs/peft/main/en/package_reference/lora#peft.LoraModel.add_weighted_adapter) method.\n\nTIES\n\nDARE\n", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Model merging", "description": "Hugging Face Official Documentation of Model merging", "url": "https://huggingface.co/docs/peft/main/en/developer_guides/model_merging", "source": "hf", "id": "c5c5a0eb-b74e-4161-9a20-1befa4037b68"}, "page_content": " 32005\nmodel.resize_token_embeddings(32005)\n\nmodel = PeftModel.from_pretrained(model, \"smangrul/tinyllama_lora_norobots\", adapter_name=\"norobots\")\n_ = model.load_adapter(\"smangrul/tinyllama_lora_sql\", adapter_name=\"sql\")\n_ = model.load_adapter(\"smangrul/tinyllama_lora_adcopy\", adapter_name=\"adcopy\")\n```\n\nSet the adapters, weights, `adapter_name`, `combination_type`, and `density` with the [add\\_weighted\\_adapter()](https://huggingface.co/docs/peft/main/en/package_reference/lora#peft.LoraModel.add_weighted_adapter) method.\n\nTIES\n\nDARE\n\nWeight values greater than `1.0` typically produce better results because they preserve the correct scale. A good default starting value for the weights is to set all values to `1.0`.\n\nCopied\n\n```\nadapters = [\"norobots\", \"adcopy\", \"sql\"]\nweights = [2.0, 1.0, 1.0]\nadapter_name = \"merge\"\ndensity = 0.2\nmodel.add_weighted_adapter(adapters, weights, adapter_name, combination_type=\"ties\", density=density)\n```\n\nSet the newly merged model as the active model with the [set\\_adapter()](https://huggingface.co/docs/peft/main/en/package_reference/tuners#peft.tuners.tuners_utils.BaseTuner.set_adapter) method.\n\nCopied\n\n```\nmodel.set_adapter(\"merge\")\n```\n\nNow you can use the merged model as an instruction-tuned model to write ad copy or SQL queries!\n\ninstruct\n\nad copy\n\nSQL\n\nCopied\n\n```\ndevice = torch.accelerator.current_accelerator().type if hasattr(torch, \"accelerator\") else \"cuda\"\nmessages = [\\\n    {\"role\": \"user\", \"content\": \"Write an essay about Generative AI.\"},\\\n]\ntext = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\ninputs = tokenizer(text, return_tensors=\"pt\")\ninputs = {k: v.to(device) for k, v in inputs.items()}\noutputs = model.generate(**inputs, max_new_tokens=256, do_sample=True, top_p=0.95, temperature=0.2, repetition_penalty=1.2, eos_token_id=tokenizer.eos_token_id)\nprint(tokenizer.decode(outputs[0]))\n```\n\n## Merging (IA)\u00b3 Models\n\nThe (IA)\u00b3 models facilitate linear merging of adapters. To merge adapters in an (IA)\u00b3 model, utilize the `add_weighted_adapter` method from the `IA3Model` class. This method is analogous to the `add_weighted_adapter` method used in `LoraModel`, with the key difference being the absence of the `combination_type` parameter. For example, to merge three (IA)\u00b3 adapters into a PEFT model, you would proceed as follows:\n\nCopied\n\n```\nadapters = [\"adapter1\", \"adapter2\", \"adapter3\"]\nweights = [0.4, 0.3, 0.3]\nadapter_name = \"merge\"\nmodel.add_weighted_adapter(adapters, weights, adapter_name)\n```\n\nIt is recommended that the weights sum to 1.0 to preserve the scale of the model. The merged model can then be set as the active model using the `set_adapter` method:\n\nCopied\n\n```\nmodel.set_adapter(\"merge\")\n```\n\n[Update on GitHub](https://github.com/huggingface/peft/blob/main/docs/source/developer_guides/model_merging.md)\n\n[\u2190IA3](https://huggingface.co/docs/peft/main/en/task_guides/ia3) [Quantization\u2192](https://huggingface.co/docs/peft/main/en/developer_guides/quantization)[SEP]", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Model merging", "description": "Hugging Face Official Documentation of Model merging", "url": "https://huggingface.co/docs/peft/main/en/developer_guides/model_merging", "source": "hf", "id": "88e3b854-40c4-4e24-b9b9-23d19691f8f0"}, "page_content": "/quantization)[SEP]", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Quantization", "description": "Hugging Face Official Documentation of Quantization", "url": "https://huggingface.co/docs/peft/main/en/developer_guides/quantization", "source": "hf", "id": "b78d4045-bc19-4979-a0b6-71980ed54713"}, "page_content": "[CLS]# Quantization\n\nQuantization represents data with fewer bits, making it a useful technique for reducing memory-usage and accelerating inference especially when it comes to large language models (LLMs). There are several ways to quantize a model including:\n\n- optimizing which model weights are quantized with the [AWQ](https://hf.co/papers/2306.00978) algorithm\n- independently quantizing each row of a weight matrix with the [GPTQ](https://hf.co/papers/2210.17323) algorithm\n- quantizing to 8-bit and 4-bit precision with the [bitsandbytes](https://github.com/TimDettmers/bitsandbytes) library\n- quantizing to as low as 2-bit precision with the [AQLM](https://huggingface.co/papers/2401.06118) algorithm\n\nHowever, after a model is quantized it isn\u2019t typically further trained for downstream tasks because training can be unstable due to the lower precision of the weights and activations. But since PEFT methods only add _extra_ trainable parameters, this allows you to train a quantized model with a PEFT adapter on top! Combining quantization with PEFT can be a good strategy for training even the largest models on a single GPU. For example, [QLoRA](https://hf.co/papers/2305.14314) is a method that quantizes a model to 4-bits and then trains it with LoRA. This method allows you to finetune a 65B parameter model on a single 48GB GPU!\n\nIn this guide, you\u2019ll see how to quantize a model to 4-bits and train it with LoRA.\n\n## Quantize a model\n\n[bitsandbytes](https://github.com/TimDettmers/bitsandbytes) is a quantization library with a Transformers integration. With this integration, you can quantize a model to 8 or 4-bits and enable many other options by configuring the [BitsAndBytesConfig](https://huggingface.co/docs/transformers/main/en/main_classes/quantization#transformers.BitsAndBytesConfig) class. For example, you can:\n\n- set `load_in_4bit=True` to quantize the model to 4-bits when you load it\n- set `bnb_4bit_quant_type=\"nf4\"` to use a special 4-bit data type for weights initialized from a normal distribution\n- set `bnb_4bit_use_double_quant=True` to use a nested quantization scheme to quantize the already quantized weights\n- set `bnb_4bit_compute_dtype=torch.bfloat16` to use bfloat16 for faster computation\n\nCopied\n\n```\nimport torch\nfrom transformers import BitsAndBytesConfig\n\nconfig = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=torch.bfloat16,\n)\n```\n\nPass the `config` to the [from\\_pretrained](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoModelForCausalLM.from_pretrained) method.\n\nCopied\n\n```\nfrom transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", quantization_config=config)\n```\n\nNext, you should call the [prepare\\_model\\_for\\_kbit\\_training()](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.prepare_model_for_kbit_training) function to preprocess the quantized model for training.\n\nCopied\n\n```\nfrom peft import prepare_model_for_kbit_training\n\nmodel = prepare_model_for_kbit_training(model)\n```\n\nNow that the quantized model is ready, let\u2019s set up a configuration.\n\n## LoraConfig\n\nCreate a [LoraConfig](https://huggingface.co/docs/peft/main/en/package_reference/lora#peft.LoraConfig) with the following parameters (or choose your own):\n\nCopied\n\n```\nfrom peft import LoraConfig\n\nconfig = LoraConfig(\n    r=16,\n    lora_alpha=8,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n```\n\nThen use the [get\\_peft\\_model()](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.get_peft_model) function to create a [PeftModel](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.PeftModel) from the quantized model and configuration.\n\n", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Quantization", "description": "Hugging Face Official Documentation of Quantization", "url": "https://huggingface.co/docs/peft/main/en/developer_guides/quantization", "source": "hf", "id": "d93cc602-0cb4-4dd0-94c7-3e5eee20fa65"}, "page_content": "Copied\n\n```\nfrom peft import LoraConfig\n\nconfig = LoraConfig(\n    r=16,\n    lora_alpha=8,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n```\n\nThen use the [get\\_peft\\_model()](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.get_peft_model) function to create a [PeftModel](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.PeftModel) from the quantized model and configuration.\n\nCopied\n\n```\nfrom peft import get_peft_model\n\nmodel = get_peft_model(model, config)\n```\n\nYou\u2019re all set for training with whichever training method you prefer!\n\n### LoftQ initialization\n\n[LoftQ](https://hf.co/papers/2310.08659) initializes LoRA weights such that the quantization error is minimized, and it can improve performance when training quantized models. To get started, follow [these instructions](https://github.com/huggingface/peft/tree/main/examples/loftq_finetuning).\n\nIn general, for LoftQ to work best, it is recommended to target as many layers with LoRA as possible, since those not targeted cannot have LoftQ applied. This means that passing `LoraConfig(..., target_modules=\"all-linear\")` will most likely give the best results. Also, you should use `nf4` as quant type in your quantization config when using 4bit quantization, i.e. `BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\")`.\n\n### QLoRA-style training\n\nQLoRA adds trainable weights to all the linear layers in the transformer architecture. Since the attribute names for these linear layers can vary across architectures, set `target_modules` to `\"all-linear\"` to add LoRA to all the linear layers:\n\nCopied\n\n```\nconfig = LoraConfig(target_modules=\"all-linear\",...)\n```\n\n## GPTQ quantization\n\nYou can learn more about gptq based `[2, 3, 4, 8]` bits quantization at [GPTQModel](https://github.com/ModelCloud/GPTQModel) and the Transformers [GPTQ](https://huggingface.co/docs/transformers/quantization/gptq) doc. Post-quant training, PEFT can use both [GPTQModel](https://github.com/ModelCloud/GPTQModel) or [AutoGPTQ](https://github.com/autogptq/autogptq) libraries, but we recommend GPTQModel because AutoGPTQ will be deprecated in a future release.\n\nCopied\n\n```\n# gptqmodel install\npip install gptqmodel --no-build-isolation\n```\n\nCopied\n\n```\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, GPTQConfig\n\nmodel_id = \"facebook/opt-125m\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\ngptq_config = GPTQConfig(bits=4, group_size=128, dataset=\"wikitext2\", tokenizer=tokenizer)\n\nquantized_model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", quantization_config=gptq_config)\n\n# save quantized model\nquantized_model.save_pretrained(\"./opt-125m-gptq\")\ntokenizer.save_pretrained(\"./opt-125m-gptq\")\n```\n\nOnce quantized, you can post-train GPTQ models with PEFT APIs.\n\n## AQLM quantization\n\nAdditive Quantization of Language Models ( [AQLM](https://huggingface.co/papers/2401.06118)) is a Large Language Models compression method. It quantizes multiple weights together and takes advantage of interdependencies between them. AQLM represents groups of 8-16 weights as a sum of multiple vector codes. This allows it to compress models down to as low as 2-bit with considerably low accuracy losses.\n\nSince the AQLM quantization process is computationally expensive, the use of prequantized models is recommended. A partial list of available models can be found in the official aqlm [repository](https://github.com/Vahe1994/AQLM).\n\nThe models support LoRA adapter tuning. To tune the quantized model you\u2019ll need to install the `aqlm` inference library: `pip install aqlm>=1.0.2`. Finetuned LoRA adapters shall be saved separately, as merging them with AQLM quantized weights is not possible.\n\nCopied\n\n```\nquantized_model = AutoModelForCausalLM.from_pretrained(\n    \"BlackSamorez/Mixtral-", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Quantization", "description": "Hugging Face Official Documentation of Quantization", "url": "https://huggingface.co/docs/peft/main/en/developer_guides/quantization", "source": "hf", "id": "b79dcb4e-9fa8-4ca0-8302-c0517efd03a2"}, "page_content": " AQLM represents groups of 8-16 weights as a sum of multiple vector codes. This allows it to compress models down to as low as 2-bit with considerably low accuracy losses.\n\nSince the AQLM quantization process is computationally expensive, the use of prequantized models is recommended. A partial list of available models can be found in the official aqlm [repository](https://github.com/Vahe1994/AQLM).\n\nThe models support LoRA adapter tuning. To tune the quantized model you\u2019ll need to install the `aqlm` inference library: `pip install aqlm>=1.0.2`. Finetuned LoRA adapters shall be saved separately, as merging them with AQLM quantized weights is not possible.\n\nCopied\n\n```\nquantized_model = AutoModelForCausalLM.from_pretrained(\n    \"BlackSamorez/Mixtral-8x7b-AQLM-2Bit-1x16-hf-test-dispatch\",\n    dtype=\"auto\", device_map=\"auto\", low_cpu_mem_usage=True,\n)\n\npeft_config = LoraConfig(...)\n\nquantized_model = get_peft_model(quantized_model, peft_config)\n```\n\nYou can refer to the [Google Colab](https://colab.research.google.com/drive/12GTp1FCj5_0SnnNQH18h_2XFh9vS_guX?usp=sharing) example for an overview of AQLM+LoRA finetuning.\n\n## EETQ quantization\n\nYou can also perform LoRA fine-tuning on EETQ quantized models. [EETQ](https://github.com/NetEase-FuXi/EETQ) package offers simple and efficient way to perform 8-bit quantization, which is claimed to be faster than the `LLM.int8()` algorithm. First, make sure that you have a transformers version that is compatible with EETQ (e.g. by installing it from latest pypi or from source).\n\nCopied\n\n```\nimport torch\nfrom transformers import EetqConfig\n\nconfig = EetqConfig(\"int8\")\n```\n\nPass the `config` to the [from\\_pretrained](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoModelForCausalLM.from_pretrained) method.\n\nCopied\n\n```\nfrom transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", quantization_config=config)\n```\n\nand create a `LoraConfig` and pass it to `get_peft_model`:\n\nCopied\n\n```\nfrom peft import LoraConfig, get_peft_model\n\nconfig = LoraConfig(\n    r=16,\n    lora_alpha=8,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, config)\n```\n\n## HQQ quantization\n\nThe models that are quantized using Half-Quadratic Quantization of Large Machine Learning Models ( [HQQ](https://mobiusml.github.io/hqq_blog/)) support LoRA adapter tuning. To tune the quantized model, you\u2019ll need to install the `hqq` library with: `pip install hqq`.\n\nCopied\n\n```\nfrom hqq.engine.hf import HQQModelForCausalLM\n\ndevice = torch.accelerator.current_accelerator().type if hasattr(torch, \"accelerator\") else \"cuda\"\n\nquantized_model = HQQModelForCausalLM.from_quantized(save_dir_or_hfhub, device=device)\npeft_config = LoraConfig(...)\nquantized_model = get_peft_model(quantized_model, peft_config)\n```\n\nOr using transformers version that is compatible with HQQ (e.g. by installing it from latest pypi or from source).\n\nCopied\n\n```\nfrom transformers import HqqConfig, AutoModelForCausalLM\n\nquant_config = HqqConfig(nbits=4, group_size=64)\nquantized_model = AutoModelForCausalLM.from_pretrained(save_dir_or_hfhub, device_map=device_map, quantization_config=quant_config)\npeft_config = LoraConfig(...)\nquantized_model = get_peft_model(quantized_model, peft_config)\n```\n\n## torchao (PyTorch Architecture Optimization)\n\nPEFT supports models quantized with [torchao](https://github.com/pytorch/ao) (\u201cao\u201d) for int8 quantization.\n\nCopied\n\n```\nfrom peft import LoraConfig, get_peft_model\nfrom transformers import AutoModelForCausalLM", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Quantization", "description": "Hugging Face Official Documentation of Quantization", "url": "https://huggingface.co/docs/peft/main/en/developer_guides/quantization", "source": "hf", "id": "f254a236-2f5c-445b-8a89-764159abe943"}, "page_content": "\n\n```\nfrom transformers import HqqConfig, AutoModelForCausalLM\n\nquant_config = HqqConfig(nbits=4, group_size=64)\nquantized_model = AutoModelForCausalLM.from_pretrained(save_dir_or_hfhub, device_map=device_map, quantization_config=quant_config)\npeft_config = LoraConfig(...)\nquantized_model = get_peft_model(quantized_model, peft_config)\n```\n\n## torchao (PyTorch Architecture Optimization)\n\nPEFT supports models quantized with [torchao](https://github.com/pytorch/ao) (\u201cao\u201d) for int8 quantization.\n\nCopied\n\n```\nfrom peft import LoraConfig, get_peft_model\nfrom transformers import AutoModelForCausalLM, TorchAoConfig\n\nmodel_id =...\nquantization_config = TorchAoConfig(quant_type=\"int8_weight_only\")\nbase_model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config)\npeft_config = LoraConfig(...)\nmodel = get_peft_model(base_model, peft_config)\n```\n\n### Caveats:\n\n- Use the most recent versions of torchao (>= v0.4.0) and transformers (> 4.42).\n- Only linear layers are currently supported.\n- `quant_type = \"int4_weight_only\"` is currently not supported.\n- `NF4` is not implemented in transformers as of yet and is thus also not supported.\n- DoRA only works with `quant_type = \"int8_weight_only\"` at the moment.\n- There is explicit support for torchao when used with LoRA. However, when torchao quantizes a layer, its class does not change, only the type of the underlying tensor. For this reason, PEFT methods other than LoRA will generally also work with torchao, even if not explicitly supported. Be aware, however, that **merging only works correctly with LoRA and with `quant_type = \"int8_weight_only\"`**. If you use a different PEFT method or dtype, merging will likely result in an error, and even it doesn\u2019t, the results will still be incorrect.\n\n## INC quantization\n\nIntel Neural Compressor ( [INC](https://github.com/intel/neural-compressor)) enables model quantization for various devices,\nincluding Intel Gaudi accelerators (also known as HPU devices). You can perform LoRA fine-tuning on models that have been\nquantized using INC. To use INC with PyTorch models, install the library with: `pip install neural-compressor[pt]`.\nQuantizing a model to FP8 precision for HPU devices can be done with the following single-step quantization workflow:\n\nCopied\n\n```\nimport torch\nfrom neural_compressor.torch.quantization import FP8Config, convert, finalize_calibration, prepare\nquant_configs = {\n   ...\n}\nconfig = FP8Config(**quant_configs)\n```\n\nPass the config to the `prepare` method, run inference to gather calibration stats, and call `finalize_calibration`\nand `convert` methods to quantize model to FP8 precision:\n\nCopied\n\n```\nmodel = prepare(model, config)\n# Run inference to collect calibration statistics\n...\n# Finalize calibration and convert the model to FP8 precision\nfinalize_calibration(model)\nmodel = convert(model)\n# Load PEFT LoRA adapter as usual\n...\n```\n\nAn example demonstrating how to load a PEFT LoRA adapter into an INC-quantized FLUX text-to-image model for HPU\ndevices is provided [here](https://github.com/huggingface/peft/blob/main/examples/stable_diffusion/inc_flux_lora_hpu.py).\n\n### Caveats:\n\n- `merge()` and `unmerge()` methods are currently not supported for INC-quantized models.\n- Currently, only **Linear** INC-quantized layers are supported when loading PEFT adapters.\n\n## Other Supported PEFT Methods\n\nBesides LoRA, the following PEFT methods also support quantization:\n\n- **VeRA** (supports bitsandbytes quantization)\n- **AdaLoRA** (supports both bitsandbytes and GPTQ quantization)\n- **(IA)\u00b3** (supports bitsandbytes quantization)\n\n## Next steps\n\nIf you\u2019re interested in learning more about quantization, the following may be helpful:\n\n- Learn more details about QLoRA and check out some benchmarks on its impact in the [Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes) blog post.\n- Read more about different quantization schemes in the Transformers [Quantization](https://hf.co/docs/transformers/main/quantization) guide.\n\n[Update on GitHub](https://github.com/huggingface/peft/blob/main/docs/source/developer_guides/quantization.md)\n\n[\u2190Model merging", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Quantization", "description": "Hugging Face Official Documentation of Quantization", "url": "https://huggingface.co/docs/peft/main/en/developer_guides/quantization", "source": "hf", "id": "0ed29336-2307-4992-ab96-a3e7158fae88"}, "page_content": "LoRA** (supports both bitsandbytes and GPTQ quantization)\n- **(IA)\u00b3** (supports bitsandbytes quantization)\n\n## Next steps\n\nIf you\u2019re interested in learning more about quantization, the following may be helpful:\n\n- Learn more details about QLoRA and check out some benchmarks on its impact in the [Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes) blog post.\n- Read more about different quantization schemes in the Transformers [Quantization](https://hf.co/docs/transformers/main/quantization) guide.\n\n[Update on GitHub](https://github.com/huggingface/peft/blob/main/docs/source/developer_guides/quantization.md)\n\n[\u2190Model merging](https://huggingface.co/docs/peft/main/en/developer_guides/model_merging) [LoRA\u2192](https://huggingface.co/docs/peft/main/en/developer_guides/lora)[SEP]", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "LoRA", "description": "Hugging Face Official Documentation of LoRA", "url": "https://huggingface.co/docs/peft/main/en/developer_guides/lora", "source": "hf", "id": "b0e1d2ce-c63b-49ca-b225-ff5a68d3bf72"}, "page_content": "[CLS]# LoRA\n\nLoRA is low-rank decomposition method to reduce the number of trainable parameters which speeds up finetuning large models and uses less memory. In PEFT, using LoRA is as easy as setting up a [LoraConfig](https://huggingface.co/docs/peft/main/en/package_reference/lora#peft.LoraConfig) and wrapping it with [get\\_peft\\_model()](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.get_peft_model) to create a trainable [PeftModel](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.PeftModel).\n\nThis guide explores in more detail other options and features for using LoRA.\n\n## Initialization\n\nThe initialization of LoRA weights is controlled by the parameter `init_lora_weights` in [LoraConfig](https://huggingface.co/docs/peft/main/en/package_reference/lora#peft.LoraConfig). By default, PEFT initializes LoRA weights with Kaiming-uniform for weight A and zeros for weight B resulting in an identity transform (same as the reference [implementation](https://github.com/microsoft/LoRA)).\n\nIt is also possible to pass `init_lora_weights=\"gaussian\"`. As the name suggests, this initializes weight A with a Gaussian distribution and zeros for weight B (this is how [Diffusers](https://huggingface.co/docs/diffusers/index) initializes LoRA weights).\n\nCopied\n\n```\nfrom peft import LoraConfig\n\nconfig = LoraConfig(init_lora_weights=\"gaussian\",...)\n```\n\nThere is also an option to set `init_lora_weights=False` which is useful for debugging and testing. This should be the only time you use this option. When choosing this option, the LoRA weights are initialized such that they do _not_ result in an identity transform.\n\nCopied\n\n```\nfrom peft import LoraConfig\n\nconfig = LoraConfig(init_lora_weights=False,...)\n```\n\n### PiSSA\n\n[PiSSA](https://huggingface.co/papers/2404.02948) initializes the LoRA adapter using the principal singular values and singular vectors. This straightforward modification allows PiSSA to converge more rapidly than LoRA and ultimately attain superior performance. Moreover, PiSSA reduces the quantization error compared to QLoRA, leading to further enhancements.\n\nConfigure the initialization method to \u201cpissa\u201d, which may take several minutes to execute SVD on the pre-trained model:\n\nCopied\n\n```\nfrom peft import LoraConfig\nconfig = LoraConfig(init_lora_weights=\"pissa\",...)\n```\n\nAlternatively, execute fast SVD, which takes only a few seconds. The number of iterations determines the trade-off between the error and computation time:\n\nCopied\n\n```\nlora_config = LoraConfig(init_lora_weights=\"pissa_niter_[number of iters]\",...)\n```\n\nFor detailed instruction on using PiSSA, please follow [these instructions](https://github.com/huggingface/peft/tree/main/examples/pissa_finetuning).\n\n### CorDA\n\n[CorDA](https://huggingface.co/papers/2406.05223) builds task-aware LoRA adapters from weight decomposition oriented by the context of downstream task to learn (instruction-previewed mode, IPM) or world knowledge to maintain (knowledge-preserved mode, KPM).\nThe KPM not only achieves better performance than LoRA on fine-tuning tasks, but also mitigates the catastrophic forgetting of pre-trained world knowledge.\nWhen preserving pre-trained knowledge is not a concern,\nthe IPM is favored because it can further accelerate convergence and enhance the fine-tuning performance.\n\nYou need to configure the initialization method to \u201ccorda\u201d, and specify the mode of IPM or KPM and the dataset to collect covariance matrices.\n\nCopied\n\n```\n@torch.no_grad()\ndef run_model():\n    # Assume `model` and `dataset` is in context...\n    model.eval()\n    for batch in dataset:\n        model(**batch)\n\ncorda_config = CordaConfig(\n    corda_method=\"kpm\",\n)\nlora_config = LoraConfig(\n    init_lora_weights=\"corda\",\n    corda_config=corda_config,\n)\npreprocess_corda(model, lora_config, run_model=run_model)\npeft_model = get_peft_model(model, lora_config)\n```\n\nFor detailed instruction on using CorDA, please follow [these instructions](https://github.com/huggingface/peft/tree/main/examples/corda_finetuning).\n\n### OLoRA\n\n[OLoRA](https://huggingface.co/papers/2406.01775) utilizes QR decomposition to initialize the LoRA adapters. OLoRA translates the base weights of the model by a", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "LoRA", "description": "Hugging Face Official Documentation of LoRA", "url": "https://huggingface.co/docs/peft/main/en/developer_guides/lora", "source": "hf", "id": "4af6a544-60cd-49f7-bd1f-4b147ee124ce"}, "page_content": ")\n\ncorda_config = CordaConfig(\n    corda_method=\"kpm\",\n)\nlora_config = LoraConfig(\n    init_lora_weights=\"corda\",\n    corda_config=corda_config,\n)\npreprocess_corda(model, lora_config, run_model=run_model)\npeft_model = get_peft_model(model, lora_config)\n```\n\nFor detailed instruction on using CorDA, please follow [these instructions](https://github.com/huggingface/peft/tree/main/examples/corda_finetuning).\n\n### OLoRA\n\n[OLoRA](https://huggingface.co/papers/2406.01775) utilizes QR decomposition to initialize the LoRA adapters. OLoRA translates the base weights of the model by a factor of their QR decompositions, i.e., it mutates the weights before performing any training on them. This approach significantly improves stability, accelerates convergence speed, and ultimately achieves superior performance.\n\nYou just need to pass a single additional option to use OLoRA:\n\nCopied\n\n```\nfrom peft import LoraConfig\nconfig = LoraConfig(init_lora_weights=\"olora\",...)\n```\n\nFor more advanced usage, please refer to our [documentation](https://github.com/huggingface/peft/tree/main/examples/olora_finetuning).\n\n### EVA\n\n[EVA](https://huggingface.co/papers/2410.07170) performs SVD on the input activations of each layer and uses the right-singular vectors to initialize LoRA weights. It is therefore a data-driven initialization scheme. Furthermore EVA adaptively allocates ranks across layers based on their \u201cexplained variance ratio\u201d - a metric derived from the SVD analysis.\n\nYou can use EVA by setting `init_lora_weights=\"eva\"` and defining [EvaConfig](https://huggingface.co/docs/peft/main/en/package_reference/lora#peft.EvaConfig) in [LoraConfig](https://huggingface.co/docs/peft/main/en/package_reference/lora#peft.LoraConfig):\n\nCopied\n\n```\nfrom peft import LoraConfig, EvaConfig\npeft_config = LoraConfig(\n    init_lora_weights = \"eva\",\n    eva_config = EvaConfig(rho = 2.0),\n   ...\n)\n```\n\nThe parameter `rho` (\u2265 1.0) determines how much redistribution is allowed. When `rho=1.0` and `r=16`, LoRA adapters are limited to exactly 16 ranks, preventing any redistribution from occurring. A recommended value for EVA with redistribution is 2.0, meaning the maximum rank allowed for a layer is 2r.\n\nIt is recommended to perform EVA initialization on an accelerator(e.g. CUDA GPU, Intel XPU) as it is much faster. To optimize the amount of available memory for EVA, you can use the `low_cpu_mem_usage` flag in [get\\_peft\\_model()](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.get_peft_model):\n\nCopied\n\n```\npeft_model = get_peft_model(model, peft_config, low_cpu_mem_usage=True)\n```\n\nThen, call [initialize\\_lora\\_eva\\_weights()](https://huggingface.co/docs/peft/main/en/package_reference/lora#peft.initialize_lora_eva_weights) to initialize the EVA weights (in most cases the dataloader used for eva initialization can be the same as the one used for finetuning):\n\nCopied\n\n```\ninitialize_lora_eva_weights(peft_model, dataloader)\n```\n\nEVA works out of the box with bitsandbytes. Simply initialize the model with `quantization_config` and call [initialize\\_lora\\_eva\\_weights()](https://huggingface.co/docs/peft/main/en/package_reference/lora#peft.initialize_lora_eva_weights) as usual.\n\n> For further instructions on using EVA, please refer to our [documentation](https://github.com/huggingface/peft/tree/main/examples/eva_finetuning).\n\n### LoftQ\n\n#### Standard approach\n\nWhen quantizing the base model for QLoRA training, consider using the [LoftQ initialization](https://huggingface.co/papers/2310.08659), which has been shown to improve performance when training quantized models. The idea is that the LoRA weights are initialized such that the quantization error is minimized. To use LoftQ, follow [these instructions](https://github.com/huggingface/peft/tree/main/examples/loftq_finetuning).\n\nIn general, for LoftQ to work best, it is recommended to target as many layers with LoRA as possible, since those not targeted cannot have LoftQ applied. This means that passing", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "LoRA", "description": "Hugging Face Official Documentation of LoRA", "url": "https://huggingface.co/docs/peft/main/en/developer_guides/lora", "source": "hf", "id": "c2862a82-3086-42a4-81c4-1bc681d7e65d"}, "page_content": " using EVA, please refer to our [documentation](https://github.com/huggingface/peft/tree/main/examples/eva_finetuning).\n\n### LoftQ\n\n#### Standard approach\n\nWhen quantizing the base model for QLoRA training, consider using the [LoftQ initialization](https://huggingface.co/papers/2310.08659), which has been shown to improve performance when training quantized models. The idea is that the LoRA weights are initialized such that the quantization error is minimized. To use LoftQ, follow [these instructions](https://github.com/huggingface/peft/tree/main/examples/loftq_finetuning).\n\nIn general, for LoftQ to work best, it is recommended to target as many layers with LoRA as possible, since those not targeted cannot have LoftQ applied. This means that passing `LoraConfig(..., target_modules=\"all-linear\")` will most likely give the best results. Also, you should use `nf4` as quant type in your quantization config when using 4bit quantization, i.e. `BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\")`.\n\n#### A more convenient way\n\nAn easier but more limited way to apply LoftQ initialization is to use the convenience function `replace_lora_weights_loftq`. This takes the quantized PEFT model as input and replaces the LoRA weights in-place with their LoftQ-initialized counterparts.\n\nCopied\n\n```\nfrom peft import replace_lora_weights_loftq\nfrom transformers import BitsAndBytesConfig\n\nbnb_config = BitsAndBytesConfig(load_in_4bit=True,...)\nbase_model = AutoModelForCausalLM.from_pretrained(..., quantization_config=bnb_config)\n# note: don't pass init_lora_weights=\"loftq\" or loftq_config!\nlora_config = LoraConfig(task_type=\"CAUSAL_LM\")\npeft_model = get_peft_model(base_model, lora_config)\nreplace_lora_weights_loftq(peft_model)\n```\n\n`replace_lora_weights_loftq` also allows you to pass a `callback` argument to give you more control over which layers should be modified or not, which empirically can improve the results quite a lot. To see a more elaborate example of this, check out [this notebook](https://github.com/huggingface/peft/blob/main/examples/loftq_finetuning/LoftQ_weight_replacement.ipynb).\n\n`replace_lora_weights_loftq` implements only one iteration step of LoftQ. This means that only the LoRA weights are updated, instead of iteratively updating LoRA weights and quantized base model weights. This may lead to lower performance but has the advantage that we can use the original quantized weights derived from the base model, instead of having to keep an extra copy of modified quantized weights. Whether this tradeoff is worthwhile depends on the use case.\n\nAt the moment, `replace_lora_weights_loftq` has these additional limitations:\n\n- Model files must be stored as a `safetensors` file.\n- Only bitsandbytes 4bit quantization is supported.\n\n> Learn more about how PEFT works with quantization in the [Quantization](https://huggingface.co/docs/peft/main/en/developer_guides/quantization) guide.\n\n### Rank-stabilized LoRA\n\nAnother way to initialize [LoraConfig](https://huggingface.co/docs/peft/main/en/package_reference/lora#peft.LoraConfig) is with the [rank-stabilized LoRA (rsLoRA)](https://huggingface.co/papers/2312.03732) method. The LoRA architecture scales each adapter during every forward pass by a fixed scalar which is set at initialization and depends on the rank `r`. The scalar is given by `lora_alpha/r` in the original implementation, but rsLoRA uses `lora_alpha/math.sqrt(r)` which stabilizes the adapters and increases the performance potential from using a higher `r`.\n\nCopied\n\n```\nfrom peft import LoraConfig\n\nconfig = LoraConfig(use_rslora=True,...)\n```\n\n### Activated LoRA (aLoRA)\n\nActivated LoRA (aLoRA) is a low rank adapter architecture for Causal LMs that allows for reusing existing base model KV cache for more efficient inference. This approach is best suited for inference pipelines which rely on the base model for most tasks/generations, but use aLoRA adapter(s) to perform specialized task(s) within the chain. For example, checking or correcting generated outputs of the base model. In these settings, inference times can be sped up by an order of magnitude or more. For more information on aLoRA and many example use cases, see [https://huggingface.co/papers/2504.12397](https://huggingface.co/papers/2504.12397).\n\nThis technique scans for", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "LoRA", "description": "Hugging Face Official Documentation of LoRA", "url": "https://huggingface.co/docs/peft/main/en/developer_guides/lora", "source": "hf", "id": "7c588d96-0c94-4598-be0b-261d0c0c2207"}, "page_content": " LoraConfig\n\nconfig = LoraConfig(use_rslora=True,...)\n```\n\n### Activated LoRA (aLoRA)\n\nActivated LoRA (aLoRA) is a low rank adapter architecture for Causal LMs that allows for reusing existing base model KV cache for more efficient inference. This approach is best suited for inference pipelines which rely on the base model for most tasks/generations, but use aLoRA adapter(s) to perform specialized task(s) within the chain. For example, checking or correcting generated outputs of the base model. In these settings, inference times can be sped up by an order of magnitude or more. For more information on aLoRA and many example use cases, see [https://huggingface.co/papers/2504.12397](https://huggingface.co/papers/2504.12397).\n\nThis technique scans for the last occurence of an invocation sequence (`alora_invocation_tokens`) in each input (this can be as short as 1 token), and activates the adapter weights on tokens starting with the beginning of the invocation sequence (any inputs after the invocation sequence are also adapted, and all generated tokens will use the adapted weights). Weights on prior tokens are left un-adapted \u2014 making the cache for those tokens interchangeable with base model cache due to the causal attention mask in Causal LMs. Usage is very similar to standard LoRA, with the key difference that this invocation sequence must be specified when the adapter is created:\n\nCopied\n\n```\nfrom peft import LoraConfig\n\nconfig = LoraConfig(alora_invocation_tokens=alora_invocation_tokens, task_type=\"CAUSAL_LM\",...)\n```\n\nwhere `alora_invocation_tokens` is a list of integer token ids. Given a desired invocation string, this can be obtained as\n\nCopied\n\n```\ninvocation_string = \"placeholder\"\nalora_invocation_tokens = tokenizer.encode(invocation_string, add_special_tokens=False).\n```\n\nwhere the tokenizer is the tokenizer for the base model. Note that we have `add_special_tokens=False` to avoid adding SOS/EOS tokens in our search string (which will most likely cause failure to find).\n\n**Notes**\n\n- aLoRA is only supported for `task_type=CAUSAL_LM` tasks due to its focus on cache reuse.\n- Since the weights are adapted on fewer tokens, often (not always) aLoRA requires higher rank (`r`) than LoRA. `r=32` can be a good starting point.\n- aLoRA weights cannot be merged into the base model by definition, since the adapter weights are selectively applied to a subset of tokens. Attempts to merge will throw errors.\n- Beam search is not yet supported.\n- It is generally not recommended to add new tokens to the tokenizer that are not present in the base model, as this can complicate the target use case of both the base model and adapter model operating on overlapping context. That said, there is a possible workaround by first efficiently adding [trainable tokens](https://huggingface.co/docs/peft/en/package_reference/trainable_tokens) to the base model prior to training the adapter.\n\n#### Choice of invocation sequence and SFT design\n\nEach input must have the `alora_invocation_tokens` sequence present, it is not added automatically. To maximize model performance without compromising cache reuse, it is recommended to have the adapter weights activated early, i.e. at the start of any adapter-specific prompting, but after any long inputs such as prior generations or documents. As with any model,\nformatting should be consistent between train and test.\n\nConsider the following example, where the base model has a chat template,\nand the goal is to train the adapter to generate a desired output.\n\n- Option 1: If there is no task-specific prompt, i.e. the input is a chat history with the `assistant` prompt, then the chat template\u2019s `assistant` prompt (e.g. `<|start_of_role|>assistant<|end_of_role|>`) is a natural choice for the invocation string. See the model\u2019s chat template to find the prompt for the model.\n- Option 2: If there is a task-specific prompt for the adapter that describes the task the adapter is learning, and that prompt is put as a `user` turn immediately prior to the generation, then the chat template\u2019s `user` prompt (e.g. `<|start_of_role|>user<|end_of_role|>`) is a natural choice for the invocation string.\n\nOnce deciding on an invocation string, get the model tokenizer and obtain `alora_invocation_tokens` as\n\nCopied\n\n```\nalora_invocation_tokens = tokenizer.encode(invocation_string, add_special_tokens=False).\n```\n\nAn example inference setup is at [alora finetuning](https://github.com/huggingface/peft/blob/main/examples/alora_finetuning/alora_finetuning.py).\n\n**Note** If using custom strings for the invocation string, make sure that the start and end of the string are special tokens to avoid issues with tokenization at the boundaries.\n\nTo see why, imagine that \u2018a\u2019, \u2018b\u2019,", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "LoRA", "description": "Hugging Face Official Documentation of LoRA", "url": "https://huggingface.co/docs/peft/main/en/developer_guides/lora", "source": "hf", "id": "7f29f81a-04d1-47c0-9fb7-cd0402dd7d63"}, "page_content": "user` prompt (e.g. `<|start_of_role|>user<|end_of_role|>`) is a natural choice for the invocation string.\n\nOnce deciding on an invocation string, get the model tokenizer and obtain `alora_invocation_tokens` as\n\nCopied\n\n```\nalora_invocation_tokens = tokenizer.encode(invocation_string, add_special_tokens=False).\n```\n\nAn example inference setup is at [alora finetuning](https://github.com/huggingface/peft/blob/main/examples/alora_finetuning/alora_finetuning.py).\n\n**Note** If using custom strings for the invocation string, make sure that the start and end of the string are special tokens to avoid issues with tokenization at the boundaries.\n\nTo see why, imagine that \u2018a\u2019, \u2018b\u2019, \u2018c\u2019, and \u2018ab\u2019 are tokens in your tokenizer (numbers 1, 2, 3, 4 respectively). Suppose that your alora\\_invocation\\_tokens = \\[2, 3\\]. Now imagine your input string is \u201cabc\u201d. Because \u201cab\u201d is a token, this will get tokenized as \\[4,3\\]. So the alora\\_invocation\\_tokens will fail to be found, despite the string \u201cbc\u201d being in it. If the start and end of the invocation string are special tokens, however, this failure case will never happen since special tokens are never tokenized into the same token with other characters.\n\n#### Using (and reusing) cache for generation\n\nThe main purpose of Activated LoRA is to make KV cache interchangeable between the base model and aLoRA adapter models **prior to the invocation sequence** since base and adapted KV values are not compatible. Specifically, keys and values stored during one model generation can be used in subsequent generations to avoid expensive prefill operations for context tokens. When sharing cache between the base model and aLoRA adapters, there are 2 main patterns:\n\n1. The base model has generated something, and an aLoRA adapter is then called to do a followup generation. Example: the base model answers a question, and an aLoRA trained to detect hallucinations checks the base model response.\n2. An aLoRA adapter has generated something, and the base model or a different aLoRA adapter is called to do a followup generation where there is partial context overlap with the original aLoRA. Example: The user provides a query, and an aLoRA rewrites the query to be more self-contained and improve retrieval in a RAG system. Then, documents are retrieved and loaded into context, an aLoRA checks if these documents are indeed relevant to the question, and then the base model generates an answer.\n\nTo demonstrate the above behaviors when using caching, we\u2019re using [DynamicCache](https://huggingface.co/docs/transformers/en/kv_cache) from `transformers`. Care must be taken to ensure that adapted cache values are not mixed with base cache values. In particular, an extra step is required for sharing the cache when there is partial context overlap (pattern 2).\n\n**Pattern 1: Base model followed by aLoRA** Here, the entire input and generation from the base model is input into the aLoRA adapter, along with the invocation sequence:\n\nCopied\n\n```\nfrom transformers import DynamicCache\n...\ncache = DynamicCache()\ninputs_base = tokenizer(prompt_base, return_tensors=\"pt\")\n# Generate from base model and save cache\nwith model_alora.disable_adapter():\n    output = model_alora.generate(inputs_base[\"input_ids\"].to(device),attention_mask=inputs_base[\"attention_mask\"].to(device),past_key_values = cache,return_dict_in_generate=True)\noutput_text_base = tokenizer.decode(output.sequences[0])\ncache = output.past_key_values\n\n# Generate with aLoRA adapter from cache\nprompt_alora = output_text + INVOCATION_STRING\ninputs_alora = tokenizer(prompt_alora, return_tensors=\"pt\").to(device)\noutput = model_alora.generate(**inputs_alora, past_key_values=cache)\noutput_text_alora = tokenizer.decode(output[0])\n\n# Note: cache is now tainted with adapter values and cannot be used in base model from here on!\n```\n\n**Pattern 2: aLoRA generation followed by base model (or another aLoRA) with partial context overlap** Here, we prefill the shared context using the base model, and then generate.\n\nCopied\n\n```\nfrom transformers import DynamicCache\nimport copy\n...\ncache = DynamicCache()\ninputs_shared = tokenizer(prompt_shared, return_tensors=\"pt\").to(device)\n\n# Prefill from base model and save cache\nwith model_alora.disable_adapter():\n    with torch.no_grad():\n        model_alora(**inputs_shared, past_key_values=cache)\ncache_copy = copy.deepcopy(cache)\n\n# Generate from aLoRA using prefilled cache\nprompt_alora = prompt_shared + INVOCATION_STRING\ninputs_alora = tokenizer(prompt_alora, return_tensors=\"pt\").to(device)\noutput = model_alora.generate(**", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "LoRA", "description": "Hugging Face Official Documentation of LoRA", "url": "https://huggingface.co/docs/peft/main/en/developer_guides/lora", "source": "hf", "id": "3b79bb98-af9e-4e2c-9b4c-b06802b9b56b"}, "page_content": " another aLoRA) with partial context overlap** Here, we prefill the shared context using the base model, and then generate.\n\nCopied\n\n```\nfrom transformers import DynamicCache\nimport copy\n...\ncache = DynamicCache()\ninputs_shared = tokenizer(prompt_shared, return_tensors=\"pt\").to(device)\n\n# Prefill from base model and save cache\nwith model_alora.disable_adapter():\n    with torch.no_grad():\n        model_alora(**inputs_shared, past_key_values=cache)\ncache_copy = copy.deepcopy(cache)\n\n# Generate from aLoRA using prefilled cache\nprompt_alora = prompt_shared + INVOCATION_STRING\ninputs_alora = tokenizer(prompt_alora, return_tensors=\"pt\").to(device)\noutput = model_alora.generate(**inputs_alora, past_key_values=cache)\noutput_text_alora = tokenizer.decode(output[0])\n\n# Generate from base model using saved cache not tainted by aLoRA KV values\nprompt_base = prompt_shared\ninputs_base = tokenizer(prompt_base, return_tensors=\"pt\").to(device)\nwith model_alora.disable_adapter():\n    output = model_alora.generate(**inputs_base, past_key_values=cache_copy)\noutput_text_base = tokenizer.decode(output[0])\n```\n\n### Weight-Decomposed Low-Rank Adaptation (DoRA)\n\nThis technique decomposes the updates of the weights into two parts, magnitude and direction. Direction is handled by normal LoRA, whereas the magnitude is handled by a separate learnable parameter. This can improve the performance of LoRA, especially at low ranks. For more information on DoRA, see [https://huggingface.co/papers/2402.09353](https://huggingface.co/papers/2402.09353).\n\nCopied\n\n```\nfrom peft import LoraConfig\n\nconfig = LoraConfig(use_dora=True,...)\n```\n\nIf parts of the model or the DoRA adapter are offloaded to CPU you can get a significant speedup at the cost of some temporary (ephemeral) VRAM overhead by using `ephemeral_gpu_offload=True` in `config.runtime_config`.\n\nCopied\n\n```\nfrom peft import LoraConfig, LoraRuntimeConfig\n\nconfig = LoraConfig(use_dora=True, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=True),...)\n```\n\nA `PeftModel` with a DoRA adapter can also be loaded with `ephemeral_gpu_offload=True` flag using the `from_pretrained` method as well as the `load_adapter` method.\n\nCopied\n\n```\nfrom peft import PeftModel\n\nmodel = PeftModel.from_pretrained(base_model, peft_model_id, ephemeral_gpu_offload=True)\n```\n\nDoRA is optimized (computes faster and takes less memory) for models in the evaluation mode, or when dropout is set to 0. We reuse the\nbase result at those times to get the speedup.\nRunning [dora finetuning](https://github.com/huggingface/peft/blob/main/examples/dora_finetuning/dora_finetuning.py)\nwith `CUDA_VISIBLE_DEVICES=0 ZE_AFFINITY_MASK=0 time python examples/dora_finetuning/dora_finetuning.py --quantize --lora_dropout 0 --batch_size 16 --eval_step 2 --use_dora`\non a 4090 with gradient accumulation set to 2 and max step to 20 resulted with the following observations:\n\n|  | Without Optimization | With Optimization |\n| :-: | :-: | :-: |\n| train\\_runtime | 359.7298 | **279.2676** |\n| train\\_samples\\_per\\_second | 1.779 | **2.292** |\n| train\\_steps\\_per\\_second | 0.056 | **0.072** |\n\n#### Caveats\n\n- DoRA only supports embedding, linear, and Conv2d layers at the moment.\n- DoRA introduces a bigger overhead than pure LoRA, so it is recommended to merge weights for inference, see [LoraModel.merge\\_and\\_unload()](https://huggingface.co/docs/peft/main/en/package_reference/tuners#peft.tuners.tuners_utils.BaseTuner.merge_and_unload).\n- DoRA should work with weights quantized with bitsandbytes (\u201cQDoRA\u201d). However, issues have been reported when using QDoRA with DeepSpeed Zero2.\n\n### QLoRA-style training\n\nThe default LoRA settings in PEFT add trainable weights to the query and value layers of each attention block. But [QLoRA](https://hf.co/papers/2305.14314), which adds trainable weights to all the linear layers of a transformer model, can provide performance equal to a fully finetuned model. To apply Lo", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "LoRA", "description": "Hugging Face Official Documentation of LoRA", "url": "https://huggingface.co/docs/peft/main/en/developer_guides/lora", "source": "hf", "id": "e9ed14a6-b20b-44c9-9a8f-7dc7102d329d"}, "page_content": " than pure LoRA, so it is recommended to merge weights for inference, see [LoraModel.merge\\_and\\_unload()](https://huggingface.co/docs/peft/main/en/package_reference/tuners#peft.tuners.tuners_utils.BaseTuner.merge_and_unload).\n- DoRA should work with weights quantized with bitsandbytes (\u201cQDoRA\u201d). However, issues have been reported when using QDoRA with DeepSpeed Zero2.\n\n### QLoRA-style training\n\nThe default LoRA settings in PEFT add trainable weights to the query and value layers of each attention block. But [QLoRA](https://hf.co/papers/2305.14314), which adds trainable weights to all the linear layers of a transformer model, can provide performance equal to a fully finetuned model. To apply LoRA to all the linear layers, like in QLoRA, set `target_modules=\"all-linear\"` (easier than specifying individual modules by name which can vary depending on the architecture).\n\nCopied\n\n```\nconfig = LoraConfig(target_modules=\"all-linear\",...)\n```\n\n### Memory efficient Layer Replication with LoRA\n\nAn approach used to improve the performance of models is to expand a model by duplicating layers in the model to build a larger model from a pretrained model of a given size. For example increasing a 7B model to a 10B model as described in the [SOLAR](https://huggingface.co/papers/2312.15166) paper. PEFT LoRA supports this kind of expansion in a memory efficient manner that supports further fine-tuning using LoRA adapters attached to the layers post replication of the layers. The replicated layers do not take additional memory as they share the underlying weights so the only additional memory required is the memory for the adapter weights. To use this feature you would create a config with the `layer_replication` argument.\n\nCopied\n\n```\nconfig = LoraConfig(layer_replication=[[0,4], [2,5]],...)\n```\n\nAssuming the original model had 5 layers `[0, 1, 2,3, 4]`, this would create a model with 7 layers arranged as `[0, 1, 2, 3, 2, 3, 4]`. This follows the [mergekit](https://github.com/arcee-ai/mergekit) pass through merge convention where sequences of layers specified as start inclusive and end exclusive tuples are stacked to build the final model. Each layer in the final model gets its own distinct set of LoRA adapters.\n\n[Fewshot-Metamath-OrcaVicuna-Mistral-10B](https://huggingface.co/abacusai/Fewshot-Metamath-OrcaVicuna-Mistral-10B) is an example of a model trained using this method on Mistral-7B expanded to 10B. The\n[adapter\\_config.json](https://huggingface.co/abacusai/Fewshot-Metamath-OrcaVicuna-Mistral-10B/blob/main/adapter_config.json) shows a sample LoRA adapter config applying this method for fine-tuning.\n\n### Fine grained control over ranks and alpha (scaling)\n\nBy default, all layers targeted with LoRA will have the same rank `r` and the same `lora_alpha` (which determines the LoRA scaling), depending on what was specified in the [LoraConfig](https://huggingface.co/docs/peft/main/en/package_reference/lora#peft.LoraConfig). In some cases, however, you may want to indicate different values for different layers. This is possible by passing the `rank_pattern` and `alpha_pattern` arguments to [LoraConfig](https://huggingface.co/docs/peft/main/en/package_reference/lora#peft.LoraConfig). These arguments should be dictionaries with the key being the layer name and the value being the rank/alpha value. The keys can be [regular expressions](https://docs.python.org/3/library/re.html) (regex). All LoRA layers that are not explicitly mentioned in `rank_pattern` and `alpha_pattern` will take the default `r` and `lora_alpha` values.\n\nTo give an example, let\u2019s assume that we have a model with the following structure:\n\nCopied\n\n```\n>>> print(model)\nOuter(\n  (foo): Linear(...)\n  (module): Middle(\n    (foo): Linear(...)\n    (foobar): Linear(...)\n    (module): Inner(\n      (foo): Linear(...)\n      (barfoo): Linear(...)\n    )\n  )\n)\n```\n\n- `rank_pattern={\"foo\": 42}` will match all 3 `foo` layers. Neither `foobar` nor `barfoo` are matched.\n- `rank_pattern={\"^foo\": 42}` will only match the `foo` layer of the model, but neither `module.foo` nor `module.module.foo`. This is because the `^` means \u201cstart of string\u201d when using regular expressions, and only `foo` starts with `\"foo\"`, the other layer names", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "LoRA", "description": "Hugging Face Official Documentation of LoRA", "url": "https://huggingface.co/docs/peft/main/en/developer_guides/lora", "source": "hf", "id": "73163a53-9cd0-4a82-afe2-17087f50e63a"}, "page_content": " model with the following structure:\n\nCopied\n\n```\n>>> print(model)\nOuter(\n  (foo): Linear(...)\n  (module): Middle(\n    (foo): Linear(...)\n    (foobar): Linear(...)\n    (module): Inner(\n      (foo): Linear(...)\n      (barfoo): Linear(...)\n    )\n  )\n)\n```\n\n- `rank_pattern={\"foo\": 42}` will match all 3 `foo` layers. Neither `foobar` nor `barfoo` are matched.\n- `rank_pattern={\"^foo\": 42}` will only match the `foo` layer of the model, but neither `module.foo` nor `module.module.foo`. This is because the `^` means \u201cstart of string\u201d when using regular expressions, and only `foo` starts with `\"foo\"`, the other layer names have prefixes.\n- `rank_pattern={\"^module.foo\": 42}` matches only `module.foo`, but not `module.module.foo`, for the same reason.\n- `rank_pattern={\"module.foo\": 42}` matches both `module.foo` and `module.module.foo`, but not `foo`.\n- `rank_pattern={\"^foo\": 42, \"^module.module.foo\": 55}` matches `foo` and `module.module.foo`, respectively, but not `module.foo`.\n- There is no need to indicate `$` to mark the end of the match, as this is added automatically by PEFT.\n\nThe same logic applies to `alpha_pattern`. If you\u2019re in doubt, don\u2019t try to get fancy with regular expressions \u2014 just pass the full name for each module with a different rank/alpha, preceded by the `^` prefix, and you should be good.\n\n### Targeting nn.Parameter directly\n\n> This feature is experimental and subject to change.\n\nGenerally, you should use `target_modules` to target the module (e.g. `nn.Linear`). However, in some circumstances, this is not possible. E.g., in many mixture of expert (MoE) layers in HF Transformers, instead of using `nn.Linear`, an `nn.Parameter` is used. PEFT normally overwrites the `forward` method for LoRA, but for `nn.Parameter`, there is none. Therefore, to apply LoRA to that parameter, it needs to be targeted with `target_parameters`. As an example, for [Llama4](https://huggingface.co/collections/meta-llama/llama-4-67f0c30d9fe03840bc9d0164), you can pass: `target_parameters=['feed_forward.experts.gate_up_proj', 'feed_forward.experts.down_proj]`.\n\n#### Caveats\n\n- At the moment, this argument allows to target 2-dim or 3-dim `nn.Parameter`s. It is assumed that in the case of a 3-dim parameter, the 0th dimension is the expert dimension.\n- It is currently not possible to add multiple LoRA adapters (via `model.add_adapter` or `model.load_adapter`) that use `target_parameters` at the same time.\n\n## Optimizers\n\nLoRA training can optionally include special purpose optimizers. Currently PEFT supports LoRA-FA and LoRA+.\n\n### LoRA-FA Optimizer\n\nLoRA training can be more effective and efficient using LoRA-FA, as described in [LoRA-FA](https://huggingface.co/papers/2308.03303). LoRA-FA reduces activation memory consumption by fixing the matrix A and only tuning the matrix B. During training, the gradient of B is optimized to approximate the full parameter fine-tuning gradient. Moreover, the memory consumption of LoRA-FA is not sensitive to the rank (since it erases the activation of $A$), therefore it can improve performance by enlarging lora rank without increasing memory consumption.\n\nCopied\n\n```\nfrom peft import LoraConfig, get_peft_model\nfrom peft.optimizers import create_lorafa_optimizer\nfrom transformers import Trainer, get_cosine_schedule_with_warmup\n\nbase_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n\nconfig = LoraConfig(...)\nmodel = get_peft_model(base_model, config)\n\noptimizer = create_lorafa_optimizer(\n    model=model,\n    r=128,\n    lora_alpha=32,\n    lr=7e-5,\n)\n\nscheduler = get_cosine_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=100,\n    num_training_steps=1000,\n)\n\ntrainer = Trainer(\n   ...,\n    optimizers=(optimizer, scheduler),\n)\n```\n\n### LoRA+ optimized LoRA\n\nLoRA training can be optimized using [LoRA+](https://huggingface.co/papers/2402.12354), which uses different learning rates for the adapter matrices A and B, shown to increase finetuning speed by up to 2x and performance by 1-2%.", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "LoRA", "description": "Hugging Face Official Documentation of LoRA", "url": "https://huggingface.co/docs/peft/main/en/developer_guides/lora", "source": "hf", "id": "3f8cd777-1d63-469a-95aa-c61d400c149f"}, "page_content": "_peft_model(base_model, config)\n\noptimizer = create_lorafa_optimizer(\n    model=model,\n    r=128,\n    lora_alpha=32,\n    lr=7e-5,\n)\n\nscheduler = get_cosine_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=100,\n    num_training_steps=1000,\n)\n\ntrainer = Trainer(\n   ...,\n    optimizers=(optimizer, scheduler),\n)\n```\n\n### LoRA+ optimized LoRA\n\nLoRA training can be optimized using [LoRA+](https://huggingface.co/papers/2402.12354), which uses different learning rates for the adapter matrices A and B, shown to increase finetuning speed by up to 2x and performance by 1-2%.\n\nCopied\n\n```\nfrom peft import LoraConfig, get_peft_model\nfrom peft.optimizers import create_loraplus_optimizer\nfrom transformers import Trainer\nimport bitsandbytes as bnb\n\nbase_model =...\nconfig = LoraConfig(...)\nmodel = get_peft_model(base_model, config)\n\noptimizer = create_loraplus_optimizer(\n    model=model,\n    optimizer_cls=bnb.optim.Adam8bit,\n    lr=5e-5,\n    loraplus_lr_ratio=16,\n)\nscheduler = None\n\n...\ntrainer = Trainer(\n   ...,\n    optimizers=(optimizer, scheduler),\n)\n```\n\n## Efficiently train tokens alongside LoRA\n\nSometimes it is necessary to not only change some layer\u2019s weights but to add new tokens as well. With larger models this can be a memory-costly endeavour. PEFT LoRA adapters support the `trainable_token_indices` parameter which allows tuning of other tokens alongside fine-tuning of specific layers with LoRA. This method only trains the tokens you specify and leaves all other tokens untouched. This saves memory and doesn\u2019t throw away learned context of existing token embeddings in contrast to when training the whole embedding matrix. Under the hood this method uses the layer of [TrainableTokensModel](https://huggingface.co/docs/peft/main/en/package_reference/trainable_tokens#peft.TrainableTokensModel).\n\nCopied\n\n```\n# for layer 'embed_tokens'\nconfig = LoraConfig(trainable_token_indices=[idx_1, idx_2,...],...)\n\n# specific embedding layer\nconfig = LoraConfig(trainable_token_indices={'emb_tokens': [idx_1, idx_2,...]},...)\n```\n\nIn the snippet below we show how to add new tokens to the model and how to train it alongside the other layers in the model.\n\nCopied\n\n```\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import get_peft_model, LoraConfig\n\nbase_model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n\n# we define our new tokens and add them to the tokenizer as special tokens\nspecial_tokens = ['<|start_think|>', '<|stop_think|>']\ntokenizer.add_special_tokens({'additional_special_tokens': special_tokens})\n\n# make room for new tokens in the embedding matrix if it isn't big enough already\nbase_model.resize_token_embeddings(max(len(tokenizer), base_model.model.embed_tokens.num_embeddings))\n\n# typical LoRA config with `trainable_token_indices` targeting embedding layer `embed_tokens`\n# and specifically our new tokens we just added\nlora_config = LoraConfig(\n    target_modules='all-linear',\n    trainable_token_indices={'embed_tokens': tokenizer.convert_tokens_to_ids(special_tokens)},\n)\npeft_model = get_peft_model(base_model, lora_config)\n\n# proceed to train the model like normal\n[...]\n```\n\nThe token weights are part of your adapter state dict and saved alongside the LoRA weights.\nIf we would have used full fine-tuning with `modules_to_save=['embed_tokens']` we would have stored the full embedding matrix in the checkpoint, leading to a much bigger file.\n\nTo give a bit of an indication how much VRAM can be saved, a rudimentary comparison of the above example was made between training the embedding matrix fully (`modules_to_save=[\"embed_tokens\"]`), using a LoRA for the embedding matrix (`target_modules=[..., \"embed_tokens\"]`, rank 32) and trainable tokens (`trainable_token_indices=[...]`, 6 tokens). Trainable tokens used about as much VRAM (15,562MB vs. 15,581MB) as LoRA while being specific to the tokens and saved ~1GB of VRAM over fully training the embedding matrix.\n\n## Merge LoRA", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "LoRA", "description": "Hugging Face Official Documentation of LoRA", "url": "https://huggingface.co/docs/peft/main/en/developer_guides/lora", "source": "hf", "id": "26bf73d4-dd84-4b31-a70d-153aa88d660e"}, "page_content": " are part of your adapter state dict and saved alongside the LoRA weights.\nIf we would have used full fine-tuning with `modules_to_save=['embed_tokens']` we would have stored the full embedding matrix in the checkpoint, leading to a much bigger file.\n\nTo give a bit of an indication how much VRAM can be saved, a rudimentary comparison of the above example was made between training the embedding matrix fully (`modules_to_save=[\"embed_tokens\"]`), using a LoRA for the embedding matrix (`target_modules=[..., \"embed_tokens\"]`, rank 32) and trainable tokens (`trainable_token_indices=[...]`, 6 tokens). Trainable tokens used about as much VRAM (15,562MB vs. 15,581MB) as LoRA while being specific to the tokens and saved ~1GB of VRAM over fully training the embedding matrix.\n\n## Merge LoRA weights into the base model\n\nWhile LoRA is significantly smaller and faster to train, you may encounter latency issues during inference due to separately loading the base model and the LoRA adapter. To eliminate latency, use the [merge\\_and\\_unload()](https://huggingface.co/docs/peft/main/en/package_reference/tuners#peft.tuners.tuners_utils.BaseTuner.merge_and_unload) function to merge the adapter weights with the base model. This allows you to use the newly merged model as a standalone model. The [merge\\_and\\_unload()](https://huggingface.co/docs/peft/main/en/package_reference/tuners#peft.tuners.tuners_utils.BaseTuner.merge_and_unload) function doesn\u2019t keep the adapter weights in memory.\n\nBelow is a diagram that explains the intuition of LoRA adapter merging:\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/lora_diagram.png)\n\nWe show in the snippets below how to run that using PEFT.\n\nCopied\n\n```\nfrom transformers import AutoModelForCausalLM\nfrom peft import PeftModel\n\nbase_model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\npeft_model_id = \"alignment-handbook/zephyr-7b-sft-lora\"\nmodel = PeftModel.from_pretrained(base_model, peft_model_id)\nmodel = model.merge_and_unload()\n```\n\nIt is important to assign the returned model to a variable and use it, [merge\\_and\\_unload()](https://huggingface.co/docs/peft/main/en/package_reference/tuners#peft.tuners.tuners_utils.BaseTuner.merge_and_unload) is not an in-place operation. If you need to keep a copy of the weights so you can unmerge the adapter later or delete and load different ones, you should use the [merge\\_adapter()](https://huggingface.co/docs/peft/main/en/package_reference/tuners#peft.tuners.tuners_utils.BaseTuner.merge_adapter) function instead. Now you have the option to use [unmerge\\_adapter()](https://huggingface.co/docs/peft/main/en/package_reference/tuners#peft.tuners.tuners_utils.BaseTuner.unmerge_adapter) to return the base model.\n\nCopied\n\n```\nfrom transformers import AutoModelForCausalLM\nfrom peft import PeftModel\n\nbase_model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\npeft_model_id = \"alignment-handbook/zephyr-7b-sft-lora\"\nmodel = PeftModel.from_pretrained(base_model, peft_model_id)\nmodel.merge_adapter()\n\n# unmerge the LoRA layers from the base model\nmodel.unmerge_adapter()\n```\n\nThe [add\\_weighted\\_adapter()](https://huggingface.co/docs/peft/main/en/package_reference/lora#peft.LoraModel.add_weighted_adapter) function is useful for merging multiple LoRAs into a new adapter based on a user provided weighting scheme in the `weights` parameter. Below is an end-to-end example.\n\nFirst load the base model:\n\nCopied\n\n```\nfrom transformers import AutoModelForCausalLM\nfrom peft import PeftModel\nimport torch\n\nbase_model = AutoModelForCausalLM.from_pretrained(\n    \"mistralai/Mistral-7B-v0.1\", dtype=torch.float16, device_map=\"auto\"\n)\n```\n\nThen we load the first adapter:\n\nCopied\n\n```\npeft_model_id = \"alignment-handbook/zephyr-7b-sft-lora\"\nmodel", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "LoRA", "description": "Hugging Face Official Documentation of LoRA", "url": "https://huggingface.co/docs/peft/main/en/developer_guides/lora", "source": "hf", "id": "ee8460e3-3060-4003-bb50-447b740a1a14"}, "page_content": "ft/main/en/package_reference/lora#peft.LoraModel.add_weighted_adapter) function is useful for merging multiple LoRAs into a new adapter based on a user provided weighting scheme in the `weights` parameter. Below is an end-to-end example.\n\nFirst load the base model:\n\nCopied\n\n```\nfrom transformers import AutoModelForCausalLM\nfrom peft import PeftModel\nimport torch\n\nbase_model = AutoModelForCausalLM.from_pretrained(\n    \"mistralai/Mistral-7B-v0.1\", dtype=torch.float16, device_map=\"auto\"\n)\n```\n\nThen we load the first adapter:\n\nCopied\n\n```\npeft_model_id = \"alignment-handbook/zephyr-7b-sft-lora\"\nmodel = PeftModel.from_pretrained(base_model, peft_model_id, adapter_name=\"sft\")\n```\n\nThen load a different adapter and merge it with the first one:\n\nCopied\n\n```\nweighted_adapter_name = \"sft-dpo\"\nmodel.load_adapter(\"alignment-handbook/zephyr-7b-dpo-lora\", adapter_name=\"dpo\")\nmodel.add_weighted_adapter(\n    adapters=[\"sft\", \"dpo\"],\n    weights=[0.7, 0.3],\n    adapter_name=weighted_adapter_name,\n    combination_type=\"linear\"\n)\nmodel.set_adapter(weighted_adapter_name)\n```\n\n> There are several supported methods for `combination_type`. Refer to the [documentation](https://huggingface.co/docs/peft/main/en/package_reference/lora#peft.LoraModel.add_weighted_adapter) for more details. Note that \u201csvd\u201d as the `combination_type` is not supported when using `torch.float16` or `torch.bfloat16` as the datatype.\n\nNow, perform inference:\n\nCopied\n\n```\ndevice = torch.accelerator.current_accelerator().type if hasattr(torch, \"accelerator\") else \"cuda\"\n\ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n\nprompt = \"Hey, are you conscious? Can you talk to me?\"\ninputs = tokenizer(prompt, return_tensors=\"pt\")\ninputs = {k: v.to(device) for k, v in inputs.items()}\n\nwith torch.no_grad():\n    generate_ids = model.generate(**inputs, max_length=30)\noutputs = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\nprint(outputs)\n```\n\n## Load adapters\n\nAdapters can be loaded onto a pretrained model with [load\\_adapter()](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.PeftModel.load_adapter), which is useful for trying out different adapters whose weights aren\u2019t merged. Set the active adapter weights with the [set\\_adapter()](https://huggingface.co/docs/peft/main/en/package_reference/tuners#peft.tuners.tuners_utils.BaseTuner.set_adapter) function.\n\nCopied\n\n```\nfrom transformers import AutoModelForCausalLM\nfrom peft import PeftModel\n\nbase_model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\npeft_model_id = \"alignment-handbook/zephyr-7b-sft-lora\"\nmodel = PeftModel.from_pretrained(base_model, peft_model_id)\n\n# load different adapter\nmodel.load_adapter(\"alignment-handbook/zephyr-7b-dpo-lora\", adapter_name=\"dpo\")\n\n# set adapter as active\nmodel.set_adapter(\"dpo\")\n```\n\nTo return the base model, you could use [unload()](https://huggingface.co/docs/peft/main/en/package_reference/tuners#peft.tuners.tuners_utils.BaseTuner.unload) to unload all of the LoRA modules or [delete\\_adapter()](https://huggingface.co/docs/peft/main/en/package_reference/tuners#peft.tuners.tuners_utils.BaseTuner.delete_adapter) to delete the adapter entirely. [unload()](https://huggingface.co/docs/peft/main/en/package_reference/tuners#peft.tuners.tuners_utils.BaseTuner.unload) is not an in-place operation, remember to assign the returned model to a variable and use it.\n\nCopied\n\n```\n# unload", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "LoRA", "description": "Hugging Face Official Documentation of LoRA", "url": "https://huggingface.co/docs/peft/main/en/developer_guides/lora", "source": "hf", "id": "36dce4a0-6116-475c-acef-a098849f05b7"}, "page_content": "://huggingface.co/docs/peft/main/en/package_reference/tuners#peft.tuners.tuners_utils.BaseTuner.unload) to unload all of the LoRA modules or [delete\\_adapter()](https://huggingface.co/docs/peft/main/en/package_reference/tuners#peft.tuners.tuners_utils.BaseTuner.delete_adapter) to delete the adapter entirely. [unload()](https://huggingface.co/docs/peft/main/en/package_reference/tuners#peft.tuners.tuners_utils.BaseTuner.unload) is not an in-place operation, remember to assign the returned model to a variable and use it.\n\nCopied\n\n```\n# unload adapter\nmodel = model.unload()\n\n# delete adapter\nmodel.delete_adapter(\"dpo\")\n```\n\n## Inference with different LoRA adapters in the same batch\n\nNormally, each inference batch has to use the same adapter(s) in PEFT. This can sometimes be annoying, because we may have batches that contain samples intended to be used with different LoRA adapters. For example, we could have a base model that works well in English and two more LoRA adapters, one for French and one for German. Usually, we would have to split our batches such that each batch only contains samples of one of the languages, we cannot combine different languages in the same batch.\n\nThankfully, it is possible to mix different LoRA adapters in the same batch using the `adapter_name` argument. Below, we show an example of how this works in practice. First, let\u2019s load the base model, English, and the two adapters, French and German, like this:\n\nCopied\n\n```\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\n\nmodel_id =...\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id)\n# load the LoRA adapter for French\npeft_model = PeftModel.from_pretrained(model, <path>, adapter_name=\"adapter_fr\")\n# next, load the LoRA adapter for German\npeft_model.load_adapter(<path>, adapter_name=\"adapter_de\")\n```\n\nNow, we want to generate text on a sample that contains all three languages: The first three samples are in English, the next three are in French, and the last three are in German. We can use the `adapter_names` argument to specify which adapter to use for each sample. Since our base model is used for English, we use the special string `\"__base__\"` for these samples. For the next three samples, we indicate the adapter name of the French LoRA fine-tune, in this case `\"adapter_fr\"`. For the last three samples, we indicate the adapter name of the German LoRA fine-tune, in this case `\"adapter_de\"`. This way, we can use the base model and the two adapters in a single batch.\n\nCopied\n\n```\ninputs = tokenizer(\n    [\\\n        \"Hello, my dog is cute\",\\\n        \"Hello, my cat is awesome\",\\\n        \"Hello, my fish is great\",\\\n        \"Salut, mon chien est mignon\",\\\n        \"Salut, mon chat est g\u00e9nial\",\\\n        \"Salut, mon poisson est super\",\\\n        \"Hallo, mein Hund ist s\u00fc\u00df\",\\\n        \"Hallo, meine Katze ist toll\",\\\n        \"Hallo, mein Fisch ist gro\u00dfartig\",\\\n    ],\n    return_tensors=\"pt\",\n    padding=True,\n)\n\nadapter_names = [\\\n    \"__base__\", \"__base__\", \"__base__\",\\\n    \"adapter_fr\", \"adapter_fr\", \"adapter_fr\",\\\n    \"adapter_de\", \"adapter_de\", \"adapter_de\",\\\n]\noutput = peft_model.generate(**inputs, adapter_names=adapter_names, max_new_tokens=20)\n```\n\nNote that the order does not matter here, i.e. the samples in the batch don\u2019t need to be grouped by adapter as in the example above. We just need to ensure that the `adapter_names` argument is aligned correctly with the samples.\n\nAdditionally, the same approach also works with the `modules_to_save` feature, which allows for saving and reusing specific neural network layers, such as custom heads for classification tasks, across different LoRA adapters.\n\n### Caveats\n\nUsing this feature has some drawbacks, namely:\n\n- It only works for inference, not for training.\n- Disabling adapters using the `with model.disable_adapter()` context takes precedence over `adapter_names`.\n- You cannot pass `adapter_names` when some adapter weights were merged with base weight using the `merge_adapter` method. Please unmerge all adapters first by calling `model.unmerge_adapter()`.\n- For obvious reasons, this cannot be used after calling `merge_and_unload()`, since all", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "LoRA", "description": "Hugging Face Official Documentation of LoRA", "url": "https://huggingface.co/docs/peft/main/en/developer_guides/lora", "source": "hf", "id": "0605c0c1-cd68-4c12-8d48-2ed91c9163e5"}, "page_content": " be grouped by adapter as in the example above. We just need to ensure that the `adapter_names` argument is aligned correctly with the samples.\n\nAdditionally, the same approach also works with the `modules_to_save` feature, which allows for saving and reusing specific neural network layers, such as custom heads for classification tasks, across different LoRA adapters.\n\n### Caveats\n\nUsing this feature has some drawbacks, namely:\n\n- It only works for inference, not for training.\n- Disabling adapters using the `with model.disable_adapter()` context takes precedence over `adapter_names`.\n- You cannot pass `adapter_names` when some adapter weights were merged with base weight using the `merge_adapter` method. Please unmerge all adapters first by calling `model.unmerge_adapter()`.\n- For obvious reasons, this cannot be used after calling `merge_and_unload()`, since all the LoRA adapters will be merged into the base weights in this case.\n- This feature does not currently work with DoRA, so set `use_dora=False` in your `LoraConfig` if you want to use it.\n- The `modules_to_save` feature is currently only supported for the layers of types `Linear`, `Embedding`, `Conv2d` and `Conv1d`.\n- There is an expected overhead for inference with `adapter_names`, especially if the amount of different adapters in the batch is high. This is because the batch size is effectively reduced to the number of samples per adapter. If runtime performance is your top priority, try the following:\n\n  - Increase the batch size.\n  - Try to avoid having a large number of different adapters in the same batch, prefer homogeneous batches. This can be achieved by buffering samples with the same adapter and only perform inference with a small handful of different adapters.\n  - Take a look at alternative implementations such as [LoRAX](https://github.com/predibase/lorax), [punica](https://github.com/punica-ai/punica), or [S-LoRA](https://github.com/S-LoRA/S-LoRA), which are specialized to work with a large number of different adapters.\n\n## Composing and Reusing LoRA Adapters\n\n### Arrow\n\n[Arrow](https://huggingface.co/papers/2405.11157) is a modular routing algorithm designed to combine multiple pre-trained task-specific LoRA adapters to solve a given task. Rather than merging all adapters naively, Arrow introduces a **gradient-free, token-wise mixture-of-experts (MoE) routing mechanism**. At inference time, it first computes a _prototype_ for each LoRA by extracting the top right singular vector from its SVD decomposition. Each token representation is then compared to these prototypes via cosine similarity to obtain routing coefficients. Tokens are assigned to the top-k most relevant LoRA adapters, with the coefficients normalized through softmax, and their outputs linearly combined. This allows effective reuse of existing LoRA modules for new tasks and leads to stronger zero-shot generalization.\n\nIn PEFT, Arrow is enabled through `ArrowConfig` and `create_arrow_model`. You can also configure parameters such as `top_k` (the number of LoRA adapters combined per token), `router_temperature` (the softmax temperature applied to the routing coefficients), and `rng_seed` (for reproducibility).\n\nCopied\n\n```\nfrom peft import create_arrow_model, ArrowConfig\nfrom transformers import AutoModelForCausalLM\n\n# Loading the model\nbase_model = AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n\n# Creating the Arrow config\narrow_config = ArrowConfig(\n    top_k=3,\n    router_temperature=1.0,\n    rng_seed=42,\n)\n\n# The LoRA adapters below were trained on a clustered FLAN dataset.\n# Task clustering was performed using the Model-Based Clustering (MBC) method,\n# as described in the Arrow paper.\n# While one could train a separate LoRA for each task and let Arrow route tokens among them,\n# training LoRAs on clusters of tasks instead provides an indirect optimization for\n# transfer across the multi-task dataset.\ntask_specific_adapter_paths = [\\\n        f\"TahaBa/phi3-mini-clustered-flan/ts_expert_{i}\" for i in range(10)\\\n    ]\n\n# Creating the Arrow model\nmodel = create_arrow_model(\n        base_model=base_model,\n        task_specific_adapter_paths=task_specific_adapter_paths,\n        arrow_config=arrow_config,\n    )\n\n# Now the forward path could be called on this model, like a normal PeftModel.\n```\n\nFurthermore, you can add or remove adapters after calling `create_arrow_model`\u2014for example, to fine-tune a new adapter or discard an unnecessary one. Once the adapters are in place, you can activate the `\"arrow_router\"` for inference to use Arrow. Note that if you add a new LoRA adapter after `create_arrow_model` and want to fine-tune it, you must explicitly set the new adapter as active, since `\"arrow_router\"` is activated by default in `", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "LoRA", "description": "Hugging Face Official Documentation of LoRA", "url": "https://huggingface.co/docs/peft/main/en/developer_guides/lora", "source": "hf", "id": "20b372fd-dc14-4a54-9416-b53e6af9ba08"}, "page_content": " in range(10)\\\n    ]\n\n# Creating the Arrow model\nmodel = create_arrow_model(\n        base_model=base_model,\n        task_specific_adapter_paths=task_specific_adapter_paths,\n        arrow_config=arrow_config,\n    )\n\n# Now the forward path could be called on this model, like a normal PeftModel.\n```\n\nFurthermore, you can add or remove adapters after calling `create_arrow_model`\u2014for example, to fine-tune a new adapter or discard an unnecessary one. Once the adapters are in place, you can activate the `\"arrow_router\"` for inference to use Arrow. Note that if you add a new LoRA adapter after `create_arrow_model` and want to fine-tune it, you must explicitly set the new adapter as active, since `\"arrow_router\"` is activated by default in `create_arrow_model`.\n\nCopied\n\n```\nfrom trl import SFTTrainer, SFTConfig\n\n# Adding a new adapter and activating it\nmodel.add_adapter(adapter_name='new_adapter')\nmodel.set_adapter('new_adapter')\n\n# Now the model could be trained along the `new_adapter`.\ntrainer = SFTTrainer(\n        model=model,\n        args=SFTConfig(...),\n       ...\n    )\n\n# Once the training is done, you can activate `arrow_router` and use it in inference\nmodel.set_adapter('arrow_router')    # Model is ready to be used at inference time now\n```\n\n### GenKnowSub\n\n[GenKnowSub](https://aclanthology.org/2025.acl-short.54/) augments Arrow by purifying task-specific LoRA adapters before routing. The key idea is to subtract general knowledge encoded in LoRA space\u2014based on the [forgetting-via-negation principle](https://huggingface.co/papers/2212.04089)\u2014so that task adapters become more isolated and focused on task-relevant signals. Concretely, GenKnowSub estimates a low-dimensional \u201cgeneral\u201d subspace from a set of general (non task-specific) LoRA adapters and removes this component from each task adapter\u2019s LoRA update prior to Arrow\u2019s token-wise routing. This typically improves compositionality and reduces interference when combining many task adapters.\n\nIn PEFT, enable GenKnowSub by setting `use_gks=True` in ArrowConfig, and providing `general_adapter_paths` in `create_arrow_model`:\n\nCopied\n\n```\nfrom peft import create_arrow_model, ArrowConfig\nfrom transformers import AutoModelForCausalLM\n\n# Loading the model\nbase_model = AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n\n# Creating the Arrow config\narrow_config = ArrowConfig(\n    top_k=3,\n    router_temperature=1.0,\n    use_gks=True,\n    rng_seed=42,\n)\n\n# Path to task-specific, trained on flan clustered dataset (as we explained before.)\ntask_specific_adapter_paths = [\\\n        f\"TahaBa/phi3-mini-clustered-flan/ts_expert_{i}\" for i in range(10)\\\n    ]\n# These general adapters are trained on English, German, and French Wikipedia dataset,\n# with causal language modelling objective, each pair like: (507 token tsentence, 5 token completion), and the loss computed on the completion\ngeneral_adapter_paths = [\\\n        \"TahaBa/phi3-mini-general-adapters/cluster0_batch16_prop1.0_langen/checkpoint-17\",\\\n        \"TahaBa/phi3-mini-general-adapters/cluster0_batch16_prop1.0_langfr/checkpoint-35\",\\\n        \"TahaBa/phi3-mini-general-adapters/cluster0_batch16_prop1.0_langger/checkpoint-17\"\\\n    ]\n\n# Creating the Arrow model\nmodel = create_arrow_model(\n        base_model=base_model,\n        task_specific_adapter_paths=task_specific_adapter_paths,\n        general_adapter_paths=general_adapter_paths,\n        arrow_config=arrow_config,\n    )\n\n# Now the forward path could be called on this model, like a normal PeftModel.\n```\n\nTo encode general knowledge, GenKnowSub subtracts the average of the provided general adapters from each task-specific adapter once, before routing begins. Furthermore, the ability to add or remove adapters after calling `create_arrow_model` (as described in the Arrow section) is still supported in this case.\n\n> **Things to keep in mind when using Arrow + GenKnowSub:**\n>\n> - All LoRA adapters (task-specific and general) must share the same `rank` and `target_modules`.\n>\n> - Any inconsistency in these settings will raise an error in `create_arrow_model`.\n>\n> - Having different scaling factors (`lora_alpha`) across task adapters is supported \u2014 Arrow handles them automatically.\n>\n> - Merging the `\"arrow_router\"`", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "LoRA", "description": "Hugging Face Official Documentation of LoRA", "url": "https://huggingface.co/docs/peft/main/en/developer_guides/lora", "source": "hf", "id": "421cf152-87c7-4166-9e1a-c810c5210817"}, "page_content": "\n\n# Now the forward path could be called on this model, like a normal PeftModel.\n```\n\nTo encode general knowledge, GenKnowSub subtracts the average of the provided general adapters from each task-specific adapter once, before routing begins. Furthermore, the ability to add or remove adapters after calling `create_arrow_model` (as described in the Arrow section) is still supported in this case.\n\n> **Things to keep in mind when using Arrow + GenKnowSub:**\n>\n> - All LoRA adapters (task-specific and general) must share the same `rank` and `target_modules`.\n>\n> - Any inconsistency in these settings will raise an error in `create_arrow_model`.\n>\n> - Having different scaling factors (`lora_alpha`) across task adapters is supported \u2014 Arrow handles them automatically.\n>\n> - Merging the `\"arrow_router\"` is not supported, due to its dynamic routing behavior.\n>\n> - In create\\_arrow\\_model, task adapters are loaded as `task_i` and general adapters as `gks_j` (where `i` and `j` are indices). The function ensures consistency of `target_modules`, `rank`, and whether adapters are applied to `Linear` or `Linear4bit` layers. It then adds the `\"arrow_router\"` module and activates it. Any customization of this process requires overriding `create_arrow_model`.\n>\n> - This implementation is compatible with 4-bit quantization (via bitsandbytes):\n>\n>\n>\n>\n>\n>\n>\n>\n>   Copied\n>\n>\n>\n>\n>   ```\n>   from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n>   import torch\n>\n>   # Quantisation config\n>   bnb_config = BitsAndBytesConfig(\n>           load_in_4bit=True,\n>           bnb_4bit_quant_type=\"nf4\",\n>           bnb_4bit_compute_dtype=torch.bfloat16,\n>           bnb_4bit_use_double_quant=False,\n>       )\n>\n>   # Loading the model\n>   base_model = AutoModelForCausalLM.from_pretrained(\n>       \"microsoft/Phi-3-mini-4k-instruct\",\n>       dtype=torch.bfloat16,\n>       device_map=\"auto\",\n>       quantization_config=bnb_config,\n>   )\n>\n>   # Now call create_arrow_model() as we explained before.\n>   ```\n\n[Update on GitHub](https://github.com/huggingface/peft/blob/main/docs/source/developer_guides/lora.md)\n\n[\u2190Quantization](https://huggingface.co/docs/peft/main/en/developer_guides/quantization) [Custom models\u2192](https://huggingface.co/docs/peft/main/en/developer_guides/custom_models)[SEP]", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Custom models", "description": "Hugging Face Official Documentation of Custom models", "url": "https://huggingface.co/docs/peft/main/en/developer_guides/custom_models", "source": "hf", "id": "192c67ea-e54f-4db1-92c4-d8e5b2a2f286"}, "page_content": "[CLS]# Custom models\n\nSome fine-tuning techniques, such as prompt tuning, are specific to language models. That means in \ud83e\udd17 PEFT, it is\nassumed a \ud83e\udd17 Transformers model is being used. However, other fine-tuning techniques - like\n[LoRA](https://huggingface.co/docs/peft/main/en/conceptual_guides/lora) \\- are not restricted to specific model types.\n\nIn this guide, we will see how LoRA can be applied to a multilayer perceptron, a computer vision model from the [timm](https://huggingface.co/docs/timm/index) library, or a new \ud83e\udd17 Transformers architecture.\n\n## Multilayer perceptron\n\nLet\u2019s assume that we want to fine-tune a multilayer perceptron with LoRA. Here is the definition:\n\nCopied\n\n```\nfrom torch import nn\n\nclass MLP(nn.Module):\n    def __init__(self, num_units_hidden=2000):\n        super().__init__()\n        self.seq = nn.Sequential(\n            nn.Linear(20, num_units_hidden),\n            nn.ReLU(),\n            nn.Linear(num_units_hidden, num_units_hidden),\n            nn.ReLU(),\n            nn.Linear(num_units_hidden, 2),\n            nn.LogSoftmax(dim=-1),\n        )\n\n    def forward(self, X):\n        return self.seq(X)\n```\n\nThis is a straightforward multilayer perceptron with an input layer, a hidden layer, and an output layer.\n\n> For this toy example, we choose an exceedingly large number of hidden units to highlight the efficiency gains\n> from PEFT, but those gains are in line with more realistic examples.\n\nThere are a few linear layers in this model that could be tuned with LoRA. When working with common \ud83e\udd17 Transformers\nmodels, PEFT will know which layers to apply LoRA to, but in this case, it is up to us as a user to choose the layers.\nTo determine the names of the layers to tune:\n\nCopied\n\n```\nprint([(n, type(m)) for n, m in MLP().named_modules()])\n```\n\nThis should print:\n\nCopied\n\n```\n[('', __main__.MLP),\\\n ('seq', torch.nn.modules.container.Sequential),\\\n ('seq.0', torch.nn.modules.linear.Linear),\\\n ('seq.1', torch.nn.modules.activation.ReLU),\\\n ('seq.2', torch.nn.modules.linear.Linear),\\\n ('seq.3', torch.nn.modules.activation.ReLU),\\\n ('seq.4', torch.nn.modules.linear.Linear),\\\n ('seq.5', torch.nn.modules.activation.LogSoftmax)]\n```\n\nLet\u2019s say we want to apply LoRA to the input layer and to the hidden layer, those are `'seq.0'` and `'seq.2'`. Moreover,\nlet\u2019s assume we want to update the output layer without LoRA, that would be `'seq.4'`. The corresponding config would\nbe:\n\nCopied\n\n```\nfrom peft import LoraConfig\n\nconfig = LoraConfig(\n    target_modules=[\"seq.0\", \"seq.2\"],\n    modules_to_save=[\"seq.4\"],\n)\n```\n\nWith that, we can create our PEFT model and check the fraction of parameters trained:\n\nCopied\n\n```\nfrom peft import get_peft_model\n\nmodel = MLP()\npeft_model = get_peft_model(model, config)\npeft_model.print_trainable_parameters()\n# prints trainable params: 56,164 || all params: 4,100,164 || trainable%: 1.369798866581922\n```\n\nFinally, we can use any training framework we like, or write our own fit loop, to train the `peft_model`.\n\nFor a complete example, check out [this notebook](https://github.com/huggingface/peft/blob/main/examples/multilayer_perceptron/multilayer_perceptron_lora.ipynb).\n\n## timm models\n\nThe [timm](https://huggingface.co/docs/timm/index) library contains a large number of pretrained computer vision models.\nThose can also be fine-tuned with PEFT. Let\u2019s check out how this works in practice.\n\nTo start, ensure that timm is installed in the Python environment:\n\nCopied\n\n```\npython -m pip install -U timm\n```\n\nNext we load a timm model for an image classification task:\n\nCopied\n\n```\nimport timm\n\nnum_classes =...\nmodel_id = \"timm/poolformer_m36.sail_in1k\"\nmodel = timm.create_model(model_id, pretrained=True, num_classes=num_classes)\n```\n\nAgain, we need to make a decision about what layers to apply LoRA to. Since LoRA supports 2D conv layers, and since\nthose are a", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Custom models", "description": "Hugging Face Official Documentation of Custom models", "url": "https://huggingface.co/docs/peft/main/en/developer_guides/custom_models", "source": "hf", "id": "1ae9c13f-8101-41d2-991c-4ce697f0ec0c"}, "page_content": "face.co/docs/timm/index) library contains a large number of pretrained computer vision models.\nThose can also be fine-tuned with PEFT. Let\u2019s check out how this works in practice.\n\nTo start, ensure that timm is installed in the Python environment:\n\nCopied\n\n```\npython -m pip install -U timm\n```\n\nNext we load a timm model for an image classification task:\n\nCopied\n\n```\nimport timm\n\nnum_classes =...\nmodel_id = \"timm/poolformer_m36.sail_in1k\"\nmodel = timm.create_model(model_id, pretrained=True, num_classes=num_classes)\n```\n\nAgain, we need to make a decision about what layers to apply LoRA to. Since LoRA supports 2D conv layers, and since\nthose are a major building block of this model, we should apply LoRA to the 2D conv layers. To identify the names of\nthose layers, let\u2019s look at all the layer names:\n\nCopied\n\n```\nprint([(n, type(m)) for n, m in model.named_modules()])\n```\n\nThis will print a very long list, we\u2019ll only show the first few:\n\nCopied\n\n```\n[('', timm.models.metaformer.MetaFormer),\\\n ('stem', timm.models.metaformer.Stem),\\\n ('stem.conv', torch.nn.modules.conv.Conv2d),\\\n ('stem.norm', torch.nn.modules.linear.Identity),\\\n ('stages', torch.nn.modules.container.Sequential),\\\n ('stages.0', timm.models.metaformer.MetaFormerStage),\\\n ('stages.0.downsample', torch.nn.modules.linear.Identity),\\\n ('stages.0.blocks', torch.nn.modules.container.Sequential),\\\n ('stages.0.blocks.0', timm.models.metaformer.MetaFormerBlock),\\\n ('stages.0.blocks.0.norm1', timm.layers.norm.GroupNorm1),\\\n ('stages.0.blocks.0.token_mixer', timm.models.metaformer.Pooling),\\\n ('stages.0.blocks.0.token_mixer.pool', torch.nn.modules.pooling.AvgPool2d),\\\n ('stages.0.blocks.0.drop_path1', torch.nn.modules.linear.Identity),\\\n ('stages.0.blocks.0.layer_scale1', timm.models.metaformer.Scale),\\\n ('stages.0.blocks.0.res_scale1', torch.nn.modules.linear.Identity),\\\n ('stages.0.blocks.0.norm2', timm.layers.norm.GroupNorm1),\\\n ('stages.0.blocks.0.mlp', timm.layers.mlp.Mlp),\\\n ('stages.0.blocks.0.mlp.fc1', torch.nn.modules.conv.Conv2d),\\\n ('stages.0.blocks.0.mlp.act', torch.nn.modules.activation.GELU),\\\n ('stages.0.blocks.0.mlp.drop1', torch.nn.modules.dropout.Dropout),\\\n ('stages.0.blocks.0.mlp.norm', torch.nn.modules.linear.Identity),\\\n ('stages.0.blocks.0.mlp.fc2', torch.nn.modules.conv.Conv2d),\\\n ('stages.0.blocks.0.mlp.drop2', torch.nn.modules.dropout.Dropout),\\\n ('stages.0.blocks.0.drop_path2', torch.nn.modules.linear.Identity),\\\n ('stages.0.blocks.0.layer_scale2', timm.models.metaformer.Scale),\\\n ('stages.0.blocks.0.res_scale2', torch.nn.modules.linear.Identity),\\\n ('stages.0.blocks.1', timm.models.metaformer.MetaFormerBlock),\\\n ('stages.0.blocks.1.norm1', timm.layers.norm.GroupNorm1),\\\n ('stages.0.blocks.1.token_mixer', timm.models.metaformer.Pooling),\\\n ('stages.0.blocks.1.token_mixer.pool', torch.nn.modules.pooling.AvgPool2d),\\\n...\\\n ('head.global_pool.flatten', torch.nn.modules.linear.Identity),\\\n ('head.norm', timm.layers.norm.LayerNorm2d),\\\n ('head.flatten', torch.nn.modules.flatten.Flatten),\\\n ('head.drop', torch.nn.modules.linear.Identity),\\\n ('head.fc', torch.nn.modules.linear.Linear)]\n ]\n```\n\nUpon closer inspection, we see that the 2D conv layers have names such as `\"stages.0.blocks.0.mlp.fc1\"` and\n`\"stages.0.blocks.0.mlp.fc2\"`. How can we match those layer names specifically? You can write a [regular\\\\\nexpressions](https://docs", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Custom models", "description": "Hugging Face Official Documentation of Custom models", "url": "https://huggingface.co/docs/peft/main/en/developer_guides/custom_models", "source": "hf", "id": "7179113e-d928-46e1-9e32-56502b1a2178"}, "page_content": ".1.token_mixer.pool', torch.nn.modules.pooling.AvgPool2d),\\\n...\\\n ('head.global_pool.flatten', torch.nn.modules.linear.Identity),\\\n ('head.norm', timm.layers.norm.LayerNorm2d),\\\n ('head.flatten', torch.nn.modules.flatten.Flatten),\\\n ('head.drop', torch.nn.modules.linear.Identity),\\\n ('head.fc', torch.nn.modules.linear.Linear)]\n ]\n```\n\nUpon closer inspection, we see that the 2D conv layers have names such as `\"stages.0.blocks.0.mlp.fc1\"` and\n`\"stages.0.blocks.0.mlp.fc2\"`. How can we match those layer names specifically? You can write a [regular\\\\\nexpressions](https://docs.python.org/3/library/re.html) to match the layer names. For our case, the regex\n`r\".*\\.mlp\\.fc\\d\"` should do the job.\n\nFurthermore, as in the first example, we should ensure that the output layer, in this case the classification head, is\nalso updated. Looking at the end of the list printed above, we can see that it\u2019s named `'head.fc'`. With that in mind,\nhere is our LoRA config:\n\nCopied\n\n```\nconfig = LoraConfig(target_modules=r\".*\\.mlp\\.fc\\d\", modules_to_save=[\"head.fc\"])\n```\n\nThen we only need to create the PEFT model by passing our base model and the config to `get_peft_model`:\n\nCopied\n\n```\npeft_model = get_peft_model(model, config)\npeft_model.print_trainable_parameters()\n# prints trainable params: 1,064,454 || all params: 56,467,974 || trainable%: 1.88505789139876\n```\n\nThis shows us that we only need to train less than 2% of all parameters, which is a huge efficiency gain.\n\nFor a complete example, check out [this notebook](https://github.com/huggingface/peft/blob/main/examples/image_classification/image_classification_timm_peft_lora.ipynb).\n\n## New transformers architectures\n\nWhen new popular transformers architectures are released, we do our best to quickly add them to PEFT. If you come across a transformers model that is not supported out of the box, don\u2019t worry, it will most likely still work if the config is set correctly. Specifically, you have to identify the layers that should be adapted and set them correctly when initializing the corresponding config class, e.g. `LoraConfig`. Here are some tips to help with this.\n\nAs a first step, it is a good idea to check the existing models for inspiration. You can find them inside of [constants.py](https://github.com/huggingface/peft/blob/main/src/peft/utils/constants.py) in the PEFT repository. Often, you\u2019ll find a similar architecture that uses the same names. For example, if the new model architecture is a variation of the \u201cmistral\u201d model and you want to apply LoRA, you can see that the entry for \u201cmistral\u201d in `TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING` contains `[\"q_proj\", \"v_proj\"]`. This tells you that for \u201cmistral\u201d models, the `target_modules` for LoRA should be `[\"q_proj\", \"v_proj\"]`:\n\nCopied\n\n```\nfrom peft import LoraConfig, get_peft_model\n\nmy_mistral_model =...\nconfig = LoraConfig(\n    target_modules=[\"q_proj\", \"v_proj\"],\n   ...,  # other LoRA arguments\n)\npeft_model = get_peft_model(my_mistral_model, config)\n```\n\nIf that doesn\u2019t help, check the existing modules in your model architecture with the `named_modules` method and try to identify the attention layers, especially the key, query, and value layers. Those will often have names such as `c_attn`, `query`, `q_proj`, etc. The key layer is not always adapted, and ideally, you should check whether including it results in better performance.\n\nAdditionally, linear layers are common targets to be adapted (e.g. in [QLoRA paper](https://huggingface.co/papers/2305.14314), authors suggest to adapt them as well). Their names will often contain the strings `fc` or `dense`.\n\nIf you want to add a new model to PEFT, please create an entry in [constants.py](https://github.com/huggingface/peft/blob/main/src/peft/utils/constants.py) and open a pull request on the [repository](https://github.com/huggingface/peft/pulls). Don\u2019t forget to update the [README](https://github.com/huggingface/peft#models-support-matrix) as well.\n\n## Verify parameters and layers", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Custom models", "description": "Hugging Face Official Documentation of Custom models", "url": "https://huggingface.co/docs/peft/main/en/developer_guides/custom_models", "source": "hf", "id": "7624e49e-aded-4c7f-9b6b-40ccbff61135"}, "page_content": " ideally, you should check whether including it results in better performance.\n\nAdditionally, linear layers are common targets to be adapted (e.g. in [QLoRA paper](https://huggingface.co/papers/2305.14314), authors suggest to adapt them as well). Their names will often contain the strings `fc` or `dense`.\n\nIf you want to add a new model to PEFT, please create an entry in [constants.py](https://github.com/huggingface/peft/blob/main/src/peft/utils/constants.py) and open a pull request on the [repository](https://github.com/huggingface/peft/pulls). Don\u2019t forget to update the [README](https://github.com/huggingface/peft#models-support-matrix) as well.\n\n## Verify parameters and layers\n\nYou can verify whether you\u2019ve correctly applied a PEFT method to your model in a few ways.\n\n- Check the fraction of parameters that are trainable with the [print\\_trainable\\_parameters()](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.PeftModel.print_trainable_parameters) method. If this number is lower or higher than expected, check the model `repr` by printing the model. This shows the names of all the layer types in the model. Ensure that only the intended target layers are replaced by the adapter layers. For example, if LoRA is applied to `nn.Linear` layers, then you should only see `lora.Linear` layers being used.\n\nCopied\n\n```\npeft_model.print_trainable_parameters()\n```\n\n- Another way you can view the adapted layers is to use the `targeted_module_names` attribute to list the name of each module that was adapted.\n\nCopied\n\n```\nprint(peft_model.targeted_module_names)\n```\n\n## Unsupported module types\n\nMethods like LoRA only work if the target modules are supported by PEFT. For example, it\u2019s possible to apply LoRA to `nn.Linear` and `nn.Conv2d` layers, but not, for instance, to `nn.LSTM`. If you find a layer class you want to apply PEFT to is not supported, you can:\n\n- define a custom mapping to dynamically dispatch custom modules in LoRA\n- open an [issue](https://github.com/huggingface/peft/issues) and request the feature where maintainers will implement it or guide you on how to implement it yourself if demand for this module type is sufficiently high\n\n### Experimental support for dynamic dispatch of custom modules in LoRA\n\n> This feature is experimental and subject to change, depending on its reception by the community. We will introduce a public and stable API if there is significant demand for it.\n\nPEFT supports an experimental API for custom module types for LoRA. Let\u2019s assume you have a LoRA implementation for LSTMs. Normally, you would not be able to tell PEFT to use it, even if it would theoretically work with PEFT. However, this is possible with dynamic dispatch of custom layers.\n\nThe experimental API currently looks like this:\n\nCopied\n\n```\nclass MyLoraLSTMLayer:\n   ...\n\nbase_model =...  # load the base model that uses LSTMs\n\n# add the LSTM layer names to target_modules\nconfig = LoraConfig(..., target_modules=[\"lstm\"])\n# define a mapping from base layer type to LoRA layer type\ncustom_module_mapping = {nn.LSTM: MyLoraLSTMLayer}\n# register the new mapping\nconfig._register_custom_module(custom_module_mapping)\n# after registration, create the PEFT model\npeft_model = get_peft_model(base_model, config)\n# do training\n```\n\n> When you call [get\\_peft\\_model()](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.get_peft_model), you will see a warning because PEFT does not recognize the targeted module type. In this case, you can ignore this warning.\n\nBy supplying a custom mapping, PEFT first checks the base model\u2019s layers against the custom mapping and dispatches to the custom LoRA layer type if there is a match. If there is no match, PEFT checks the built-in LoRA layer types for a match.\n\nTherefore, this feature can also be used to override existing dispatch logic, e.g. if you want to use your own LoRA layer for `nn.Linear` instead of using the one provided by PEFT.\n\nWhen creating your custom LoRA module, please follow the same rules as the [existing LoRA modules](https://github.com/huggingface/peft/blob/main/src/peft/tuners/lora/layer.py). Some important constraints to consider:\n\n- The custom module should inherit from `nn.Module` and `peft.tuners.lora.layer.LoraLayer`.\n- The `__init__` method of the custom module should have the positional arguments `base_layer` and `adapter_name`. After this, there are additional `", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Custom models", "description": "Hugging Face Official Documentation of Custom models", "url": "https://huggingface.co/docs/peft/main/en/developer_guides/custom_models", "source": "hf", "id": "86dc31c1-4801-4daf-b2d3-997755cd0cf6"}, "page_content": " match. If there is no match, PEFT checks the built-in LoRA layer types for a match.\n\nTherefore, this feature can also be used to override existing dispatch logic, e.g. if you want to use your own LoRA layer for `nn.Linear` instead of using the one provided by PEFT.\n\nWhen creating your custom LoRA module, please follow the same rules as the [existing LoRA modules](https://github.com/huggingface/peft/blob/main/src/peft/tuners/lora/layer.py). Some important constraints to consider:\n\n- The custom module should inherit from `nn.Module` and `peft.tuners.lora.layer.LoraLayer`.\n- The `__init__` method of the custom module should have the positional arguments `base_layer` and `adapter_name`. After this, there are additional `**kwargs` that you are free to use or ignore.\n- The learnable parameters should be stored in an `nn.ModuleDict` or `nn.ParameterDict`, where the key corresponds to the name of the specific adapter (remember that a model can have more than one adapter at a time).\n- The name of these learnable parameter attributes should start with `\"lora_\"`, e.g. `self.lora_new_param =...`.\n- Some methods are optional, e.g. you only need to implement `merge` and `unmerge` if you want to support weight merging.\n\nCurrently, the information about the custom module does not persist when you save the model. When loading the model, you have to register the custom modules again.\n\nCopied\n\n```\n# saving works as always and includes the parameters of the custom modules\npeft_model.save_pretrained(<model-path>)\n\n# loading the model later:\nbase_model =...\n# load the LoRA config that you saved earlier\nconfig = LoraConfig.from_pretrained(<model-path>)\n# register the custom module again, the same way as the first time\ncustom_module_mapping = {nn.LSTM: MyLoraLSTMLayer}\nconfig._register_custom_module(custom_module_mapping)\n# pass the config instance to from_pretrained:\npeft_model = PeftModel.from_pretrained(model, tmp_path / \"lora-custom-module\", config=config)\n```\n\nIf you use this feature and find it useful, or if you encounter problems, let us know by creating an issue or a discussion on GitHub. This allows us to estimate the demand for this feature and add a public API if it is sufficiently high.\n\n[Update on GitHub](https://github.com/huggingface/peft/blob/main/docs/source/developer_guides/custom_models.md)\n\n[\u2190LoRA](https://huggingface.co/docs/peft/main/en/developer_guides/lora) [Adapter injection\u2192](https://huggingface.co/docs/peft/main/en/developer_guides/low_level_api)[SEP]", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Adapter injection", "description": "Hugging Face Official Documentation of Adapter injection", "url": "https://huggingface.co/docs/peft/main/en/developer_guides/low_level_api", "source": "hf", "id": "22fa8d08-7721-40e6-bda5-2503adfaa50b"}, "page_content": "[CLS]# Adapter injection\n\nWith PEFT, you can inject trainable adapters into any `torch` module which allows you to use adapter methods without relying on the modeling classes in PEFT. This works for all adapters except for those based on prompt learning (e.g. prefix tuning or p-tuning).\n\nCheck the table below to see when you should inject adapters.\n\n| Pros | Cons |\n| --- | --- |\n| the model is modified inplace, keeping all the original attributes and methods | manually write the `from_pretrained` and `save_pretrained` utility functions from Hugging Face to save and load adapters |\n| works for any `torch` module and modality | doesn\u2019t work with any of the utility methods provided by `PeftModel` such as disabling and merging adapters |\n\n## Creating a new PEFT model\n\nTo perform the adapter injection, use the [inject\\_adapter\\_in\\_model()](https://huggingface.co/docs/peft/main/en/package_reference/functional#peft.inject_adapter_in_model) method. This method takes 3 arguments, the PEFT config, the model, and an optional adapter name. You can also attach multiple adapters to the model if you call [inject\\_adapter\\_in\\_model()](https://huggingface.co/docs/peft/main/en/package_reference/functional#peft.inject_adapter_in_model) multiple times with different adapter names.\n\nFor example, to inject LoRA adapters into the `linear` submodule of the `DummyModel` module:\n\nCopied\n\n```\nimport torch\nfrom peft import inject_adapter_in_model, LoraConfig\n\nclass DummyModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.embedding = torch.nn.Embedding(10, 10)\n        self.linear = torch.nn.Linear(10, 10)\n        self.lm_head = torch.nn.Linear(10, 10)\n\n    def forward(self, input_ids):\n        x = self.embedding(input_ids)\n        x = self.linear(x)\n        x = self.lm_head(x)\n        return x\n\nlora_config = LoraConfig(\n    lora_alpha=16,\n    lora_dropout=0.1,\n    r=64,\n    bias=\"none\",\n    target_modules=[\"linear\"],\n)\n\nmodel = DummyModel()\nmodel = inject_adapter_in_model(lora_config, model)\n\ndummy_inputs = torch.LongTensor([[0, 1, 2, 3, 4, 5, 6, 7]])\ndummy_outputs = model(dummy_inputs)\n```\n\nPrint the model to see that the adapters have been correctly injected.\n\nCopied\n\n```\nDummyModel(\n  (embedding): Embedding(10, 10)\n  (linear): Linear(\n    in_features=10, out_features=10, bias=True\n    (lora_dropout): ModuleDict(\n      (default): Dropout(p=0.1, inplace=False)\n    )\n    (lora_A): ModuleDict(\n      (default): Linear(in_features=10, out_features=64, bias=False)\n    )\n    (lora_B): ModuleDict(\n      (default): Linear(in_features=64, out_features=10, bias=False)\n    )\n    (lora_embedding_A): ParameterDict()\n    (lora_embedding_B): ParameterDict()\n  )\n  (lm_head): Linear(in_features=10, out_features=10, bias=True)\n)\n```\n\n### Injection based on a state\\_dict\n\nSometimes, it is possible that there is a PEFT adapter checkpoint but the corresponding PEFT config is not known for whatever reason. To inject the PEFT layers for this checkpoint, you would usually have to reverse-engineer the corresponding PEFT config, most notably the `target_modules` argument, based on the `state_dict` from the checkpoint. This can be cumbersome and error prone. To avoid this, it is also possible to call [inject\\_adapter\\_in\\_model()](https://huggingface.co/docs/peft/main/en/package_reference/functional#peft.inject_adapter_in_model) and pass the loaded `state_dict` as an argument:\n\nCopied\n\n```\nfrom safetensors.torch import load_file\n\nmodel =...\nstate_dict = load_file(<path-to-safetensors-file>)\nlora_config = LoraConfig(...)\nmodel = inject_adapter_in_model(lora_config, model, state_dict=state_dict)\n```\n\nIn this case, PEFT will use the `state_dict` as reference for which layers to target instead of using the PEFT config. As a user, you don\u2019t have to set the exact `target_modules` of the PEFT config for this to work. However, you should still pass a PEFT config of the right type, in this example `LoraConfig`,", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Adapter injection", "description": "Hugging Face Official Documentation of Adapter injection", "url": "https://huggingface.co/docs/peft/main/en/developer_guides/low_level_api", "source": "hf", "id": "8c9f71e9-7677-4778-92e2-3d75ca3c9747"}, "page_content": "/main/en/package_reference/functional#peft.inject_adapter_in_model) and pass the loaded `state_dict` as an argument:\n\nCopied\n\n```\nfrom safetensors.torch import load_file\n\nmodel =...\nstate_dict = load_file(<path-to-safetensors-file>)\nlora_config = LoraConfig(...)\nmodel = inject_adapter_in_model(lora_config, model, state_dict=state_dict)\n```\n\nIn this case, PEFT will use the `state_dict` as reference for which layers to target instead of using the PEFT config. As a user, you don\u2019t have to set the exact `target_modules` of the PEFT config for this to work. However, you should still pass a PEFT config of the right type, in this example `LoraConfig`, you can leave the `target_modules` as `None`.\n\nBe aware that this still only creates the uninitialized PEFT layers, the values from the `state_dict` are not used to populate the model weights. To populate the weights, proceed with calling [set\\_peft\\_model\\_state\\_dict()](https://huggingface.co/docs/peft/main/en/package_reference/functional#peft.set_peft_model_state_dict) as described below.\n\n\u26a0\ufe0f Note that if there is a mismatch between what is configured in the PEFT config and what is found in the `state_dict`, PEFT will warn you about this. You can ignore the warning if you know that the PEFT config is not correctly specified.\n\n> If the original PEFT adapters was using `target_parameters` instead of `target_modules`, injecting from a `state_dict` will not work correctly. In this case, it is mandatory to use the correct PEFT config for injection.\n\n## Saving the model\n\nTo only save the adapter, use the [get\\_peft\\_model\\_state\\_dict()](https://huggingface.co/docs/peft/main/en/package_reference/functional#peft.get_peft_model_state_dict) function:\n\nCopied\n\n```\nfrom peft import get_peft_model_state_dict\n\npeft_state_dict = get_peft_model_state_dict(model)\nprint(peft_state_dict)\n```\n\nOtherwise, `model.state_dict()` returns the full state dict of the model.\n\n## Loading the model\n\nAfter loading the saved `state_dict`, it can be applied using the [set\\_peft\\_model\\_state\\_dict()](https://huggingface.co/docs/peft/main/en/package_reference/functional#peft.set_peft_model_state_dict) function:\n\nCopied\n\n```\nfrom peft import set_peft_model_state_dict\n\nmodel = DummyModel()\nmodel = inject_adapter_in_model(lora_config, model)\noutcome = set_peft_model_state_dict(model, peft_state_dict)\n# check that there were no wrong keys\nprint(outcome.unexpected_keys)\n```\n\nIf injecting the adapter is slow or you need to load a large number of adapters, you may use an optimization that allows to create an \u201cempty\u201d adapter on meta device and only fills the weights with real weights when the [set\\_peft\\_model\\_state\\_dict()](https://huggingface.co/docs/peft/main/en/package_reference/functional#peft.set_peft_model_state_dict) is called. To do this, pass `low_cpu_mem_usage=True` to both [inject\\_adapter\\_in\\_model()](https://huggingface.co/docs/peft/main/en/package_reference/functional#peft.inject_adapter_in_model) and [set\\_peft\\_model\\_state\\_dict()](https://huggingface.co/docs/peft/main/en/package_reference/functional#peft.set_peft_model_state_dict).\n\nCopied\n\n```\nmodel = DummyModel()\nmodel = inject_adapter_in_model(lora_config, model, low_cpu_mem_usage=True)\n\nprint(model.linear.lora_A[\"default\"].weight.device.type == \"meta\")  # should be True\nset_peft_model_state_dict(model, peft_state_dict, low_cpu_mem_usage=True)\nprint(model.linear.lora_A[\"default\"].weight.device.type == \"cpu\")  # should be True\n```\n\n[Update on GitHub](https://github.com/huggingface/peft/blob/main/docs/source/developer_guides/low_level_api.md)\n\n[\u2190Custom models](https://huggingface.co/docs/peft/main/en/developer_guides/custom_models) [Mixed adapter types\u2192](https://huggingface.co/docs/peft/main/en/developer_guides/mixed_models", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Adapter injection", "description": "Hugging Face Official Documentation of Adapter injection", "url": "https://huggingface.co/docs/peft/main/en/developer_guides/low_level_api", "source": "hf", "id": "12a1e171-9473-4f7a-92e7-57a9ada7eef7"}, "page_content": "\n\nprint(model.linear.lora_A[\"default\"].weight.device.type == \"meta\")  # should be True\nset_peft_model_state_dict(model, peft_state_dict, low_cpu_mem_usage=True)\nprint(model.linear.lora_A[\"default\"].weight.device.type == \"cpu\")  # should be True\n```\n\n[Update on GitHub](https://github.com/huggingface/peft/blob/main/docs/source/developer_guides/low_level_api.md)\n\n[\u2190Custom models](https://huggingface.co/docs/peft/main/en/developer_guides/custom_models) [Mixed adapter types\u2192](https://huggingface.co/docs/peft/main/en/developer_guides/mixed_models)[SEP]", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Mixed adapter types", "description": "Hugging Face Official Documentation of Mixed adapter types", "url": "https://huggingface.co/docs/peft/main/en/developer_guides/mixed_models", "source": "hf", "id": "61e7f521-7208-44a2-95c3-a705d0b63274"}, "page_content": "[CLS]# Mixed adapter types\n\nNormally, it isn\u2019t possible to mix different adapter types in \ud83e\udd17 PEFT. You can create a PEFT model with two different LoRA adapters (which can have different config options), but it is not possible to combine a LoRA and LoHa adapter. With [PeftMixedModel](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.PeftMixedModel) however, this works as long as the adapter types are compatible. The main purpose of allowing mixed adapter types is to combine trained adapters for inference. While it is possible to train a mixed adapter model, this has not been tested and is not recommended.\n\nTo load different adapter types into a PEFT model, use [PeftMixedModel](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.PeftMixedModel) instead of [PeftModel](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.PeftModel):\n\nCopied\n\n```\nfrom peft import PeftMixedModel\n\nbase_model =...  # load the base model, e.g. from transformers\n# load first adapter, which will be called \"default\"\npeft_model = PeftMixedModel.from_pretrained(base_model, <path_to_adapter1>)\npeft_model.load_adapter(<path_to_adapter2>, adapter_name=\"other\")\npeft_model.set_adapter([\"default\", \"other\"])\n```\n\nThe [set\\_adapter()](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.PeftMixedModel.set_adapter) method is necessary to activate both adapters, otherwise only the first adapter would be active. You can keep adding more adapters by calling [add\\_adapter()](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.PeftModel.add_adapter) repeatedly.\n\n[PeftMixedModel](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.PeftMixedModel) does not support saving and loading mixed adapters. The adapters should already be trained, and loading the model requires a script to be run each time.\n\n## Tips\n\n- Not all adapter types can be combined. See [`peft.tuners.mixed.COMPATIBLE_TUNER_TYPES`](https://github.com/huggingface/peft/blob/1c1c7fdaa6e6abaa53939b865dee1eded82ad032/src/peft/tuners/mixed/model.py#L35) for a list of compatible types. An error will be raised if you try to combine incompatible adapter types.\n- It is possible to mix multiple adapters of the same type which can be useful for combining adapters with very different configs.\n- If you want to combine a lot of different adapters, the most performant way to do it is to consecutively add the same adapter types. For example, add LoRA1, LoRA2, LoHa1, LoHa2 in this order, instead of LoRA1, LoHa1, LoRA2, and LoHa2. While the order can affect the output, there is no inherently _best_ order, so it is best to choose the fastest one.\n\n[Update on GitHub](https://github.com/huggingface/peft/blob/main/docs/source/developer_guides/mixed_models.md)\n\n[\u2190Adapter injection](https://huggingface.co/docs/peft/main/en/developer_guides/low_level_api) [torch.compile\u2192](https://huggingface.co/docs/peft/main/en/developer_guides/torch_compile)[SEP]", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Troubleshooting", "description": "Hugging Face Official Documentation of Troubleshooting", "url": "https://huggingface.co/docs/peft/main/en/developer_guides/troubleshooting", "source": "hf", "id": "c4ac6293-404a-4c52-a4e9-ae2c6faf441e"}, "page_content": "[CLS]# Troubleshooting\n\nIf you encounter any issue when using PEFT, please check the following list of common issues and their solutions.\n\n## Examples don\u2019t work\n\nExamples often rely on the most recent package versions, so please ensure they\u2019re up-to-date. In particular, check the following package versions:\n\n- `peft`\n- `transformers`\n- `accelerate`\n- `torch`\n\nIn general, you can update the package version by running this command inside your Python environment:\n\nCopied\n\n```\npython -m pip install -U <package_name>\n```\n\nInstalling PEFT from source is useful for keeping up with the latest developments:\n\nCopied\n\n```\npython -m pip install git+https://github.com/huggingface/peft\n```\n\n## Dtype-related issues\n\n### ValueError: Attempting to unscale FP16 gradients\n\nThis error probably occurred because the model was loaded with `dtype=torch.float16` and then used in an automatic mixed precision (AMP) context, e.g. by setting `fp16=True` in the [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) class from \ud83e\udd17 Transformers. The reason is that when using AMP, trainable weights should never use fp16. To make this work without loading the whole model in fp32, add the following to your code:\n\nCopied\n\n```\npeft_model = get_peft_model(...)\n\n# add this:\nfor param in model.parameters():\n    if param.requires_grad:\n        param.data = param.data.float()\n\n# proceed as usual\ntrainer = Trainer(model=peft_model, fp16=True,...)\ntrainer.train()\n```\n\nAlternatively, you can use the [cast\\_mixed\\_precision\\_params()](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.cast_mixed_precision_params) function to correctly cast the weights:\n\nCopied\n\n```\nfrom peft import cast_mixed_precision_params\n\npeft_model = get_peft_model(...)\ncast_mixed_precision_params(peft_model, dtype=torch.float16)\n\n# proceed as usual\ntrainer = Trainer(model=peft_model, fp16=True,...)\ntrainer.train()\n```\n\n> Starting from PEFT version v0.12.0, PEFT automatically promotes the dtype of adapter weights from `torch.float16` and `torch.bfloat16` to `torch.float32` where appropriate. To _prevent_ this behavior, you can pass `autocast_adapter_dtype=False` to [~get\\_peft\\_model()](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.get_peft_model), to [from\\_pretrained()](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.PeftModel.from_pretrained), and to [load\\_adapter()](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.PeftModel.load_adapter).\n\n### Selecting the dtype of the adapter\n\nMost PEFT methods, like LoRA, work by adding trainable adapter weights. By default, those weights are stored in float32 dtype (fp32), i.e. at a relatively high precision. Therefore, even if the base model is loaded in float16 (fp16) or bfloat16 (bf16), the adapter weights are float32. When the adapter results are calculated during the forward pass, the input will typically be in the dtype of the base model, thus it will be upcast to float32 if necessary, then cast back to the original dtype.\n\nIf you prefer to have the adapter weights in the lower precision of the base model, i.e. in float16 or bfloat16, you can pass `autocast_adapter_dtype=False` when creating the model ( [~get\\_peft\\_model()](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.get_peft_model)) or loading the model ( [from\\_pretrained()](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.PeftModel.from_pretrained)). There are some advantages and disadvantages to this:\n\nAdvantages of half precision adapter:\n\n- computation slightly faster\n- slightly less memory\n- smaller file size of checkpoint (half the size)\n\nDisadvantages of half precision adapter:\n\n- slightly worse loss\n- higher risk of overflow or underflow\n\nNote that for most use cases, overall runtime and memory cost will be determined by the size of the base model and by the dataset, while the dtype of the PEFT adapter will only have a small impact.\n\n## Bad results from a loaded PEFT model\n\nThere", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Troubleshooting", "description": "Hugging Face Official Documentation of Troubleshooting", "url": "https://huggingface.co/docs/peft/main/en/developer_guides/troubleshooting", "source": "hf", "id": "e3a241f2-6949-4a62-b27b-5a90773f8542"}, "page_content": "/main/en/package_reference/peft_model#peft.get_peft_model)) or loading the model ( [from\\_pretrained()](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.PeftModel.from_pretrained)). There are some advantages and disadvantages to this:\n\nAdvantages of half precision adapter:\n\n- computation slightly faster\n- slightly less memory\n- smaller file size of checkpoint (half the size)\n\nDisadvantages of half precision adapter:\n\n- slightly worse loss\n- higher risk of overflow or underflow\n\nNote that for most use cases, overall runtime and memory cost will be determined by the size of the base model and by the dataset, while the dtype of the PEFT adapter will only have a small impact.\n\n## Bad results from a loaded PEFT model\n\nThere can be several reasons for getting a poor result from a loaded PEFT model which are listed below. If you\u2019re still unable to troubleshoot the problem, see if anyone else had a similar [issue](https://github.com/huggingface/peft/issues) on GitHub, and if you can\u2019t find any, open a new issue.\n\nWhen opening an issue, it helps a lot if you provide a minimal code example that reproduces the issue. Also, please report if the loaded model performs at the same level as the model did before fine-tuning, if it performs at a random level, or if it is only slightly worse than expected. This information helps us identify the problem more quickly.\n\n### Random deviations\n\nIf your model outputs are not exactly the same as previous runs, there could be an issue with random elements. For example:\n\n1. please ensure it is in `.eval()` mode, which is important, for instance, if the model uses dropout\n2. if you use [generate](https://huggingface.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationMixin.generate) on a language model, there could be random sampling, so obtaining the same result requires setting a random seed\n3. if you used quantization and merged the weights, small deviations are expected due to rounding errors\n\n### Incorrectly loaded model\n\nPlease ensure that you load the model correctly. A common error is trying to load a _trained_ model with [get\\_peft\\_model()](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.get_peft_model) which is incorrect. Instead, the loading code should look like this:\n\nCopied\n\n```\nfrom peft import PeftModel, PeftConfig\n\nbase_model =...  # to load the base model, use the same code as when you trained it\nconfig = PeftConfig.from_pretrained(peft_model_id)\npeft_model = PeftModel.from_pretrained(base_model, peft_model_id)\n```\n\n### Randomly initialized layers\n\nFor some tasks, it is important to correctly configure `modules_to_save` in the config to account for randomly initialized layers.\n\nAs an example, this is necessary if you use LoRA to fine-tune a language model for sequence classification because \ud83e\udd17 Transformers adds a randomly initialized classification head on top of the model. If you do not add this layer to `modules_to_save`, the classification head won\u2019t be saved. The next time you load the model, you\u2019ll get a _different_ randomly initialized classification head, resulting in completely different results.\n\nPEFT tries to correctly guess the `modules_to_save` if you provide the `task_type` argument in the config. This should work for transformers models that follow the standard naming scheme. It is always a good idea to double check though because we can\u2019t guarantee all models follow the naming scheme.\n\nWhen you load a transformers model that has randomly initialized layers, you should see a warning along the lines of:\n\nCopied\n\n```\nSome weights of <MODEL> were not initialized from the model checkpoint at <ID> and are newly initialized: [<LAYER_NAMES>].\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n```\n\nThe mentioned layers should be added to `modules_to_save` in the config to avoid the described problem.\n\n> As an example, when loading a model that is using the DeBERTa architecture for sequence classification, you\u2019ll see a warning that the following weights are newly initialized: `['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']`. From this, it follows that the `classifier` and `pooler` layers should be added to: `modules_to_save=[\"classifier\", \"pooler\"]`.\n\n### Extending the vocabulary\n\nFor many language fine-tuning tasks, extending the model\u2019s vocabulary is necessary since new tokens are being introduced. This requires extending the embedding layer to account for the new tokens and, depending on the fine-tuning method, also storing the embedding layer in addition to the adapter weights when saving the adapter. There are a few ways of achieving this ordered by parameter effectiveness:\n\n- [trainable tokens](https://huggingface.co/", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Troubleshooting", "description": "Hugging Face Official Documentation of Troubleshooting", "url": "https://huggingface.co/docs/peft/main/en/developer_guides/troubleshooting", "source": "hf", "id": "eb11f8c0-9346-42c4-a976-e2f0a5167d1e"}, "page_content": " loading a model that is using the DeBERTa architecture for sequence classification, you\u2019ll see a warning that the following weights are newly initialized: `['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']`. From this, it follows that the `classifier` and `pooler` layers should be added to: `modules_to_save=[\"classifier\", \"pooler\"]`.\n\n### Extending the vocabulary\n\nFor many language fine-tuning tasks, extending the model\u2019s vocabulary is necessary since new tokens are being introduced. This requires extending the embedding layer to account for the new tokens and, depending on the fine-tuning method, also storing the embedding layer in addition to the adapter weights when saving the adapter. There are a few ways of achieving this ordered by parameter effectiveness:\n\n- [trainable tokens](https://huggingface.co/docs/peft/main/en/package_reference/trainable_tokens), train only the specified tokens, optionally store only the updated values\n- training an adapter on the embedding matrix, optionally store only the updated values\n- full-finetuning of the embedding layer\n\n#### Using trainable tokens\n\nLet\u2019s start with trainable tokens, in this case its [LoRA integration](https://huggingface.co/docs/peft/main/en/developer_guides/lora#efficiently-train-tokens-alongside-lora). If you\u2019re interested in only training the new embeddings and nothing else, refer to the [standalone documentation](https://huggingface.co/docs/peft/main/en/package_reference/trainable_tokens).\n\nTo enable selective token training of the embedding layer, you\u2019ll need to supply the token ids of your newly added tokens via the `trainable_token_indices` parameter. Optionally you can specify which layer to target if there is more than one embedding layer. For a Mistral model this could look like this:\n\nCopied\n\n```\nnew_tokens = ['<think>', '</think>']\ntokenizer.add_tokens(new_tokens)\nbase_model.resize_token_embeddings(len(tokenizer))\n\nlora_config = LoraConfig(\n   ...,\n    trainable_token_indices={'embed_tokens': tokenizer.convert_tokens_to_ids(new_tokens)},\n)\n```\n\nIf your model uses tied weights (such as the `lm_head`), trainable tokens will try to resolve those and keep them updated as well, so in that case there should be no need for adding `modules_to_save=[\"lm_head\"]`. This only works if the model uses the Transformers convention for tying weights.\n\nSaving the model with `model.save_pretrained` may save the full embedding matrix instead of\nonly the difference as a precaution because the embedding matrix was resized. To save space you can disable this behavior by setting `save_embedding_layers=False` when calling `save_pretrained`. This is safe to do as long as you don\u2019t modify the embedding matrix through other means as well, as such changes will be not tracked by trainable tokens.\n\n#### Using an adapter, e.g. LoRA\n\nPrepare the embedding layer by adding it to the `target_modules` of your adapter config. For example, the Mistral config could look like this:\n\nCopied\n\n```\nconfig = LoraConfig(..., target_modules=[\"embed_tokens\", \"lm_head\", \"q_proj\", \"v_proj\"])\n```\n\nOnce added to `target_modules`, PEFT automatically stores the embedding layer when saving the adapter if the model has the `get_input_embeddings` and `get_output_embeddings`. This is generally the case for Transformers models.\n\nIf the model\u2019s embedding layer doesn\u2019t follow the Transformer\u2019s naming scheme but nevertheless implements `get_input_embeddings`, you can still save it by manually passing `save_embedding_layers=True` when saving the adapter:\n\nCopied\n\n```\nmodel = get_peft_model(...)\n# train the model\nmodel.save_pretrained(\"my_adapter\", save_embedding_layers=True)\n```\n\nFor inference, load the base model first and resize it the same way you did before you trained the model. After you\u2019ve resized the base model, you can load the PEFT checkpoint.\n\nFor a complete example, please check out [this notebook](https://github.com/huggingface/peft/blob/main/examples/causal_language_modeling/peft_lora_clm_with_additional_tokens.ipynb).\n\n#### Full fine-tuning\n\nFull fine-tuning is more costly in terms of VRAM or storage space but if all else fails, you can fall back to this and see if it works for you. Achieve it by adding the name of the embedding layer to `modules_to_save`. Note that you need to add tied layers as well, e.g. `lm_head`. Example for a Mistral model with LoRA:\n\nCopied\n\n```\nconfig = LoraConfig(..., modules_to_save=[\"embed_tokens\", \"lm_head\"], target_modules=[\"q_proj\", \"v_proj\"])\n```\n\n### Getting a", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Troubleshooting", "description": "Hugging Face Official Documentation of Troubleshooting", "url": "https://huggingface.co/docs/peft/main/en/developer_guides/troubleshooting", "source": "hf", "id": "07e8c7c2-3a5e-4c03-8ca0-be7c168123eb"}, "page_content": " check out [this notebook](https://github.com/huggingface/peft/blob/main/examples/causal_language_modeling/peft_lora_clm_with_additional_tokens.ipynb).\n\n#### Full fine-tuning\n\nFull fine-tuning is more costly in terms of VRAM or storage space but if all else fails, you can fall back to this and see if it works for you. Achieve it by adding the name of the embedding layer to `modules_to_save`. Note that you need to add tied layers as well, e.g. `lm_head`. Example for a Mistral model with LoRA:\n\nCopied\n\n```\nconfig = LoraConfig(..., modules_to_save=[\"embed_tokens\", \"lm_head\"], target_modules=[\"q_proj\", \"v_proj\"])\n```\n\n### Getting a warning about \u201cweights not being initialized from the model checkpoint\u201d\n\nWhen you load your PEFT model which has been trained on a task (for example, classification), you may get a warning like:\n\n> Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: \\[\u2018score.weight\u2019\\]. You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\nAlthough this looks scary, it is most likely nothing to worry about. This warning comes from Transformers, and it isn\u2019t a PEFT specific warning. It lets you know that a randomly initialized classification head (`score`) is attached to the base model, and the head must be trained to produce sensible predictions.\n\nWhen you get this warning _before_ training the model, PEFT automatically takes care of making the classification head trainable if you correctly passed the `task_type` argument to the PEFT config.\n\nCopied\n\n```\nfrom peft import LoraConfig, TaskType\n\nlora_config = LoraConfig(..., task_type=TaskType.SEQ_CLS)\n```\n\nIf your classification head does not follow the usual naming conventions from Transformers (which is rare), you have to explicitly tell PEFT the name of the head in `modules_to_save`.\n\nCopied\n\n```\nlora_config = LoraConfig(..., modules_to_save=[\"name-of-classification-head\"])\n```\n\nTo check the name of the classification head, print the model and it should be the last module.\n\nIf you get this warning from your inference code, i.e. _after_ training the model, when you load the PEFT model, you always have to load the Transformers model first. Since Transformers does not know that you will load PEFT weights afterwards, it still gives the warning.\n\nAs always, it is best practice to ensure the model works correctly for inference by running some validation on it.\n\n### Check layer and model status\n\nSometimes a PEFT model can end up in a bad state, especially when handling multiple adapters. There can be some confusion around what adapters exist, which one is active, which one is merged, etc. To help investigate this issue, call the [get\\_layer\\_status()](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.PeftModel.get_layer_status) and the [get\\_model\\_status()](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.PeftModel.get_model_status) methods.\n\nThe [get\\_layer\\_status()](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.PeftModel.get_layer_status) method gives you a detailed overview of each targeted layer\u2019s active, merged, and available adapters.\n\nCopied\n\n```\n>>> from transformers import AutoModel\n>>> from peft import get_peft_model, LoraConfig\n\n>>> model_id = \"google/flan-t5-small\"\n>>> model = AutoModel.from_pretrained(model_id)\n>>> model = get_peft_model(model, LoraConfig())\n\n>>> model.get_layer_status()\n[TunerLayerStatus(name='model.encoder.block.0.layer.0.SelfAttention.q',\\\n                  module_type='lora.Linear',\\\n                  enabled=True,\\\n                  active_adapters=['default'],\\\n                  merged_adapters=[],\\\n                  requires_grad={'default': True},\\\n                  available_adapters=['default']),\\\n TunerLayerStatus(name='model.encoder.block.0.layer.0.SelfAttention.v',\\\n                  module_type='lora.Linear',\\\n                  enabled=True,\\\n                  active_adapters=['default'],\\\n                  merged_adapters=[],\\\n                  requires_grad={'default': True},\\\n                  available_adapters=['default']),\\\n...]\n\n>>> model.get_model_status()\nTunerModelStatus(\n    base_model_type='T5Model',\n    adapter_model_type='LoraModel',\n    peft_types={'default': 'L", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Troubleshooting", "description": "Hugging Face Official Documentation of Troubleshooting", "url": "https://huggingface.co/docs/peft/main/en/developer_guides/troubleshooting", "source": "hf", "id": "4f210cfc-14f7-4c44-b9a4-7b960d1d458a"}, "page_content": "q',\\\n                  module_type='lora.Linear',\\\n                  enabled=True,\\\n                  active_adapters=['default'],\\\n                  merged_adapters=[],\\\n                  requires_grad={'default': True},\\\n                  available_adapters=['default']),\\\n TunerLayerStatus(name='model.encoder.block.0.layer.0.SelfAttention.v',\\\n                  module_type='lora.Linear',\\\n                  enabled=True,\\\n                  active_adapters=['default'],\\\n                  merged_adapters=[],\\\n                  requires_grad={'default': True},\\\n                  available_adapters=['default']),\\\n...]\n\n>>> model.get_model_status()\nTunerModelStatus(\n    base_model_type='T5Model',\n    adapter_model_type='LoraModel',\n    peft_types={'default': 'LORA'},\n    trainable_params=344064,\n    total_params=60855680,\n    num_adapter_layers=48,\n    enabled=True,\n    active_adapters=['default'],\n    merged_adapters=[],\n    requires_grad={'default': True},\n    available_adapters=['default'],\n)\n```\n\nIn the model state output, you should look out for entries that say `\"irregular\"`. This means PEFT detected an inconsistent state in the model. For instance, if `merged_adapters=\"irregular\"`, it means that for at least one adapter, it was merged on some target modules but not on others. The inference results will most likely be incorrect as a result.\n\nThe best way to resolve this issue is to reload the whole model and adapter checkpoint(s). Ensure that you don\u2019t perform any incorrect operations on the model, e.g. manually merging adapters on some modules but not others.\n\nConvert the layer status into a pandas `DataFrame` for an easier visual inspection.\n\nCopied\n\n```\nfrom dataclasses import asdict\nimport pandas as pd\n\ndf = pd.DataFrame(asdict(layer) for layer in model.get_layer_status())\n```\n\nIt is possible to get this information for non-PEFT models if they are using PEFT layers under the hood, but some information like the `base_model_type` or the `peft_types` cannot be determined in that case. As an example, you can call this on a [diffusers](https://huggingface.co/docs/diffusers/index) model like so:\n\nCopied\n\n```\n>>> import torch\n>>> from diffusers import StableDiffusionPipeline\n>>> from peft import get_model_status, get_layer_status\n\n>>> path = \"runwayml/stable-diffusion-v1-5\"\n>>> lora_id = \"takuma104/lora-test-text-encoder-lora-target\"\n>>> pipe = StableDiffusionPipeline.from_pretrained(path, dtype=torch.float16)\n>>> pipe.load_lora_weights(lora_id, adapter_name=\"adapter-1\")\n>>> pipe.load_lora_weights(lora_id, adapter_name=\"adapter-2\")\n>>> pipe.set_lora_device([\"adapter-2\"], \"cuda\")\n>>> get_layer_status(pipe.text_encoder)\n[TunerLayerStatus(name='text_model.encoder.layers.0.self_attn.k_proj',\\\n                  module_type='lora.Linear',\\\n                  enabled=True,\\\n                  active_adapters=['adapter-2'],\\\n                  merged_adapters=[],\\\n                  requires_grad={'adapter-1': False, 'adapter-2': True},\\\n                  available_adapters=['adapter-1', 'adapter-2'],\\\n                  devices={'adapter-1': ['cpu'], 'adapter-2': ['cuda']}),\\\n TunerLayerStatus(name='text_model.encoder.layers.0.self_attn.v_proj',\\\n                  module_type='lora.Linear',\\\n                  enabled=True,\\\n                  active_adapters=['adapter-2'],\\\n                  merged_adapters=[],\\\n                  requires_grad={'adapter-1': False, 'adapter-2': True},\\\n                  devices={'adapter-1': ['cpu'], 'adapter-2': ['cuda']}),\\\n...]\n\n>>> get_model_status(pipe.unet)\nTunerModelStatus(\n    base_model_type='other',\n    adapter_model_type='None',\n    peft_types={},\n    trainable_params=797184,\n    total_params=861115332,\n    num_adapter_layers=128,\n    enabled=True,\n    active_adapters=['adapter-2'],\n    merged_adapters=[],\n    requires_grad={'adapter-1': False, 'adapter-2': True},\n    available_adapters=['adapter-1', 'adapter-2'],\n    devices={'adapter-1': ['cpu'], 'adapter-2': ['cuda']},\n)\n```\n\n## Speed\n\n### Loading adapter weights is slow\n\nLoading adapters like LoRA weights should generally be fast compared to loading the base model. However", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Troubleshooting", "description": "Hugging Face Official Documentation of Troubleshooting", "url": "https://huggingface.co/docs/peft/main/en/developer_guides/troubleshooting", "source": "hf", "id": "d843a6d5-f269-4f55-9930-a2c04c94e77e"}, "page_content": ">>> get_model_status(pipe.unet)\nTunerModelStatus(\n    base_model_type='other',\n    adapter_model_type='None',\n    peft_types={},\n    trainable_params=797184,\n    total_params=861115332,\n    num_adapter_layers=128,\n    enabled=True,\n    active_adapters=['adapter-2'],\n    merged_adapters=[],\n    requires_grad={'adapter-1': False, 'adapter-2': True},\n    available_adapters=['adapter-1', 'adapter-2'],\n    devices={'adapter-1': ['cpu'], 'adapter-2': ['cuda']},\n)\n```\n\n## Speed\n\n### Loading adapter weights is slow\n\nLoading adapters like LoRA weights should generally be fast compared to loading the base model. However, there can be use cases where the adapter weights are quite large or where users need to load a large number of adapters \u2014 the loading time can add up in this case. The reason for this is that the adapter weights are first initialized and then overridden by the loaded weights, which is wasteful. To speed up the loading time, you can pass the `low_cpu_mem_usage=True` argument to [from\\_pretrained()](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.PeftModel.from_pretrained) and [load\\_adapter()](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.PeftModel.load_adapter).\n\n> If this option works well across different use cases, it may become the default for adapter loading in the future.\n\n## Reproducibility\n\n### Models using batch norm\n\nWhen loading a trained PEFT model where the base model uses batch norm (e.g. `torch.nn.BatchNorm1d` or `torch.nn.BatchNorm2d`), you may find that you cannot reproduce the exact same outputs. This is because the batch norm layers keep track of running stats during training, but these stats are not part of the PEFT checkpoint. Therefore, when you load the PEFT model, the running stats of the base model will be used (i.e. from before training with PEFT).\n\nDepending on your use case, this may not be a big deal. If, however, you need your outputs to be 100% reproducible, you can achieve this by adding the batch norm layers to `modules_to_save`. Below is an example of this using resnet and LoRA. Notice that we set `modules_to_save=[\"classifier\", \"normalization\"]`. We need the `\"classifier\"` argument because our task is image classification, and we add the `\"normalization\"` argument to ensure that the batch norm layers are saved in the PEFT checkpoint.\n\nCopied\n\n```\nfrom transformers import AutoModelForImageClassification\nfrom peft import LoraConfig, get_peft_model\n\nmodel_id = \"microsoft/resnet-18\"\nbase_model = AutoModelForImageClassification.from_pretrained(self.model_id)\nconfig = LoraConfig(\n    target_modules=[\"convolution\"],\n    modules_to_save=[\"classifier\", \"normalization\"],\n),\n```\n\nDepending on the type of model you use, the batch norm layers could have different names than `\"normalization\"`, so please ensure that the name matches your model architecture.\n\n## Version mismatch\n\n### Error while loading the config because of an unexpected keyword argument\n\nWhen you encounter an error like the one shown below, it means the adapter you\u2019re trying to load was trained with a more recent version of PEFT than the version you have installed on your system.\n\nCopied\n\n```\nTypeError: LoraConfig.__init__() got an unexpected keyword argument <argument-name>\n```\n\nThe best way to resolve this issue is to install the latest PEFT version:\n\nCopied\n\n```\npython -m pip install -U PEFT\n```\n\nIf the adapter was trained from a source install of PEFT (an unreleased version of PEFT), then you also need to install PEFT from source.\n\nCopied\n\n```\npython -m pip install -U git+https://github.com/huggingface/peft.git\n```\n\nIf it is not possible for you to upgrade PEFT, there is a workaround you can try.\n\nAssume the error message says that the unknown keyword argument is named `foobar`. Search inside the `adapter_config.json` of this PEFT adapter for the `foobar` entry and delete it from the file. Then save the file and try loading the model again.\n\nThis solution works most of the time. As long as it is the default value for `foobar`, it can be ignored. However, when it is set to some other value, you will get incorrect results. Upgrading PEFT is the recommended solution.\n\n## Adapter handling\n\n### Using multiple adapters at the same time\n\nPEFT allows you to create more than one adapter on the same model. This can be useful in many situations. For example, for inference, you may want to serve two fine-tuned models from the same base model instead of loading the base", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Troubleshooting", "description": "Hugging Face Official Documentation of Troubleshooting", "url": "https://huggingface.co/docs/peft/main/en/developer_guides/troubleshooting", "source": "hf", "id": "2665de95-4eba-45ca-9136-0865fd856d1e"}, "page_content": " is not possible for you to upgrade PEFT, there is a workaround you can try.\n\nAssume the error message says that the unknown keyword argument is named `foobar`. Search inside the `adapter_config.json` of this PEFT adapter for the `foobar` entry and delete it from the file. Then save the file and try loading the model again.\n\nThis solution works most of the time. As long as it is the default value for `foobar`, it can be ignored. However, when it is set to some other value, you will get incorrect results. Upgrading PEFT is the recommended solution.\n\n## Adapter handling\n\n### Using multiple adapters at the same time\n\nPEFT allows you to create more than one adapter on the same model. This can be useful in many situations. For example, for inference, you may want to serve two fine-tuned models from the same base model instead of loading the base model once for each fine-tuned model, which would cost more memory. However, multiple adapters can be activated at the same time. This way, the model may leverage the learnings from all those adapters at the same time. As an example, if you have a diffusion model, you may want to use one LoRA adapter to change the style and a different one to change the subject.\n\nActivating multiple adapters at the same time is generally possible on all PEFT methods (LoRA, LoHa, IA\u00b3, etc.) except for prompt learning methods (p-tuning, prefix tuning, etc.). The following example illustrates how to achieve this:\n\nCopied\n\n```\nfrom transformers import AutoModelForCausalLM\nfrom peft import PeftModel\n\nmodel_id =...\nbase_model = AutoModelForCausalLM.from_pretrained(model_id)\nmodel = PeftModel.from_pretrained(base_model, lora_path_0)  # default adapter_name is 'default'\nmodel.load_adapter(lora_path_1, adapter_name=\"other\")\n# the 'other' adapter was loaded but it's not active yet, so to activate both adapters:\nmodel.base_model.set_adapter([\"default\", \"other\"])\n```\n\n> In the example above, you can see that we need to call `model.base_model.set_adapter([\"default\", \"other\"])`. Why can we not call `model.set_adapter([\"default\", \"other\"])`? This is unfortunately not possible because, as explained earlier, some PEFT methods don\u2019t support activating more than one adapter at a time.\n\nIt is also possible to train two adapters at the same time, but you should be careful to ensure that the weights of both adapters are known to the optimizer. Otherwise, only one adapter will receive updates.\n\nCopied\n\n```\nfrom transformers import AutoModelForCausalLM\nfrom peft import LoraConfig, get_peft_model\n\nmodel_id =...\nbase_model = AutoModelForCausalLM.from_pretrained(model_id)\nlora_config_0 = LoraConfig(...)\nlora_config_1 = LoraConfig(...)\nmodel = get_peft_model(base_model, lora_config_0)\nmodel.add_adapter(adapter_name=\"other\", peft_config=lora_config_1)\n```\n\nIf we would now call:\n\nCopied\n\n```\nfrom transformers import Trainer\n\ntrainer = Trainer(model=model, ...)\ntrainer.train()\n```\n\nor\n\nCopied\n\n```\noptimizer = torch.optim.AdamW([param for param in model.parameters() if param.requires_grad],...)\n```\n\nthen the second LoRA adapter (`\"other\"`) would not be trained. This is because it is inactive at this moment, which means the `requires_grad` attribute on its parameters is set to `False` and the optimizer will ignore it. Therefore, make sure to activate all adapters that should be trained _before_ initializing the optimizer:\n\nCopied\n\n```\n# activate all adapters\nmodel.base_model.set_adapter([\"default\", \"other\"])\ntrainer = Trainer(model=model, ...)\ntrainer.train()\n```\n\n> This section deals with using multiple adapters _of the same type_ on the same model, for example, using multiple LoRA adapters at the same time. It does not apply to using _different types_ of adapters on the same model, for example one LoRA adapter and one LoHa adapter. For this, please check [`PeftMixedModel`](https://huggingface.co/docs/peft/developer_guides/mixed_models).\n\n[Update on GitHub](https://github.com/huggingface/peft/blob/main/docs/source/developer_guides/troubleshooting.md)\n\n[\u2190Contribute to PEFT](https://huggingface.co/docs/peft/main/en/developer_guides/contributing) [PEFT checkpoint format\u2192](https://huggingface.co/docs/peft/main/en/developer_guides/checkpoint)[SEP]", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Troubleshooting", "description": "Hugging Face Official Documentation of Troubleshooting", "url": "https://huggingface.co/docs/peft/main/en/developer_guides/troubleshooting", "source": "hf", "id": "e0a0ae98-17d1-4546-95f9-9f80153157fe"}, "page_content": " to using _different types_ of adapters on the same model, for example one LoRA adapter and one LoHa adapter. For this, please check [`PeftMixedModel`](https://huggingface.co/docs/peft/developer_guides/mixed_models).\n\n[Update on GitHub](https://github.com/huggingface/peft/blob/main/docs/source/developer_guides/troubleshooting.md)\n\n[\u2190Contribute to PEFT](https://huggingface.co/docs/peft/main/en/developer_guides/contributing) [PEFT checkpoint format\u2192](https://huggingface.co/docs/peft/main/en/developer_guides/checkpoint)[SEP]", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "PEFT checkpoint format", "description": "Hugging Face Official Documentation of PEFT checkpoint format", "url": "https://huggingface.co/docs/peft/main/en/developer_guides/checkpoint", "source": "hf", "id": "989d6a3e-180b-4644-8956-92e62c6cd6b4"}, "page_content": "[CLS]# PEFT checkpoint format\n\nThis document describes how PEFT\u2019s checkpoint files are structured and how to convert between the PEFT format and other formats.\n\n## PEFT files\n\nPEFT (parameter-efficient fine-tuning) methods only update a small subset of a model\u2019s parameters rather than all of them. This is nice because checkpoint files can generally be much smaller than the original model files and are easier to store and share. However, this also means that to load a PEFT model, you need to have the original model available as well.\n\nWhen you call [save\\_pretrained()](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.PeftModel.save_pretrained) on a PEFT model, the PEFT model saves three files, described below:\n\n1. `adapter_model.safetensors` or `adapter_model.bin`\n\nBy default, the model is saved in the `safetensors` format, a secure alternative to the `bin` format, which is known to be susceptible to [security vulnerabilities](https://huggingface.co/docs/hub/security-pickle) because it uses the pickle utility under the hood. Both formats store the same `state_dict` though, and are interchangeable.\n\nThe `state_dict` only contains the parameters of the adapter module, not the base model. To illustrate the difference in size, a normal BERT model requires ~420MB of disk space, whereas an IA\u00b3 adapter on top of this BERT model only requires ~260KB.\n\n2. `adapter_config.json`\n\nThe `adapter_config.json` file contains the configuration of the adapter module, which is necessary to load the model. Below is an example of an `adapter_config.json` for an IA\u00b3 adapter with standard settings applied to a BERT model:\n\nCopied\n\n```\n{\n  \"auto_mapping\": {\n    \"base_model_class\": \"BertModel\",\n    \"parent_library\": \"transformers.models.bert.modeling_bert\"\n  },\n  \"base_model_name_or_path\": \"bert-base-uncased\",\n  \"fan_in_fan_out\": false,\n  \"feedforward_modules\": [\\\n    \"output.dense\"\\\n  ],\n  \"inference_mode\": true,\n  \"init_ia3_weights\": true,\n  \"modules_to_save\": null,\n  \"peft_type\": \"IA3\",\n  \"revision\": null,\n  \"target_modules\": [\\\n    \"key\",\\\n    \"value\",\\\n    \"output.dense\"\\\n  ],\n  \"task_type\": null\n}\n```\n\nThe configuration file contains:\n\n- the adapter module type stored, `\"peft_type\": \"IA3\"`\n- information about the base model like `\"base_model_name_or_path\": \"bert-base-uncased\"`\n- the revision of the model (if any), `\"revision\": null`\n\nIf the base model is not a pretrained Transformers model, the latter two entries will be `null`. Other than that, the settings are all related to the specific IA\u00b3 adapter that was used to fine-tune the model.\n\n3. `README.md`\n\nThe generated `README.md` is the model card of a PEFT model and contains a few pre-filled entries. The intent of this is to make it easier to share the model with others and to provide some basic information about the model. This file is not needed to load the model.\n\n## Convert to PEFT format\n\nWhen converting from another format to the PEFT format, we require both the `adapter_model.safetensors` (or `adapter_model.bin`) file and the `adapter_config.json` file.\n\n### adapter\\_model\n\nFor the model weights, it is important to use the correct mapping from parameter name to value for PEFT to load the file. Getting this mapping right is an exercise in checking the implementation details, as there is no generally agreed upon format for PEFT adapters.\n\nFortunately, figuring out this mapping is not overly complicated for common base cases. Let\u2019s look at a concrete example, the [`LoraLayer`](https://github.com/huggingface/peft/blob/main/src/peft/tuners/lora/layer.py):\n\nCopied\n\n```\n# showing only part of the code\n\nclass LoraLayer(BaseTunerLayer):\n    # All names of layers that may contain (trainable) adapter weights\n    adapter_layer_names = (\"lora_A\", \"lora_B\", \"lora_embedding_A\", \"lora_embedding_B\")\n    # All names of other parameters that may contain adapter-related parameters\n    other_param_names = (\"r\", \"lora_alpha\", \"scaling\", \"lora_dropout\")\n\n    def __init__(self, base_layer: nn.Module, **kwargs) -> None:\n        self.base_layer = base_layer\n        self.r = {}\n        self.lora_alpha = {}\n        self.scaling = {}\n        self.lora_dropout = nn.ModuleDict({})\n        ", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "PEFT checkpoint format", "description": "Hugging Face Official Documentation of PEFT checkpoint format", "url": "https://huggingface.co/docs/peft/main/en/developer_guides/checkpoint", "source": "hf", "id": "bfa41165-d416-47a2-8081-5f75946f805f"}, "page_content": "):\n\nCopied\n\n```\n# showing only part of the code\n\nclass LoraLayer(BaseTunerLayer):\n    # All names of layers that may contain (trainable) adapter weights\n    adapter_layer_names = (\"lora_A\", \"lora_B\", \"lora_embedding_A\", \"lora_embedding_B\")\n    # All names of other parameters that may contain adapter-related parameters\n    other_param_names = (\"r\", \"lora_alpha\", \"scaling\", \"lora_dropout\")\n\n    def __init__(self, base_layer: nn.Module, **kwargs) -> None:\n        self.base_layer = base_layer\n        self.r = {}\n        self.lora_alpha = {}\n        self.scaling = {}\n        self.lora_dropout = nn.ModuleDict({})\n        self.lora_A = nn.ModuleDict({})\n        self.lora_B = nn.ModuleDict({})\n        # For Embedding layer\n        self.lora_embedding_A = nn.ParameterDict({})\n        self.lora_embedding_B = nn.ParameterDict({})\n        # Mark the weight as unmerged\n        self._disable_adapters = False\n        self.merged_adapters = []\n        self.use_dora: dict[str, bool] = {}\n        self.lora_magnitude_vector: Optional[torch.nn.ParameterDict] = None  # for DoRA\n        self._caches: dict[str, Any] = {}\n        self.kwargs = kwargs\n```\n\nIn the `__init__` code used by all `LoraLayer` classes in PEFT, there are a bunch of parameters used to initialize the model, but only a few are relevant for the checkpoint file: `lora_A`, `lora_B`, `lora_embedding_A`, and `lora_embedding_B`. These parameters are listed in the class attribute `adapter_layer_names` and contain the learnable parameters, so they must be included in the checkpoint file. All the other parameters, like the rank `r`, are derived from the `adapter_config.json` and must be included there (unless the default value is used).\n\nLet\u2019s check the `state_dict` of a PEFT LoRA model applied to BERT. When printing the first five keys using the default LoRA settings (the remaining keys are the same, just with different layer numbers), we get:\n\n- `base_model.model.encoder.layer.0.attention.self.query.lora_A.weight`\n- `base_model.model.encoder.layer.0.attention.self.query.lora_B.weight`\n- `base_model.model.encoder.layer.0.attention.self.value.lora_A.weight`\n- `base_model.model.encoder.layer.0.attention.self.value.lora_B.weight`\n- `base_model.model.encoder.layer.1.attention.self.query.lora_A.weight`\n- etc.\n\nLet\u2019s break this down:\n\n- By default, for BERT models, LoRA is applied to the `query` and `value` layers of the attention module. This is why you see `attention.self.query` and `attention.self.value` in the key names for each layer.\n- LoRA decomposes the weights into two low-rank matrices, `lora_A` and `lora_B`. This is where `lora_A` and `lora_B` come from in the key names.\n- These LoRA matrices are implemented as `nn.Linear` layers, so the parameters are stored in the `.weight` attribute (`lora_A.weight`, `lora_B.weight`).\n- By default, LoRA isn\u2019t applied to BERT\u2019s embedding layer, so there are _no entries_ for `lora_A_embedding` and `lora_B_embedding`.\n- The keys of the `state_dict` always start with `\"base_model.model.\"`. The reason is that, in PEFT, we wrap the base model inside a tuner-specific model (`LoraModel` in this case), which itself is wrapped in a general PEFT model (`PeftModel`). For this reason, these two prefixes are added to the keys. When converting to the PEFT format, it is required to add these prefixes.\n\n> This last point is not true for prefix tuning techniques like prompt tuning. There, the extra embeddings are directly stored in the `state_dict` without any prefixes added to the keys.\n\nWhen inspecting the parameter names in the loaded model, you might be surprised to find that they look a bit different, e.g. `base_model.model.encoder.layer.0.attention.self.query.lora_A.default.weight`. The difference is the _`.default`_ part in the second to last segment. This part exists because PEFT generally allows the addition of multiple adapters at once (using an `nn.ModuleDict` or `nn.ParameterDict` to store them). For example, if you add another adapter called \u201cother\u201d, the key for that adapter would be `base_model.model", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "PEFT checkpoint format", "description": "Hugging Face Official Documentation of PEFT checkpoint format", "url": "https://huggingface.co/docs/peft/main/en/developer_guides/checkpoint", "source": "hf", "id": "6e4c63ee-118d-4ba5-a97a-1f14ddfa78f1"}, "page_content": " are added to the keys. When converting to the PEFT format, it is required to add these prefixes.\n\n> This last point is not true for prefix tuning techniques like prompt tuning. There, the extra embeddings are directly stored in the `state_dict` without any prefixes added to the keys.\n\nWhen inspecting the parameter names in the loaded model, you might be surprised to find that they look a bit different, e.g. `base_model.model.encoder.layer.0.attention.self.query.lora_A.default.weight`. The difference is the _`.default`_ part in the second to last segment. This part exists because PEFT generally allows the addition of multiple adapters at once (using an `nn.ModuleDict` or `nn.ParameterDict` to store them). For example, if you add another adapter called \u201cother\u201d, the key for that adapter would be `base_model.model.encoder.layer.0.attention.self.query.lora_A.other.weight`.\n\nWhen you call [save\\_pretrained()](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.PeftModel.save_pretrained), the adapter name is stripped from the keys. The reason is that the adapter name is not an important part of the model architecture; it is just an arbitrary name. When loading the adapter, you could choose a totally different name, and the model would still work the same way. This is why the adapter name is not stored in the checkpoint file.\n\n> If you call `save_pretrained(\"some/path\")` and the adapter name is not `\"default\"`, the adapter is stored in a sub-directory with the same name as the adapter. So if the name is \u201cother\u201d, it would be stored inside of `some/path/other`.\n\nIn some circumstances, deciding which values to add to the checkpoint file can become a bit more complicated. For example, in PEFT, DoRA is implemented as a special case of LoRA. If you want to convert a DoRA model to PEFT, you should create a LoRA checkpoint with extra entries for DoRA. You can see this in the `__init__` of the previous `LoraLayer` code:\n\nCopied\n\n```\nself.lora_magnitude_vector: Optional[torch.nn.ParameterDict] = None  # for DoRA\n```\n\nThis indicates that there is an optional extra parameter per layer for DoRA.\n\n### adapter\\_config\n\nAll the other information needed to load a PEFT model is contained in the `adapter_config.json` file. Let\u2019s check this file for a LoRA model applied to BERT:\n\nCopied\n\n```\n{\n  \"alpha_pattern\": {},\n  \"auto_mapping\": {\n    \"base_model_class\": \"BertModel\",\n    \"parent_library\": \"transformers.models.bert.modeling_bert\"\n  },\n  \"base_model_name_or_path\": \"bert-base-uncased\",\n  \"bias\": \"none\",\n  \"fan_in_fan_out\": false,\n  \"inference_mode\": true,\n  \"init_lora_weights\": true,\n  \"layer_replication\": null,\n  \"layers_pattern\": null,\n  \"layers_to_transform\": null,\n  \"loftq_config\": {},\n  \"lora_alpha\": 8,\n  \"lora_dropout\": 0.0,\n  \"megatron_config\": null,\n  \"megatron_core\": \"megatron.core\",\n  \"modules_to_save\": null,\n  \"peft_type\": \"LORA\",\n  \"r\": 8,\n  \"rank_pattern\": {},\n  \"revision\": null,\n  \"target_modules\": [\\\n    \"query\",\\\n    \"value\"\\\n  ],\n  \"task_type\": null,\n  \"use_dora\": false,\n  \"use_rslora\": false\n}\n```\n\nThis contains a lot of entries, and at first glance, it could feel overwhelming to figure out all the right values to put in there. However, most of the entries are not necessary to load the model. This is either because they use the default values and don\u2019t need to be added or because they only affect the initialization of the LoRA weights, which is irrelevant when it comes to loading the model. If you find that you don\u2019t know what a specific parameter does, e.g., `\"use_rslora\",` don\u2019t add it, and you should be fine. Also note that as more options are added, this file will get more entries in the future, but it should be backward compatible.\n\nAt the minimum, you should include the following entries:\n\nCopied\n\n```\n{\n  \"target_modules\": [\"query\", \"value\"],\n  \"peft_type\": \"LORA\"\n}\n```\n\nHowever, adding as many entries as possible, like the rank `r` or the `base_model_name_or_path` (if it\u2019s a Transformers model) is recommended. This information can help others understand the model better and share it more easily. To check which keys and values are expected, check out the [config.py](https://github.com", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "PEFT checkpoint format", "description": "Hugging Face Official Documentation of PEFT checkpoint format", "url": "https://huggingface.co/docs/peft/main/en/developer_guides/checkpoint", "source": "hf", "id": "9e168fbc-166d-4c1b-911b-09dd20c9fe89"}, "page_content": " model. If you find that you don\u2019t know what a specific parameter does, e.g., `\"use_rslora\",` don\u2019t add it, and you should be fine. Also note that as more options are added, this file will get more entries in the future, but it should be backward compatible.\n\nAt the minimum, you should include the following entries:\n\nCopied\n\n```\n{\n  \"target_modules\": [\"query\", \"value\"],\n  \"peft_type\": \"LORA\"\n}\n```\n\nHowever, adding as many entries as possible, like the rank `r` or the `base_model_name_or_path` (if it\u2019s a Transformers model) is recommended. This information can help others understand the model better and share it more easily. To check which keys and values are expected, check out the [config.py](https://github.com/huggingface/peft/blob/main/src/peft/tuners/lora/config.py) file (as an example, this is the config file for LoRA) in the PEFT source code.\n\n## Model storage\n\nIn some circumstances, you might want to store the whole PEFT model, including the base weights. This can be necessary if, for instance, the base model is not available to the users trying to load the PEFT model. You can merge the weights first or convert it into a Transformer model.\n\n### Merge the weights\n\nThe most straightforward way to store the whole PEFT model is to merge the adapter weights into the base weights:\n\nCopied\n\n```\nmerged_model = model.merge_and_unload()\nmerged_model.save_pretrained(...)\n```\n\nThere are some disadvantages to this approach, though:\n\n- Once [merge\\_and\\_unload()](https://huggingface.co/docs/peft/main/en/package_reference/tuners#peft.tuners.tuners_utils.BaseTuner.merge_and_unload) is called, you get a basic model without any PEFT-specific functionality. This means you can\u2019t use any of the PEFT-specific methods anymore.\n- You cannot unmerge the weights, load multiple adapters at once, disable the adapter, etc.\n- Not all PEFT methods support merging weights.\n- Some PEFT methods may generally allow merging, but not with specific settings (e.g. when using certain quantization techniques).\n- The whole model will be much larger than the PEFT model, as it will contain all the base weights as well.\n\nBut inference with a merged model should be a bit faster.\n\n### Convert to a Transformers model\n\nAnother way to save the whole model, assuming the base model is a Transformers model, is to use this hacky approach to directly insert the PEFT weights into the base model and save it, which only works if you \u201ctrick\u201d Transformers into believing the PEFT model is not a PEFT model. This only works with LoRA because other adapters are not implemented in Transformers.\n\nCopied\n\n```\nmodel =...  # the PEFT model\n...\n# after you finish training the model, save it in a temporary location\nmodel.save_pretrained(<temp_location>)\n# now load this model directly into a transformers model, without the PEFT wrapper\n# the PEFT weights are directly injected into the base model\nmodel_loaded = AutoModel.from_pretrained(<temp_location>)\n# now make the loaded model believe that it is _not_ a PEFT model\nmodel_loaded._hf_peft_config_loaded = False\n# now when we save it, it will save the whole model\nmodel_loaded.save_pretrained(<final_location>)\n# or upload to Hugging Face Hub\nmodel_loaded.push_to_hub(<final_location>)\n```\n\n[Update on GitHub](https://github.com/huggingface/peft/blob/main/docs/source/developer_guides/checkpoint.md)\n\n[\u2190Troubleshooting](https://huggingface.co/docs/peft/main/en/developer_guides/troubleshooting) [DeepSpeed\u2192](https://huggingface.co/docs/peft/main/en/accelerate/deepspeed)[SEP]", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "DeepSpeed", "description": "Hugging Face Official Documentation of DeepSpeed", "url": "https://huggingface.co/docs/peft/main/en/accelerate/deepspeed", "source": "hf", "id": "fba8714d-a70c-4e3d-a632-5bcfcd5e2395"}, "page_content": "[CLS]# DeepSpeed\n\n[DeepSpeed](https://www.deepspeed.ai/) is a library designed for speed and scale for distributed training of large models with billions of parameters. At its core is the Zero Redundancy Optimizer (ZeRO) that shards optimizer states (ZeRO-1), gradients (ZeRO-2), and parameters (ZeRO-3) across data parallel processes. This drastically reduces memory usage, allowing you to scale your training to billion parameter models. To unlock even more memory efficiency, ZeRO-Offload reduces GPU compute and memory by leveraging CPU resources during optimization.\n\nBoth of these features are supported in \ud83e\udd17 Accelerate, and you can use them with \ud83e\udd17 PEFT.\n\n## Compatibility with bitsandbytes quantization + LoRA\n\nBelow is a table that summarizes the compatibility between PEFT\u2019s LoRA, [`bitsandbytes`](https://github.com/TimDettmers/bitsandbytes) library and DeepSpeed Zero stages with respect to fine-tuning. DeepSpeed Zero-1 and 2 will have no effect at inference as stage 1 shards the optimizer states and stage 2 shards the optimizer states and gradients:\n\n| DeepSpeed stage | Is compatible? |\n| --- | --- |\n| Zero-1 | \ud83d\udfe2 |\n| Zero-2 | \ud83d\udfe2 |\n| Zero-3 | \ud83d\udfe2 |\n\nFor DeepSpeed Stage 3 + QLoRA, please refer to the section [Use PEFT QLoRA and DeepSpeed with ZeRO3 for finetuning large models on multiple GPUs](https://huggingface.co/docs/peft/main/en/accelerate/deepspeed#use-peft-qlora-and-deepspeed-with-zero3-for-finetuning-large-models-on-multiple-gpus) below.\n\nFor confirming these observations, we ran the SFT (Supervised Fine-tuning) [offical example scripts](https://github.com/huggingface/trl/tree/main/examples) of the [Transformers Reinforcement Learning (TRL) library](https://github.com/huggingface/trl) using QLoRA + PEFT and the accelerate configs available [here](https://github.com/huggingface/trl/tree/main/examples/accelerate_configs). We ran these experiments on a 2x NVIDIA T4 GPU.\n\n# Use PEFT and DeepSpeed with ZeRO3 for finetuning large models on multiple devices and multiple nodes\n\nThis section of guide will help you learn how to use our DeepSpeed [training script](https://github.com/huggingface/peft/blob/main/examples/sft/train.py) for performing SFT. You\u2019ll configure the script to do SFT (supervised fine-tuning) of Llama-70B model with LoRA and ZeRO-3 on 8xH100 80GB GPUs on a single machine. You can configure it to scale to multiple machines by changing the accelerate config.\n\n## Configuration\n\nStart by running the following command to [create a DeepSpeed configuration file](https://huggingface.co/docs/accelerate/quicktour#launching-your-distributed-script) with \ud83e\udd17 Accelerate. The `--config_file` flag allows you to save the configuration file to a specific location, otherwise it is saved as a `default_config.yaml` file in the \ud83e\udd17 Accelerate cache.\n\nThe configuration file is used to set the default options when you launch the training script.\n\nCopied\n\n```\naccelerate config --config_file deepspeed_config.yaml\n```\n\nYou\u2019ll be asked a few questions about your setup, and configure the following arguments. In this example, you\u2019ll use ZeRO-3 so make sure you pick those options.\n\nCopied\n\n```\n`zero_stage`: [0] Disabled, [1] optimizer state partitioning, [2] optimizer+gradient state partitioning and [3] optimizer+gradient+parameter partitioning\n`gradient_accumulation_steps`: Number of training steps to accumulate gradients before averaging and applying them. Pass the same value as you would pass via cmd argument else you will encounter mismatch error.\n`gradient_clipping`: Enable gradient clipping with value. Don't set this as you will be passing it via cmd arguments.\n`offload_optimizer_device`: [none] Disable optimizer offloading, [cpu] offload optimizer to CPU, [nvme] offload optimizer to NVMe SSD. Only applicable with ZeRO >= Stage-2. Set this as `none` as don't want to enable offloading.\n`offload_param_device`: [none] Disable parameter offloading, [cpu] offload parameters to CPU, [nvme] offload parameters to NVMe SSD. Only applicable with ZeRO Stage-3. Set this as `none` as don't want to enable offloading.\n`zero3_init_flag`: Decides whether to enable `deepspeed.zero.Init` for constructing massive models. Only applicable with ZeRO Stage-3. Set this to `True`.\n`zero3_save_16bit_model`: Decides whether to save 16-bit model weights when using ZeRO Stage-3. Set this to `True`.\n`mixed_precision`: `no", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "DeepSpeed", "description": "Hugging Face Official Documentation of DeepSpeed", "url": "https://huggingface.co/docs/peft/main/en/accelerate/deepspeed", "source": "hf", "id": "1251e935-6fd8-4ad2-9da1-8ae65753b871"}, "page_content": ", [cpu] offload optimizer to CPU, [nvme] offload optimizer to NVMe SSD. Only applicable with ZeRO >= Stage-2. Set this as `none` as don't want to enable offloading.\n`offload_param_device`: [none] Disable parameter offloading, [cpu] offload parameters to CPU, [nvme] offload parameters to NVMe SSD. Only applicable with ZeRO Stage-3. Set this as `none` as don't want to enable offloading.\n`zero3_init_flag`: Decides whether to enable `deepspeed.zero.Init` for constructing massive models. Only applicable with ZeRO Stage-3. Set this to `True`.\n`zero3_save_16bit_model`: Decides whether to save 16-bit model weights when using ZeRO Stage-3. Set this to `True`.\n`mixed_precision`: `no` for FP32 training, `fp16` for FP16 mixed-precision training and `bf16` for BF16 mixed-precision training. Set this to `True`.\n```\n\nOnce this is done, the corresponding config should look like below and you can find it in config folder at [deepspeed\\_config.yaml](https://github.com/huggingface/peft/blob/main/examples/sft/configs/deepspeed_config.yaml):\n\nCopied\n\n```\ncompute_environment: LOCAL_MACHINE\ndebug: false\ndeepspeed_config:\n  deepspeed_multinode_launcher: standard\n  gradient_accumulation_steps: 4\n  offload_optimizer_device: none\n  offload_param_device: none\n  zero3_init_flag: true\n  zero3_save_16bit_model: true\n  zero_stage: 3\ndistributed_type: DEEPSPEED\ndowncast_bf16: 'no'\nmachine_rank: 0\nmain_training_function: main\nmixed_precision: bf16\nnum_machines: 1\nnum_processes: 8\nrdzv_backend: static\nsame_network: true\ntpu_env: []\ntpu_use_cluster: false\ntpu_use_sudo: false\nuse_cpu: false\n```\n\n## Launch command\n\nThe launch command is available at [run\\_peft\\_deepspeed.sh](https://github.com/huggingface/peft/blob/main/examples/sft/run_peft_deepspeed.sh) and it is also shown below:\n\nCopied\n\n```\naccelerate launch --config_file \"configs/deepspeed_config.yaml\"  train.py \\\n--seed 100 \\\n--model_name_or_path \"meta-llama/Llama-2-70b-hf\" \\\n--dataset_name \"smangrul/ultrachat-10k-chatml\" \\\n--chat_template_format \"chatml\" \\\n--add_special_tokens False \\\n--append_concat_token False \\\n--splits \"train,test\" \\\n--max_seq_len 2048 \\\n--num_train_epochs 1 \\\n--logging_steps 5 \\\n--log_level \"info\" \\\n--logging_strategy \"steps\" \\\n--eval_strategy \"epoch\" \\\n--save_strategy \"epoch\" \\\n--push_to_hub \\\n--hub_private_repo True \\\n--hub_strategy \"every_save\" \\\n--bf16 True \\\n--packing True \\\n--learning_rate 1e-4 \\\n--lr_scheduler_type \"cosine\" \\\n--weight_decay 1e-4 \\\n--warmup_ratio 0.0 \\\n--max_grad_norm 1.0 \\\n--output_dir \"llama-sft-lora-deepspeed\" \\\n--per_device_train_batch_size 8 \\\n--per_device_eval_batch_size 8 \\\n--gradient_accumulation_steps 4 \\\n--gradient_checkpointing True \\\n--use_reentrant False \\\n--dataset_text_field \"content\" \\\n--use_flash_attn True \\\n--use_peft_lora True \\\n--lora_r 8 \\\n--lora_alpha 16 \\\n--lora_dropout 0.1 \\\n--lora_target_modules \"all-linear\" \\\n--use_4bit_quantization False\n```\n\nNotice that we are using LoRA with rank=8, alpha=16 and targeting all linear layers. We are passing the deepspeed config file and finetuning 70B Llama model on a subset of the ultrachat dataset.\n\n## The important parts\n\nLet\u2019s dive a little deeper into the script so you can see what\u2019s going on, and understand how it works.\n\nThe first thing to know is that the script uses DeepSpeed for distributed training as the DeepSpeed config has been passed. The [SFTTrainer](https://huggingface.co/docs/trl/main/en/sft_trainer#trl.SFTTrainer) class handles all the heavy lifting of creating the PEFT model using the peft", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "DeepSpeed", "description": "Hugging Face Official Documentation of DeepSpeed", "url": "https://huggingface.co/docs/peft/main/en/accelerate/deepspeed", "source": "hf", "id": "7e519e73-a031-406d-8e53-f230e5825b9c"}, "page_content": "lora_dropout 0.1 \\\n--lora_target_modules \"all-linear\" \\\n--use_4bit_quantization False\n```\n\nNotice that we are using LoRA with rank=8, alpha=16 and targeting all linear layers. We are passing the deepspeed config file and finetuning 70B Llama model on a subset of the ultrachat dataset.\n\n## The important parts\n\nLet\u2019s dive a little deeper into the script so you can see what\u2019s going on, and understand how it works.\n\nThe first thing to know is that the script uses DeepSpeed for distributed training as the DeepSpeed config has been passed. The [SFTTrainer](https://huggingface.co/docs/trl/main/en/sft_trainer#trl.SFTTrainer) class handles all the heavy lifting of creating the PEFT model using the peft config that is passed. After that, when you call `trainer.train()`, [SFTTrainer](https://huggingface.co/docs/trl/main/en/sft_trainer#trl.SFTTrainer) internally uses \ud83e\udd17 Accelerate to prepare the model, optimizer and trainer using the DeepSpeed config to create DeepSpeed engine which is then trained. The main code snippet is below:\n\nCopied\n\n```\n# trainer\ntrainer = SFTTrainer(\n    model=model,\n    processing_class=tokenizer,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    peft_config=peft_config,\n)\ntrainer.accelerator.print(f\"{trainer.model}\")\n\n# train\ncheckpoint = None\nif training_args.resume_from_checkpoint is not None:\n    checkpoint = training_args.resume_from_checkpoint\ntrainer.train(resume_from_checkpoint=checkpoint)\n\n# saving final model\ntrainer.save_model()\n```\n\n## Memory usage\n\nIn the above example, the memory consumed per GPU is 64 GB (80%) as seen in the screenshot below:\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/peft_deepspeed_mem_usage.png)\n\nGPU memory usage for the training run\n\n## More resources\n\nYou can also refer this blog post [Falcon 180B Finetuning using \ud83e\udd17 PEFT and DeepSpeed](https://medium.com/@sourabmangrulkar/falcon-180b-finetuning-using-peft-and-deepspeed-b92643091d99) on how to finetune 180B Falcon model on 16 A100 GPUs on 2 machines.\n\n# Use PEFT QLoRA and DeepSpeed with ZeRO3 for finetuning large models on multiple GPUs\n\nIn this section, we will look at how to use QLoRA and DeepSpeed Stage-3 for finetuning 70B llama model on 2X40GB GPUs.\nFor this, we first need `bitsandbytes>=0.43.3`, `accelerate>=1.0.1`, `transformers>4.44.2`, `trl>0.11.4` and `peft>0.13.0`. We need to set `zero3_init_flag` to true when using Accelerate config. Below is the config which can be found at [deepspeed\\_config\\_z3\\_qlora.yaml](https://github.com/huggingface/peft/blob/main/examples/sft/configs/deepspeed_config_z3_qlora.yaml):\n\nCopied\n\n```\ncompute_environment: LOCAL_MACHINE\ndebug: false\ndeepspeed_config:\n  deepspeed_multinode_launcher: standard\n  offload_optimizer_device: none\n  offload_param_device: none\n  zero3_init_flag: true\n  zero3_save_16bit_model: true\n  zero_stage: 3\ndistributed_type: DEEPSPEED\ndowncast_bf16: 'no'\nmachine_rank: 0\nmain_training_function: main\nmixed_precision: bf16\nnum_machines: 1\nnum_processes: 2\nrdzv_backend: static\nsame_network: true\ntpu_env: []\ntpu_use_cluster: false\ntpu_use_sudo: false\nuse_cpu: false\n```\n\nLaunch command is given below which is available at [run\\_peft\\_qlora\\_deepspeed\\_stage3.sh](https://github.com/huggingface/peft/blob/main/examples/sft/run_peft_qlora_deepspeed_stage3.sh):\n\nCopied\n\n```\naccelerate launch --config_file \"configs/deepspeed_config_z3_qlora.yaml\"  train.py \\\n--seed 100 \\\n--model_name_or_path \"meta-llama/Llama-2-70b-hf\"", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "DeepSpeed", "description": "Hugging Face Official Documentation of DeepSpeed", "url": "https://huggingface.co/docs/peft/main/en/accelerate/deepspeed", "source": "hf", "id": "3e4d0045-b98a-470d-9dc0-029a5af8c91a"}, "page_content": ": 1\nnum_processes: 2\nrdzv_backend: static\nsame_network: true\ntpu_env: []\ntpu_use_cluster: false\ntpu_use_sudo: false\nuse_cpu: false\n```\n\nLaunch command is given below which is available at [run\\_peft\\_qlora\\_deepspeed\\_stage3.sh](https://github.com/huggingface/peft/blob/main/examples/sft/run_peft_qlora_deepspeed_stage3.sh):\n\nCopied\n\n```\naccelerate launch --config_file \"configs/deepspeed_config_z3_qlora.yaml\"  train.py \\\n--seed 100 \\\n--model_name_or_path \"meta-llama/Llama-2-70b-hf\" \\\n--dataset_name \"smangrul/ultrachat-10k-chatml\" \\\n--chat_template_format \"chatml\" \\\n--add_special_tokens False \\\n--append_concat_token False \\\n--splits \"train,test\" \\\n--max_seq_len 2048 \\\n--num_train_epochs 1 \\\n--logging_steps 5 \\\n--log_level \"info\" \\\n--logging_strategy \"steps\" \\\n--eval_strategy \"epoch\" \\\n--save_strategy \"epoch\" \\\n--push_to_hub \\\n--hub_private_repo True \\\n--hub_strategy \"every_save\" \\\n--bf16 True \\\n--packing True \\\n--learning_rate 1e-4 \\\n--lr_scheduler_type \"cosine\" \\\n--weight_decay 1e-4 \\\n--warmup_ratio 0.0 \\\n--max_grad_norm 1.0 \\\n--output_dir \"llama-sft-qlora-dsz3\" \\\n--per_device_train_batch_size 2 \\\n--per_device_eval_batch_size 2 \\\n--gradient_accumulation_steps 2 \\\n--gradient_checkpointing True \\\n--use_reentrant True \\\n--dataset_text_field \"content\" \\\n--use_flash_attn True \\\n--use_peft_lora True \\\n--lora_r 8 \\\n--lora_alpha 16 \\\n--lora_dropout 0.1 \\\n--lora_target_modules \"all-linear\" \\\n--use_4bit_quantization True \\\n--use_nested_quant True \\\n--bnb_4bit_compute_dtype \"bfloat16\" \\\n--bnb_4bit_quant_storage_dtype \"bfloat16\"\n```\n\nNotice the new argument being passed `bnb_4bit_quant_storage_dtype` which denotes the data type for packing the 4-bit parameters. For example, when it is set to `bfloat16`, **32/4 = 8** 4-bit params are packed together post quantization.\n\nIn terms of training code, the important code changes are:\n\nCopied\n\n```\n...\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=args.use_4bit_quantization,\n    bnb_4bit_quant_type=args.bnb_4bit_quant_type,\n    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=args.use_nested_quant,\n+   bnb_4bit_quant_storage=quant_storage_dtype,\n)\n\n...\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    args.model_name_or_path,\n    quantization_config=bnb_config,\n    trust_remote_code=True,\n    attn_implementation=\"flash_attention_2\" if args.use_flash_attn else \"eager\",\n+   dtype=quant_storage_dtype or torch.float32,\n)\n```\n\nNotice that `dtype` for `AutoModelForCausalLM` is same as the `bnb_4bit_quant_storage` data type. That\u2019s it. Everything else is handled by Trainer and TRL.\n\n## Memory usage\n\nIn the above example, the memory consumed per GPU is **36.6 GB**. Therefore, what took 8X80GB GPUs with DeepSpeed Stage 3+LoRA and a couple of 80GB GPUs with DDP+QLoRA now requires 2X40GB GPUs. This makes finetuning of large models more accessible.\n\n# Use PEFT and DeepSpeed with ZeRO3 and CPU Offloading for finetuning large models on a single GPU\n\nThis section of guide will help you learn how to use our DeepSpeed [training script](https://github.com/huggingface/peft/blob/main/examples/conditional_generation/peft_lora_seq2seq_accelerate_ds_zero3_offload.py). You\u2019ll configure the script to train a large model for conditional generation with ZeRO-3 and CPU Offload.\n\n> \ud83d\udca1 To help you", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "DeepSpeed", "description": "Hugging Face Official Documentation of DeepSpeed", "url": "https://huggingface.co/docs/peft/main/en/accelerate/deepspeed", "source": "hf", "id": "21acc3b0-827a-4672-9a1a-facfafa2ca4c"}, "page_content": "\n\nIn the above example, the memory consumed per GPU is **36.6 GB**. Therefore, what took 8X80GB GPUs with DeepSpeed Stage 3+LoRA and a couple of 80GB GPUs with DDP+QLoRA now requires 2X40GB GPUs. This makes finetuning of large models more accessible.\n\n# Use PEFT and DeepSpeed with ZeRO3 and CPU Offloading for finetuning large models on a single GPU\n\nThis section of guide will help you learn how to use our DeepSpeed [training script](https://github.com/huggingface/peft/blob/main/examples/conditional_generation/peft_lora_seq2seq_accelerate_ds_zero3_offload.py). You\u2019ll configure the script to train a large model for conditional generation with ZeRO-3 and CPU Offload.\n\n> \ud83d\udca1 To help you get started, check out our example training scripts for [causal language modeling](https://github.com/huggingface/peft/blob/main/examples/causal_language_modeling/peft_lora_clm_accelerate_ds_zero3_offload.py) and [conditional generation](https://github.com/huggingface/peft/blob/main/examples/conditional_generation/peft_lora_seq2seq_accelerate_ds_zero3_offload.py). You can adapt these scripts for your own applications or even use them out of the box if your task is similar to the one in the scripts.\n\n## Configuration\n\nStart by running the following command to [create a DeepSpeed configuration file](https://huggingface.co/docs/accelerate/quicktour#launching-your-distributed-script) with \ud83e\udd17 Accelerate. The `--config_file` flag allows you to save the configuration file to a specific location, otherwise it is saved as a `default_config.yaml` file in the \ud83e\udd17 Accelerate cache.\n\nThe configuration file is used to set the default options when you launch the training script.\n\nCopied\n\n```\naccelerate config --config_file ds_zero3_cpu.yaml\n```\n\nYou\u2019ll be asked a few questions about your setup, and configure the following arguments. In this example, you\u2019ll use ZeRO-3 along with CPU-Offload so make sure you pick those options.\n\nCopied\n\n```\n`zero_stage`: [0] Disabled, [1] optimizer state partitioning, [2] optimizer+gradient state partitioning and [3] optimizer+gradient+parameter partitioning\n`gradient_accumulation_steps`: Number of training steps to accumulate gradients before averaging and applying them.\n`gradient_clipping`: Enable gradient clipping with value.\n`offload_optimizer_device`: [none] Disable optimizer offloading, [cpu] offload optimizer to CPU, [nvme] offload optimizer to NVMe SSD. Only applicable with ZeRO >= Stage-2.\n`offload_param_device`: [none] Disable parameter offloading, [cpu] offload parameters to CPU, [nvme] offload parameters to NVMe SSD. Only applicable with ZeRO Stage-3.\n`zero3_init_flag`: Decides whether to enable `deepspeed.zero.Init` for constructing massive models. Only applicable with ZeRO Stage-3.\n`zero3_save_16bit_model`: Decides whether to save 16-bit model weights when using ZeRO Stage-3.\n`mixed_precision`: `no` for FP32 training, `fp16` for FP16 mixed-precision training and `bf16` for BF16 mixed-precision training.\n```\n\nAn example [configuration file](https://github.com/huggingface/peft/blob/main/examples/conditional_generation/accelerate_ds_zero3_cpu_offload_config.yaml) might look like the following. The most important thing to notice is that `zero_stage` is set to `3`, and `offload_optimizer_device` and `offload_param_device` are set to the `cpu`.\n\nCopied\n\n```\ncompute_environment: LOCAL_MACHINE\ndeepspeed_config:\n  gradient_accumulation_steps: 1\n  gradient_clipping: 1.0\n  offload_optimizer_device: cpu\n  offload_param_device: cpu\n  zero3_init_flag: true\n  zero3_save_16bit_model: true\n  zero_stage: 3\ndistributed_type: DEEPSPEED\ndowncast_bf16: 'no'\ndynamo_backend: 'NO'\nfsdp_config: {}\nmachine_rank: 0\nmain_training_function: main\nmegatron_lm_config: {}\nmixed_precision: 'no'\nnum_machines: 1\nnum_processes: 1\nrdzv_backend: static\nsame_network: true\nuse_cpu: false\n```\n\n## The important parts\n\nLet\u2019s dive a little deeper into the script so you can see what\u2019s going on, and understand how it works.\n\nWithin the [`main`](https://github.com/huggingface/peft/blob/2822398fbe896f", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "DeepSpeed", "description": "Hugging Face Official Documentation of DeepSpeed", "url": "https://huggingface.co/docs/peft/main/en/accelerate/deepspeed", "source": "hf", "id": "618b6af1-7693-4685-a0c7-850aa256fbac"}, "page_content": "\n  zero3_init_flag: true\n  zero3_save_16bit_model: true\n  zero_stage: 3\ndistributed_type: DEEPSPEED\ndowncast_bf16: 'no'\ndynamo_backend: 'NO'\nfsdp_config: {}\nmachine_rank: 0\nmain_training_function: main\nmegatron_lm_config: {}\nmixed_precision: 'no'\nnum_machines: 1\nnum_processes: 1\nrdzv_backend: static\nsame_network: true\nuse_cpu: false\n```\n\n## The important parts\n\nLet\u2019s dive a little deeper into the script so you can see what\u2019s going on, and understand how it works.\n\nWithin the [`main`](https://github.com/huggingface/peft/blob/2822398fbe896f25d4dac5e468624dc5fd65a51b/examples/conditional_generation/peft_lora_seq2seq_accelerate_ds_zero3_offload.py#L103) function, the script creates an [Accelerator](https://huggingface.co/docs/accelerate/main/en/package_reference/accelerator#accelerate.Accelerator) class to initialize all the necessary requirements for distributed training.\n\n> \ud83d\udca1 Feel free to change the model and dataset inside the `main` function. If your dataset format is different from the one in the script, you may also need to write your own preprocessing function.\n\nThe script also creates a configuration for the \ud83e\udd17 PEFT method you\u2019re using, which in this case, is LoRA. The [LoraConfig](https://huggingface.co/docs/peft/main/en/package_reference/lora#peft.LoraConfig) specifies the task type and important parameters such as the dimension of the low-rank matrices, the matrices scaling factor, and the dropout probability of the LoRA layers. If you want to use a different \ud83e\udd17 PEFT method, make sure you replace `LoraConfig` with the appropriate [class](https://huggingface.co/docs/peft/main/en/package_reference/tuners).\n\nCopied\n\n```\n def main():\n+    accelerator = Accelerator()\n     model_name_or_path = \"facebook/bart-large\"\n     dataset_name = \"twitter_complaints\"\n+    peft_config = LoraConfig(\n         task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1\n     )\n```\n\nThroughout the script, you\u2019ll see the [main\\_process\\_first](https://huggingface.co/docs/accelerate/main/en/package_reference/accelerator#accelerate.Accelerator.main_process_first) and [wait\\_for\\_everyone](https://huggingface.co/docs/accelerate/main/en/package_reference/accelerator#accelerate.Accelerator.wait_for_everyone) functions which help control and synchronize when processes are executed.\n\nThe [get\\_peft\\_model()](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.get_peft_model) function takes a base model and the `peft_config` you prepared earlier to create a [PeftModel](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.PeftModel):\n\nCopied\n\n```\n  model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)\n+ model = get_peft_model(model, peft_config)\n```\n\nPass all the relevant training objects to \ud83e\udd17 Accelerate\u2019s [prepare](https://huggingface.co/docs/accelerate/main/en/package_reference/accelerator#accelerate.Accelerator.prepare) which makes sure everything is ready for training:\n\nCopied\n\n```\nmodel, train_dataloader, eval_dataloader, test_dataloader, optimizer, lr_scheduler = accelerator.prepare(\n    model, train_dataloader, eval_dataloader, test_dataloader, optimizer, lr_scheduler\n)\n```\n\nThe next bit of code checks whether the DeepSpeed plugin is used in the `Accelerator`, and if the plugin exists, then we check if we are using ZeRO-3. This conditional flag is used when calling `generate` function call during inference for syncing GPUs when the model parameters are sharded:\n\nCopied\n\n```\nis_ds_zero_3 = False\nif getattr(accelerator.state, \"deepspeed_plugin\", None):\n    is_ds_zero_3 = accelerator.state.deepspeed_plugin.zero_stage == 3\n```\n\nInside the training loop, the usual `loss.backward()` is replaced", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "DeepSpeed", "description": "Hugging Face Official Documentation of DeepSpeed", "url": "https://huggingface.co/docs/peft/main/en/accelerate/deepspeed", "source": "hf", "id": "b46fbccd-120c-44ec-87ca-ba11cf15578a"}, "page_content": " optimizer, lr_scheduler = accelerator.prepare(\n    model, train_dataloader, eval_dataloader, test_dataloader, optimizer, lr_scheduler\n)\n```\n\nThe next bit of code checks whether the DeepSpeed plugin is used in the `Accelerator`, and if the plugin exists, then we check if we are using ZeRO-3. This conditional flag is used when calling `generate` function call during inference for syncing GPUs when the model parameters are sharded:\n\nCopied\n\n```\nis_ds_zero_3 = False\nif getattr(accelerator.state, \"deepspeed_plugin\", None):\n    is_ds_zero_3 = accelerator.state.deepspeed_plugin.zero_stage == 3\n```\n\nInside the training loop, the usual `loss.backward()` is replaced by \ud83e\udd17 Accelerate\u2019s [backward](https://huggingface.co/docs/accelerate/main/en/package_reference/accelerator#accelerate.Accelerator.backward) which uses the correct `backward()` method based on your configuration:\n\nCopied\n\n```\n  for epoch in range(num_epochs):\n      with TorchTracemalloc() as tracemalloc:\n          model.train()\n          total_loss = 0\n          for step, batch in enumerate(tqdm(train_dataloader)):\n              outputs = model(**batch)\n              loss = outputs.loss\n              total_loss += loss.detach().float()\n+             accelerator.backward(loss)\n              optimizer.step()\n              lr_scheduler.step()\n              optimizer.zero_grad()\n```\n\nThat is all! The rest of the script handles the training loop, evaluation, and even pushes it to the Hub for you.\n\n## Train\n\nRun the following command to launch the training script. Earlier, you saved the configuration file to `ds_zero3_cpu.yaml`, so you\u2019ll need to pass the path to the launcher with the `--config_file` argument like this:\n\nCopied\n\n```\naccelerate launch --config_file ds_zero3_cpu.yaml examples/peft_lora_seq2seq_accelerate_ds_zero3_offload.py\n```\n\nYou\u2019ll see some output logs that track memory usage during training, and once it\u2019s completed, the script returns the accuracy and compares the predictions to the labels:\n\nCopied\n\n```\nGPU Memory before entering the train : 1916\nGPU Memory consumed at the end of the train (end-begin): 66\nGPU Peak Memory consumed during the train (max-begin): 7488\nGPU Total Peak Memory consumed during the train (max): 9404\nCPU Memory before entering the train : 19411\nCPU Memory consumed at the end of the train (end-begin): 0\nCPU Peak Memory consumed during the train (max-begin): 0\nCPU Total Peak Memory consumed during the train (max): 19411\nepoch=4: train_ppl=tensor(1.0705, device='cuda:0') train_epoch_loss=tensor(0.0681, device='cuda:0')\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7/7 [00:27<00:00,  3.92s/it]\nGPU Memory before entering the eval : 1982\nGPU Memory consumed at the end of the eval (end-begin): -66\nGPU Peak Memory consumed during the eval (max-begin): 672\nGPU Total Peak Memory consumed during the eval (max): 2654\nCPU Memory before entering the eval : 19411\nCPU Memory consumed at the end of the eval (end-begin): 0\nCPU Peak Memory consumed during the eval (max-begin): 0\nCPU Total Peak Memory consumed during the eval (max): 19411\naccuracy=100.0\neval_preds[:10]=['no complaint', 'no complaint', 'complaint', 'complaint', 'no complaint', 'no complaint', 'no complaint', 'complaint', 'complaint', 'no complaint']\ndataset['train'][label_column][:10]=['no complaint', 'no complaint', 'complaint', 'complaint', 'no complaint', 'no complaint', 'no complaint', 'complaint', 'complaint', 'no complaint']\n```\n\n# Caveats\n\n1. Merging when using PEFT and DeepSpeed is currently unsupported and will raise error.\n2. When using CPU offloading, the major gains from using PEFT to shrink the optimizer states and gradients to that of the adapter weights would be realized on CPU RAM and there won\u2019t be savings with respect to GPU memory.\n3. DeepSpeed Stage 3 and qlora when used with CPU offloading leads to more GPU memory usage when compared to disabling CPU offloading.\n\n> \ud83d\udca1 When you have code that requires merging (and unmerging) of weights, try to manually collect the parameters with DeepSpeed Zero-3 beforehand:\n>\n> Copied\n>\n> ```\n> import deepspeed\n>\n> is_ds_zero_3 =... # check if Zero-3\n>", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "DeepSpeed", "description": "Hugging Face Official Documentation of DeepSpeed", "url": "https://huggingface.co/docs/peft/main/en/accelerate/deepspeed", "source": "hf", "id": "0e24697d-8313-479b-9844-8260964db09b"}, "page_content": "', 'no complaint', 'complaint', 'complaint', 'no complaint']\n```\n\n# Caveats\n\n1. Merging when using PEFT and DeepSpeed is currently unsupported and will raise error.\n2. When using CPU offloading, the major gains from using PEFT to shrink the optimizer states and gradients to that of the adapter weights would be realized on CPU RAM and there won\u2019t be savings with respect to GPU memory.\n3. DeepSpeed Stage 3 and qlora when used with CPU offloading leads to more GPU memory usage when compared to disabling CPU offloading.\n\n> \ud83d\udca1 When you have code that requires merging (and unmerging) of weights, try to manually collect the parameters with DeepSpeed Zero-3 beforehand:\n>\n> Copied\n>\n> ```\n> import deepspeed\n>\n> is_ds_zero_3 =... # check if Zero-3\n>\n> with deepspeed.zero.GatheredParameters(list(model.parameters()), enabled= is_ds_zero_3):\n>     model.merge_adapter()\n>     # do whatever is needed, then unmerge in the same context if unmerging is required\n>    ...\n>     model.unmerge_adapter()\n> ```\n\n[Update on GitHub](https://github.com/huggingface/peft/blob/main/docs/source/accelerate/deepspeed.md)\n\n[\u2190PEFT checkpoint format](https://huggingface.co/docs/peft/main/en/developer_guides/checkpoint) [Fully Sharded Data Parallel\u2192](https://huggingface.co/docs/peft/main/en/accelerate/fsdp)[SEP]", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Fully Sharded Data Parallel", "description": "Hugging Face Official Documentation of Fully Sharded Data Parallel", "url": "https://huggingface.co/docs/peft/main/en/accelerate/fsdp", "source": "hf", "id": "9ae16a08-4642-4472-b26a-d68711510e21"}, "page_content": "[CLS]# Fully Sharded Data Parallel\n\n[Fully sharded data parallel](https://pytorch.org/docs/stable/fsdp.html) (FSDP) is developed for distributed training of large pretrained models up to 1T parameters. FSDP achieves this by sharding the model parameters, gradients, and optimizer states across data parallel processes and it can also offload sharded model parameters to a CPU. The memory efficiency afforded by FSDP allows you to scale training to larger batch or model sizes.\n\nBoth of these features are supported in \ud83e\udd17 Accelerate, and you can use them with \ud83e\udd17 PEFT.\n\n# Use PEFT and FSDP\n\nThis section of guide will help you learn how to use our DeepSpeed [training script](https://github.com/huggingface/peft/blob/main/examples/sft/train.py) for performing SFT. You\u2019ll configure the script to do SFT (supervised fine-tuning) of Llama-70B model with LoRA and FSDP on 8xH100 80GB GPUs on a single machine. You can configure it to scale to multiple machines by changing the accelerate config.\n\n## Configuration\n\nStart by running the following command to [create a FSDP configuration file](https://huggingface.co/docs/accelerate/quicktour#launching-your-distributed-script) with \ud83e\udd17 Accelerate. The `--config_file` flag allows you to save the configuration file to a specific location, otherwise it is saved as a `default_config.yaml` file in the \ud83e\udd17 Accelerate cache.\n\nThe configuration file is used to set the default options when you launch the training script.\n\nCopied\n\n```\naccelerate config --config_file fsdp_config.yaml\n```\n\nYou\u2019ll be asked a few questions about your setup, and configure the following arguments. In this example, you\u2019ll answer the questionnaire as shown in the image below.\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/fsdp-peft-config.png)\n\nCreating Accelerate's config to use FSDP\n\nOnce this is done, the corresponding config should look like below and you can find it in config folder at [fsdp\\_config.yaml](https://github.com/huggingface/peft/blob/main/examples/sft/configs/fsdp_config.yaml):\n\nCopied\n\n```\ncompute_environment: LOCAL_MACHINE\ndebug: false\ndistributed_type: FSDP\ndowncast_bf16: 'no'\nfsdp_config:\n  fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP\n  fsdp_backward_prefetch: BACKWARD_PRE\n  fsdp_cpu_ram_efficient_loading: true\n  fsdp_forward_prefetch: false\n  fsdp_offload_params: false\n  fsdp_sharding_strategy: FULL_SHARD\n  fsdp_state_dict_type: SHARDED_STATE_DICT\n  fsdp_sync_module_states: true\n  fsdp_use_orig_params: false\nmachine_rank: 0\nmain_training_function: main\nmixed_precision: bf16\nnum_machines: 1\nnum_processes: 8\nrdzv_backend: static\nsame_network: true\ntpu_env: []\ntpu_use_cluster: false\ntpu_use_sudo: false\nuse_cpu: false\n```\n\n## Launch command\n\nThe launch command is available at [run\\_peft\\_fsdp.sh](https://github.com/huggingface/peft/blob/main/examples/sft/run_peft_fsdp.sh) and it is also shown below:\n\nCopied\n\n```\naccelerate launch --config_file \"configs/fsdp_config.yaml\"  train.py \\\n--seed 100 \\\n--model_name_or_path \"meta-llama/Llama-2-70b-hf\" \\\n--dataset_name \"smangrul/ultrachat-10k-chatml\" \\\n--chat_template_format \"chatml\" \\\n--add_special_tokens False \\\n--append_concat_token False \\\n--splits \"train,test\" \\\n--max_seq_len 2048 \\\n--num_train_epochs 1 \\\n--logging_steps 5 \\\n--log_level \"info\" \\\n--logging_strategy \"steps\" \\\n--eval_strategy \"epoch\" \\\n--save_strategy \"epoch\" \\\n--push_to_hub \\\n--hub_private_repo True \\\n--hub_strategy \"every_save\" \\\n--bf16 True \\\n--packing True \\\n--learning_rate 1e-4 \\\n--lr_scheduler_type \"cosine\" \\\n--weight_decay 1e-4 \\\n--warmup_ratio 0.0 \\\n--max_grad_norm 1.0 \\\n--output_dir \"llama-sft-lora-fsdp\" \\\n--", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Fully Sharded Data Parallel", "description": "Hugging Face Official Documentation of Fully Sharded Data Parallel", "url": "https://huggingface.co/docs/peft/main/en/accelerate/fsdp", "source": "hf", "id": "c5487893-e02b-4bbd-8587-81d08b61968b"}, "page_content": "_token False \\\n--splits \"train,test\" \\\n--max_seq_len 2048 \\\n--num_train_epochs 1 \\\n--logging_steps 5 \\\n--log_level \"info\" \\\n--logging_strategy \"steps\" \\\n--eval_strategy \"epoch\" \\\n--save_strategy \"epoch\" \\\n--push_to_hub \\\n--hub_private_repo True \\\n--hub_strategy \"every_save\" \\\n--bf16 True \\\n--packing True \\\n--learning_rate 1e-4 \\\n--lr_scheduler_type \"cosine\" \\\n--weight_decay 1e-4 \\\n--warmup_ratio 0.0 \\\n--max_grad_norm 1.0 \\\n--output_dir \"llama-sft-lora-fsdp\" \\\n--per_device_train_batch_size 8 \\\n--per_device_eval_batch_size 8 \\\n--gradient_accumulation_steps 4 \\\n--gradient_checkpointing True \\\n--use_reentrant False \\\n--dataset_text_field \"content\" \\\n--use_flash_attn True \\\n--use_peft_lora True \\\n--lora_r 8 \\\n--lora_alpha 16 \\\n--lora_dropout 0.1 \\\n--lora_target_modules \"all-linear\" \\\n--use_4bit_quantization False\n```\n\nNotice that we are using LoRA with rank=8, alpha=16 and targeting all linear layers. We are passing the FSDP config file and finetuning the 70B Llama model on a subset of the [ultrachat dataset](https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k).\n\n## The important parts\n\nLet\u2019s dive a little deeper into the script so you can see what\u2019s going on, and understand how it works.\n\nThe first thing to know is that the script uses FSDP for distributed training as the FSDP config has been passed. The [SFTTrainer](https://huggingface.co/docs/trl/main/en/sft_trainer#trl.SFTTrainer) class handles all the heavy lifting of creating PEFT model using the peft config that is passed. After that when you call `trainer.train()`, Trainer internally uses \ud83e\udd17 Accelerate to prepare model, optimizer and trainer using the FSDP config to create FSDP wrapped model which is then trained. The main code snippet is below:\n\nCopied\n\n```\n# trainer\ntrainer = SFTTrainer(\n    model=model,\n    processing_class=tokenizer,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    peft_config=peft_config,\n)\ntrainer.accelerator.print(f\"{trainer.model}\")\nif model_args.use_peft_lora:\n    # handle PEFT+FSDP case\n    trainer.model.print_trainable_parameters()\n    if getattr(trainer.accelerator.state, \"fsdp_plugin\", None):\n        from peft.utils.other import fsdp_auto_wrap_policy\n\n        fsdp_plugin = trainer.accelerator.state.fsdp_plugin\n        fsdp_plugin.auto_wrap_policy = fsdp_auto_wrap_policy(trainer.model)\n\n# train\ncheckpoint = None\nif training_args.resume_from_checkpoint is not None:\n    checkpoint = training_args.resume_from_checkpoint\ntrainer.train(resume_from_checkpoint=checkpoint)\n\n# saving final model\nif trainer.is_fsdp_enabled:\n    trainer.accelerator.state.fsdp_plugin.set_state_dict_type(\"FULL_STATE_DICT\")\ntrainer.save_model()\n```\n\nHere, one main thing to note currently when using FSDP with PEFT is that `use_orig_params` needs to be `False` to realize GPU memory savings. Due to `use_orig_params=False`, the auto wrap policy for FSDP needs to change so that trainable and non-trainable parameters are wrapped separately. This is done by the code snippt below which uses the util function `fsdp_auto_wrap_policy` from PEFT:\n\nCopied\n\n```\nif getattr(trainer.accelerator.state, \"fsdp_plugin\", None):\n    from peft.utils.other import fsdp_auto_wrap_policy\n\n    fsdp_plugin = trainer.accelerator.state.fsdp_plugin\n    fsdp_plugin.auto_wrap_policy = fsdp_auto_wrap_policy(trainer.model)\n```\n\n## Memory usage\n\nIn the above example, the memory consumed per GPU is 72-80 GB (90-98%) as seen in the screenshot below. The slight increase in GPU memory at the end is when saving the model using `FULL_STATE_DICT` state dict type instead of the `SHARDED_", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Fully Sharded Data Parallel", "description": "Hugging Face Official Documentation of Fully Sharded Data Parallel", "url": "https://huggingface.co/docs/peft/main/en/accelerate/fsdp", "source": "hf", "id": "35524ffb-7008-4e1d-a981-a26b1d5309de"}, "page_content": "able parameters are wrapped separately. This is done by the code snippt below which uses the util function `fsdp_auto_wrap_policy` from PEFT:\n\nCopied\n\n```\nif getattr(trainer.accelerator.state, \"fsdp_plugin\", None):\n    from peft.utils.other import fsdp_auto_wrap_policy\n\n    fsdp_plugin = trainer.accelerator.state.fsdp_plugin\n    fsdp_plugin.auto_wrap_policy = fsdp_auto_wrap_policy(trainer.model)\n```\n\n## Memory usage\n\nIn the above example, the memory consumed per GPU is 72-80 GB (90-98%) as seen in the screenshot below. The slight increase in GPU memory at the end is when saving the model using `FULL_STATE_DICT` state dict type instead of the `SHARDED_STATE_DICT` so that the model has adapter weights that can be loaded normally with `from_pretrained` method during inference:\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/peft_fsdp_mem_usage.png)\n\nGPU memory usage for the training run\n\n# Use PEFT QLoRA and FSDP for finetuning large models on multiple GPUs\n\nIn this section, we will look at how to use QLoRA and FSDP for finetuning 70B llama model on 2X24GB GPUs. [Answer.AI](https://www.answer.ai/) in collaboration with bitsandbytes and Hugging Face \ud83e\udd17 open sourced code enabling the usage of FSDP+QLoRA and explained the whole process in their insightful blogpost [You can now train a 70b language model at home](https://www.answer.ai/posts/2024-03-06-fsdp-qlora.html). This is now integrated in Hugging Face ecosystem.\n\nFor this, we first need `bitsandbytes>=0.43.3`, `accelerate>=1.0.1`, `transformers>4.44.2`, `trl>0.11.4` and `peft>0.13.0`. We need to set `fsdp_cpu_ram_efficient_loading=true`, `fsdp_use_orig_params=false` and `fsdp_offload_params=true`(cpu offloading) when using Accelerate config. When not using accelerate launcher, you can alternately set the environment variable `export FSDP_CPU_RAM_EFFICIENT_LOADING=true`. Here, we will be using accelerate config and below is the config which can be found at [fsdp\\_config\\_qlora.yaml](https://github.com/huggingface/peft/blob/main/examples/sft/configs/fsdp_config_qlora.yaml):\n\nCopied\n\n```\ncompute_environment: LOCAL_MACHINE\ndebug: false\ndistributed_type: FSDP\ndowncast_bf16: 'no'\nfsdp_config:\n  fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP\n  fsdp_backward_prefetch: BACKWARD_PRE\n  fsdp_cpu_ram_efficient_loading: true\n  fsdp_forward_prefetch: false\n  fsdp_offload_params: true\n  fsdp_sharding_strategy: FULL_SHARD\n  fsdp_state_dict_type: SHARDED_STATE_DICT\n  fsdp_sync_module_states: true\n  fsdp_use_orig_params: false\nmachine_rank: 0\nmain_training_function: main\nmixed_precision: 'no'\nnum_machines: 1\nnum_processes: 2\nrdzv_backend: static\nsame_network: true\ntpu_env: []\ntpu_use_cluster: false\ntpu_use_sudo: false\nuse_cpu: false\n```\n\nLaunch command is given below which is available at [run\\_peft\\_qlora\\_fsdp.sh](https://github.com/huggingface/peft/blob/main/examples/sft/run_peft_qlora_fsdp.sh):\n\nCopied\n\n```\naccelerate launch --config_file \"configs/fsdp_config_qlora.yaml\"  train.py \\\n--seed 100 \\\n--model_name_or_path \"meta-llama/Llama-2-70b-hf\" \\\n--dataset_name \"smangrul/ultrachat-10k-chatml\" \\\n--chat_template_format \"chatml\" \\\n--add_special_tokens False \\\n--append_concat_token False \\\n--splits \"train,test\" \\\n--max_seq_len 2048 \\\n--num_train_epochs 1 \\\n--logging_steps 5 \\\n--log_level \"info\" \\\n--logging_strategy \"steps\" \\\n--eval_strategy \"epoch\" \\\n--save_strategy \"epoch\" \\\n--push_to_hub \\\n--hub_private_repo True \\\n--hub_strategy", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Fully Sharded Data Parallel", "description": "Hugging Face Official Documentation of Fully Sharded Data Parallel", "url": "https://huggingface.co/docs/peft/main/en/accelerate/fsdp", "source": "hf", "id": "9a21ae90-debf-4168-ab5b-fe20892fd845"}, "page_content": "_config_qlora.yaml\"  train.py \\\n--seed 100 \\\n--model_name_or_path \"meta-llama/Llama-2-70b-hf\" \\\n--dataset_name \"smangrul/ultrachat-10k-chatml\" \\\n--chat_template_format \"chatml\" \\\n--add_special_tokens False \\\n--append_concat_token False \\\n--splits \"train,test\" \\\n--max_seq_len 2048 \\\n--num_train_epochs 1 \\\n--logging_steps 5 \\\n--log_level \"info\" \\\n--logging_strategy \"steps\" \\\n--eval_strategy \"epoch\" \\\n--save_strategy \"epoch\" \\\n--push_to_hub \\\n--hub_private_repo True \\\n--hub_strategy \"every_save\" \\\n--bf16 True \\\n--packing True \\\n--learning_rate 1e-4 \\\n--lr_scheduler_type \"cosine\" \\\n--weight_decay 1e-4 \\\n--warmup_ratio 0.0 \\\n--max_grad_norm 1.0 \\\n--output_dir \"llama-sft-qlora-fsdp\" \\\n--per_device_train_batch_size 2 \\\n--per_device_eval_batch_size 2 \\\n--gradient_accumulation_steps 2 \\\n--gradient_checkpointing True \\\n--use_reentrant True \\\n--dataset_text_field \"content\" \\\n--use_flash_attn True \\\n--use_peft_lora True \\\n--lora_r 8 \\\n--lora_alpha 16 \\\n--lora_dropout 0.1 \\\n--lora_target_modules \"all-linear\" \\\n--use_4bit_quantization True \\\n--use_nested_quant True \\\n--bnb_4bit_compute_dtype \"bfloat16\" \\\n--bnb_4bit_quant_storage_dtype \"bfloat16\"\n```\n\nNotice the new argument being passed, `bnb_4bit_quant_storage_dtype`, which denotes the data type for packing the 4-bit parameters. For example, when it is set to `bfloat16`, **16/4 = 4** 4-bit params are packed together post quantization. When using mixed precision training with `bfloat16`, `bnb_4bit_quant_storage_dtype` can be either `bfloat16` for pure `bfloat16` finetuning, or `float32` for automatic mixed precision (this consumes more GPU memory). When using mixed precision training with `float16`, `bnb_4bit_quant_storage_dtype` should be set to `float32` for stable automatic mixed precision training.\n\nIn terms of training code, the important code changes are:\n\nCopied\n\n```\n...\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=args.use_4bit_quantization,\n    bnb_4bit_quant_type=args.bnb_4bit_quant_type,\n    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=args.use_nested_quant,\n+   bnb_4bit_quant_storage=quant_storage_dtype,\n)\n\n...\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    args.model_name_or_path,\n    quantization_config=bnb_config,\n    trust_remote_code=True,\n    attn_implementation=\"flash_attention_2\" if args.use_flash_attn else \"eager\",\n+   dtype=quant_storage_dtype or torch.float32,\n)\n```\n\nNotice that `dtype` for `AutoModelForCausalLM` is same as the `bnb_4bit_quant_storage` data type. That\u2019s it. Everything else is handled by Trainer and TRL.\n\n## Memory usage\n\nIn the above example, the memory consumed per GPU is **19.6 GB** while CPU RAM usage is around **107 GB**. When disabling CPU offloading, the GPU memory usage is **35.6 GB/ GPU**. Therefore, what took 16X80GB GPUs for full finetuning, 8X80GB GPUs with FSDP+LoRA, and a couple of 80GB GPUs with DDP+QLoRA, now requires 2X24GB GPUs. This makes finetuning of large models more accessible.\n\n## More resources\n\nYou can also refer the [llama-recipes](https://github.com/facebookresearch/llama-recipes/?tab=readme-ov-file#fine-tuning) repo and [Getting started with Llama](https://llama.meta.com/get-started/#fine-tuning) guide on how to finetune using FSDP and PEFT.\n\n## Caveats\n\n1. Merging when using PEFT and FSDP is currently unsupported and will raise error.\n2. Passing `modules_to_save` config parameter to is untested at present.\n3.", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Fully Sharded Data Parallel", "description": "Hugging Face Official Documentation of Fully Sharded Data Parallel", "url": "https://huggingface.co/docs/peft/main/en/accelerate/fsdp", "source": "hf", "id": "f87312da-0204-4094-9732-6f4b7c3ea96d"}, "page_content": " GPUs for full finetuning, 8X80GB GPUs with FSDP+LoRA, and a couple of 80GB GPUs with DDP+QLoRA, now requires 2X24GB GPUs. This makes finetuning of large models more accessible.\n\n## More resources\n\nYou can also refer the [llama-recipes](https://github.com/facebookresearch/llama-recipes/?tab=readme-ov-file#fine-tuning) repo and [Getting started with Llama](https://llama.meta.com/get-started/#fine-tuning) guide on how to finetune using FSDP and PEFT.\n\n## Caveats\n\n1. Merging when using PEFT and FSDP is currently unsupported and will raise error.\n2. Passing `modules_to_save` config parameter to is untested at present.\n3. GPU Memory saving when using CPU Offloading is untested at present.\n4. When using FSDP+QLoRA, `paged_adamw_8bit` currently results in an error when saving a checkpoint.\n5. DoRA training with FSDP should work (albeit at lower speed than LoRA). If combined with bitsandbytes (QDoRA), 4-bit quantization should also work, but 8-bit quantization has known issues and is not recommended.\n\n[Update on GitHub](https://github.com/huggingface/peft/blob/main/docs/source/accelerate/fsdp.md)\n\n[\u2190DeepSpeed](https://huggingface.co/docs/peft/main/en/accelerate/deepspeed) [Adapters\u2192](https://huggingface.co/docs/peft/main/en/conceptual_guides/adapter)[SEP]", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Adapters", "description": "Hugging Face Official Documentation of Adapters", "url": "https://huggingface.co/docs/peft/main/en/conceptual_guides/adapter", "source": "hf", "id": "89115dab-15fe-47c2-9c60-201673761329"}, "page_content": "[CLS]# Adapters\n\nAdapter-based methods add extra trainable parameters after the attention and fully-connected layers of a frozen pretrained model to reduce memory-usage and speed up training. The method varies depending on the adapter, it could simply be an extra added layer or it could be expressing the weight updates \u2206W as a low-rank decomposition of the weight matrix. Either way, the adapters are typically small but demonstrate comparable performance to a fully finetuned model and enable training larger models with fewer resources.\n\nThis guide will give you a brief overview of the adapter methods supported by PEFT (if you\u2019re interested in learning more details about a specific method, take a look at the linked paper).\n\n## Low-Rank Adaptation (LoRA)\n\n> LoRA is one of the most popular PEFT methods and a good starting point if you\u2019re just getting started with PEFT. It was originally developed for large language models but it is a tremendously popular training method for diffusion models because of its efficiency and effectiveness.\n\nAs mentioned briefly earlier, [LoRA](https://hf.co/papers/2106.09685) is a technique that accelerates finetuning large models while consuming less memory.\n\nLoRA represents the weight updates \u2206W with two smaller matrices (called _update matrices_) through low-rank decomposition. These new matrices can be trained to adapt to the new data while keeping the overall number of parameters low. The original weight matrix remains frozen and doesn\u2019t receive any further updates. To produce the final results, the original and extra adapted weights are combined. You could also merge the adapter weights with the base model to eliminate inference latency.\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/lora_animated.gif)\n\nThis approach has a number of advantages:\n\n- LoRA makes finetuning more efficient by drastically reducing the number of trainable parameters.\n- The original pretrained weights are kept frozen, which means you can have multiple lightweight and portable LoRA models for various downstream tasks built on top of them.\n- LoRA is orthogonal to other parameter-efficient methods and can be combined with many of them.\n- Performance of models finetuned using LoRA is comparable to the performance of fully finetuned models.\n\nIn principle, LoRA can be applied to any subset of weight matrices in a neural network to reduce the number of trainable parameters. However, for simplicity and further parameter efficiency, LoRA is typically only applied to the attention blocks in Transformer models. The resulting number of trainable parameters in a LoRA model depends on the size of the update matrices, which is determined mainly by the rank `r` and the shape of the original weight matrix.\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/lora.png)\n\n[Navigating Text-To-Image Customization: From LyCORIS Fine-Tuning to Model Evaluation](https://hf.co/papers/2103.10385)\n\n## Mixture of LoRA Experts (X-LoRA)\n\n[X-LoRA](https://huggingface.co/papers/2402.07148) is a mixture of experts method for LoRA which works by using dense or sparse gating to dynamically activate LoRA experts. The LoRA experts as well as the base model are frozen during training, resulting in a low parameter count as only the gating layers must be trained. In particular, the gating layers output scalings which (depending on config) are granular on the layer and token level. Additionally, during inference, X-LoRA dynamically activates LoRA adapters to recall knowledge and effectively mix them:\n\nThe below graphic demonstrates how the scalings change for different prompts for each token. This highlights the activation of different adapters as the generation progresses and the sequence creates new context.\n\n![Token-by-token scalings](https://github.com/EricLBuehler/xlora/raw/master/res/token_by_token_scalings.gif)\n\nFor each step, X-LoRA requires the base model to be run twice: first, to get hidden states without any LoRA adapters, and secondly, the hidden states are used to calculate scalings which are applied to the LoRA adapters and the model is run a second time. The output of the second run is the result of the model step.\n\nUltimately, X-LoRA allows the model to reflect upon its knowledge because of the dual forward pass scheme, and dynamically reconfigure the architecture.\n\n## Low-Rank Hadamard Product (LoHa)\n\nLow-rank decomposition can impact performance because the weight updates are limited to the low-rank space, which can constrain a model\u2019s expressiveness. However, you don\u2019t necessarily want to use a larger rank because it increases the number of trainable parameters. To address this, [LoHa](https://huggingface.co/papers/2108.06098) (a method originally developed for computer vision) was applied to diffusion models where the ability to generate diverse images is an important consideration. LoHa should also work with general model types, but the embedding layers aren\u2019t currently implemented in PEFT.\n\nLoHa uses the [Hadamard product](https://en.wikipedia.org/wiki/Hadamard_product_(matrices)) (element-wise product) instead of the matrix product. \u2206W is represented by four smaller matrices instead of two -", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Adapters", "description": "Hugging Face Official Documentation of Adapters", "url": "https://huggingface.co/docs/peft/main/en/conceptual_guides/adapter", "source": "hf", "id": "d43a11d5-5d9c-465a-ab95-e19421cdd6b6"}, "page_content": ".\n\n## Low-Rank Hadamard Product (LoHa)\n\nLow-rank decomposition can impact performance because the weight updates are limited to the low-rank space, which can constrain a model\u2019s expressiveness. However, you don\u2019t necessarily want to use a larger rank because it increases the number of trainable parameters. To address this, [LoHa](https://huggingface.co/papers/2108.06098) (a method originally developed for computer vision) was applied to diffusion models where the ability to generate diverse images is an important consideration. LoHa should also work with general model types, but the embedding layers aren\u2019t currently implemented in PEFT.\n\nLoHa uses the [Hadamard product](https://en.wikipedia.org/wiki/Hadamard_product_(matrices)) (element-wise product) instead of the matrix product. \u2206W is represented by four smaller matrices instead of two - like in LoRA - and each pair of these low-rank matrices are combined with the Hadamard product. As a result, \u2206W can have the same number of trainable parameters but a higher rank and expressivity.\n\n## Low-Rank Kronecker Product (LoKr)\n\n[LoKr](https://hf.co/papers/2309.14859) is very similar to LoRA and LoHa, and it is also mainly applied to diffusion models, though you could also use it with other model types. LoKr replaces the matrix product with the [Kronecker product](https://en.wikipedia.org/wiki/Kronecker_product) instead. The Kronecker product decomposition creates a block matrix which preserves the rank of the original weight matrix. Another benefit of the Kronecker product is that it can be vectorized by stacking the matrix columns. This can speed up the process because you\u2019re avoiding fully reconstructing \u2206W.\n\n## Orthogonal Finetuning (OFT)\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/oft.png)\n\n[Controlling Text-to-Image Diffusion by Orthogonal Finetuning](https://hf.co/papers/2306.07280)\n\n[OFT](https://hf.co/papers/2306.07280) is a method that primarily focuses on preserving a pretrained model\u2019s generative performance in the finetuned model. It tries to maintain the same cosine similarity (hyperspherical energy) between all pairwise neurons in a layer because this better captures the semantic information among neurons. This means OFT is more capable at preserving the subject and it is better for controllable generation (similar to [ControlNet](https://huggingface.co/docs/diffusers/using-diffusers/controlnet)).\n\nOFT preserves the hyperspherical energy by learning an orthogonal transformation for neurons to keep the cosine similarity between them unchanged. In practice, this means taking the matrix product of an orthogonal matrix with the pretrained weight matrix. However, to be parameter-efficient, the orthogonal matrix is represented as a block-diagonal matrix with rank `r` blocks. Whereas LoRA reduces the number of trainable parameters with low-rank structures, OFT reduces the number of trainable parameters with a sparse block-diagonal matrix structure.\n\n## Orthogonal Butterfly (BOFT)\n\n[BOFT](https://hf.co/papers/2311.06243) is an improved orthogonal finetuning method that focuses on preserving a pretrained model\u2019s generative capabilities while being significantly more parameter-efficient than standard OFT. Like OFT, BOFT maintains the same cosine similarity (hyperspherical energy) between all pairwise neurons in a layer by applying an orthogonal transformation to the pretrained weight matrix, ensuring the semantic relationships among neurons are preserved.\n\nInstead of using a block-diagonal orthogonal matrix, BOFT factorizes the orthogonal transformation into a product of **sparse butterfly matrices** (originally introduced in the [Cooley\u2013Tukey FFT](https://en.wikipedia.org/wiki/Cooley%E2%80%93Tukey_FFT_algorithm)). Unlike OFT\u2019s block-diagonal rotations, which only mix inputs within each block, the butterfly structure guarantees that every input can influence every output, producing a **dense connectivity** with just `O(d log d)` parameters. This factorization preserves expressivity while drastically reducing the parameter count compared to OFT (at the expense of computation time).\n\nIn practice, BOFT multiplies each pretrained weight matrix by a sequence of butterfly-structured orthogonal factors, enabling efficient and expressive neuron rotations. This makes BOFT well-suited for controllable generation and tasks where maintaining the pretrained model\u2019s subject representation is critical, while also scaling to larger models with lower memory and compute overhead.\n\n## Adaptive Low-Rank Adaptation (AdaLoRA)\n\n[AdaLoRA](https://hf.co/papers/2303.10512) manages the parameter budget introduced from LoRA by allocating more parameters - in other words, a higher rank `r` \\- for important weight matrices that are better adapted for a task and pruning less important ones. The rank is controlled by a method similar to singular value decomposition (SVD). The \u2206W is parameterized with two orthogonal matrices and a diagonal matrix which contains singular values. This parametrization method avoids iteratively applying SVD which is computationally expensive. Based on this method, the rank of \u2206W is adjusted according to an importance score. \u2206W is divided into triplets and", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Adapters", "description": "Hugging Face Official Documentation of Adapters", "url": "https://huggingface.co/docs/peft/main/en/conceptual_guides/adapter", "source": "hf", "id": "928eda2f-989b-4180-bc66-de35d950e9d6"}, "page_content": " rotations. This makes BOFT well-suited for controllable generation and tasks where maintaining the pretrained model\u2019s subject representation is critical, while also scaling to larger models with lower memory and compute overhead.\n\n## Adaptive Low-Rank Adaptation (AdaLoRA)\n\n[AdaLoRA](https://hf.co/papers/2303.10512) manages the parameter budget introduced from LoRA by allocating more parameters - in other words, a higher rank `r` \\- for important weight matrices that are better adapted for a task and pruning less important ones. The rank is controlled by a method similar to singular value decomposition (SVD). The \u2206W is parameterized with two orthogonal matrices and a diagonal matrix which contains singular values. This parametrization method avoids iteratively applying SVD which is computationally expensive. Based on this method, the rank of \u2206W is adjusted according to an importance score. \u2206W is divided into triplets and each triplet is scored according to its contribution to model performance. Triplets with low importance scores are pruned and triplets with high importance scores are kept for finetuning.\n\nTraining with AdaLoRA has three phases: the init phase, the budgeting phase and the final phase. In the initial phase, no budgeting is applied, therefore the ranks are not touched. During the budgeting phase the process described above is applied and the rank is redistributed according to a budget, aiming to give more important adapters more rank and less important layers less. When reaching the final phase, budgeting has ended, the ranks are redistributed but we may continue training for a while with the redistributed ranks to further improve performance.\n\n## Llama-Adapter\n\n[Llama-Adapter](https://hf.co/papers/2303.16199) is a method for adapting Llama into an instruction-following model. To help adapt the model for instruction-following, the adapter is trained with a 52K instruction-output dataset.\n\nA set of learnable adaption prompts are prefixed to the input instruction tokens. These are inserted into the upper layers of the model because it is better to learn with the higher-level semantics of the pretrained model. The instruction-output tokens prefixed to the input guide the adaption prompt to generate a contextual response.\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/llama-adapter.png)\n\n[LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention](https://hf.co/papers/2303.16199)\n\nTo avoid adding noise to the tokens, the adapter uses zero-initialized attention. On top of this, the adapter adds a learnable gating factor (initialized with zeros) to progressively add information to the model during training. This prevents overwhelming the model\u2019s pretrained knowledge with the newly learned instructions.\n\n## Householder Reflection Adaptation (HRA)\n\n[HRA](https://huggingface.co/papers/2405.17484) provides a new perspective connecting LoRA to OFT, which means it can harness the advantages of both strategies, reduce parameters and computation costs while penalizing the loss of pre-training knowledge.\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/hra.png)\n\n[Bridging The Gap between Low-rank and Orthogonal Adaptation via Householder Reflection Adaptation](https://huggingface.co/papers/2405.17484)\n\nHRA constructs a chain of `r` trainable Householder reflections (HRs). Because the Householder reflection matrix is an orthogonal matrix and the product of orthogonal matrices is also an orthogonal matrix, HRA satisfies the theoretical guarantee of Orthogonal Finetuning (OFT). Meanwhile, HRA can also be viewed as a low-rank fine-tuning adapter by rewriting formula.\n\nThe higher `r`, the more trainable parameters, resulting in a larger model capacity and better performance. Besides, due to the chain structure, the orthogonality of HR planes impacts the capacity and regularity of HRA. To achieve a trade-off between the model capacity and regularity, an orthogonality regularizer of the HR planes is added to the loss function. The weight\u03bb\\\\lambda\u03bb can control the strength of the regularizer.\n\n## Bone\n\n[MiSS](https://huggingface.co/papers/2409.15371) New version of paper(MiSS: Balancing LoRA Performance and Efficiency with Simple Shard Sharing)\nIf you already have a Bone checkpoint, you can use `/scripts/convert-bone-to-miss.py` to convert it into a MiSS checkpoint and proceed with training using MiSS.\n\n## MiSS\n\n[MiSS](https://huggingface.co/papers/2409.15371) MiSS (Matrix Shard Sharing) is a novel Parameter-Efficient Fine-Tuning (PEFT) method designed to address the trade-off between adaptability and efficiency in Large Language Models. The core approach of MiSS involves a simple shard-sharing mechanism. It achieves low-rank adaptation by decomposing a weight matrix into multiple fragments and then utilizing a shared, trainable \u201ccommon fragment.\u201d The final low-rank update matrix is constructed by replicating these shared, partitioned shards. (MiSS is a novel PEFT method that adopts a low-rank structure, requires only a single trainable matrix, and introduces", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Adapters", "description": "Hugging Face Official Documentation of Adapters", "url": "https://huggingface.co/docs/peft/main/en/conceptual_guides/adapter", "source": "hf", "id": "a1f90887-a914-46dd-92d7-91ba1f760a94"}, "page_content": "If you already have a Bone checkpoint, you can use `/scripts/convert-bone-to-miss.py` to convert it into a MiSS checkpoint and proceed with training using MiSS.\n\n## MiSS\n\n[MiSS](https://huggingface.co/papers/2409.15371) MiSS (Matrix Shard Sharing) is a novel Parameter-Efficient Fine-Tuning (PEFT) method designed to address the trade-off between adaptability and efficiency in Large Language Models. The core approach of MiSS involves a simple shard-sharing mechanism. It achieves low-rank adaptation by decomposing a weight matrix into multiple fragments and then utilizing a shared, trainable \u201ccommon fragment.\u201d The final low-rank update matrix is constructed by replicating these shared, partitioned shards. (MiSS is a novel PEFT method that adopts a low-rank structure, requires only a single trainable matrix, and introduces a new update mechanism distinct from LoRA, achieving an excellent balance between performance and efficiency.)\n\n[MiSS: Balancing LoRA Performance and Efficiency with Simple Shard Sharing](https://huggingface.co/papers/2409.15371)\n\nIntuitively, the shape of a single trainable matrix in MiSS is consistent with `lora_B`, so the `r` parameter in MiSS is less than the `r` in LoRA by (`in_feature * r`).\n\nNote: Bat\u2019s r (b) is special and requires that weight W satisfies the conditions `in_features % r == 0` and `out_features % r == 0`. Additionally, when `in_features == out_features` and MiSS-r equals LoRA-r, MiSS\u2019s number of trainable parameters is only half that of LoRA.\n\nAlthough the nonlinear updates of Bat bring some performance improvements, they also increase computational overhead. Its main purpose is to provide researchers with a direction for improvement. Therefore, we recommend fine-tuning the comprehensive MiSS model instead.\n\n[Update on GitHub](https://github.com/huggingface/peft/blob/main/docs/source/conceptual_guides/adapter.md)\n\n[\u2190Fully Sharded Data Parallel](https://huggingface.co/docs/peft/main/en/accelerate/fsdp) [Soft prompts\u2192](https://huggingface.co/docs/peft/main/en/conceptual_guides/prompting)[SEP]", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Soft prompts", "description": "Hugging Face Official Documentation of Soft prompts", "url": "https://huggingface.co/docs/peft/main/en/conceptual_guides/prompting", "source": "hf", "id": "2489d5d0-38e8-470a-99ad-db0e41df1378"}, "page_content": "[CLS]# Soft prompts\n\nTraining large pretrained language models is very time-consuming and compute-intensive. As they continue to grow in size, there is increasing interest in more efficient training methods such as _prompting_. Prompting primes a frozen pretrained model for a specific downstream task by including a text prompt that describes the task or even demonstrates an example of the task. With prompting, you can avoid fully training a separate model for each downstream task, and use the same frozen pretrained model instead. This is a lot easier because you can use the same model for several different tasks, and it is significantly more efficient to train and store a smaller set of prompt parameters than to train all the model\u2019s parameters.\n\nThere are two categories of prompting methods:\n\n- hard prompts are manually handcrafted text prompts with discrete input tokens; the downside is that it requires a lot of effort to create a good prompt\n- soft prompts are learnable tensors concatenated with the input embeddings that can be optimized to a dataset; the downside is that they aren\u2019t human readable because you aren\u2019t matching these \u201cvirtual tokens\u201d to the embeddings of a real word\n\nThis conceptual guide provides a brief overview of the soft prompt methods included in \ud83e\udd17 PEFT: prompt tuning, prefix tuning, P-tuning, and multitask prompt tuning.\n\n## Prompt tuning\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/prompt-tuning.png)\n\nOnly train and store a significantly smaller set of task-specific prompt parameters [(image source)](https://hf.co/papers/2104.08691).\n\n[Prompt tuning](https://hf.co/papers/2104.08691) was developed for text classification tasks on T5 models, and all downstream tasks are cast as a text generation task. For example, sequence classification usually assigns a single class label to a sequence of text. By casting it as a text generation task, the tokens that make up the class label are _generated_. Prompts are added to the input as a series of tokens. Typically, the model parameters are fixed which means the prompt tokens are also fixed by the model parameters.\n\nThe key idea behind prompt tuning is that prompt tokens have their own parameters that are updated independently. This means you can keep the pretrained model\u2019s parameters frozen, and only update the gradients of the prompt token embeddings. The results are comparable to the traditional method of training the entire model, and prompt tuning performance scales as model size increases.\n\nTake a look at [Prompt tuning for causal language modeling](https://huggingface.co/docs/peft/main/en/task_guides/clm-prompt-tuning) for a step-by-step guide on how to train a model with prompt tuning.\n\n## Prefix tuning\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/prefix-tuning.png)\n\nOptimize the prefix parameters for each task [(image source)](https://hf.co/papers/2101.00190).\n\n[Prefix tuning](https://hf.co/papers/2101.00190) was designed for natural language generation (NLG) tasks on GPT models. It is very similar to prompt tuning; prefix tuning also prepends a sequence of task-specific vectors to the input that can be trained and updated while keeping the rest of the pretrained model\u2019s parameters frozen.\n\nThe main difference is that the prefix parameters are inserted in **all** of the model layers, whereas prompt tuning only adds the prompt parameters to the model input embeddings. The prefix parameters are also optimized by a separate feed-forward network (FFN) instead of training directly on the soft prompts because it causes instability and hurts performance. The FFN is discarded after updating the soft prompts.\n\nAs a result, the authors found that prefix tuning demonstrates comparable performance to fully finetuning a model, despite having 1000x fewer parameters, and it performs even better in low-data settings.\n\nTake a look at [Prefix tuning for conditional generation](https://huggingface.co/docs/peft/main/en/task_guides/seq2seq-prefix-tuning) for a step-by-step guide on how to train a model with prefix tuning.\n\n## P-tuning\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/p-tuning.png)\n\nPrompt tokens can be inserted anywhere in the input sequence, and they are optimized by a prompt encoder [(image source)](https://hf.co/papers/2103.10385).\n\n[P-tuning](https://hf.co/papers/2103.10385) is designed for natural language understanding (NLU) tasks and all language models.\nIt is another variation of a soft prompt method; P-tuning also adds a trainable embedding tensor that can be optimized to find better prompts, and it uses a prompt encoder (a bidirectional long-short term memory network or LSTM) to optimize the prompt parameters. Unlike prefix tuning though:\n\n- the prompt tokens can be inserted anywhere in the input sequence, and it isn\u2019t restricted to only the beginning\n- the prompt tokens are only added to the input instead of adding them to every layer of the model\n- introducing _anchor_ tokens can improve performance because they indicate characteristics of a component in the input sequence\n\nThe results suggest that P-t", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Soft prompts", "description": "Hugging Face Official Documentation of Soft prompts", "url": "https://huggingface.co/docs/peft/main/en/conceptual_guides/prompting", "source": "hf", "id": "b95e1e35-dbcc-41c0-934a-ec8b65c8a9c6"}, "page_content": " are optimized by a prompt encoder [(image source)](https://hf.co/papers/2103.10385).\n\n[P-tuning](https://hf.co/papers/2103.10385) is designed for natural language understanding (NLU) tasks and all language models.\nIt is another variation of a soft prompt method; P-tuning also adds a trainable embedding tensor that can be optimized to find better prompts, and it uses a prompt encoder (a bidirectional long-short term memory network or LSTM) to optimize the prompt parameters. Unlike prefix tuning though:\n\n- the prompt tokens can be inserted anywhere in the input sequence, and it isn\u2019t restricted to only the beginning\n- the prompt tokens are only added to the input instead of adding them to every layer of the model\n- introducing _anchor_ tokens can improve performance because they indicate characteristics of a component in the input sequence\n\nThe results suggest that P-tuning is more efficient than manually crafting prompts, and it enables GPT-like models to compete with BERT-like models on NLU tasks.\n\nTake a look at [P-tuning for sequence classification](https://huggingface.co/docs/peft/main/en/task_guides/ptuning-seq-classification) for a step-by-step guide on how to train a model with P-tuning.\n\n## Multitask prompt tuning\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/mpt.png)\n\n[Multitask prompt tuning enables parameter-efficient transfer learning](https://hf.co/papers/2303.02861).\n\n[Multitask prompt tuning (MPT)](https://hf.co/papers/2303.02861) learns a single prompt from data for multiple task types that can be shared for different target tasks. Other existing approaches learn a separate soft prompt for each task that need to be retrieved or aggregated for adaptation to target tasks. MPT consists of two stages:\n\n1. source training - for each task, its soft prompt is decomposed into task-specific vectors. The task-specific vectors are multiplied together to form another matrix W, and the Hadamard product is used between W and a shared prompt matrix P to generate a task-specific prompt matrix. The task-specific prompts are distilled into a single prompt matrix that is shared across all tasks. This prompt is trained with multitask training.\n2. target adaptation - to adapt the single prompt for a target task, a target prompt is initialized and expressed as the Hadamard product of the shared prompt matrix and the task-specific low-rank prompt matrix.\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/mpt-decomposition.png)\n\n[Prompt decomposition](https://hf.co/papers/2103.10385).\n\n## Context-Aware Prompt Tuning (CPT)\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/cpt.png)\n\nCPT optimizing only specific token embeddings while keeping the rest of the model frozen [(image source)](https://huggingface.co/papers/2410.17222).\n\n[Context-Aware Prompt Tuning (CPT)](https://huggingface.co/papers/2410.17222) is designed to enhance few-shot classification by refining only context embeddings.\nThis approach combines ideas from In-Context Learning (ICL), Prompt Tuning (PT), and adversarial optimization, focusing on making model adaptation both parameter-efficient and effective.\nIn CPT, only specific context token embeddings are optimized, while the rest of the model remains frozen.\nTo prevent overfitting and maintain stability, CPT uses controlled perturbations to limit the allowed changes to context embeddings within a defined range.\nAdditionally, to address the phenomenon of recency bias\u2014where examples near the end of the context tend to be prioritized over earlier ones\u2014CPT applies a decay loss factor.\n\nTake a look at [Example](https://github.com/huggingface/peft/blob/main/examples/cpt_finetuning/README.md) for a step-by-step guide on how to train a model with CPT.\n\n[Update on GitHub](https://github.com/huggingface/peft/blob/main/docs/source/conceptual_guides/prompting.md)\n\n[\u2190Adapters](https://huggingface.co/docs/peft/main/en/conceptual_guides/adapter) [IA3\u2192](https://huggingface.co/docs/peft/main/en/conceptual_guides/ia3)[SEP]", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Soft prompts", "description": "Hugging Face Official Documentation of Soft prompts", "url": "https://huggingface.co/docs/peft/main/en/conceptual_guides/prompting", "source": "hf", "id": "5fd488c4-220d-4f5a-98d1-400fac44654d"}, "page_content": "Adapters](https://huggingface.co/docs/peft/main/en/conceptual_guides/adapter) [IA3\u2192](https://huggingface.co/docs/peft/main/en/conceptual_guides/ia3)[SEP]", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "IA3", "description": "Hugging Face Official Documentation of IA3", "url": "https://huggingface.co/docs/peft/main/en/conceptual_guides/ia3", "source": "hf", "id": "69a31de2-7e2a-4a0e-8cde-534df6db13bb"}, "page_content": "[CLS]# IA3\n\nThis conceptual guide gives a brief overview of [IA3](https://huggingface.co/papers/2205.05638), a parameter-efficient fine tuning technique that is\nintended to improve over [LoRA](https://huggingface.co/docs/peft/main/en/conceptual_guides/lora).\n\nTo make fine-tuning more efficient, IA3 (Infused Adapter by Inhibiting and Amplifying Inner Activations)\nrescales inner activations with learned vectors. These learned vectors are injected in the attention and feedforward modules\nin a typical transformer-based architecture. These learned vectors are the only trainable parameters during fine-tuning, and thus the original\nweights remain frozen. Dealing with learned vectors (as opposed to learned low-rank updates to a weight matrix like LoRA)\nkeeps the number of trainable parameters much smaller.\n\nBeing similar to LoRA, IA3 carries many of the same advantages:\n\n- IA3 makes fine-tuning more efficient by drastically reducing the number of trainable parameters. (For T0, an IA3 model only has about 0.01% trainable parameters, while even LoRA has > 0.1%)\n- The original pre-trained weights are kept frozen, which means you can have multiple lightweight and portable IA3 models for various downstream tasks built on top of them.\n- Performance of models fine-tuned using IA3 is comparable to the performance of fully fine-tuned models.\n- IA3 does not add any inference latency because adapter weights can be merged with the base model.\n\nIn principle, IA3 can be applied to any subset of weight matrices in a neural network to reduce the number of trainable\nparameters. Following the authors\u2019 implementation, IA3 weights are added to the key, value and feedforward layers\nof a Transformer model. To be specific, for transformer models, IA3 weights are added to the outputs of key and value layers, and to the input of the second feedforward layer\nin each transformer block.\n\nGiven the target layers for injecting IA3 parameters, the number of trainable parameters\ncan be determined based on the size of the weight matrices.\n\n## Common IA3 parameters in PEFT\n\nAs with other methods supported by PEFT, to fine-tune a model using IA3, you need to:\n\n1. Instantiate a base model.\n2. Create a configuration (`IA3Config`) where you define IA3-specific parameters.\n3. Wrap the base model with `get_peft_model()` to get a trainable `PeftModel`.\n4. Train the `PeftModel` as you normally would train the base model.\n\n`IA3Config` allows you to control how IA3 is applied to the base model through the following parameters:\n\n- `target_modules`: The modules (for example, attention blocks) to apply the IA3 vectors.\n- `feedforward_modules`: The list of modules to be treated as feedforward layers in `target_modules`. While learned vectors are multiplied with\nthe output activation for attention blocks, the vectors are multiplied with the input for classic feedforward layers. Note that `feedforward_modules` must be a subset of `target_modules`.\n- `modules_to_save`: List of modules apart from IA3 layers to be set as trainable and saved in the final checkpoint. These typically include model\u2019s custom head that is randomly initialized for the fine-tuning task.\n\n## Example Usage\n\nFor the task of sequence classification, one can initialize the IA3 config for a Llama model as follows:\n\nCopied\n\n```\npeft_config = IA3Config(\n    task_type=TaskType.SEQ_CLS, target_modules=[\"k_proj\", \"v_proj\", \"down_proj\"], feedforward_modules=[\"down_proj\"]\n)\n```\n\n[Update on GitHub](https://github.com/huggingface/peft/blob/main/docs/source/conceptual_guides/ia3.md)\n\n[\u2190Soft prompts](https://huggingface.co/docs/peft/main/en/conceptual_guides/prompting) [OFT/BOFT\u2192](https://huggingface.co/docs/peft/main/en/conceptual_guides/oft)[SEP]", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Orthogonal Finetuning (OFT and BOFT)", "description": "Hugging Face Official Documentation of Orthogonal Finetuning (OFT and BOFT)", "url": "https://huggingface.co/docs/peft/main/en/conceptual_guides/oft", "source": "hf", "id": "66c633f3-e79e-4e7b-9b90-2a9f925f6d60"}, "page_content": "[CLS]# Orthogonal Finetuning (OFT and BOFT)\n\nThis conceptual guide gives a brief overview of [OFT](https://huggingface.co/papers/2306.07280), [OFTv2](https://huggingface.co/papers/2506.19847) and [BOFT](https://huggingface.co/papers/2311.06243), a parameter-efficient fine-tuning technique that utilizes orthogonal matrix to multiplicatively transform the pretrained weight matrices.\n\nTo achieve efficient fine-tuning, OFT represents the weight updates with an orthogonal transformation. The orthogonal transformation is parameterized by an orthogonal matrix multiplied to the pretrained weight matrix. These new matrices can be trained to adapt to the new data while keeping the overall number of changes low. The original weight matrix remains frozen and doesn\u2019t receive any further adjustments. To produce the final results, both the original and the adapted weights are multiplied togethor.\n\nOrthogonal Butterfly (BOFT) generalizes OFT with Butterfly factorization and further improves its parameter efficiency and finetuning flexibility. In short, OFT can be viewed as a special case of BOFT. Different from LoRA that uses additive low-rank weight updates, BOFT uses multiplicative orthogonal weight updates. The comparison is shown below.\n\n![](https://raw.githubusercontent.com/wy1iu/butterfly-oft/main/assets/BOFT_comparison.png)\n\nBOFT has some advantages compared to LoRA:\n\n- BOFT proposes a simple yet generic way to finetune pretrained models to downstream tasks, yielding a better preservation of pretraining knowledge and a better parameter efficiency.\n- Through the orthogonality, BOFT introduces a structural constraint, i.e., keeping the [hyperspherical energy](https://huggingface.co/papers/1805.09298) unchanged during finetuning. This can effectively reduce the forgetting of pretraining knowledge.\n- BOFT uses the butterfly factorization to efficiently parameterize the orthogonal matrix, which yields a compact yet expressive learning space (i.e., hypothesis class).\n- The sparse matrix decomposition in BOFT brings in additional inductive biases that are beneficial to generalization.\n\nIn principle, BOFT can be applied to any subset of weight matrices in a neural network to reduce the number of trainable parameters. Given the target layers for injecting BOFT parameters, the number of trainable parameters can be determined based on the size of the weight matrices.\n\n## Merge OFT/BOFT weights into the base model\n\nSimilar to LoRA, the weights learned by OFT/BOFT can be integrated into the pretrained weight matrices using the merge\\_and\\_unload() function. This function merges the adapter weights with the base model which allows you to effectively use the newly merged model as a standalone model.\n\n![](https://raw.githubusercontent.com/wy1iu/butterfly-oft/main/assets/boft_merge.png)\n\nThis works because during training, the orthogonal weight matrix (R in the diagram above) and the pretrained weight matrices are separate. But once training is complete, these weights can actually be merged (multiplied) into a new weight matrix that is equivalent.\n\n## Utils for OFT / BOFT\n\n### Common OFT / BOFT parameters in PEFT\n\nAs with other methods supported by PEFT, to fine-tune a model using OFT or BOFT, you need to:\n\n1. Instantiate a base model.\n2. Create a configuration (`OFTConfig` or `BOFTConfig`) where you define OFT/BOFT-specific parameters.\n3. Wrap the base model with `get_peft_model()` to get a trainable `PeftModel`.\n4. Train the `PeftModel` as you normally would train the base model.\n\n### OFT-specific parameters\n\n`OFTConfig` allows you to control how OFT is applied to the base model through the following parameters:\n\n- `r`: OFT rank, number of OFT blocks per injected layer. **Bigger**`r` results in more sparse update matrices with **fewer** trainable paramters. **Note**: You can only specify either `r` or `oft_block_size`, but not both simultaneously, because `r` \u00d7 `oft_block_size` = layer dimension. For simplicity, we let the user speficy either `r` or `oft_block_size` and infer the other one. Default set to `r = 0`, the user is advised to set the `oft_block_size` instead for better clarity.\n- `oft_block_size`: OFT block size across different layers. **Bigger**`oft_block_size` results in more dense update matrices with **more** trainable parameters. **Note**: Please choose `oft_block_size` to be divisible by layer\u2019s input dimension (`in_features`), e.g., 4, 8, 16. You can only specify either `r` or `oft_block_size`, but not both simultaneously, because `r` \u00d7 `oft_block_size` = layer dimension. For simplicity, we let the user speficy either `r` or `oft_block_size` and infer the other one. Default set to `oft_block_size = 32`.\n- `use_cayley_neumann`: Specifies whether to use the Cayley-Neumann parameterization (efficient but approximate) or the vanilla Cayley", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Orthogonal Finetuning (OFT and BOFT)", "description": "Hugging Face Official Documentation of Orthogonal Finetuning (OFT and BOFT)", "url": "https://huggingface.co/docs/peft/main/en/conceptual_guides/oft", "source": "hf", "id": "e40cd740-2a51-4279-8100-1dc6c6babf96"}, "page_content": "_size` instead for better clarity.\n- `oft_block_size`: OFT block size across different layers. **Bigger**`oft_block_size` results in more dense update matrices with **more** trainable parameters. **Note**: Please choose `oft_block_size` to be divisible by layer\u2019s input dimension (`in_features`), e.g., 4, 8, 16. You can only specify either `r` or `oft_block_size`, but not both simultaneously, because `r` \u00d7 `oft_block_size` = layer dimension. For simplicity, we let the user speficy either `r` or `oft_block_size` and infer the other one. Default set to `oft_block_size = 32`.\n- `use_cayley_neumann`: Specifies whether to use the Cayley-Neumann parameterization (efficient but approximate) or the vanilla Cayley parameterization (exact but computationally expensive because of matrix inverse). We recommend to set it to `True` for better efficiency, but performance may be slightly worse because of the approximation error. Please test both settings (`True` and `False`) depending on your needs. Default is `False`.\n- `module_dropout`: The multiplicative dropout probability, by setting OFT blocks to identity during training, similar to the dropout layer in LoRA.\n- `bias`: specify if the `bias` parameters should be trained. Can be `\"none\"`, `\"all\"` or `\"oft_only\"`.\n- `target_modules`: The modules (for example, attention blocks) to inject the OFT matrices.\n- `modules_to_save`: List of modules apart from OFT matrices to be set as trainable and saved in the final checkpoint. These typically include model\u2019s custom head that is randomly initialized for the fine-tuning task.\n\n### BOFT-specific parameters\n\n`BOFTConfig` allows you to control how BOFT is applied to the base model through the following parameters:\n\n- `boft_block_size`: the BOFT matrix block size across different layers, expressed in `int`. **Bigger**`boft_block_size` results in more dense update matrices with **more** trainable parameters. **Note**, please choose `boft_block_size` to be divisible by most layer\u2019s input dimension (`in_features`), e.g., 4, 8, 16. Also, please only\nspecify either `boft_block_size` or `boft_block_num`, but not both simultaneously or leaving both to 0, because `boft_block_size` x `boft_block_num` must equal the layer\u2019s input dimension.\n- `boft_block_num`: the number of BOFT matrix blocks across different layers, expressed in `int`. **Bigger**`boft_block_num` result in sparser update matrices with **fewer** trainable parameters. **Note**, please choose `boft_block_num` to be divisible by most layer\u2019s input dimension (`in_features`), e.g., 4, 8, 16. Also, please only\nspecify either `boft_block_size` or `boft_block_num`, but not both simultaneously or leaving both to 0, because `boft_block_size` x `boft_block_num` must equal the layer\u2019s input dimension.\n- `boft_n_butterfly_factor`: the number of butterfly factors. **Note**, for `boft_n_butterfly_factor=1`, BOFT is the same as vanilla OFT, for `boft_n_butterfly_factor=2`, the effective block size of OFT becomes twice as big and the number of blocks become half.\n- `bias`: specify if the `bias` parameters should be trained. Can be `\"none\"`, `\"all\"` or `\"boft_only\"`.\n- `boft_dropout`: specify the probability of multiplicative dropout.\n- `target_modules`: The modules (for example, attention blocks) to inject the OFT/BOFT matrices.\n- `modules_to_save`: List of modules apart from OFT/BOFT matrices to be set as trainable and saved in the final checkpoint. These typically include model\u2019s custom head that is randomly initialized for the fine-tuning task.\n\n## OFT Example Usage\n\nFor using OFT for quantized finetuning with [TRL](https://github.com/huggingface/trl) for `SFT`, `PPO`, or `DPO` fine-tuning, follow the following outline:\n\nCopied\n\n```\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom trl import SFTTrainer\nfrom peft import OFTConfig\n\nif use_quantization:\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=torch.bfloat16,\n        bnb_4bit_use_double_quant=True,\n        bnb_4bit_quant_storage=torch.bfloat16,\n    )\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"model_name\",\n    quantization_config=bnb_", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Orthogonal Finetuning (OFT and BOFT)", "description": "Hugging Face Official Documentation of Orthogonal Finetuning (OFT and BOFT)", "url": "https://huggingface.co/docs/peft/main/en/conceptual_guides/oft", "source": "hf", "id": "94f74ba5-a1c2-4543-9c6a-3dde309b5d2e"}, "page_content": "O`, or `DPO` fine-tuning, follow the following outline:\n\nCopied\n\n```\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom trl import SFTTrainer\nfrom peft import OFTConfig\n\nif use_quantization:\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=torch.bfloat16,\n        bnb_4bit_use_double_quant=True,\n        bnb_4bit_quant_storage=torch.bfloat16,\n    )\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"model_name\",\n    quantization_config=bnb_config\n)\ntokenizer = AutoTokenizer.from_pretrained(\"model_name\")\n\n# Configure OFT\npeft_config = OFTConfig(\n    oft_block_size=32,\n    use_cayley_neumann=True,\n    target_modules=\"all-linear\",\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=ds['train'],\n    peft_config=peft_config,\n    processing_class=tokenizer,\n    args=training_arguments,\n    data_collator=collator,\n)\n\ntrainer.train()\n```\n\n## BOFT Example Usage\n\nFor an example of the BOFT method application to various downstream tasks, please refer to the following guides:\n\nTake a look at the following step-by-step guides on how to finetune a model with BOFT:\n\n- [Dreambooth finetuning with BOFT](https://github.com/huggingface/peft/blob/main/examples/boft_dreambooth/boft_dreambooth.md)\n- [Controllable generation finetuning with BOFT (ControlNet)](https://github.com/huggingface/peft/blob/main/examples/boft_controlnet/boft_controlnet.md)\n\nFor the task of image classification, one can initialize the BOFT config for a DinoV2 model as follows:\n\nCopied\n\n```\nimport transformers\nfrom transformers import AutoModelForSeq2SeqLM, BOFTConfig\nfrom peft import BOFTConfig, get_peft_model\n\nconfig = BOFTConfig(\n    boft_block_size=4,\n    boft_n_butterfly_factor=2,\n    target_modules=[\"query\", \"value\", \"key\", \"output.dense\", \"mlp.fc1\", \"mlp.fc2\"],\n    boft_dropout=0.1,\n    bias=\"boft_only\",\n    modules_to_save=[\"classifier\"],\n)\n\nmodel = transformers.Dinov2ForImageClassification.from_pretrained(\n    \"facebook/dinov2-large\",\n    num_labels=100,\n)\n\nboft_model = get_peft_model(model, config)\n```\n\n[Update on GitHub](https://github.com/huggingface/peft/blob/main/docs/source/conceptual_guides/oft.md)\n\n[\u2190IA3](https://huggingface.co/docs/peft/main/en/conceptual_guides/ia3) [AutoPeftModel\u2192](https://huggingface.co/docs/peft/main/en/package_reference/auto_class)[SEP]", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities", "description": "Arxiv Paper titled 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities' authored by Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid", "url": "https://arxiv.org/pdf/2408.13296v1", "source": "arxiv", "id": "05f31571-47a3-411a-af46-723e727fc940"}, "page_content": "[CLS]This technical report thoroughly examines the process of fine-tuning Large Language Models (LLMs),\nintegrating theoretical insights and practical applications. It begins by tracing the historical develop-\nment of LLMs, emphasising their evolution from traditional Natural Language Processing (NLP) models\nand their pivotal role in modern AI systems. The analysis differentiates between various fine-tuning\nmethodologies, including supervised, unsupervised, and instruction-based approaches, underscoring their\nrespective implications for specific tasks.\nA structured seven-stage pipeline for LLM fine-tuning is introduced, covering the complete lifecycle\nfrom data preparation to model deployment.\nKey considerations include data collection strategies,\nhandling of imbalanced datasets, model initialisation, and optimisation techniques, with a particular\nfocus on hyperparameter tuning.\nThe report also highlights parameter-efficient fine-tuning methods\nsuch as Low-Rank Adaptation (LoRA) and Half Fine-Tuning, which balance resource constraints with\noptimal model performance.\nThe exploration extends to advanced fine-tuning techniques and configurations like memory fine-\ntuning, Mixture of Experts (MoE) and Mixture of Agents (MoA), demonstrating how these methods\nharness specialised networks and multi-agent collaboration for improved outcomes.\nProximal Policy\nOptimisation (PPO) and Direct Preference Optimisation (DPO) are discussed as innovative approaches\nto aligning models with human preferences, while the benefits of pruning and routing optimisations are\nexamined for enhancing efficiency.\nIn the latter sections, the report delves into validation frameworks, post-deployment monitoring, and\noptimisation techniques for inference. It also addresses the deployment of LLMs on distributed and\ncloud-based platforms. Additionally, cutting-edge topics such as multimodal LLMs and fine-tuning for\naudio and speech processing are covered, alongside emerging challenges related to scalability, privacy,\nand accountability.\nThis report aims to serve as a comprehensive guide for researchers and practitioners, offering action-\nable insights into fine-tuning LLMs while navigating the challenges and opportunities inherent in this\nrapidly evolving field.\nContents\n1\nIntroduction\n6\n1.1\nBackground of Large Language Models (LLMs)........................\n6\n1.2\nHistorical Development and Key Milestones..........................\n6\n1.3\nEvolution from Traditional NLP Models to State-of-the-Art LLMs\n.............\n6\n1.3.1\nStatistical Language Models (SLMs)..........................\n6\n1.3.2\nNeural Language Models (NLMs)............................\n7\n1.3.3\nPre-trained Language Models (PLMs).........................\n7\n1.3.4\nLarge Language Models (LLMs)\n............................\n7\n1.4\nOverview of Current Leading LLMs\n..............................\n8\n1.5\nWhat is Fine-Tuning?\n......................................\n8\n1.6\nTypes of LLM Fine-Tuning\n...................................\n9\n1.6.1\nUnsupervised Fine-Tuning................................\n9\n1.6.2\nSupervised Fine-Tuning (SFT).............................\n9\n1.6.3\nInstruction Fine-Tuning via Prompt Engineering...................\n10\n1.7\nPre-training vs Fine-tuning...................................\n10\n1.8\nImportance of Fine-Tuning LLMs................................\n10\n1.9\nRetrieval Augmented Generation (RAG)............................\n11\n1.9.1\nTraditional RAG Pipeline and Steps.....", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities", "description": "Arxiv Paper titled 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities' authored by Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid", "url": "https://arxiv.org/pdf/2408.13296v1", "source": "arxiv", "id": "d7e2b91c-835c-49d2-a8a6-3c2d0016c0d2"}, "page_content": "1.6.3\nInstruction Fine-Tuning via Prompt Engineering...................\n10\n1.7\nPre-training vs Fine-tuning...................................\n10\n1.8\nImportance of Fine-Tuning LLMs................................\n10\n1.9\nRetrieval Augmented Generation (RAG)............................\n11\n1.9.1\nTraditional RAG Pipeline and Steps..........................\n11\n1.9.2\nBenefits of Using RAG..................................\n12\n1.9.3\nChallenges and Considerations in Serving RAG....................\n12\n1.9.4\nUse Cases and Examples.................................\n12\n1.9.5\nConsiderations for Choosing Between RAG and Fine-Tuning\n............\n12\n1.10 Objectives of the Report.....................................\n13\n1.10.1 Goals and Scope.....................................\n13\n1.10.2 Key Questions and Issues Addressed..........................\n13\n1.10.3 Overview of the Report Structure............................\n13\n2\nSeven Stage Fine-Tuning Pipeline for LLM\n14\n2.1\nStage 1: Dataset Preparation..................................\n14\n2.2\nStage 2: Model Initialisation\n..................................\n14\n2.3\nStage 3: Training Environment Setup\n.............................\n14\n2.4\nStage 4: Partial or Full Fine-Tuning\n..............................\n15\n2.5\nStage 5: Evaluation and Validation...............................\n15\n2.6\nStage 6: Deployment.......................................\n16\n2.7\nStage 7: Monitoring and Maintenance\n.............................\n16\n3\nStage 1: Data Preparation\n17\n3.1\nSteps Involved in Data Preparation...............................\n17\n3.1.1\nData Collection......................................\n17\n3.1.2\nData Preprocessing and Formatting\n..........................\n17\n3.1.3\nHandling Data Imbalance................................\n17\n3.1.4\nSplitting Dataset.....................................\n19\n3.2\nExisting and Potential Research Methodologies........................\n19\n3.2.", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities", "description": "Arxiv Paper titled 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities' authored by Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid", "url": "https://arxiv.org/pdf/2408.13296v1", "source": "arxiv", "id": "7a23da09-da1f-4391-ae41-3eac4c421ec7"}, "page_content": "...............\n17\n3.1.2\nData Preprocessing and Formatting\n..........................\n17\n3.1.3\nHandling Data Imbalance................................\n17\n3.1.4\nSplitting Dataset.....................................\n19\n3.2\nExisting and Potential Research Methodologies........................\n19\n3.2.1\nData Annotation.....................................\n19\n3.2.2\nData Augmentation\n...................................\n20\n3.2.3\nSynthetic Data Generation using LLMs\n........................\n20\n3.3\nChallenges in Data Preparation for Fine-Tuning LLMs....................\n20\n1\n3.4\nAvailable LLM Fine-Tuning Datasets..............................\n21\n3.5\nBest Practices...........................................\n21\n3.5.1\nHigh-Quality Data Collection..............................\n21\n3.5.2\nEffective Data Preprocessing\n..............................\n21\n3.5.3\nManaging Data Imbalance................................\n21\n3.5.4\nAugmenting and Annotating Data...........................\n21\n3.5.5\nEthical Data Handling..................................\n21\n3.5.6\nRegular Evaluation and Iteration............................\n21\n4\nStage 2: Model Initialisation\n22\n4.1\nSteps Involved in Model Initialisation\n.............................\n22\n4.2\nTools and Libraries for Model Initialisation\n..........................\n23\n4.3\nChallenges in Model Initialisation................................\n24\n4.4\nTutorials..............................................\n24\n5\nStage 3: Training Setup\n26\n5.1\nSteps Involved in Training Setup................................\n26\n5.2\nSetting up Training Environment................................\n26\n5.3\nDefining Hyperparameters....................................\n27\n5.3.1\nMethods for Hyperparameter Tuning..........................\n27\n5.4\nInitialising Optimisers and Loss Functions...........................\n28\n5.4.1\nGradient Descent................", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities", "description": "Arxiv Paper titled 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities' authored by Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid", "url": "https://arxiv.org/pdf/2408.13296v1", "source": "arxiv", "id": "5bee8733-cf13-4081-8567-12cfc5b71e70"}, "page_content": "\n26\n5.2\nSetting up Training Environment................................\n26\n5.3\nDefining Hyperparameters....................................\n27\n5.3.1\nMethods for Hyperparameter Tuning..........................\n27\n5.4\nInitialising Optimisers and Loss Functions...........................\n28\n5.4.1\nGradient Descent.....................................\n28\n5.4.2\nStochastic Gradient Descent (SGD)\n..........................\n28\n5.4.3\nMini-batch Gradient Descent..............................\n29\n5.4.4\nAdaGrad\n.........................................\n29\n5.4.5\nRMSprop.........................................\n30\n5.4.6\nAdaDelta.........................................\n30\n5.4.7\nAdam...........................................\n31\n5.4.8\nAdamW..........................................\n31\n5.5\nChallenges in Training Setup\n..................................\n32\n5.6\nBest Practices...........................................\n32\n6\nStage 4: Selection of Fine-Tuning Techniques and Appropriate Model Configurations 34\n6.1\nSteps Involved in Fine-Tuning..................................\n34\n6.2\nFine-Tuning Strategies for LLMs................................\n35\n6.2.1\nTask-Specific Fine-Tuning................................\n35\n6.2.2\nDomain-Specific Fine-Tuning..............................\n35\n6.3\nParameter-Efficient Fine-Tuning (PEFT) Techniques.....................\n37\n6.3.1\nAdapters\n.........................................\n37\n6.3.2\nLow-Rank Adaptation (LoRA).............................\n38\n6.3.3\nQLoRA..........................................\n40\n6.3.4\nWeight-Decomposed Low-Rank Adaptation (DoRA).................\n41\n6.3.5\nFine-Tuning with Multiple Adapters..........................\n43\n6.4\nHalf Fine Tuning................", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities", "description": "Arxiv Paper titled 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities' authored by Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid", "url": "https://arxiv.org/pdf/2408.13296v1", "source": "arxiv", "id": "0570bf42-e501-4934-bbc3-43535a2eaca1"}, "page_content": "Rank Adaptation (LoRA).............................\n38\n6.3.3\nQLoRA..........................................\n40\n6.3.4\nWeight-Decomposed Low-Rank Adaptation (DoRA).................\n41\n6.3.5\nFine-Tuning with Multiple Adapters..........................\n43\n6.4\nHalf Fine Tuning.........................................\n45\n6.4.1\nBenefits of using Half Fine tuning\n...........................\n45\n6.4.2\nComparison between HFT and LoRA\n.........................\n46\n6.5\nLamini Memory Tuning.....................................\n47\n6.5.1\nLamini-1 - A model architecture based on Lamini...................\n47\n6.6\nMixture of Experts........................................\n48\n6.6.1\nMixtral 8x7B Architecture and Performance......................\n48\n6.7\nMixture of Agents\n........................................\n49\n6.7.1\nMethodology.......................................\n49\n6.7.2\nAnalogy with MoE....................................\n50\n6.7.3\nWhat makes MoA works well?\n.............................\n50\n6.8\nProximal Policy Optimisation (PPO)..............................\n50\n6.8.1\nBenefits of PPO\n.....................................\n51\n6.8.2\nLimitations of PPO\n...................................\n52\n6.8.3\nTutorial for training models using PPO technique\n..................\n52\n6.9\nDirect Preference Optimisation (DPO).............................\n52\n2\n6.9.1\nBenefits of DPO\n.....................................\n53\n6.9.2\nBest Practices for DPO\n.................................\n53\n6.9.3\nTutorial for training models using DPO technique\n..................\n53\n6.9.4\nIs DPO Superior to PPO for LLM Alignment?\n....................\n53\n6.10 Optimised Routing and Pruning Operations (ORPO).....................\n54\n6.10.1 When to Prune AI Models?...............................\n54\n6", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities", "description": "Arxiv Paper titled 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities' authored by Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid", "url": "https://arxiv.org/pdf/2408.13296v1", "source": "arxiv", "id": "d57c967f-6339-4e65-aa65-d87ba236d396"}, "page_content": "...............................\n53\n6.9.3\nTutorial for training models using DPO technique\n..................\n53\n6.9.4\nIs DPO Superior to PPO for LLM Alignment?\n....................\n53\n6.10 Optimised Routing and Pruning Operations (ORPO).....................\n54\n6.10.1 When to Prune AI Models?...............................\n54\n6.10.2 Benefits of Pruning....................................\n55\n6.10.3 Challenges of Pruning..................................\n55\n7\nStage 5: Evaluation and Validation\n56\n7.1\nSteps Involved in Evaluating and Validating Fine-Tuned Models\n..............\n56\n7.2\nSetting Up Evaluation Metrics\n.................................\n56\n7.2.1\nImportance of Cross-Entropy for LLM Training and Evaluation...........\n56\n7.2.2\nBeyond Cross-Entropy: Advanced LLM Evaluation Metrics.............\n56\n7.3\nUnderstanding the Training Loss Curve\n............................\n57\n7.3.1\nInterpreting Loss Curves.................................\n58\n7.3.2\nAvoiding Overfitting...................................\n58\n7.3.3\nSources of Noisy Gradients\n...............................\n59\n7.4\nRunning Validation Loops....................................\n59\n7.5\nMonitoring and Interpreting Results..............................\n59\n7.6\nHyperparameter Tuning and Other Adjustments\n.......................\n59\n7.6.1\nData Size and Quality..................................\n59\n7.7\nBenchmarking Fine-Tuned LLMs................................\n60\n7.8\nEvaluating Fine-Tuned LLMs on Safety Benchmark......................\n61\n7.9\nEvaluating Safety of Fine-Tuned LLM using AI Models\n...................\n61\n7.9.1\nLlama Guard.......................................\n61\n7.9.2\nShield Gemma\n......................................\n62\n7.9.3\nWILDGUARD......................................\n62\n8\nStage 6: Deployment\n64\n8.1\nSteps Involved in Deploying the Fine-Tuned Model......................\n64\n8.2\nCloud-Based Providers for LLM Deployment\n.........................\n64\n", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities", "description": "Arxiv Paper titled 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities' authored by Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid", "url": "https://arxiv.org/pdf/2408.13296v1", "source": "arxiv", "id": "a6529e1a-9d4d-4ebf-9716-f73a536bddcc"}, "page_content": "\n7.9.2\nShield Gemma\n......................................\n62\n7.9.3\nWILDGUARD......................................\n62\n8\nStage 6: Deployment\n64\n8.1\nSteps Involved in Deploying the Fine-Tuned Model......................\n64\n8.2\nCloud-Based Providers for LLM Deployment\n.........................\n64\n8.3\nTechniques for Optimising Model Performance During Inference...............\n65\n8.3.1\nTraditional On-Premises GPU-Based Deployments..................\n65\n8.3.2\nDistributed LLM: Torrent-Style Deployment and Parallel Forward Passes\n.....\n66\n8.3.3\nWebGPU-Based Deployment of LLM..........................\n67\n8.3.4\nLLM on WebGPU using WebLLM...........................\n67\n8.3.5\nQuantised LLMs.....................................\n69\n8.3.6\nvLLMs...........................................\n69\n8.4\nKey Considerations for Deployment of LLMs\n.........................\n69\n9\nStage 7: Monitoring and Maintenance\n71\n9.1\nSteps Involved in Monitoring and Maintenance of Deployed Fine-Tuned LLMs.......\n71\n9.2\nContinuous Monitoring of Model Performance.........................\n72\n9.2.1\nFunctional Monitoring..................................\n72\n9.2.2\nPrompt Monitoring....................................\n72\n9.2.3\nResponse Monitoring...................................\n72\n9.2.4\nAlerting Mechanisms and Thresholds..........................\n72\n9.2.5\nMonitoring User Interface (UI).............................\n73\n9.3\nUpdating LLM Knowledge....................................\n73\n9.3.1\nRetraining Methods\n...................................\n73\n9.3.2\nAdditional Methods\n...................................\n73\n9.3.3\nKey Considerations....................................\n73\n9.4\nThe Future of LLM Updates\n..................................\n74\n3\n10 Industrial Fine-Tuning Platforms and Frameworks for LLMs\n75\n10.1 Autotrain.............................................\n77\n10.1.1 Steps Involved in Fine-Tuning Using Autotrain....................\n77\n", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities", "description": "Arxiv Paper titled 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities' authored by Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid", "url": "https://arxiv.org/pdf/2408.13296v1", "source": "arxiv", "id": "92fb4804-4a3a-4f81-a09b-b5dce3026e69"}, "page_content": "...................................\n73\n9.4\nThe Future of LLM Updates\n..................................\n74\n3\n10 Industrial Fine-Tuning Platforms and Frameworks for LLMs\n75\n10.1 Autotrain.............................................\n77\n10.1.1 Steps Involved in Fine-Tuning Using Autotrain....................\n77\n10.1.2 Best Practices of Using Autotrain\n...........................\n79\n10.1.3 Challenges of Using Autotrain\n.............................\n79\n10.1.4 When to Use Autotrain.................................\n79\n10.1.5 Tutorials\n.........................................\n79\n10.2 Transformers Library and Trainer API.............................\n79\n10.2.1 Limitations of the Transformers Library and Trainer API..............\n80\n10.3 Optimum: Enhancing LLM Deployment Efficiency......................\n80\n10.3.1 Best Practices of Using Optimum............................\n81\n10.3.2 Tutorials\n.........................................\n81\n10.4 Amazon SageMaker JumpStart.................................\n81\n10.4.1 Steps Involved in Using JumpStart...........................\n81\n10.4.2 Best Practices for Using JumpStart\n..........................\n82\n10.4.3 Limitations of Using JumpStart.............................\n83\n10.4.4 Tutorials\n.........................................\n83\n10.5 Amazon Bedrock.........................................\n83\n10.5.1 Steps Involved in Using Amazon Bedrock.......................\n83\n10.5.2 Limitations of Using Amazon Bedrock.........................\n84\n10.5.3 Tutorials\n.........................................\n84\n10.6 OpenAI\u2019s Fine-Tuning API\n...................................\n84\n10.6.1 Steps Involved in Using OpenAI\u2019s Fine-Tuning API\n.................\n84\n10.6.2 Limitations of OpenAI\u2019s Fine-Tuning API.......................\n85\n10.6.3 Tutorials\n.........................................\n85\n10.7 NVID", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities", "description": "Arxiv Paper titled 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities' authored by Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid", "url": "https://arxiv.org/pdf/2408.13296v1", "source": "arxiv", "id": "f5d28a03-2ff4-41a3-8223-4ea1cde22e4b"}, "page_content": ".....\n84\n10.6 OpenAI\u2019s Fine-Tuning API\n...................................\n84\n10.6.1 Steps Involved in Using OpenAI\u2019s Fine-Tuning API\n.................\n84\n10.6.2 Limitations of OpenAI\u2019s Fine-Tuning API.......................\n85\n10.6.3 Tutorials\n.........................................\n85\n10.7 NVIDIA NeMo Customizer\n...................................\n85\n10.7.1 Key Features of NVIDIA NeMo.............................\n86\n10.7.2 Components of NVIDIA NeMo.............................\n86\n10.7.3 Customising Large Language Models (LLMs).....................\n86\n10.7.4 Tutorials\n.........................................\n87\n11 Multimodal LLMs and their Fine-tuning\n88\n11.1 Vision Language Model (VLMs)\n................................\n89\n11.1.1 Architecture\n.......................................\n89\n11.1.2 Contrastive Learning...................................\n89\n11.2 Fine-tuning of multimodal models\n...............................\n90\n11.2.1 Full-parameter Fine-Tuning...............................\n90\n11.2.2 Case study of fine-tuning MLLMs for Medical domain................\n91\n11.3 Applications of Multimodal models...............................\n92\n11.4 Audio or Speech LLMs Or Large Audio Models........................\n92\n11.4.1 Tokenization and Preprocessing.............................\n94\n11.4.2 Fine-Tuning Techniques.................................\n94\n11.4.3 Fine-Tuning Whisper for Automatic Speech Recognition (ASR)...........\n94\n11.4.4 Case Studies and Applications\n.............................\n95\n12 Open Challenges and Research Directions\n96\n12.1 Scalability Issues.........................................\n96\n12.1.1 Challenges in Scaling Fine-Tuning Processes......................\n96\n12.1.2 Research Directions for Scalable Solutions.......................\n97\n12.1.3 Hardware and Algorithm Co-Design..........................\n98\n12.2 Ethical Considerations in Fine-Tuning LLMs\n.........................\n98\n12.2.1 Bias and Fairness......", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities", "description": "Arxiv Paper titled 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities' authored by Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid", "url": "https://arxiv.org/pdf/2408.13296v1", "source": "arxiv", "id": "82070b55-1b5c-44f3-b4ad-e75546adc55c"}, "page_content": "..................\n96\n12.1.1 Challenges in Scaling Fine-Tuning Processes......................\n96\n12.1.2 Research Directions for Scalable Solutions.......................\n97\n12.1.3 Hardware and Algorithm Co-Design..........................\n98\n12.2 Ethical Considerations in Fine-Tuning LLMs\n.........................\n98\n12.2.1 Bias and Fairness.....................................\n98\n12.2.2 Privacy Concerns.....................................\n99\n12.2.3 Security Risks\n......................................\n99\n12.3 Accountability and Transparency................................ 100\n12.3.1 The Need for Accountability and Transparency.................... 100\n12.3.2 Recent Research and Industry Practices........................ 100\n12.3.3 Promoting Accountability and Transparency\n..................... 100\n4\n12.3.4 Proposed frameworks/techniques for Ethical Fine-Tuning\n.............. 100\n12.4 Integration with Emerging Technologies............................ 101\n12.4.1 Opportunities....................................... 101\n12.4.2 Challenges\n........................................ 102\n12.5 Future Research Areas...................................... 102\nGlossary\n103\n5\nChapter 1\nIntroduction\n1.1\nBackground of Large Language Models (LLMs)\nLarge Language Models (LLMs) represent a significant leap in computational systems capable of under-\nstanding and generating human language. Building on traditional language models (LMs) like N-gram\nmodels [1], LLMs address limitations such as rare word handling, overfitting, and capturing complex\nlinguistic patterns. Notable examples, such as GPT-3 and GPT-4 [2], leverage the self-attention mecha-\nnism within Transformer architectures to efficiently manage sequential data and understand long-range\ndependencies. Key advancements include in-context learning for generating coherent text from prompts\nand Reinforcement Learning from Human Feedback (RLHF) [3] for refining models using human re-\nsponses. Techniques like prompt engineering, question-answering, and conversational interactions have\nsignificantly advanced the field of natural language processing (NLP) [4].\n1.2\nHistorical Development and Key Milestones\nLanguage models are fundamental to natural language processing (NLP), leveraging mathematical tech-\nniques to generalise linguistic rules and knowledge for tasks involving prediction and generation. Over\nseveral decades, language modelling has evolved from early statistical language models (SLMs) to to-\nday\u2019s advanced large language models (LLMs). This rapid advancement has enabled LLMs to process,\ncomprehend, and generate text at a level comparable to human capabilities [5, 6].\nFigure 1.1 shows the evolution of large language models from early statistical approaches to current\nadvanced models.\n1.3\nEvolution from Traditional NLP Models to State-of-the-Art\nLLMs\nUnderstanding LLMs requires tracing the development of language models through stages such as Statis-\ntical Language Models (SLMs), Neural Language Models (NLMs), Pre-trained Language Models (PLMs),\nand LLMs.\n1.3.1\nStatistical Language Models (SLMs)\nEmerging in the 1990s, SLMs analyse natural language using probabilistic methods to determine the\nlikelihood of sentences within texts.\nFor instance, the probability P(S) of the sentence \u201cI am very\nhappy\u201d is given by:\nP(S) = P(\u03c91, \u03c92,", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities", "description": "Arxiv Paper titled 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities' authored by Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid", "url": "https://arxiv.org/pdf/2408.13296v1", "source": "arxiv", "id": "56770811-82af-4b16-acba-2ca536d14be3"}, "page_content": " has enabled LLMs to process,\ncomprehend, and generate text at a level comparable to human capabilities [5, 6].\nFigure 1.1 shows the evolution of large language models from early statistical approaches to current\nadvanced models.\n1.3\nEvolution from Traditional NLP Models to State-of-the-Art\nLLMs\nUnderstanding LLMs requires tracing the development of language models through stages such as Statis-\ntical Language Models (SLMs), Neural Language Models (NLMs), Pre-trained Language Models (PLMs),\nand LLMs.\n1.3.1\nStatistical Language Models (SLMs)\nEmerging in the 1990s, SLMs analyse natural language using probabilistic methods to determine the\nlikelihood of sentences within texts.\nFor instance, the probability P(S) of the sentence \u201cI am very\nhappy\u201d is given by:\nP(S) = P(\u03c91, \u03c92, \u03c93, \u03c94) = P(I, am, very, happy)\n(1.1)\nThis probability can be calculated using conditional probabilities:\nP(I, am, very, happy) = P(I) \u00b7 P(am | I) \u00b7 P(very | I, am) \u00b7 P(happy | I, am, very)\n(1.2)\nConditional probabilities are estimated using Maximum Likelihood Estimation (MLE):\n6\nFigure 1.1: A chronological timeline showcasing the evolution of Large Language Models (LLMs) from\n1990 to 2023. This progression begins with early statistical models such as N-grams, transitions through\nneural language models like Word2Vec and RNN/LSTM, and advances into the era of pre-trained mod-\nels with the introduction of transformers and attention mechanisms. The figure highlights significant\nmilestones, including the development of BERT, GPT series, and recent innovations such as GPT-4 and\nChatGPT, demonstrating the rapid advancements in LLM technology over time. (adapted from [6])\nP(\u03c9i | \u03c91\u03c92 \u00b7 \u00b7 \u00b7 \u03c9i\u22121) =\nC(\u03c91\u03c92 \u00b7 \u00b7 \u00b7 \u03c9i)\nC(\u03c91\u03c92 \u00b7 \u00b7 \u00b7 \u03c9i\u22121)\n(1.3)\n1.3.2\nNeural Language Models (NLMs)\nNLMs leverage neural networks to predict word sequences, overcoming SLM limitations. Word vectors\nenable computers to understand word meanings. Tools like Word2Vec [7] represent words in a vector\nspace where semantic relationships are reflected in vector angles. NLMs consist of interconnected neurons\norganised into layers, resembling the human brain\u2019s structure. The input layer concatenates word vectors,\nthe hidden layer applies a non-linear activation function, and the output layer predicts subsequent words\nusing the Softmax function to transform values into a probability distribution.\nFigure 1.2 illustrates the structure of Neural Language Models, highlighting the layers and connections\nused to predict subsequent words.\n1.3.3\nPre-trained Language Models (PLMs)\nPLMs are initially trained on extensive volumes of unlabelled text to understand fundamental language\nstructures (pre-training). They are then fine-tuned on a smaller, task-specific dataset. This \u201dpre-training\nand fine-tuning\u201d paradigm, exemplified by GPT-2 [8] and BERT [9], has led to diverse and effective model\narchitectures.\n1.3.4\nLarge Language Models (LLMs)\nLLMs like GPT-3, GPT-4, PaLM [10], and LLaMA [11] are trained on massive text corpora with tens of\nbillions of parameters. LLMs undergo a two-stage process: initial pre-training on a vast corpus followed\n7\nFigure 1.2: A schematic representation of Neural Language Models, showcasing the layered architecture\nwhere the input layer processes sequential data, the hidden layer captures dependencies, and the output\nlayer generates predictions. The figure emphasises the flow of information through concatenation and\nmatrix multiplications, culminating in a probability distribution via the softmax function. (adopted from\n[6])\nby alignment with human values. This approach enables LLMs to understand human commands and\nvalues better.\n1.4\nOverview of Current Leading LLMs\nLLMs are powerful tools in NLP, capable of performing tasks such as translation, summarisation, and\nconversational interaction. Advances in transformer architectures, computational power, and extensive\ndatasets have driven their success. These models approximate human-level performance, making them\ninvaluable for research and practical implementations. LLMs\u2019 rapid development has spurred research\ninto architectural innovations, training strategies, extending context lengths, fine-tuning techniques, and\nintegrating multi-modal data. Their applications extend beyond NLP, aiding in human-robot interactions\nand creating intuitive AI systems. This highlights the importance of comprehensive reviews consolidating\nthe latest developments [12].\nFigure 1.3 provides an overview of current leading LLMs, highlighting their capabilities and applications.\n1.5\nWhat is Fine-Tuning?\nFine-tuning uses a pre-trained model, such as OpenAI\u2019s GPT series, as a foundation.\nThe process\ninvolves further training on a smaller, domain-specific dataset. This approach builds upon the model\u2019s\npre-existing knowledge, enhancing performance on specific tasks with reduced data and computational\nrequirements.\nFine-tuning transfers the pre-trained model\u2019s learned patterns and features to new tasks, improving\nperformance and reducing training data needs. It", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities", "description": "Arxiv Paper titled 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities' authored by Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid", "url": "https://arxiv.org/pdf/2408.13296v1", "source": "arxiv", "id": "592eedf4-1dbe-4d90-a3cf-dcec6fa81a0d"}, "page_content": "urred research\ninto architectural innovations, training strategies, extending context lengths, fine-tuning techniques, and\nintegrating multi-modal data. Their applications extend beyond NLP, aiding in human-robot interactions\nand creating intuitive AI systems. This highlights the importance of comprehensive reviews consolidating\nthe latest developments [12].\nFigure 1.3 provides an overview of current leading LLMs, highlighting their capabilities and applications.\n1.5\nWhat is Fine-Tuning?\nFine-tuning uses a pre-trained model, such as OpenAI\u2019s GPT series, as a foundation.\nThe process\ninvolves further training on a smaller, domain-specific dataset. This approach builds upon the model\u2019s\npre-existing knowledge, enhancing performance on specific tasks with reduced data and computational\nrequirements.\nFine-tuning transfers the pre-trained model\u2019s learned patterns and features to new tasks, improving\nperformance and reducing training data needs. It has become popular in NLP for tasks like text classi-\nfication, sentiment analysis, and question-answering.\n8\nFigure 1.3: Mind map depicting various dimensions of Large Language Models (LLMs), covering aspects\nfrom pre-training and fine-tuning methodologies to efficiency, evaluation, inference, and application do-\nmains. Each dimension is linked to specific techniques, challenges, and examples of models that exemplify\nthe discussed characteristics. This diagram serves as an overview of the multifaceted considerations in\nthe development and deployment of LLMs. (adapted from [13])\n1.6\nTypes of LLM Fine-Tuning\n1.6.1\nUnsupervised Fine-Tuning\nThis method does not require labelled data. Instead, the LLM is exposed to a large corpus of unla-\nbelled text from the target domain, refining its understanding of language. This approach is useful for\nnew domains like legal or medical fields but is less precise for specific tasks such as classification or\nsummarisation.\n1.6.2\nSupervised Fine-Tuning (SFT)\nSFT involves providing the LLM with labelled data tailored to the target task. For example, fine-tuning\nan LLM for text classification in a business context uses a dataset of text snippets with class labels.\nWhile effective, this method requires substantial labelled data, which can be costly and time-consuming\nto obtain.\n9\n1.6.3\nInstruction Fine-Tuning via Prompt Engineering\nThis method relies on providing the LLM with natural language instructions, useful for creating spe-\ncialised assistants. It reduces the need for vast amounts of labelled data but depends heavily on the\nquality of the prompts.\n1.7\nPre-training vs Fine-tuning\nTable 1.1 provides a comparison between pre-training and fine-tuning, highlighting their respective char-\nacteristics and processes.\nAspect\nPre-training\nFine-tuning\nDefinition\nTraining on a vast amount of\nunlabelled text data\nAdapting a pre-trained model to\nspecific tasks\nData Requirement\nExtensive\nand\ndiverse\nunla-\nbelled text data\nSmaller,\ntask-specific labelled\ndata\nObjective\nBuild general linguistic knowl-\nedge\nSpecialise\nmodel\nfor\nspecific\ntasks\nProcess\nData\ncollection,\ntraining\non\nlarge\ndataset,\npredict\nnext\nword/sequence\nTask-specific\ndata\ncollection,\nmodify last layer for task, train\non new dataset, generate output\nbased on tasks\nModel Modification\nEntire model trained\nLast layers adapted for new task\nComputational Cost\nHigh (large dataset,\ncomplex\nmodel)\nLower (smaller dataset,\nfine-\ntuning layers)\nTraining Duration\nWeeks to months\nDays to weeks\nPurpose\nGeneral language understand-\ning\nTask-specific performance im-\nprovement\nExamples\nGPT, LLaMA 3\nFine-tuning LLaMA 3 for sum-\nmarisation\nTable 1.1: A Comparative Overview of Pre-training and Fine-tuning in Large Language Models (LLMs).\nThe table outlines key differences between the pre-training and fine-tuning phases across various aspects\nsuch as definition, data requirements, objectives, processes, model modification, computational costs,\ntraining duration, and their respective purposes, with examples highlighting specific models and tasks.\nPre-training involves extensive training on vast amounts of unlabelled data to build general linguistic\nknowledge, while fine-tuning adapts the pre-trained models to specialised tasks using smaller, labelled\ndatasets, focusing on task-specific performance improvements.\n1.8\nImportance of Fine-Tuning LLMs\n1. Transfer Learning: Fine-tuning leverages the knowledge acquired during pre-training, adapting\nit to specific tasks with reduced computation time and resources.\n2. Reduced Data Requirements: Fine-tuning requires less labelled data, focusing on tailoring\npre-trained features to the target task.\n3. Improved Generalisation: Fine-tuning enhances the model\u2019s ability to generalise to specific\ntasks or domains, capturing general language features and customising them.\n4. Efficient Model Deployment: Fine-tuned models are more efficient for real-world applications,\nbeing computationally efficient and well-suited for specific tasks.\n5. Adaptability to Various Tasks: Fine-tuned LLMs can adapt to a broad range of tasks, per-\nforming well across various applications without task-specific architectures.\n6. Domain-Specific Performance: Fine", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities", "description": "Arxiv Paper titled 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities' authored by Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid", "url": "https://arxiv.org/pdf/2408.13296v1", "source": "arxiv", "id": "f213c9f1-5312-4eb8-be38-71825950d151"}, "page_content": "\n1.8\nImportance of Fine-Tuning LLMs\n1. Transfer Learning: Fine-tuning leverages the knowledge acquired during pre-training, adapting\nit to specific tasks with reduced computation time and resources.\n2. Reduced Data Requirements: Fine-tuning requires less labelled data, focusing on tailoring\npre-trained features to the target task.\n3. Improved Generalisation: Fine-tuning enhances the model\u2019s ability to generalise to specific\ntasks or domains, capturing general language features and customising them.\n4. Efficient Model Deployment: Fine-tuned models are more efficient for real-world applications,\nbeing computationally efficient and well-suited for specific tasks.\n5. Adaptability to Various Tasks: Fine-tuned LLMs can adapt to a broad range of tasks, per-\nforming well across various applications without task-specific architectures.\n6. Domain-Specific Performance: Fine-tuning allows models to excel in domain-specific tasks by\nadjusting to the nuances and vocabulary of the target domain.\n10\n7. Faster Convergence: Fine-tuning usually achieves faster convergence, starting with weights that\nalready capture general language features.\n1.9\nRetrieval Augmented Generation (RAG)\nA popular method to utilise your own data is by incorporating it into the prompt when querying the LLM\nmodel. This approach, known as Retrieval-Augmented Generation (RAG), involves retrieving relevant\ndata and using it as additional context for the LLM. Instead of depending solely on knowledge from the\ntraining data, a RAG workflow pulls pertinent information, connecting static LLMs with real-time data\nretrieval. With RAG architecture, organisations can deploy any LLM model and enhance it to return\nrelevant results by providing a small amount of their own data (see Figure1.4 for visual workflow). This\nprocess avoids the costs and time associated with fine-tuning or pre-training the model.\nFigure 1.4: An illustration of the Traditional Retrieval-Augmented Generation (RAG) pipeline steps,\ndepicting the sequential process from client query to response generation.\nThe pipeline starts with\nthe client\u2019s question, followed by semantic search in a vector database, contextually enriching the data\nbefore generating a prompt for the large language model (LLM). The final response is post-processed\nand returned to the client.\n1.9.1\nTraditional RAG Pipeline and Steps\n1. Data Indexing: Organise data efficiently for quick retrieval. This involves processing, chunking,\nand storing data in a vector database using indexing strategies like search indexing, vector indexing,\nand hybrid indexing.\n2. Input Query Processing: Refine user queries to improve compatibility with indexed data. This\ncan include simplification or vector transformation of queries for enhanced search efficiency.\n3. Searching and Ranking: Retrieve and rank data based on relevance using search algorithms\nsuch as TF-IDF, BM25, and deep learning models like BERT to interpret the query\u2019s intent and\ncontext.\n4. Prompt Augmentation: Incorporate relevant information from the search results into the origi-\nnal query to provide the LLM with additional context, enhancing response accuracy and relevance.\n11\n5. Response Generation: Use the augmented prompt to generate responses that combine the LLM\u2019s\nknowledge with current, specific data, ensuring high-quality, contextually grounded answers.\n1.9.2\nBenefits of Using RAG\n\u2022 Up-to-Date and Accurate Responses: Enhances the LLM\u2019s responses with current external\ndata, improving accuracy and relevance.\n\u2022 Reducing Inaccurate Responses: Grounds the LLM\u2019s output in relevant knowledge, reducing\nthe risk of generating incorrect information.\n\u2022 Domain-Specific Responses: Delivers contextually relevant responses tailored to an organisa-\ntion\u2019s proprietary data.\n\u2022 Efficiency and Cost-Effectiveness: Offers a cost-effective method for customising LLMs without\nextensive model fine-tuning.\n1.9.3\nChallenges and Considerations in Serving RAG\n1. User Experience: Ensuring rapid response times suitable for real-time applications.\n2. Cost Efficiency: Managing the costs associated with serving millions of responses.\n3. Accuracy: Ensuring outputs are accurate to avoid misinformation.\n4. Recency and Relevance: Keeping responses and content current with the latest data.\n5. Business Context Awareness: Aligning LLM responses with specific business contexts.\n6. Service Scalability: Managing increased capacity while controlling costs.\n7. Security and Governance: Implementing protocols for data security, privacy, and governance.\n1.9.4\nUse Cases and Examples\n1. Question and Answer Chatbots: Integrate LLMs with chatbots to generate accurate answers\nfrom company documents, enhancing customer support.\n2. Search Augmentation: Enhance search engines with LLM-generated answers for more accurate\ninformational queries.\n3. Knowledge Engine: Use LLMs to answer questions related to internal functions, such as HR\nand compliance, using company data.\n1.9.5\nConsiderations for Choosing Between RAG and Fine-Tuning\nWhen considering external data access, RAG is likely a superior option for applications needing to access\nexternal data sources. Fine-tuning, on the other hand, is more suitable if you require the model to ad-\njust its behaviour, and writing style, or incorporate domain-specific knowledge. In terms of suppressing\n", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities", "description": "Arxiv Paper titled 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities' authored by Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid", "url": "https://arxiv.org/pdf/2408.13296v1", "source": "arxiv", "id": "4f71153f-70a3-4fc2-bafe-965c82b7539f"}, "page_content": ". Security and Governance: Implementing protocols for data security, privacy, and governance.\n1.9.4\nUse Cases and Examples\n1. Question and Answer Chatbots: Integrate LLMs with chatbots to generate accurate answers\nfrom company documents, enhancing customer support.\n2. Search Augmentation: Enhance search engines with LLM-generated answers for more accurate\ninformational queries.\n3. Knowledge Engine: Use LLMs to answer questions related to internal functions, such as HR\nand compliance, using company data.\n1.9.5\nConsiderations for Choosing Between RAG and Fine-Tuning\nWhen considering external data access, RAG is likely a superior option for applications needing to access\nexternal data sources. Fine-tuning, on the other hand, is more suitable if you require the model to ad-\njust its behaviour, and writing style, or incorporate domain-specific knowledge. In terms of suppressing\nhallucinations and ensuring accuracy, RAG systems tend to perform better as they are less prone to gen-\nerating incorrect information. If you have ample domain-specific, labelled training data, fine-tuning can\nresult in a more tailored model behaviour, whereas RAG systems are robust alternatives when such data\nis scarce. RAG systems provide an advantage with dynamic data retrieval capabilities for environments\nwhere data frequently updates or changes. Additionally, it is crucial to ensure the transparency and\ninterpret ability of the model\u2019s decision-making process. In that case, RAG systems offer insight that is\ntypically not available in models that are solely fine-tuned. Figure1.5 illustrates the visual representation\nalongside example use cases.\n12\nFigure 1.5: Graph comparing the model adaptation required versus the level of external knowledge needed\nacross different scenarios, highlighting the roles of Retrieval-Augmented Generation (RAG), Fine-Tuning,\nand their hybrid applications in various contexts such as Q&A systems, customer support automation,\nand summarisation tasks. (adapted from [14])\n1.10\nObjectives of the Report\n1.10.1\nGoals and Scope\nThe primary goal of this report is to conduct a comprehensive analysis of fine-tuning techniques for LLMs.\nThis involves exploring theoretical foundations, practical implementation strategies, and challenges. The\nreport examines various fine-tuning methodologies, their applications, and recent advancements.\n1.10.2\nKey Questions and Issues Addressed\nThis report addresses critical questions surrounding fine-tuning LLMs, starting with foundational in-\nsights into LLMs, their evolution, and significance in NLP. It defines fine-tuning, distinguishes it from\npre-training, and emphasises its role in adapting models for specific tasks. Key objectives include en-\nhancing model performance for targeted applications and domains.\nThe report outlines a structured fine-tuning process, featuring a high-level pipeline with visual rep-\nresentations and detailed stage explanations. It covers practical implementation strategies, including\nmodel initialisation, hyperparameter definition, and fine-tuning techniques such as Parameter-Efficient\nFine-Tuning (PEFT) and Retrieval-Augmented Generation (RAG). Industry applications, evaluation\nmethods, deployment challenges, and recent advancements are also explored.\n1.10.3\nOverview of the Report Structure\nThe rest of the report provides a comprehensive understanding of fine-tuning LLMs. The main chapters\ninclude an in-depth look at the fine-tuning pipeline, practical applications, model alignment, evaluation\nmetrics, and challenges. The concluding sections discuss the evolution of fine-tuning techniques, highlight\nongoing research challenges, and provide insights for researchers and practitioners.\n13\nChapter 2\nSeven Stage Fine-Tuning Pipeline\nfor LLM\nFine-tuning a Large Language Model (LLM) is a comprehensive process divided into seven distinct\nstages, each essential for adapting the pre-trained model to specific tasks and ensuring optimal per-\nformance. These stages encompass everything from initial dataset preparation to the final deployment\nand maintenance of the fine-tuned model. By following these stages systematically, the model is refined\nand tailored to meet precise requirements, ultimately enhancing its ability to generate accurate and\ncontextually appropriate responses. The seven stages include Dataset Preparation, Model Initialisation,\nTraining Environment Setup, Fine-Tuning, Evaluation and Validation, Deployment, and Monitoring and\nMaintenance.\nFigure 2.1 illustrates the comprehensive pipeline for fine-tuning LLMs, encompassing all necessary stages\nfrom dataset preparation to monitoring and maintenance.\n2.1\nStage 1: Dataset Preparation\nFine-tuning a Large Language Model (LLM) starts with adapting the pre-trained model for specific tasks\nby updating its parameters using a new dataset. This involves cleaning and formatting the dataset to\nmatch the target task, such as instruction tuning, sentiment analysis, or topic mapping. The dataset is\ncomposed of < input, output > pairs, demonstrating the desired behaviour for the model.\nFor example, in instruction tuning, the dataset may look like:\n###Human: $<Input Query>$\n###Assistant: $<Generated Output>$\nHere, the \u2019Input Query\u2019 is what the user asks, and the \u2019Generated Output\u2019 is the model\u2019s response. The\nstructure and style of these pairs can be adjusted based on the specific needs of the task.\n2.2\nStage 2: Model Initialisation\nModel initialisation is the process of setting up the initial parameters and configurations of the LLM\nbefore training or deploying it. This step is crucial for ensuring the model performs optimally, trains\nefficiently, and avoids issues such", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities", "description": "Arxiv Paper titled 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities' authored by Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid", "url": "https://arxiv.org/pdf/2408.13296v1", "source": "arxiv", "id": "3f81614a-a16c-4492-b850-5ecf9fe4ef9f"}, "page_content": "-trained model for specific tasks\nby updating its parameters using a new dataset. This involves cleaning and formatting the dataset to\nmatch the target task, such as instruction tuning, sentiment analysis, or topic mapping. The dataset is\ncomposed of < input, output > pairs, demonstrating the desired behaviour for the model.\nFor example, in instruction tuning, the dataset may look like:\n###Human: $<Input Query>$\n###Assistant: $<Generated Output>$\nHere, the \u2019Input Query\u2019 is what the user asks, and the \u2019Generated Output\u2019 is the model\u2019s response. The\nstructure and style of these pairs can be adjusted based on the specific needs of the task.\n2.2\nStage 2: Model Initialisation\nModel initialisation is the process of setting up the initial parameters and configurations of the LLM\nbefore training or deploying it. This step is crucial for ensuring the model performs optimally, trains\nefficiently, and avoids issues such as vanishing or exploding gradients.\n2.3\nStage 3: Training Environment Setup\nSetting up the training environment for LLM fine-tuning involves configuring the necessary infrastructure\nto adapt a pre-existing model for specific tasks. This includes selecting relevant training data, defining the\nmodel\u2019s architecture and hyperparameters, and running training iterations to adjust the model\u2019s weights\nand biases.\nThe aim is to enhance the LLM\u2019s performance in generating accurate and contextually\nappropriate outputs tailored to specific applications, like content creation, translation, or sentiment\nanalysis. Successful fine-tuning relies on careful preparation and rigorous experimentation.\n14\nFigure 2.1: A comprehensive pipeline for fine-tuning Large Language Models (LLMs), illustrating the\nseven essential stages: Dataset Preparation, Model Initialisation, Training Environment Setup, Fine-\nTuning, Evaluation and Validation, Deployment, and Monitoring and Maintenance. Each stage plays\na crucial role in adapting the pre-trained model to specific tasks and ensuring optimal performance\nthroughout its lifecycle.\n2.4\nStage 4: Partial or Full Fine-Tuning\nThis stage involves updating the parameters of the LLM using a task-specific dataset. Full fine-tuning up-\ndates all parameters of the model, ensuring comprehensive adaptation to the new task. Alternatively, Half\nfine-tuning (HFT) [15] or Parameter-Efficient Fine-Tuning (PEFT) approaches, such as using adapter\nlayers, can be employed to partially fine-tune the model. This method attaches additional layers to the\npre-trained model, allowing for efficient fine-tuning with fewer parameters, which can address challenges\nrelated to computational efficiency, overfitting, and optimisation.\n2.5\nStage 5: Evaluation and Validation\nEvaluation and validation involve assessing the fine-tuned LLM\u2019s performance on unseen data to ensure\nit generalises well and meets the desired objectives. Evaluation metrics, such as cross-entropy, measure\nprediction errors, while validation monitors loss curves and other performance indicators to detect issues\nlike overfitting or underfitting.\nThis stage helps guide further fine-tuning to achieve optimal model\nperformance.\n15\n2.6\nStage 6: Deployment\nDeploying an LLM means making it operational and accessible for specific applications. This involves\nconfiguring the model to run efficiently on designated hardware or software platforms, ensuring it can\nhandle tasks like natural language processing, text generation, or user query understanding. Deployment\nalso includes setting up integration, security measures, and monitoring systems to ensure reliable and\nsecure performance in real-world applications.\n2.7\nStage 7: Monitoring and Maintenance\nMonitoring and maintaining an LLM after deployment is crucial to ensure ongoing performance and\nreliability.\nThis involves continuously tracking the model\u2019s performance, addressing any issues that\narise, and updating the model as needed to adapt to new data or changing requirements.\nEffective\nmonitoring and maintenance help sustain the model\u2019s accuracy and effectiveness over time.\n16\nChapter 3\nStage 1: Data Preparation\n3.1\nSteps Involved in Data Preparation\n3.1.1\nData Collection\nThe first step in data preparation is to collect data from various sources. These sources can be in any\nformat such as CSV, web pages, SQL databases, S3 storage, etc. Python provides several libraries to\ngather the data efficiently and accurately. Table 3.1 presents a selection of commonly used data formats\nalong with the corresponding Python libraries used for data collection.\n3.1.2\nData Preprocessing and Formatting\nData preprocessing and formatting are crucial for ensuring high-quality data for fine-tuning. This step\ninvolves tasks such as cleaning the data, handling missing values, and formatting the data to match the\nspecific requirements of the task. Several libraries assist with text data processing and Table 3.2 contains\nsome of the most commonly used data preprocessing libraries in python.\n3.1.3\nHandling Data Imbalance\nHandling imbalanced datasets is crucial for ensuring balanced performance across all classes. Several\ntechniques and strategies are employed:\n1. Over-sampling and Under-sampling:\nTechniques like SMOTE (Synthetic Minority Over-\nsampling Technique) generate synthetic examples to achieve balance.\nPython Library: imbalanced-learn\nDescription: imbalanced-learn provides various methods to deal with imbalanced datasets, in-\ncluding oversampling techniques like SMOTE.\n2. Adjusting Loss Function: Modify the loss function to give more weight to the minority class,\nsetting class weights inversely proportional to the class frequencies.\n3. Focal Loss: A variant of cross-entropy", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities", "description": "Arxiv Paper titled 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities' authored by Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid", "url": "https://arxiv.org/pdf/2408.13296v1", "source": "arxiv", "id": "1ebbb36b-e327-4898-865d-21d713f36fa5"}, "page_content": ", and formatting the data to match the\nspecific requirements of the task. Several libraries assist with text data processing and Table 3.2 contains\nsome of the most commonly used data preprocessing libraries in python.\n3.1.3\nHandling Data Imbalance\nHandling imbalanced datasets is crucial for ensuring balanced performance across all classes. Several\ntechniques and strategies are employed:\n1. Over-sampling and Under-sampling:\nTechniques like SMOTE (Synthetic Minority Over-\nsampling Technique) generate synthetic examples to achieve balance.\nPython Library: imbalanced-learn\nDescription: imbalanced-learn provides various methods to deal with imbalanced datasets, in-\ncluding oversampling techniques like SMOTE.\n2. Adjusting Loss Function: Modify the loss function to give more weight to the minority class,\nsetting class weights inversely proportional to the class frequencies.\n3. Focal Loss: A variant of cross-entropy loss that adds a factor to down-weight easy examples and\nfocus training on hard negatives.\nPython Library: focal loss\nDescription: The focal loss package provides robust implementations of various focal loss func-\ntions, including BinaryFocalLoss and SparseCategoricalFocalLoss.\n4. Cost-sensitive Learning: Incorporating the cost of misclassifications directly into the learning\nalgorithm, assigning a higher cost to misclassifying minority class samples.\n5. Ensemble Methods: Using techniques like bagging and boosting to combine multiple models\nand handle class imbalance.\nPython Library: sklearn.ensemble\nDescription: scikit-learn provides robust implementations of various ensemble methods, including\nbagging and boosting.\n17\nData Format\nPython\nLi-\nbrary\nDescription\nLibrary Link\nCSV Files\npandas\npandas is a powerful library for data ma-\nnipulation and analysis. It provides the\nread csv function for easy and efficient\nreading of CSV files into DataFrame ob-\njects.\nIt also supports reading data in\nExcel, JSON, and more.\npandas documenta-\ntion\nWeb Pages\nBeautifulSoup\nand requests\nBeautifulSoup is a library for parsing\nHTML and XML documents. Combined\nwith requests for sending HTTP re-\nquests, it enables data extraction from\nweb pages, essential for web scraping\ntasks.\nBeautifulSoup\ndocumentation,\nrequests documen-\ntation\nSQL Databases\nSQLAlchemy\nSQLAlchemy\nis\na\nSQL\ntoolkit\nand\nObject-Relational Mapping (ORM) li-\nbrary for Python, providing a full suite\nof enterprise-level persistence patterns.\nSQLAlchemy docu-\nmentation\nS3 Storage\nboto3\nboto3 is the Amazon Web Services\n(AWS) SDK for Python, allowing devel-\nopers to use services like Amazon S3 and\nEC2. It enables interaction with AWS\nservices, including uploading, download-\ning, and managing S3 bucket files.\nboto3\ndocumenta-\ntion\nData\nIntegra-\ntion\nRapidMiner\nRapidMiner is a comprehensive envi-\nronment for data preparation, machine\nlearning, and predictive analytics, allow-\ning efficient processing and transforma-\ntion of raw data into actionable insights.\nRapidMiner\ndocu-\nmentation\nData Cleaning\nTrifacta\nWran-\ngler\nTrifacta Wrangler focuses on simplify-\ning and automating data wrangling pro-\ncesses, transforming raw data into clean\nand structured formats.\nTrifacta\nWrangler\ndocumentation\nTable 3.1: Python libraries and tools for data collection and integration in various formats, providing\nan overview of commonly used libraries, their functions, and links to their official documentation for\nefficient data management and processing.\n6. Stratified Sampling: Ensuring that each mini-batch during training contains an equal or pro-\nportional representation of each class.\nPython Library: sklearn.model selection.StratifiedShuffleSplit\nDescription: scikit-learn offers tools for stratified sampling, ensuring balanced representation\nacross classes.\n7. Data Cleaning: Removing noisy and mislabelled data, which can disproportionately affect the\nminority class.\nPython Library: pandas.DataFrame.sample\nDescription: pandas provides methods for sampling data from DataFrames, useful for data clean-\ning and preprocessing.\n8. Using Appropriate Metrics: Metrics like Precision-Recall AUC, F1-score, and Cohen\u2019s Kappa\nare more informative than accuracy when dealing with imbalanced datasets.\nPython Library: sklearn.metrics\nDescription: scikit-learn offers a comprehensive set of tools for evaluating the performance of\nclassification models, particularly with imbalanced datasets.\n18\nLibrary Name\nData Preprocessing Options\nLink\nspaCy\nspaCy provides robust capabilities for text prepro-\ncessing, including tokenization, lemmatization, and\nefficient sentence boundary detection.\nspaCy documentation\nNLTK\nNLTK offers a comprehensive set of tools for data\npreprocessing, such as tokenization, stemming, and\nstop word removal.\nNLTK documentation\nHuggingFace\nHuggingFace provides extensive capabilities for\ntext preprocessing through its transformers library,\nincluding functionalities for tokenization and sup-\nport for various pre-trained models.\nHuggingFace documentation\nKNIME\nKNIME Analytics Platform allows visual workflow\ndesign for data integration, preprocessing, and ad-\n", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities", "description": "Arxiv Paper titled 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities' authored by Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid", "url": "https://arxiv.org/pdf/2408.13296v1", "source": "arxiv", "id": "4ca3e81a-14d4-4086-a80f-765949bbc4cd"}, "page_content": " with imbalanced datasets.\nPython Library: sklearn.metrics\nDescription: scikit-learn offers a comprehensive set of tools for evaluating the performance of\nclassification models, particularly with imbalanced datasets.\n18\nLibrary Name\nData Preprocessing Options\nLink\nspaCy\nspaCy provides robust capabilities for text prepro-\ncessing, including tokenization, lemmatization, and\nefficient sentence boundary detection.\nspaCy documentation\nNLTK\nNLTK offers a comprehensive set of tools for data\npreprocessing, such as tokenization, stemming, and\nstop word removal.\nNLTK documentation\nHuggingFace\nHuggingFace provides extensive capabilities for\ntext preprocessing through its transformers library,\nincluding functionalities for tokenization and sup-\nport for various pre-trained models.\nHuggingFace documentation\nKNIME\nKNIME Analytics Platform allows visual workflow\ndesign for data integration, preprocessing, and ad-\nvanced manipulations like text mining and image\nanalysis.\nKNIME documentation\nTable 3.2: Outline of Python libraries commonly used for text data preprocessing, including spaCy,\nNLTK, HuggingFace, and KNIME. It details the specific preprocessing options offered by each library\nand provides links to their official documentation for users seeking more in-depth guidance on their use.\n3.1.4\nSplitting Dataset\nSplitting the dataset for fine-tuning involves dividing it into training and validation sets, typically using\nan 80:20 ratio. Different techniques include:\n1. Random Sampling: Selecting a subset of data randomly to create a representative sample.\nPython Library: sklearn.model selection.train test split\n2. Stratified Sampling: Dividing the dataset into subgroups and sampling from each to maintain\nclass balance.\nPython Library: sklearn.model selection.StratifiedShuffleSplit\n3. K-Fold Cross Validation: Splitting the dataset into K folds and performing training and vali-\ndation K times.\nPython Library: sklearn.model selection.KFold\n4. Leave-One-Out Cross Validation: Using a single data point as the validation set and the rest\nfor training, repeated for each data point.\nPython Library: sklearn.model selection.LeaveOneOut\nFurther details can be found in scikit-learn\u2019s documentation on model selection.\n3.2\nExisting and Potential Research Methodologies\n3.2.1\nData Annotation\nData annotation involves labelling or tagging textual data with specific attributes relevant to the model\u2019s\ntraining objectives.\nThis process is crucial for supervised learning tasks and greatly influences the\nperformance of the fine-tuned model. Recent research highlights various approaches to data annotation:\n\u2022 Human Annotation: Manual annotation by human experts remains a gold standard due to its\naccuracy and context understanding. However, it is time-consuming and costly for large datasets\n[16]. Tools like Excel, Prodigy1, and Innodata2 facilitate this process.\n\u2022 Semi-automatic Annotation: Combining machine learning algorithms with human review to\ncreate labelled datasets more efficiently. This approach balances efficiency and accuracy. Tools\nlike Snorkel3 use weak supervision to generate initial labels, which are then refined by human\nannotators [17].\n1https://prodi.gy\n2https://innodata.com/\n3https://snorkel.ai/\n19\n\u2022 Automatic Annotation: Fully automated annotation leverages machine learning algorithms to\nlabel data without human intervention, offering scalability and cost-effectiveness.\nServices like\nAmazon SageMaker Ground Truth4 utilise machine learning to automate data labelling, al-\nthough the accuracy may vary depending on the complexity of the task [18].\n3.2.2\nData Augmentation\nData Augmentation (DA) techniques expand training datasets artificially to address data scarcity and\nimprove model performance. Advanced techniques often used in NLP include:\n\u2022 Word Embeddings: Using word embeddings like Word2Vec and GloVe to replace words with\ntheir semantic equivalents, thereby generating new data instances [19, 20].\n\u2022 Back Translation: Translating text to another language and then back to the original language\nto create paraphrased data. This technique helps in generating diverse training samples [21]. Tools\nlike Google Translate API5 are commonly used for this purpose.\n\u2022 Adversarial Attacks: Generating augmented data through adversarial examples that slightly\nmodify the original text to create new training samples while preserving the original meaning [22].\nLibraries like TextAttack6 provide frameworks for such augmentations.\n\u2022 NLP-AUG7: This library offers a variety of augmenters for character, word, sentence, audio, and\nspectrogram augmentation, enhancing dataset diversity.\n3.2.3\nSynthetic Data Generation using LLMs\nLarge Language Models (LLMs) can generate synthetic data through innovative techniques such as:\n\u2022 Prompt Engineering: Crafting specific prompts to guide LLMs like GPT-3 in generating relevant\nand high-quality synthetic data [23].\n\u2022 Multi-Step Generation: Employing iterative generation processes where LLMs generate initial\ndata that is refined through subsequent steps [24]. This method can produce high-quality synthetic\ndata for various tasks, including summarising and bias detection.\nIt is crucial to verify the accuracy and relevance of synthetic data generated by LLMs before using\nthem for fine-tuning processes [25].\n3.3\nChallenges in Data Preparation for Fine-Tuning LLMs\nKey challenges in data preparation include:\n1. Domain Relevance: Ensuring that the data", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities", "description": "Arxiv Paper titled 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities' authored by Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid", "url": "https://arxiv.org/pdf/2408.13296v1", "source": "arxiv", "id": "83bf6700-c788-4d48-b6d4-e85613454557"}, "page_content": " character, word, sentence, audio, and\nspectrogram augmentation, enhancing dataset diversity.\n3.2.3\nSynthetic Data Generation using LLMs\nLarge Language Models (LLMs) can generate synthetic data through innovative techniques such as:\n\u2022 Prompt Engineering: Crafting specific prompts to guide LLMs like GPT-3 in generating relevant\nand high-quality synthetic data [23].\n\u2022 Multi-Step Generation: Employing iterative generation processes where LLMs generate initial\ndata that is refined through subsequent steps [24]. This method can produce high-quality synthetic\ndata for various tasks, including summarising and bias detection.\nIt is crucial to verify the accuracy and relevance of synthetic data generated by LLMs before using\nthem for fine-tuning processes [25].\n3.3\nChallenges in Data Preparation for Fine-Tuning LLMs\nKey challenges in data preparation include:\n1. Domain Relevance: Ensuring that the data is relevant to the specific domain for accurate model\nperformance. Mismatched domain data can lead to poor generalisation and inaccurate outputs\n[26].\n2. Data Diversity: Including diverse and well-balanced data to prevent model biases and improve\ngeneralisation. A lack of diversity can cause the model to perform poorly on underrepresented\nscenarios [27].\n3. Data Size: Managing and processing large datasets, with at least 1000 samples recommended for\neffective fine-tuning. However, large datasets pose challenges in terms of storage, computational\nrequirements, and processing time.\n4. Data Cleaning and Preprocessing: Removing noise, errors, and inconsistencies are critical for\nproviding clean inputs to the model. Poorly preprocessed data can degrade model performance\nsignificantly.\n4https://aws.amazon.com/sagemaker/groundtruth/\n5https://translate.google.com/?sl=auto&tl=en&op=translate\n6https://github.com/QData/TextAttack\n7https://github.com/makcedward/nlpaug\n20\n5. Data Annotation: Ensuring precise and consistent labelling is essential for tasks requiring la-\nbelled data. Inconsistent annotation can lead to unreliable model predictions.\n6. Handling Rare Cases: Adequately representing rare but important instances in the dataset to\nensure the model can generalise to less frequent but critical scenarios.\n7. Ethical Considerations: Scrutinising data for harmful or biased content to prevent unintended\nconsequences. Ethical data handling includes removing biases and ensuring privacy [28].\n3.4\nAvailable LLM Fine-Tuning Datasets\nFor a comprehensive list of datasets suitable for fine-tuning LLMs, refer to resources like LLMXplorer,\nwhich provides domain and task-specific datasets.\n3.5\nBest Practices\n3.5.1\nHigh-Quality Data Collection\nEnsuring high-quality, diverse, and representative data is critical. Leveraging curated sources and en-\nsuring comprehensive coverage across different scenarios enhances model robustness [29].\nTools like\nDataRobot Paxata8 and KNIME Analytics Platform9 offer robust data profiling and transforma-\ntion capabilities.\n3.5.2\nEffective Data Preprocessing\nProper data preprocessing is essential for model performance. Utilising libraries like spaCy, NLTK, and\nHuggingFace Transformers can streamline preprocessing tasks. Platforms like Trifacta Wrangler\nand RapidMiner automate data cleaning tasks, improving efficiency and ensuring consistency [30].\n3.5.3\nManaging Data Imbalance\nAddressing data imbalance is crucial.\nTechniques like over-sampling, under-sampling, and SMOTE\nhelp balance datasets. Libraries like imbalanced-learn and ensemble methods in scikit-learn provide\nrobust tools for managing imbalanced datasets [31].\n3.5.4\nAugmenting and Annotating Data\nData augmentation and annotation improve model robustness. Tools like NLP-AUG, TextAttack,\nand Snorkel offer sophisticated capabilities for creating diverse and well-labelled datasets [32, 33].\n3.5.5\nEthical Data Handling\nEnsuring ethical data handling involves thorough scrutiny for biases and privacy concerns. Implement-\ning privacy-preserving techniques and filtering harmful content is critical. Services like Amazon Sage-\nMaker Ground Truth ensure scalable and secure data annotation [34].\n3.5.6\nRegular Evaluation and Iteration\nContinuous evaluation and iteration of the data preparation pipeline help maintain data quality and\nrelevance. Leveraging feedback loops and performance metrics ensures ongoing improvements and adap-\ntation to new data requirements.\nBy integrating these best practices, researchers and practitioners can enhance the effectiveness of LLM\nfine-tuning, ensuring robust and reliable model performance.\n8https://www.datarobot.com/platform/preparation/\n9https://www.knime.com/\n21\nChapter 4\nStage 2: Model Initialisation\n4.1\nSteps Involved in Model Initialisation\nFigure 4.1: Sequential steps involved in Initialising a Large Language Model (LLM), illustrating the\nprocess from setting up the environment to executing tasks. Each step is critical for ensuring that the\nLLM is correctly configured and ready for operation. This includes installing necessary dependencies,\nimporting libraries, selecting and downloading the appropriate language model from a repository, and\nfinally, loading the model to perform specific tasks.\n1. Set Up the Environment: Configure your environment, such as setting up GPU/TPU usage if\navailable", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities", "description": "Arxiv Paper titled 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities' authored by Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid", "url": "https://arxiv.org/pdf/2408.13296v1", "source": "arxiv", "id": "8228b229-a85b-47f1-b89c-b74edb448957"}, "page_content": " data requirements.\nBy integrating these best practices, researchers and practitioners can enhance the effectiveness of LLM\nfine-tuning, ensuring robust and reliable model performance.\n8https://www.datarobot.com/platform/preparation/\n9https://www.knime.com/\n21\nChapter 4\nStage 2: Model Initialisation\n4.1\nSteps Involved in Model Initialisation\nFigure 4.1: Sequential steps involved in Initialising a Large Language Model (LLM), illustrating the\nprocess from setting up the environment to executing tasks. Each step is critical for ensuring that the\nLLM is correctly configured and ready for operation. This includes installing necessary dependencies,\nimporting libraries, selecting and downloading the appropriate language model from a repository, and\nfinally, loading the model to perform specific tasks.\n1. Set Up the Environment: Configure your environment, such as setting up GPU/TPU usage if\navailable, which can significantly speed up model loading and inference.\n2. Install the Dependencies: Ensure that all necessary software and libraries are installed. This\ntypically includes package managers like pip and frameworks like PyTorch or TensorFlow.\n22\n3. Import the Libraries: Import the required libraries in your script or notebook. Common libraries\ninclude transformers from Hugging Face, torch for PyTorch, and other utility libraries.\n4. Choose the Language Model: Select the appropriate pre-trained language model based on your\ntask requirements. This could be models like BERT, GPT-3, or others available on platforms like\nHugging Face\u2019s Model Hub.\n5. Download the Model from the Repository: Use the chosen framework\u2019s functions to download\nthe pre-trained model from an online repository. For instance, using transformers, you might use\nAutoModel.from pretrained(\u2019model name\u2019).\n6. Load the Model in the Memory: Load the model into memory, ready for inference or further\nfine-tuning. This step ensures the model weights are initialised and ready for use.\n7. Execute Tasks: Perform the desired tasks using the loaded model. This could involve making\npredictions, generating text, or fine-tuning the model on a new dataset.\n4.2\nTools and Libraries for Model Initialisation\nPython offers a wide range of libraries for Initialising large language models, providing access to both\nopen and closed-source models. Here are some notable libraries:\n1. Python Library: HuggingFace\nDescription: HuggingFace is renowned for its support of numerous pre-trained large language\nmodels, ranging from Phi-3 mini to Llama-3 70B. The transformers library, part of HuggingFace,\nenables users to access these models via classes such as AutoModelForCausalLM. This library\nsupports loading fine-tuned models as well as 4-bit quantised models. Additionally, the transformers\nlibrary includes the \u201dpipeline\u201d feature, making it easy to use pre-trained models for various tasks\n[35].\n2. Python Framework: PyTorch\nDescription: PyTorch offers comprehensive tools and libraries for Initialising and fine-tuning\nlarge language models. It provides a flexible and efficient platform for building and deploying deep\nlearning models. HuggingFace\u2019s transformers library bridges the gap between PyTorch and other\nframeworks, enhancing its usability for state-of-the-art language models [36].\n3. Python Framework: TensorFlow\nDescription: TensorFlow also provides extensive tools and libraries for Initialising and fine-tuning\nlarge language models. Similar to PyTorch, it benefits from the HuggingFace transformers library,\nwhich provides a versatile and user-friendly API and interface for working with the latest advance-\nments in large language models [37].\n23\n4.3\nChallenges in Model Initialisation\nChallenge\nDescription\nAlignment with the\nTarget Task\nIt\u2019s essential that the pre-trained model closely aligns with your specific\ntask or domain. This initial alignment serves as a solid foundation for\nfurther fine-tuning efforts, leading to improved efficiency and results [38].\nUnderstanding the\nPre-trained Model\nBefore making a selection, it\u2019s crucial to thoroughly comprehend the\narchitecture, capabilities, limitations, and the tasks the model was orig-\ninally trained on. Without this understanding, fine-tuning efforts may\nnot yield the desired outcomes [23].\nAvailability and\nCompatibility\nCareful consideration of a model\u2019s documentation, license, maintenance,\nand update frequency is necessary to avoid potential issues and ensure\nsmooth integration into your application.\nModel Architecture\nNot all models excel at every task.\nEach model architecture has its\nstrengths and weaknesses, so selecting one aligned with your specific\ntask is essential for favourable outcomes [39].\nResource Constraints\nLoading pre-trained LLMs is resource-heavy and requires more compu-\ntation. These models need high-performance CPUs and GPUs and a\nsignificant amount of disk space. For instance, the Llama 3 8B model\nrequires a minimum of 16GB of memory to load and run the inference.\nPrivacy\nPrivacy and confidentiality are crucial factors when selecting a large lan-\nguage model (LLM). Many businesses prefer not to share their data\nwith external LLM providers.\nIn such instances, hosting an LLM on\nlocal servers or using pre-trained LLMs available through private cloud\nproviders can be viable solutions. These approaches ensure that data\nremains within the company\u2019s premises, thereby preserving privacy and\nconfidentiality.\n", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities", "description": "Arxiv Paper titled 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities' authored by Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid", "url": "https://arxiv.org/pdf/2408.13296v1", "source": "arxiv", "id": "a9b0966a-59fe-4448-913c-d5b9e788646c"}, "page_content": " task.\nEach model architecture has its\nstrengths and weaknesses, so selecting one aligned with your specific\ntask is essential for favourable outcomes [39].\nResource Constraints\nLoading pre-trained LLMs is resource-heavy and requires more compu-\ntation. These models need high-performance CPUs and GPUs and a\nsignificant amount of disk space. For instance, the Llama 3 8B model\nrequires a minimum of 16GB of memory to load and run the inference.\nPrivacy\nPrivacy and confidentiality are crucial factors when selecting a large lan-\nguage model (LLM). Many businesses prefer not to share their data\nwith external LLM providers.\nIn such instances, hosting an LLM on\nlocal servers or using pre-trained LLMs available through private cloud\nproviders can be viable solutions. These approaches ensure that data\nremains within the company\u2019s premises, thereby preserving privacy and\nconfidentiality.\nCost and Maintenance\nHosting LLMs on local servers entails significant time and expense for\nsetup and ongoing maintenance. Conversely, utilising cloud vendors al-\nleviates concerns about resource maintenance but incurs monthly billing\ncosts. These charges are typically based on factors such as model size\nand the volume of requests per minute.\nModel Size and\nQuantisation\nutilising a pre-trained model with high memory consumption can still be\nviable by employing its quantised version. Through quantisation, pre-\ntrained weights can be loaded with reduced precision, typically 4-bit or\n8-bit floating point, substantially diminishing parameter volume while\nmaintaining considerable accuracy [40].\nPre-training Datasets\nExamine the datasets used for pre-training to gauge the model\u2019s under-\nstanding of language. These are important as there are models available\nspecifically for performing code generation, and we do not want to use\nthose models for finance text classification [41].\nBias Awareness\nBe vigilant regarding potential biases in pre-trained models, especially if\nunbiased predictions are required. The bias awareness can be evaluated\nby testing different models and backtracking the datasets used for pre-\ntraining [42].\nTable 4.1: Comprehensive Overview of Challenges in Initialising a Large Language Model (LLM). This\ntable highlights critical considerations, such as the importance of aligning pre-trained models with specific\ntasks, understanding model architecture and compatibility, managing resource constraints, and ensuring\ndata privacy. Additionally, it discusses the challenges related to cost, maintenance, and the complexities\nof model size, quantisation, and bias awareness. Each challenge is associated with specific references to\nensure thorough understanding and proper model deployment.\n4.4\nTutorials\n1. Summarisation using Llama 3\n24\n2. HuggingFace tutorial for getting started with LLMs\n3. PyTorch tutorial for fine-tuning models\n4. TensorFlow tutorial for transformer models\n25\nChapter 5\nStage 3: Training Setup\n5.1\nSteps Involved in Training Setup\n1. Setting up the training environment: When setting up the environment for training an LLM,\nit is crucial to configure high-performance hardware, such as GPUs or TPUs, and ensure proper\ninstallation of necessary software components like CUDA, cuDNN, and deep learning frameworks\nsuch as PyTorch or TensorFlow. Verify hardware recognition and compatibility with the software to\nleverage computational power effectively, reducing training time and improving model performance.\n2. Defining the Hyper-parameters: When defining hyperparameters for fine-tuning an LLM, it is\nessential to carefully tune key parameters such as learning rate, batch size, and epochs to optimise\nthe model\u2019s performance.\n3. Initialising Optimisers and Loss Functions: When initialising optimisers and loss functions\nfor fine-tuning an LLM, it is crucial to select the appropriate optimiser to efficiently update the\nmodel\u2019s weights and the correct loss function to measure model performance [43].\n5.2\nSetting up Training Environment\nWhen fine-tuning a large language model (LLM), the computational environment plays a crucial role in\nensuring efficient training. To achieve optimal performance, it\u2019s essential to configure the environment\nwith high-performance hardware such as GPUs (Graphics Processing Units) or TPUs (Tensor Processing\nUnits). GPUs, such as the NVIDIA A100 or V100, are widely used for training deep learning models\ndue to their parallel processing capabilities. For larger-scale operations, TPUs offered by Google Cloud\ncan provide even greater acceleration [44].\nFirst, ensure that your system or cloud environment has the necessary hardware installed. For GPUs,\nthis involves setting up CUDA1 (Compute Unified Device Architecture) and cuDNN2 (CUDA Deep Neu-\nral Network library) from NVIDIA, which are essential for enabling GPU acceleration. For TPU usage,\nyou would typically set up a Google Cloud environment with TPU instances, which includes configuring\nthe TPU runtime in your training scripts.\nVerify that your hardware is correctly recognised and utilised by your deep learning frameworks. In\nPyTorch, for instance, you can check GPU availability with torch.cuda.is available(). Properly setting\nup and testing the hardware ensures that the training process can leverage the computational power\neffectively, reducing training time and improving model performance [36].\nWhen fine-tuning an LLM, both software and hardware considerations are paramount to ensure a smooth\nand efficient training process. On the software side, you need a compatible deep learning framework like\nPyTorch or TensorFlow. These frameworks", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities", "description": "Arxiv Paper titled 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities' authored by Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid", "url": "https://arxiv.org/pdf/2408.13296v1", "source": "arxiv", "id": "11492882-5a42-4605-b9f7-70b39af7b29d"}, "page_content": " up CUDA1 (Compute Unified Device Architecture) and cuDNN2 (CUDA Deep Neu-\nral Network library) from NVIDIA, which are essential for enabling GPU acceleration. For TPU usage,\nyou would typically set up a Google Cloud environment with TPU instances, which includes configuring\nthe TPU runtime in your training scripts.\nVerify that your hardware is correctly recognised and utilised by your deep learning frameworks. In\nPyTorch, for instance, you can check GPU availability with torch.cuda.is available(). Properly setting\nup and testing the hardware ensures that the training process can leverage the computational power\neffectively, reducing training time and improving model performance [36].\nWhen fine-tuning an LLM, both software and hardware considerations are paramount to ensure a smooth\nand efficient training process. On the software side, you need a compatible deep learning framework like\nPyTorch or TensorFlow. These frameworks have extensive support for LLMs and provide utilities for\nefficient model training and evaluation. Installing the latest versions of these frameworks, along with\nany necessary dependencies, is crucial for leveraging the latest features and performance improvements\n1https://developer.nvidia.com/cuda-toolkit\n2https://developer.nvidia.com/cudnn\n26\n[45].\nAdditionally, use libraries like Hugging Face\u2019s transformers to simplify the process of loading pre-trained\nmodels and tokenizers. This library is particularly well-suited for working with various LLMs and offers\na user-friendly interface for model fine-tuning. Ensure that all software components, including libraries\nand dependencies, are compatible with your chosen framework and hardware setup [35].\nOn the hardware side, consider the memory requirements of the model and your dataset. LLMs typ-\nically require substantial GPU memory, so opting for GPUs with higher VRAM (e.g., 16GB or more)\ncan be beneficial. If your model is exceptionally large or if you are training with very large datasets,\ndistributed training across multiple GPUs or TPUs might be necessary. This requires a careful setup of\ndata parallelism or model parallelism techniques to efficiently utilise the available hardware [46].\nLastly, ensure robust cooling and power supply for your hardware, as training LLMs can be resource-\nintensive, generating significant heat and requiring consistent power. Proper hardware setup not only\nenhances training performance but also prolongs the lifespan of your equipment [47].\n5.3\nDefining Hyperparameters\nKey hyperparameters like learning rate, batch size, epochs are crucial for enhancing the model\u2019s perfor-\nmance and obtaining superior outcomes. This process entails adjusting hyperparameters and training\nsettings to align with your particular use case. Below are the key hyperparameters:\n1. Learning Rate: Fine-tuning an LLM involves using optimisation algorithms like stochastic gradi-\nent descent (SGD). This technique estimates the error gradient for the model\u2019s current state using\nsamples from the training dataset and subsequently updates the model\u2019s weights via the backprop-\nagation of errors algorithm. The learning rate dictates the speed at which the model adapts to the\nproblem. Smaller learning rates necessitate more training due to the minimal weight adjustments\nper update, while larger learning rates lead to quicker changes to weights [48].\n2. Batch Size: A batch refers to a subset of the training data used to update a model\u2019s weights\nduring the training process. Batch training involves dividing the entire training set into smaller\ngroups, updating the model after processing each batch. The batch size is a hyperparameter that\ndetermines the number of samples processed before the model parameters are updated.\n3. Epochs: Epoch refers to a full pass through the entire training dataset. This involves a complete\nforward and backward pass through the dataset. The dataset can be processed as a single batch\nor divided into multiple smaller batches. An epoch is considered complete once the model has\nprocessed all batches and updated its parameters based on the calculated loss.\n5.3.1\nMethods for Hyperparameter Tuning\nLLM hyperparameter tuning involves adjusting various hyperparameters during the training process\nto identify the optimal combination that yields the best output. This process often entails significant\ntrial and error, meticulously tracking each hyperparameter adjustment, and recording the resulting\nperformance. Conducting this manually can be highly time-consuming. To address this, automated\nhyperparameter tuning methods have been developed to streamline the process. The three most common\nmethods of automated hyperparameter tuning are random search, grid search, and Bayesian optimisation:\n1. Random Search: This method randomly selects and evaluates combinations of hyperparameters\nfrom a specified range. It is a straightforward and efficient approach capable of exploring a large\nparameter space. However, it may not always find the optimal combination of hyperparameters\nand can be computationally expensive [49].\n2. Grid Search: Unlike random search, grid search exhaustively evaluates every possible combination\nof hyperparameters from a given range.\nAlthough resource-intensive, this systematic approach\nensures that the optimal set of hyperparameters is found [50].\n27\n3. Bayesian Optimisation: This method uses a probabilistic model to predict the performance of\ndifferent hyperparameters and selects the best ones accordingly. It is an efficient method that can\nhandle large parameter spaces better and is less resource-intensive than grid search. However, it is\nmore complex to set up and may be less reliable in identifying the optimal set of hyperparameters\ncompared to grid search.\n4. Automated hyperparameter tuning: This facilitates the development of multiple language\nmodels, each with a unique combination of hyperparameters. By training", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities", "description": "Arxiv Paper titled 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities' authored by Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid", "url": "https://arxiv.org/pdf/2408.13296v1", "source": "arxiv", "id": "105ee3e4-a90a-4af2-9b53-7912bd99a33b"}, "page_content": " straightforward and efficient approach capable of exploring a large\nparameter space. However, it may not always find the optimal combination of hyperparameters\nand can be computationally expensive [49].\n2. Grid Search: Unlike random search, grid search exhaustively evaluates every possible combination\nof hyperparameters from a given range.\nAlthough resource-intensive, this systematic approach\nensures that the optimal set of hyperparameters is found [50].\n27\n3. Bayesian Optimisation: This method uses a probabilistic model to predict the performance of\ndifferent hyperparameters and selects the best ones accordingly. It is an efficient method that can\nhandle large parameter spaces better and is less resource-intensive than grid search. However, it is\nmore complex to set up and may be less reliable in identifying the optimal set of hyperparameters\ncompared to grid search.\n4. Automated hyperparameter tuning: This facilitates the development of multiple language\nmodels, each with a unique combination of hyperparameters. By training these models on the same\ndataset, it becomes possible to compare their outputs and determine which configuration is best\nsuited for the desired use case. Additionally, models tuned with different sets of hyperparameters\ncan be tailored to various specific applications.\n5.4\nInitialising Optimisers and Loss Functions\nChoosing the right optimiser and loss function is crucial for training and fine-tuning LLMs.\nBelow\nare descriptions of some commonly used optimisation algorithms, their advantages, disadvantages, and\nappropriate use cases:\n5.4.1\nGradient Descent\nGradient Descent is a fundamental optimisation algorithm used to minimise cost functions in machine\nlearning models. It aims to find the optimal parameters for a neural network.\nHow it Works: Gradient Descent iteratively updates model parameters in the direction of the\nnegative gradient of the cost function. It calculates gradients for each parameter and applies updates\nacross all data points until convergence. This method utilises the entire dataset to calculate gradients,\noften requiring a fixed learning rate and being sensitive to the scale of data and learning rate choice.\nPros:\n\u2022 Simple and easy to implement.\n\u2022 Intuitive and easy to understand.\n\u2022 Converges to the global minimum for convex functions.\n\u2022 Suitable for small-scale problems.\nCons:\n\u2022 Computationally expensive on large datasets.\n\u2022 May get stuck in local minima.\n\u2022 Requires a large number of iterations.\n\u2022 Sensitive to the choice of learning rate.\nWhen to Use: Gradient Descent is best used for small datasets where gradient computation is\ncheap and simplicity and clarity are preferred.\n5.4.2\nStochastic Gradient Descent (SGD)\nStochastic Gradient Descent (SGD) is a variant of Gradient Descent that focuses on reducing computation\nper iteration.\nHow it Works: SGD updates parameters using a single or few data points at each iteration, intro-\nducing randomness in updates. It reduces the computational burden per iteration and often converges\nfaster than batch Gradient Descent. However, it requires a smaller learning rate due to higher variance\nand benefits from momentum to stabilise updates.\nPros:\n\u2022 Fast and handles large datasets well.\n\u2022 Efficient memory usage.\n28\n\u2022 Simple and easy to implement.\n\u2022 Can escape local minima due to noise.\nCons:\n\u2022 High variance in updates can lead to instability.\n\u2022 Can overshoot the minimum.\n\u2022 Sensitive to the choice of learning rate.\n\u2022 Can be slower to converge compared to batch methods.\nWhen to Use: SGD is ideal for large datasets, incremental learning scenarios, and real-time learning\nenvironments where computational resources are limited.\n5.4.3\nMini-batch Gradient Descent\nMini-batch Gradient Descent combines the efficiency of SGD and the stability of batch Gradient Descent,\noffering a compromise between batch and stochastic approaches.\nHow it Works: It splits data into small batches and updates parameters using gradients averaged\nover each mini-batch. This reduces variance compared to SGD and is more efficient than batch Gradient\nDescent, helping in generalising the updates.\nPros:\n\u2022 Balances between efficiency and stability.\n\u2022 More generalisable updates.\n\u2022 Reduces the variance of parameter updates.\n\u2022 Provides a compromise between SGD and batch.\nCons:\n\u2022 Requires tuning of batch size.\n\u2022 Can still be computationally expensive for very large datasets.\n\u2022 More complex implementation.\n\u2022 Can require more iterations than full-batch Gradient Descent.\nWhen to Use: Mini-batch Gradient Descent is suitable for most deep learning tasks, especially\nwhen working with moderate to large datasets.\n5.4.4\nAdaGrad\nAdaptive Gradient Algorithm (AdaGrad) is designed for sparse data and high-dimensional models, ad-\njusting learning rates to improve performance on sparse data.\nHow it Works: AdaGrad adapts the learning rate for each parameter based on historical gradi-\nent information, accumulating squared gradients. This approach prevents large updates for frequent\nparameters and helps in dealing with sparse features.\nPros:\n\u2022 Adapts learning rate for each parameter.\n\u2022 Good for sparse data.\n\u2022 No need to manually tune learning rates.\n\u2022 Works well with high-dimensional data.\nCons:\n\u2022 Learning rate can diminish to zero, stopping learning.\n29\n\u2022 May require more tuning for convergence.\n\u2022 Accumulation of squared gradients can lead to overly small learning rates.\n\u2022 Can slow down significantly.\nWhen to Use: AdaGrad is useful for sparse datasets like text and images where learning rates need\nto adapt to feature frequency.", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities", "description": "Arxiv Paper titled 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities' authored by Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid", "url": "https://arxiv.org/pdf/2408.13296v1", "source": "arxiv", "id": "61347fcc-a1c3-4287-a8e0-1f73400f6141"}, "page_content": "\nAdaGrad\nAdaptive Gradient Algorithm (AdaGrad) is designed for sparse data and high-dimensional models, ad-\njusting learning rates to improve performance on sparse data.\nHow it Works: AdaGrad adapts the learning rate for each parameter based on historical gradi-\nent information, accumulating squared gradients. This approach prevents large updates for frequent\nparameters and helps in dealing with sparse features.\nPros:\n\u2022 Adapts learning rate for each parameter.\n\u2022 Good for sparse data.\n\u2022 No need to manually tune learning rates.\n\u2022 Works well with high-dimensional data.\nCons:\n\u2022 Learning rate can diminish to zero, stopping learning.\n29\n\u2022 May require more tuning for convergence.\n\u2022 Accumulation of squared gradients can lead to overly small learning rates.\n\u2022 Can slow down significantly.\nWhen to Use: AdaGrad is useful for sparse datasets like text and images where learning rates need\nto adapt to feature frequency.\n5.4.5\nRMSprop\nRoot Mean Square Propagation (RMSprop) is an adaptive learning rate method designed to perform\nbetter on non-stationary and online problems.\nHow it Works: RMSprop modifies AdaGrad by using a moving average of squared gradients to\nadapt learning rates based on recent gradient magnitudes. It maintains a running average of squared\ngradients to help in maintaining steady learning rates.\nPros:\n\u2022 Addresses the diminishing learning rate problem of AdaGrad.\n\u2022 Adapts learning rate based on recent gradients.\n\u2022 Effective for recurrent neural networks.\n\u2022 More robust against non-stationary targets.\nCons:\n\u2022 Can still get stuck in local minima on non-convex problems.\n\u2022 Requires hyperparameter tuning.\n\u2022 Requires careful tuning of the decay rate.\n\u2022 Can be sensitive to the initial learning rate.\nWhen to Use: RMSprop is best for non-convex optimisation problems, training RNNs and LSTMs,\nand dealing with noisy or non-stationary objectives.\n5.4.6\nAdaDelta\nAdaptive Delta (AdaDelta) improves on AdaGrad and RMSprop, focusing on adaptive learning rates\nwithout diminishing too quickly.\nHow it Works: AdaDelta eliminates the need for a default learning rate by using a moving window\nof gradient updates. It adapts learning rates based on recent gradient magnitudes to ensure consistent\nupdates even with sparse gradients.\nPros:\n\u2022 Eliminates the need to set a default learning rate.\n\u2022 Addresses the diminishing learning rate issue.\n\u2022 Does not require manual tuning of the learning rate.\n\u2022 Handles gradient sparsity well.\nCons:\n\u2022 More complex than RMSprop and AdaGrad.\n\u2022 Can have slower convergence initially.\n\u2022 Can require more iterations to converge.\n\u2022 Implementation can be more complex.\nWhen to Use: AdaDelta is suitable for scenarios similar to RMSprop but is preferred when avoiding\nmanual learning rate setting.\n30\n5.4.7\nAdam\nAdaptive Moment Estimation (Adam) combines the advantages of AdaGrad and RMSprop, making it\nsuitable for problems with large datasets and high-dimensional spaces.\nHow it Works: Adam uses running averages of both gradients and their squared values to com-\npute adaptive learning rates for each parameter. It includes bias correction and often achieves faster\nconvergence than other methods.\nPros:\n\u2022 Combines advantages of AdaGrad and RMSprop.\n\u2022 Adaptive learning rates.\n\u2022 Includes bias correction.\n\u2022 Fast convergence.\n\u2022 Works well with large datasets and high-dimensional spaces.\nCons:\n\u2022 Requires tuning of hyperparameters (though it often works well with defaults).\n\u2022 Computationally intensive.\n\u2022 Can lead to overfitting if not regularised properly.\n\u2022 Requires more memory.\nWhen to Use: Adam is widely used in most deep learning applications due to its efficiency and\neffectiveness, particularly in complex neural network architectures.\n5.4.8\nAdamW\nAdamW is an extension of Adam that includes weight decay regularisation to address overfitting issues\npresent in Adam.\nHow it Works: AdamW integrates L2 regularisation directly into the parameter updates, decoupling\nweight decay from the learning rate. This improves generalisation and is suitable for fine-tuning large\nmodels.\nPros:\n\u2022 Includes weight decay for better regularisation.\n\u2022 Combines Adam\u2019s adaptive learning rate with L2 regularisation.\n\u2022 Improves generalisation.\n\u2022 Reduces overfitting compared to Adam.\nCons:\n\u2022 Slightly more complex than Adam.\n\u2022 Requires careful tuning of the weight decay parameter.\n\u2022 Slightly slower than Adam due to additional computations.\n\u2022 Requires more memory.\nWhen to Use: AdamW is ideal for scenarios where regularisation is needed, such as preventing\noverfitting in large models and fine-tuning pre-trained models.\nA comprehensive collection of optimisation algorithms implemented within the PyTorch library can be\nfound in here. The Hugging Face Transformers package also offers a variety of optimisers for initialising\nand fine-tuning language models, available here.\n31\n5.5\nChallenges in Training Setup\n1. Ensuring compatibility and proper configuration of high-performance hardware like GPUs or TPUs\ncan be complex and time-consuming.\n2. Managing dependencies and versions of deep learning frameworks and libraries to avoid conflicts\nand leverage the latest features.\n3. Selecting an appropriate learning rate is critical, as too high a rate can cause suboptimal conver-\ngence, while too low a rate can make the training process excessively slow.\n4. Determining the optimal batch size that balances", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities", "description": "Arxiv Paper titled 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities' authored by Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid", "url": "https://arxiv.org/pdf/2408.13296v1", "source": "arxiv", "id": "b8df9242-ae37-412d-8692-d7e8b7aa4600"}, "page_content": " Use: AdamW is ideal for scenarios where regularisation is needed, such as preventing\noverfitting in large models and fine-tuning pre-trained models.\nA comprehensive collection of optimisation algorithms implemented within the PyTorch library can be\nfound in here. The Hugging Face Transformers package also offers a variety of optimisers for initialising\nand fine-tuning language models, available here.\n31\n5.5\nChallenges in Training Setup\n1. Ensuring compatibility and proper configuration of high-performance hardware like GPUs or TPUs\ncan be complex and time-consuming.\n2. Managing dependencies and versions of deep learning frameworks and libraries to avoid conflicts\nand leverage the latest features.\n3. Selecting an appropriate learning rate is critical, as too high a rate can cause suboptimal conver-\ngence, while too low a rate can make the training process excessively slow.\n4. Determining the optimal batch size that balances memory constraints and training efficiency, es-\npecially given the large memory requirements of LLMs.\n5. Choosing the right number of epochs to avoid underfitting or overfitting the model, requiring careful\nmonitoring and validation.\n6. Selecting the most suitable optimiser for the specific training task to efficiently update the model\u2019s\nweights.\n7. Choosing the correct loss function to accurately measure model performance and guide the opti-\nmisation process.\n5.6\nBest Practices\n\u2022 Optimal Learning Rate: Use a lower learning rate, typically between 1e-4 to 2e-4, to ensure\nstable convergence. A learning rate schedule, such as learning rate warm-up followed by a linear\ndecay, can also be beneficial. This helps in initially stabilising the training and then allowing the\nmodel to converge more accurately.\n\u2022 Batch Size Considerations: Opt for a batch size that balances memory constraints and training\nefficiency.\nSmaller batch sizes can help in achieving faster convergence but may require more\nfrequent updates. Conversely, larger batch sizes can be more memory-intensive but may lead to\nmore stable updates. Experiment with different batch sizes to find the optimal balance for your\nspecific use case.\n\u2022 Save Checkpoints Regularly: Regularly save model weights at various intervals across 5-8\nepochs to capture optimal performance without overfitting. Implement early stopping mechanisms\nto halt training once the model performance starts to degrade on the validation set, thereby pre-\nventing overfitting [51].\n\u2022 Hyperparameter Tuning: Utilise hyperparameter tuning methods like grid search, random\nsearch, and Bayesian optimisation to find the optimal set of hyperparameters.\nTools such as\nOptuna, Hyperopt, and Ray Tune can automate this process and help in efficiently exploring the\nhyperparameter space [49].\n\u2022 Data Parallelism and Model Parallelism: For large-scale training, consider using data paral-\nlelism or model parallelism techniques to distribute the training workload across multiple GPUs or\nTPUs. Libraries like Horovod and DeepSpeed can facilitate efficient distributed training, helping\nto reduce training time and manage memory usage effectively [52, 53].\n\u2022 Regular Monitoring and Logging: Implement robust monitoring and logging to track training\nmetrics, resource usage, and potential bottlenecks. Tools like TensorBoard, Weights & Biases, and\nMLflow can provide real-time insights into the training process, allowing for timely interventions\nand adjustments.\n\u2022 Handling Overfitting and Underfitting: Ensure that your model generalises well by imple-\nmenting techniques to handle overfitting and underfitting. regularisation techniques such as L2\nregularisation, dropout, and data augmentation can help prevent overfitting. Conversely, if your\nmodel is underfitting, consider increasing the model complexity or training for more epochs.\n32\n\u2022 Use Mixed Precision Training: Mixed precision training involves using both 16-bit and 32-bit\nfloating-point types to reduce memory usage and increase computational efficiency. This technique\ncan significantly speed up training and reduce the required memory footprint, especially when\nusing large models. NVIDIA\u2019s Apex and TensorFlow\u2019s mixed precision API provide support for\nimplementing mixed precision training [54].\n\u2022 Evaluate and Iterate: Continuously evaluate the model performance using a separate validation\nset and iterate on the training process based on the results. Regularly update your training data\nand retrain the model to keep it current with new data trends and patterns.\n\u2022 Documentation and Reproducibility: Maintain thorough documentation of your training\nsetup, including the hardware configuration, software environment, and hyperparameters used.\nEnsure reproducibility by setting random seeds and providing detailed records of the training\nprocess. This practice not only aids in debugging and further development but also facilitates\ncollaboration and sharing of results with the broader research community.\n33\nChapter 6\nStage 4: Selection of Fine-Tuning\nTechniques and Appropriate Model\nConfigurations\nThis chapter focuses on selecting appropriate fine-tuning techniques and model configurations that suit\nthe specific requirements of various tasks. Fine-tuning is a crucial stage where pre-trained models are\nadapted to specific tasks or domains.\n6.1\nSteps Involved in Fine-Tuning\nThe following steps outline the fine-tuning process, integrating advanced techniques and best practices.\n1. Initialise the Pre-Trained Tokenizer and Model: Begin by loading the pre-trained tokenizer\nand model. The tokenizer ensures that the input text is converted into a format the model can\nprocess, while the pre-trained model serves as the foundation for further adaptation. Depending", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities", "description": "Arxiv Paper titled 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities' authored by Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid", "url": "https://arxiv.org/pdf/2408.13296v1", "source": "arxiv", "id": "372f1a8b-8ec1-4490-9b67-4d0f61fa2b5f"}, "page_content": " training\nprocess. This practice not only aids in debugging and further development but also facilitates\ncollaboration and sharing of results with the broader research community.\n33\nChapter 6\nStage 4: Selection of Fine-Tuning\nTechniques and Appropriate Model\nConfigurations\nThis chapter focuses on selecting appropriate fine-tuning techniques and model configurations that suit\nthe specific requirements of various tasks. Fine-tuning is a crucial stage where pre-trained models are\nadapted to specific tasks or domains.\n6.1\nSteps Involved in Fine-Tuning\nThe following steps outline the fine-tuning process, integrating advanced techniques and best practices.\n1. Initialise the Pre-Trained Tokenizer and Model: Begin by loading the pre-trained tokenizer\nand model. The tokenizer ensures that the input text is converted into a format the model can\nprocess, while the pre-trained model serves as the foundation for further adaptation. Depending\non the task, select a model that has been pre-trained on relevant data to provide a strong starting\npoint.\n2. Modify the Model\u2019s Output Layer: Adjust the model\u2019s output layer to align with the specific\nrequirements of the target task. This may involve modifying existing layers or adding new layers.\nFor instance, tasks like classification may require a softmax layer with the appropriate number of\nclasses, while text generation tasks might involve changes in the decoding mechanism.\n3. Choose an Appropriate Fine-Tuning Strategy: Select the fine-tuning strategy that best fits\nthe task and the model architecture. Some Options include:\n\u2022 Task-Specific Fine-Tuning: For tasks such as text summarisation, code generation, classi-\nfication, and question answering, adapt the model using relevant datasets.\n\u2022 Domain-Specific Fine-Tuning: Tailor the model to comprehend and generate text relevant\nto specific domains, such as medical, financial, or legal fields.\n\u2022 Parameter-Efficient Fine-Tuning (PEFT): Techniques like LoRA, QLoRA, and adapters\nallow for fine-tuning with reduced computational costs by updating a small subset of model\nparameters.\n\u2022 Half Fine-Tuning (HFT): Balance between retaining pre-trained knowledge and learning\nnew tasks by updating only half of the model\u2019s parameters during each fine-tuning round.\n4. Set Up the Training Loop: Establish the training loop, incorporating the selected fine-tuning\nstrategy. The loop should include data loading, loss computation, backpropagation, and parameter\nupdates.\nWhen using PEFT methods, ensure that only the relevant parameters are updated\nto maximise efficiency. Implement techniques like dynamic learning rates and early stopping to\nenhance the training process.\n34\n5. Incorporate Techniques for Handling Multiple Tasks: If fine-tuning for multiple tasks,\nconsider strategies like fine-tuning with multiple adapters or leveraging Mixture of Experts (MoE)\narchitectures. These methods allow a single model to handle various tasks by utilising specialised\nsub-networks or adapters for each task.\n6. Monitor Performance on a Validation Set: Regularly evaluate the model\u2019s performance on\na validation set to ensure it generalises well to unseen data.\nAdjust hyperparameters such as\nlearning rate, batch size, and dropout rates based on the validation performance. Utilise advanced\nmonitoring tools to track metrics like accuracy, loss, and overfitting.\n7. Optimise Model Using Advanced Techniques: Employ techniques such as Proximal Policy\nOptimisation (PPO) for reinforcement learning scenarios, or Direct Preference Optimisation (DPO)\nfor aligning model outputs with human preferences. These techniques are particularly useful in\nfine-tuning models for tasks requiring nuanced decision-making or human-like responses.\n8. Prune and optimise the Model (if necessary): To deploy the model in resource-constrained\nenvironments, consider pruning techniques to reduce its size and complexity. This involves removing\nunnecessary parameters or components without significantly affecting performance. Utilise dynamic\npruning methods during inference to optimise the model on-the-fly for different scenarios.\n9. Continuous Evaluation and Iteration: Continuously evaluate the model\u2019s performance across\nvarious tasks using appropriate benchmarks. Iterate on the fine-tuning process, making adjustments\nbased on performance metrics and real-world testing. This iterative approach helps in refining the\nmodel to meet specific performance criteria.\n6.2\nFine-Tuning Strategies for LLMs\n6.2.1\nTask-Specific Fine-Tuning\nTask-specific fine-tuning adapts large language models (LLMs) for particular downstream tasks using\nappropriately formatted and cleaned data. Below is a summary of key tasks suitable for fine-tuning\nLLMs, including examples of LLMs tailored to these tasks.\nTask\nDescription\nKey Models\nText Summarisation\nCondensing long texts into coherent sum-\nmaries while retaining key information. Ap-\nproaches include Extractive (selecting key\nsentences) and Abstractive summarisation\n(generating new sentences).\nBERTSUM, GPT-3, T5\nCode Generation\nAutomatically generating programming code\nbased on natural language descriptions, par-\ntial code snippets, or structured data inputs.\nCodex, GPT-3, CodeBERT\nClassification\nCategorising text into predefined labels such\nas Sentiment Analysis, Topic Classification,\nand Entity Classification.\nBERT, RoBERTa, GPT-4\nQ&", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities", "description": "Arxiv Paper titled 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities' authored by Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid", "url": "https://arxiv.org/pdf/2408.13296v1", "source": "arxiv", "id": "97545df2-d601-4978-af6e-5ba21a5a7c18"}, "page_content": " models (LLMs) for particular downstream tasks using\nappropriately formatted and cleaned data. Below is a summary of key tasks suitable for fine-tuning\nLLMs, including examples of LLMs tailored to these tasks.\nTask\nDescription\nKey Models\nText Summarisation\nCondensing long texts into coherent sum-\nmaries while retaining key information. Ap-\nproaches include Extractive (selecting key\nsentences) and Abstractive summarisation\n(generating new sentences).\nBERTSUM, GPT-3, T5\nCode Generation\nAutomatically generating programming code\nbased on natural language descriptions, par-\ntial code snippets, or structured data inputs.\nCodex, GPT-3, CodeBERT\nClassification\nCategorising text into predefined labels such\nas Sentiment Analysis, Topic Classification,\nand Entity Classification.\nBERT, RoBERTa, GPT-4\nQ&A\nUnderstanding and generating accurate, con-\ntextually relevant answers to natural lan-\nguage questions.\nBERT, GPT-3, T5\nTable 6.1: Overview of tasks such as text summarisation, code generation, classification, and Q&A, along\nwith their key LLMs and descriptions.\n6.2.2\nDomain-Specific Fine-Tuning\nDomain-specific fine-tuning focuses on tailoring the model to comprehend and produce text relevant to\na specific domain or industry. By fine-tuning the model on a dataset derived from the target domain,\nit enhances the model\u2019s contextual understanding and expertise in domain-specific tasks. Below are\nexamples of domain-specific LLMs.\n35\nMedical Domain\nModel Description: Med-PaLM 2 is trained on meticulously curated medical datasets and is capable\nof accurately answering medical questions, achieving performance comparable to that of medical profes-\nsionals [55].\nBase Model: PaLM 2\nFine-tuned Model Parameters: Not Known\nFine-Tuning Techniques Used: Instruction fine-tuning\nDatasets Used:\n\u2022 MedQA\n\u2022 MedMCQA\n\u2022 LiveQA\n\u2022 MedicationQA\n\u2022 HealthSearchQA\nResults: Med-PaLM 2 outperformed GPT-4 in several key medical benchmarks, demonstrating superior\nperformance in handling complex medical knowledge and reasoning tasks.\nFinance Domain\nModel Description: FinGPT, an open-source LLM tailored for the financial sector, enhances financial\nresearch and cooperation by promoting data accessibility and handling finance-specific issues like data\nacquisition and quality [56].\nBase Model: LlaMA, ChatGLM, and other Transformer Models\nFine-tuned Model Parameters: Not Known\nFine-Tuning Techniques Used: LoRA, Reinforcement Learning on Stock Prices (RLSP)\nDatasets Used:\n\u2022 Financial News (Reuters, CNBC, Yahoo Finance)\n\u2022 Social Media (Twitter, Facebook, Reddit, Weibo)\n\u2022 Regulatory Filings (e.g., SEC filings)\n\u2022 Trends (Seeking Alpha, Google Trends)\n\u2022 Academic Datasets\nResults: Not Applicable\nLegal Domain\nModel Description: LAWGPT, the first open-source model specifically designed for Chinese legal\napplications, demonstrates superior capability in handling Chinese legal tasks [57].\nBase Model: Chinese Alpaca Plus 7B base model\nFine-tuned Model Parameters: Not Known\nFine-Tuning Techniques Used: LoRA with Alpaca template\nDatasets Used:\n\u2022 Open-source dataset: 200,000 examples containing crime type prediction and crime consultation\ntasks.\n\u2022 JEC-QA dataset: 20,000 examples containing legal question answering tasks.\n\u2022 Constructed legal dataset: 80,000 examples, refined from open-source and JEC-QA datasets using\nChatGPT.\nResults: LAWGPT demonstrates notable performance improvements over the LLaMA 7B model in\nvarious legal tasks, but still trails behind proprietary models like GPT-3.5 Turbo and GPT-4.\n36\nPharmaceutical Domain\nModel Description: PharmaGPT, a suite of domain-specific large language models tailored to the\nbiopharmaceutical and chemical industries, sets a new benchmark for precision in these fields [58].\nBase Model: LlaMA series\nFine-tuned Model Parameters: 13B and 70B\nFine-Tuning Techniques Used: Instruction fine-tuning and RLHF\nDatasets Used:\n\u2022 Specific-domain data from academic papers and clinical reports\n\u2022 Text data from NLP dataset formats (e.g., question answering, summarisation, dialogue)\n\u2022 Instruction fine-tuning dataset for multitask learning\n\u2022 RLHF dataset with human preference expert-annotated instructions\nResults: PharmaGPT models demonstrated impressive performance on various pharmaceutical bench-\nmarks, consistently outperforming GPT-3.5 Turbo.\nFinance Domain\nModel Description: Palmyra-Fin-70B-32K, developed by Writer, is a leading large language model\nspecifically designed for the financial sector. [59]\nBase Model: LlaMA\nFine-tuned Model Parameters: 70B\nFine-Tuning Techniques Used: Not Known\nDatasets Used: Not Known\nResults: Palmyra-Fin-70B-32K exhibits state-of-the-art performance, achieving leading results across\nvarious financial datasets and excelling in financial document analysis, market trend prediction, and risk\nassessment.\n6.3\nParameter-Efficient Fine-Tuning (PEFT) Techniques\nParameter", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities", "description": "Arxiv Paper titled 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities' authored by Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid", "url": "https://arxiv.org/pdf/2408.13296v1", "source": "arxiv", "id": "bb7557b7-7a63-474e-b8b3-aa5e03337acf"}, "page_content": "uning dataset for multitask learning\n\u2022 RLHF dataset with human preference expert-annotated instructions\nResults: PharmaGPT models demonstrated impressive performance on various pharmaceutical bench-\nmarks, consistently outperforming GPT-3.5 Turbo.\nFinance Domain\nModel Description: Palmyra-Fin-70B-32K, developed by Writer, is a leading large language model\nspecifically designed for the financial sector. [59]\nBase Model: LlaMA\nFine-tuned Model Parameters: 70B\nFine-Tuning Techniques Used: Not Known\nDatasets Used: Not Known\nResults: Palmyra-Fin-70B-32K exhibits state-of-the-art performance, achieving leading results across\nvarious financial datasets and excelling in financial document analysis, market trend prediction, and risk\nassessment.\n6.3\nParameter-Efficient Fine-Tuning (PEFT) Techniques\nParameter Efficient Fine Tuning (PEFT) is an impactful NLP technique that adeptly adapts pre-trained\nlanguage models to various applications with remarkable efficiency. PEFT methods fine-tune only a\nsmall subset of (additional) model parameters while keeping most of the pre-trained LLM parameters\nfrozen, thereby significantly reducing computational and storage costs. This approach mitigates the issue\nof catastrophic forgetting, a phenomenon where neural networks lose previously acquired knowledge and\nexperience a significant performance decline on previously learned tasks when trained on new datasets.\nPEFT methods have demonstrated superior performance compared to full fine-tuning, particularly in\nlow-data scenarios, and exhibit better generalisation to out-of-domain contexts. This technique is appli-\ncable to various modalities, such as financial sentiment classification and machine translation of medical\nterminologies. A taxonomy of PEFT-based fine-tuning approaches is provided in Figure6.1. We will\nfurther discuss a few key PEFT-based approaches in the following sections.\n6.3.1\nAdapters\nAdapter-based methods introduce additional trainable parameters after the attention and fully connected\nlayers of a frozen pre-trained model, aiming to reduce memory usage and accelerate training. The specific\napproach varies depending on the adapter; it might involve adding an extra layer or representing the\nweight updates delta (W) as a low-rank decomposition of the weight matrix. Regardless of the method,\nadapters are generally small yet achieve performance comparable to fully fine-tuned models, allowing for\nthe training of larger models with fewer resources.\nHuggingFace supports adapter configurations through the PEFT library. During fine-tuning, new adapters\nare integrated into the model using LoraConfig 1. HuggingFace uses PeftConfig to load existing pre-\ntrained models and apply PEFT techniques. Additionally, HuggingFace provides built-in support to\n1https://huggingface.co/docs/peft/en/package_reference/lora\n37\nFigure 6.1: Comprehensive Taxonomy of Parameter-Efficient Fine-Tuning (PEFT) Methods for Large\nLanguage Models (LLMs). This figure categorises various PEFT techniques, highlighting their distinct\napproaches, from additive and selective fine-tuning to reparameterised and hybrid methods. It details\nspecific strategies within each category, such as Adapter-Based Fine-Tuning, Soft Prompt-Based Fine-\nTuning, and their respective sub-techniques like LoRA and its derivatives, showcasing the diverse and\nevolving landscape of LLM fine-tuning. (adapted from [60])\nrun the fine-tuning process across any distributed configuration using Accelerate2, making large-scale\ntraining and inference simple, efficient, and adaptable.\n6.3.2\nLow-Rank Adaptation (LoRA)\nLow-Rank Adaptation (LoRA)[62] is a technique designed for fine-tuning large language models, which\nmodifies the fine-tuning process by freezing the original model weights and applying changes to a separate\nset of weights, added to the original parameters. LoRA transforms the model parameters into a lower-\nrank dimension, reducing the number of trainable parameters, speeding up the process, and lowering\ncosts. This method is particularly useful in scenarios where multiple clients require fine-tuned models\nfor different applications, allowing for the creation of specific weights for each use case without the\nneed for separate models. By employing low-rank approximation methods, LoRA effectively reduces\ncomputational and resource requirements while preserving the pre-trained model\u2019s adaptability to specific\ntasks or domains.\nBenefits of Using LoRA\n1. Parameter Efficiency: LoRA significantly reduces the number of parameters that need to be\ntrained by focusing only on the low-rank matrices, resulting in lower memory and storage require-\nments compared to full fine-tuning.\n2. Efficient Storage: The storage of the trained model is more efficient as it only requires storing\nthe low-rank matrices instead of the full model weights.\n2https://huggingface.co/docs/accelerate/en/index\n38\nFigure 6.2: Schematic representation of the Adapter Architecture used in LLMs. The diagram showcases\nthe integration of adapters within the Transformer architecture, including the feed-forward up and down\nlayers and their role in enabling efficient model adaptation by inserting additional parameters while\nmaintaining the model\u2019s core structure (adapted from [61])\n3. Reduced Computational Load: Training with low-rank matrices requires fewer computational\nresources, making it faster and more scalable.\n4. Lower Memory Footprint: Since", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities", "description": "Arxiv Paper titled 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities' authored by Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid", "url": "https://arxiv.org/pdf/2408.13296v1", "source": "arxiv", "id": "a98e37f3-4827-4bf1-878c-d542321705b2"}, "page_content": " that need to be\ntrained by focusing only on the low-rank matrices, resulting in lower memory and storage require-\nments compared to full fine-tuning.\n2. Efficient Storage: The storage of the trained model is more efficient as it only requires storing\nthe low-rank matrices instead of the full model weights.\n2https://huggingface.co/docs/accelerate/en/index\n38\nFigure 6.2: Schematic representation of the Adapter Architecture used in LLMs. The diagram showcases\nthe integration of adapters within the Transformer architecture, including the feed-forward up and down\nlayers and their role in enabling efficient model adaptation by inserting additional parameters while\nmaintaining the model\u2019s core structure (adapted from [61])\n3. Reduced Computational Load: Training with low-rank matrices requires fewer computational\nresources, making it faster and more scalable.\n4. Lower Memory Footprint: Since fewer parameters are being updated, the memory footprint\nduring training is reduced, enabling the use of larger batch sizes or more complex models within\nthe same hardware constraints.\n5. Flexibility: LoRA can be easily integrated with existing pre-trained models without extensive\nmodifications to the model architecture.\n6. Compatibility: It can be used alongside other fine-tuning techniques, such as adapter layers or\nprompt-tuning, to further enhance performance.\n7. Comparable Results: Despite the reduction in the number of trainable parameters, LoRA has\nbeen shown to achieve performance comparable to full fine-tuning in many tasks.\n8. Task-Specific Adaptation: It effectively adapts the pre-trained model to specific tasks, leverag-\ning the knowledge already embedded in the original model.\n9. Avoiding Overfitting: By focusing on low-rank updates, LoRA can help in mitigating overfitting,\nespecially when dealing with smaller task-specific datasets.\nLimitations\nWhile LoRA demonstrates considerable power, it also presents challenges:\n\u2022 Fine-tuning Scope: LoRA may face difficulties when applied to tasks demanding substantial\nalterations to the pre-trained model\u2019s internal representations.\n\u2022 Hyperparameter Optimisation: Tuning the rank parameter \u2018r\u2019 requires meticulous adjustment\nfor optimal performance.\n\u2022 Ongoing Research: Despite its promise, LoRA is still in active research stages, and its long-term\nimplications remain to be fully explored.\n39\nFigure 6.3: A comparison between weight updates in regular fine-tuning and LoRA fine-tuning.\nIn\nregular fine-tuning, the entire weight update matrix (\u2206W) is applied to the pre-trained weights. In\ncontrast, LoRA fine-tuning introduces two low-rank matrices (A and B) that approximate the weight\nupdate matrix (\u2206W), significantly reducing the number of trainable parameters by leveraging the inner\ndimension (r), which is a hyperparameter.\nThis method is more efficient in terms of memory and\ncomputation, making it ideal for fine-tuning large models. (adapted from [63])\nDespite these challenges, LoRA stands as a pioneering technique with vast potential to democratise access\nto the capabilities of LLMs. Continued research and development offer the prospect of overcoming current\nlimitations and unlocking even greater efficiency and adaptability.\nTutorial for Fine-Tuning LLM Using LoRA\nAn open-source template for fine-tuning LLMs using the LoRA method with the Hugging Face library\ncan be found here. This template is designed specifically for adapting LLMs for instruction fine-tuning\nprocesses.\n6.3.3\nQLoRA\nQLoRA[64] is an extended version of LoRA designed for greater memory efficiency in large language mod-\nels (LLMs) by quantising weight parameters to 4-bit precision. Typically, LLM parameters are stored\nin a 32-bit format, but QLoRA compresses them to 4-bit, significantly reducing the memory footprint.\nThis allows fine-tuning on less powerful hardware, including consumer GPUs. QLoRA also quantises the\nweights of the LoRA adapters from 8-bit to 4-bit, further decreasing memory and storage requirements\n(see Figure 6.4). Despite the reduction in bit precision, QLoRA maintains performance levels comparable\nto traditional 16-bit fine-tuning.\nIt achieves this by backpropagating gradients through a frozen, 4-bit quantised pre-trained language\nmodel into Low-Rank Adapters, making the fine-tuning process efficient while preserving model effective-\nness. The QLoRA configuration is supported by HuggingFace via the PEFT library, utilising LoraConfig\nand BitsAndBytesConfig for quantisation. Innovations such as an optimal 4-bit data type, double quan-\ntisation of constants, and memory spike management enable QLoRA to reduce memory usage from 96\nbits per parameter in traditional fine-tuning to 5.2 bits per parameter, an 18-fold reduction.\nPerformance-wise, QLoRA outperforms naive 4-bit quantisation and matches 16-bit quantised models\non benchmarks. Additionally, QLoRA enabled the fine-tuning of a high-quality 4-bit chatbot using a\nsingle GPU in 24 hours, achieving quality comparable to ChatGPT.\nThis tutorial explains the end-to-end steps of fine-tuning QLoRA on a custom dataset for the Phi-2\nmodel.\n40\nFigure 6.4: Quantised Low-Rank Adaptation (QLoRA) Optimisation Workflow", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities", "description": "Arxiv Paper titled 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities' authored by Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid", "url": "https://arxiv.org/pdf/2408.13296v1", "source": "arxiv", "id": "44add56c-6ca4-4684-8478-d34438c16c12"}, "page_content": " library, utilising LoraConfig\nand BitsAndBytesConfig for quantisation. Innovations such as an optimal 4-bit data type, double quan-\ntisation of constants, and memory spike management enable QLoRA to reduce memory usage from 96\nbits per parameter in traditional fine-tuning to 5.2 bits per parameter, an 18-fold reduction.\nPerformance-wise, QLoRA outperforms naive 4-bit quantisation and matches 16-bit quantised models\non benchmarks. Additionally, QLoRA enabled the fine-tuning of a high-quality 4-bit chatbot using a\nsingle GPU in 24 hours, achieving quality comparable to ChatGPT.\nThis tutorial explains the end-to-end steps of fine-tuning QLoRA on a custom dataset for the Phi-2\nmodel.\n40\nFigure 6.4: Quantised Low-Rank Adaptation (QLoRA) Optimisation Workflow. This figure illustrates\nthe QLoRA optimisation process, showing how the optimisation states, adapters, and the model interact\nduring fine-tuning. It demonstrates the use of different bit-widths (32-bit, 16-bit, and 4-bit) to optimise\nthe memory and computational efficiency during the fine-tuning of large language models (adapted from\n[65]).\n6.3.4\nWeight-Decomposed Low-Rank Adaptation (DoRA)\nIn the context of optimising model fine-tuning, the pattern analysis of LoRA and Full Fine-Tuning\n(FT) reveals significant differences in learning behaviours and updates. LoRA, employing a strategy of\nincrementally updating pre-trained weights using the product of two low-rank matrices, maintains the\noriginal weights largely static during the fine-tuning process, which allows for efficient inference. Despite\nits computational efficiency, previous studies have suggested that LoRA\u2019s limited number of trainable\nparameters might contribute to its performance discrepancies when compared to FT.\nWeight-Decomposed Low-Rank Adaptation (DoRA) [66] is a novel fine-tuning methodology designed to\noptimise pre-trained models by decomposing their weights into magnitude and directional components.\nThis approach leverages the efficiency of Low-Rank Adaptation (LoRA) for directional updates, facili-\ntating substantial parameter updates without altering the entire model architecture. DoRA addresses\nthe computational challenges associated with traditional full fine-tuning (FT) by maintaining model\nsimplicity and inference efficiency, while simultaneously bridging the performance gap typically observed\nbetween LoRA and FT. Empirical and theoretical evaluations demonstrate that DoRA not only achieves\nlearning outcomes comparable to FT across diverse tasks\u2014including natural language processing and\nvision-language applications\u2014but also consistently surpasses LoRA in performance, providing a robust\nsolution for enhancing the adaptability and efficiency of large-scale models.\nPython Library - DoRA is facilitated via the HuggingFace LoraConfig package. To incorporate DoRA\ninto the fine-tuning process, it is essential to specify the \u2019use dora = True\u2019 parameter during the Lora\nconfiguration. Further information on initialisation can be found here.\nBenefits of DoRA\n1. Enhanced Learning Capacity: DoRA achieves a learning capacity closely resembling full fine-\ntuning (FT) by decomposing pre-trained weights into magnitude and directional components, al-\nlowing for more nuanced updates.\n2. Efficient Fine-Tuning: By utilising the structural advantages of Low-Rank Adaptation (LoRA)\nfor directional updates, DoRA enables efficient fine-tuning without altering the entire model archi-\ntecture.\n3. No Additional Inference Latency: Despite its improved learning capabilities, DoRA does not\nintroduce any additional inference latency over LoRA, maintaining model simplicity and efficiency.\n4. Superior Performance: Experimental results demonstrate that DoRA consistently outperforms\nLoRA across a wide range of tasks, including natural language processing (NLP), visual instruction\ntuning, and image/video-text understanding. For example, it shows significant improvements in\ncommonsense reasoning and visual instruction tuning benchmarks.\n5. Versatility Across Backbones: DoRA has been validated across various model backbones,\nincluding large language models (LLM) and vision-language models (LVLM), indicating its broad\n41\nFigure 6.5: An overview of DoRA (Decomposed Representations for Adaptation), which is a method for\nweight decomposed low-rank adaptation. The figure illustrates how pre-trained weights are decomposed\nand adapted for fine-tuning. In the left section, pre-trained weights are decomposed into a magnitude and\ndirection. The right section shows how these decomposed weights are merged with trainable parameters\nduring fine-tuning, resulting in updated weights that combine both frozen (blue) and trainable (green)\ncomponents. The process emphasises efficient adaptation by focusing on the most significant directions\nin the parameter space, facilitating effective fine-tuning while maintaining the integrity of the original\nmodel (adapted from [66]).\napplicability and robustness in different domains.\n6. Innovative Analysis: The introduction of a novel weight decomposition analysis helps uncover\nfundamental differences in the learning patterns of FT and various parameter-efficient fine-tuning\n(PEFT) methods, contributing to a deeper understanding of model fine-tuning dynamics.\nComparison between LoRA and DoRA\nLow-Rank Adaptation (LoRA) and Weight-Decomposed Low-Rank Adaptation (DoRA) are both ad-\nvanced techniques designed to improve the efficiency and effectiveness of fine-tuning large pre", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities", "description": "Arxiv Paper titled 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities' authored by Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid", "url": "https://arxiv.org/pdf/2408.13296v1", "source": "arxiv", "id": "2335e7dc-5474-43fd-99d3-e79a5919665c"}, "page_content": " section shows how these decomposed weights are merged with trainable parameters\nduring fine-tuning, resulting in updated weights that combine both frozen (blue) and trainable (green)\ncomponents. The process emphasises efficient adaptation by focusing on the most significant directions\nin the parameter space, facilitating effective fine-tuning while maintaining the integrity of the original\nmodel (adapted from [66]).\napplicability and robustness in different domains.\n6. Innovative Analysis: The introduction of a novel weight decomposition analysis helps uncover\nfundamental differences in the learning patterns of FT and various parameter-efficient fine-tuning\n(PEFT) methods, contributing to a deeper understanding of model fine-tuning dynamics.\nComparison between LoRA and DoRA\nLow-Rank Adaptation (LoRA) and Weight-Decomposed Low-Rank Adaptation (DoRA) are both ad-\nvanced techniques designed to improve the efficiency and effectiveness of fine-tuning large pre-trained\nmodels. While they share the common goal of reducing computational overhead, they employ different\nstrategies to achieve this (see Table6.2).\n42\nCriteria\nLoRA\n(Low-Rank\nAdapta-\ntion)\nDoRA\n(Weight-Decomposed\nLow-Rank Adaptation)\nObjective\nProvide an efficient method for\nfine-tuning pre-trained models by\nusing low-rank matrix products\nto update weights incrementally\nwithout increasing inference la-\ntency.\nImproves\nlearning\ncapacity\nby\nclosely mimicking the learning pat-\nterns of full fine-tuning, optimis-\ning magnitude and direction sep-\narately.\nApproach\nImplements a low-rank decompo-\nsition where the weight update is\nmodelled as the product of two\nlow-rank matrices (B and A), keep-\ning the original weights static.\nUses weight decomposition anal-\nysis to reparameterise the weight\nmatrix into separate magnitude\nand direction components for dis-\ntinct updates.\nModel Architecture\nKeeps the pre-trained weight ma-\ntrix (W0) unchanged and applies\nupdates using low-rank matrices\n(B and A). Matrix A is initialised\nwith a uniform Kaiming distribu-\ntion, while B is set to zero initially.\nRestructures\nthe\nweight\nmatrix\ninto\nmagnitude\nand\ndirectional\ncomponents, ensuring directional\nvectors are unit vectors for more\ndetailed adjustments.\nTable 6.2:\nA detailed comparison between LoRA (Low-Rank Adaptation) and DoRA (Weight-\nDecomposed Low-Rank Adaptation), highlighting their objectives, approaches, and the specific architec-\ntural strategies they employ for fine-tuning large language models.\nTutorial for Fine-Tuning LLM using DoRA\nThis tutorial offers an in-depth guide and detailed explanation of the steps involved in implementing\nDoRA from scratch, as well as insights into the fine-tuning process essential for optimising performance.\n6.3.5\nFine-Tuning with Multiple Adapters\nDuring fine-tuning, we have explored the method of freezing the parameters of the LLM and focusing\nsolely on fine-tuning a few million trainable parameters using LoRA. For example, fine-tuning an LLM\nfor translation involves training a translation adapter with relevant data. This approach allows us to\nfine-tune separate adapters for each specific task we want the LLM to perform. However, a key question\narises: can we consolidate multiple adapters into a unified multi-task adapter? For instance, if we have\nseparate adapters for translation and summarisation tasks, can we merge them so that the LLM can\nproficiently handle both tasks? (Illustrated via Figure6.6).\nThe PEFT library simplifies the process of merging adapters with its add weighted adapter function 3,\nwhich offers three distinct methods:\n1. Concatenation: This straightforward method concatenates the parameters of the adapters. For\ninstance, if two adapters each have a rank of 16, the resulting adapter will have a rank of 32. This\nmethod is highly efficient.\n2. Linear Combination: Although less documented, this method appears to perform a weighted\nsum of the adapters\u2019 parameters.\n3. SVD: The default method employs singular value decomposition through torch.linalg.svd. While\nversatile, it is notably slower than the other methods, particularly for adapters with high ranks\n(greater than 100), which can take several hours.\nEach method allows for customising the combination by adjusting weights. For instance, when merging\ntwo adapters, X and Y, assigning more weight to X ensures that the resulting adapter prioritises behaviour\nsimilar to X over Y.\nThis approach is particularly suited for consolidating a single LLM to handle multiple tasks rather than\ncreating separate models for each task domain. By adopting this method, there is no longer a need to\n3https://huggingface.co/docs/peft/main/en/package_reference/lora#peft.LoraModel.add_weighted_adapter\n43\nindividually fine-tune a model for each task. Instead, a single adapter layer can be fine-tuned for each\ntask, allowing queries to yield the desired responses efficiently.\nFigure 6.6: Overview of how multiple adapters can be used with a pre-trained LLM to fine-tune it for\nvarious specific tasks, such as summarisation, proofreading,", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities", "description": "Arxiv Paper titled 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities' authored by Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid", "url": "https://arxiv.org/pdf/2408.13296v1", "source": "arxiv", "id": "62ac85a2-85cf-4ec4-9325-0d1342f8d8ad"}, "page_content": ". For instance, when merging\ntwo adapters, X and Y, assigning more weight to X ensures that the resulting adapter prioritises behaviour\nsimilar to X over Y.\nThis approach is particularly suited for consolidating a single LLM to handle multiple tasks rather than\ncreating separate models for each task domain. By adopting this method, there is no longer a need to\n3https://huggingface.co/docs/peft/main/en/package_reference/lora#peft.LoraModel.add_weighted_adapter\n43\nindividually fine-tune a model for each task. Instead, a single adapter layer can be fine-tuned for each\ntask, allowing queries to yield the desired responses efficiently.\nFigure 6.6: Overview of how multiple adapters can be used with a pre-trained LLM to fine-tune it for\nvarious specific tasks, such as summarisation, proofreading, sentiment analysis, and more. (adapted from\n[67])\nSteps for Fine-Tuning LLM with LoRA for Multiple Tasks and Adapters\n1. Adapter Creation: Create multiple adapters, each fine-tuned for specific tasks using different\nprompt formats or task-identifying tags (e.g., [translate fren], [chat]).\n2. LoRA Integration: Implement LoRA to efficiently integrate these adapters into the pre-trained\nLLM. Utilise LoRA\u2019s methods such as concatenation, linear combination, or singular value decom-\nposition (SVD) to combine adapters while minimising computational overhead and maintaining\nperformance.\n3. Task-Specific Adaptation: Fine-tune each adapter with task-specific data to enhance perfor-\nmance for individual tasks. Ensure adapters are trained with data relevant to their respective\ntasks, optimising their ability to generate accurate responses.\n4. Behaviour Adjustment: Monitor the behaviour of combined adapters to identify any undesired\ninherited behaviours from individual adapters (e.g., short response generation from a translation\n44\nadapter). Adjust the combination weights or types to modify adapter behaviour as needed, ensuring\neach adapter performs optimally for its intended task.\n5. Evaluation and Iteration: Evaluate the performance of the combined model across multiple\ntasks using validation datasets. Iterate on the fine-tuning process, making adjustments to adapter\ncombinations and training parameters based on performance metrics and user feedback.\nTherefore, for optimal performance, it is advisable to combine adapters that have been fine-tuned with\ndistinctly varied prompt formats. However, even when using adapters with different prompt formats, the\nresulting adapter may not exhibit desired behaviour. For example, a newly combined adapter designed for\nchatting may only generate short responses, inheriting this tendency from an adapter that was originally\ntrained to halt after producing a single sentence. To adjust the behaviour of the combined adapter,\none can prioritise the influence of a specific adapter during the combination process and/or modify the\nmethod of combination used.\nAn illustrative tutorial demonstrating the fine-tuning of large language models (LLMs) using multiple\nadapter layers for various tasks can be found here.\n6.4\nHalf Fine Tuning\nHalf Fine-Tuning (HFT)[68] is a technique designed to balance the retention of foundational knowledge\nwith the acquisition of new skills in large language models (LLMs). HFT involves freezing half of the\nmodel\u2019s parameters during each fine-tuning round while updating the other half, allowing the model to\nretain pre-trained knowledge and enhance new task performance without altering the model architecture.\nEach repetitive transformer layer is divided into three blocks: self-attention, feed-forward, and layernorm,\nwith half of the parameters in each block updated and the other half frozen, varying with each round.\nThis strategic parameter update helps maintain knowledge parity across training rounds and enhances\nscalability in successive training sessions.\nResearch on models like LLAMA 2-7B demonstrated that HFT could significantly restore forgotten basic\nknowledge while preserving high general ability performance. This method\u2019s robustness and efficiency\nmake it applicable to various fine-tuning scenarios, including supervised fine-tuning, direct preference\noptimisation, and continual learning. Additionally, HFT\u2019s ability to maintain the model architecture\nsimplifies its implementation and ensures compatibility with existing systems, further promoting its\npractical adoption.\n6.4.1\nBenefits of using Half Fine tuning\n1. Recovery of Pre-Trained Knowledge: By rolling back half of the fine-tuned parameters to their\npre-trained state, HFT effectively recovers a portion of the original knowledge, thereby mitigating\ncatastrophic forgetting of previously acquired capabilities.\n2. Enhanced Performance: Research experiments shows that HFT maintains or even surpasses\nthe performance of full fine-tuning (FFT) on downstream tasks, demonstrating its effectiveness in\nbalancing knowledge retention with task-specific learning.\n3. Robustness: The method is robust to different selection strategies and the number of parameters\nchosen for updating, ensuring consistent performance across various configurations.\n4. Simplicity and Scalability: HFT does not alter the model architecture, which simplifies im-\nplementation and allows for scalable applications, particularly beneficial in successive fine-tuning\nscenarios.\n5. Versatility: The technique has proven effective across diverse fine-tuning scenarios, including\nsupervised fine-tuning, direct preference optimisation, and continual learning.\n45\nFigure 6.7: Schematic illustration of the Half Fine-Tuning (HFT) method as applied to LLAMA 2\u2019s\narch", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities", "description": "Arxiv Paper titled 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities' authored by Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid", "url": "https://arxiv.org/pdf/2408.13296v1", "source": "arxiv", "id": "ff59aef6-cccf-42fc-9316-4d572dba2a91"}, "page_content": " of previously acquired capabilities.\n2. Enhanced Performance: Research experiments shows that HFT maintains or even surpasses\nthe performance of full fine-tuning (FFT) on downstream tasks, demonstrating its effectiveness in\nbalancing knowledge retention with task-specific learning.\n3. Robustness: The method is robust to different selection strategies and the number of parameters\nchosen for updating, ensuring consistent performance across various configurations.\n4. Simplicity and Scalability: HFT does not alter the model architecture, which simplifies im-\nplementation and allows for scalable applications, particularly beneficial in successive fine-tuning\nscenarios.\n5. Versatility: The technique has proven effective across diverse fine-tuning scenarios, including\nsupervised fine-tuning, direct preference optimisation, and continual learning.\n45\nFigure 6.7: Schematic illustration of the Half Fine-Tuning (HFT) method as applied to LLAMA 2\u2019s\narchitecture. The diagram shows multiple stages of fine-tuning, where specific model parameters are\nselectively activated (orange) while others remain frozen (blue). This approach optimises training by\nreducing computational requirements while still effectively adapting the model to new tasks or data.\n(adapted from [68])\n6.4.2\nComparison between HFT and LoRA\nCriteria\nHFT\nLoRA\nObjective\nThe goal is to retain the foun-\ndational knowledge acquired dur-\ning pre-training while learning new\ntask-specific skills, thus balancing\nbetween maintaining existing ca-\npabilities and acquiring new ones.\nLoRA aims to reduce computa-\ntional and memory requirements\nduring fine-tuning, making it more\nefficient and feasible to train large\nmodels on limited hardware re-\nsources.\nApproach\nHFT involves freezing half of the\nmodel\u2019s parameters during each\nfine-tuning round and updating\nonly the other half.\nLoRA reduces the number of train-\nable parameters by\nintroducing\nlow-rank decomposition into the\nweight matrices of the neural net-\nwork. This involves injecting low-\nrank matrices into the model\u2019s lay-\ners during fine-tuning.\nModel Architecture\nHFT does not alter the model\u2019s ar-\nchitecture or introduce new param-\neters, making it straightforward\nto apply without additional struc-\ntural changes.\nLoRA\nmodifies\nthe\nmodel\nby\nadding low-rank matrices, which\nchanges the training dynamics and\nrequires additional computations\nfor the low-rank updates.\nPerformance\nResearch has shown that HFT\ncan restore forgotten basic knowl-\nedge while maintaining high per-\nformance in general abilities.\nLoRA is designed to achieve com-\npetitive performance with full fine-\ntuning but with significantly fewer\ntrainable\nparameters\nand\nlower\ncomputational costs.\nTable 6.3: Comparative Analysis of Half Fine-Tuning (HFT) and Low-Rank Adaptation (LoRA).\n46\n6.5\nLamini Memory Tuning\nLamini [69] was introduced as a specialised approach to fine-tuning Large Language Models (LLMs),\ntargeting the reduction of hallucinations. This development was motivated by the need to enhance the\nreliability and precision of LLMs in domains requiring accurate information retrieval. Traditional training\nmethods typically consist of running stochastic gradient descent on vast datasets, which, despite fitting\nthe training data well, often produce models that fail to generalise effectively and are prone to such errors.\nFoundation models often follow a training regimen similar to the Chinchilla recipe, which prescribes\ntraining for a single epoch on a massive corpus, such as training Llama 2 7B on about one trillion\ntokens. This approach results in substantial loss and is geared more towards enhancing generalisation\nand creativity where a degree of randomness in token selection is permissible. However, it falls short for\ntasks demanding high factual precision. In contrast, Lamini Memory Tuning delves deeper by analysing\nthe loss of individual facts, significantly improving the accuracy of factual recall.\nBy augmenting a\nmodel with additional parameters specifically for memory (e.g., an 8B parameter model with an extra 2B\nparameters for weights), Lamini enables the model to memorise and accurately recall a significant number\nof facts, closely aligning performance with LLM scaling laws without compromising on generalisation.\n6.5.1\nLamini-1 - A model architecture based on Lamini\nDeparting from traditional transformer-based designs, the Lamini-1 model architecture (Figure 6.8) em-\nploys a massive mixture of memory experts (MoME). This system features a pre-trained transformer\nbackbone augmented by adapters that are dynamically selected from an index using cross-attention\nmechanisms. These adapters function similarly to experts in MoE architectures, and the network is\ntrained end-to-end while freezing the backbone. This setup allows for specific facts to be stored exactly\nin the selected experts.\nFigure 6.8: Diagram of the Lamini-1 Model Architecture, featuring a Massive Array of Memory Experts\n(MoME). This architecture integrates a pre-trained transformer backbone with dynamically selected\nadapters via cross-attention mechanisms. Each adapter, functioning as a memory expert, is capable of\nstoring specific factual data. (adopted from [69])\nAt inference time, only the relevant experts are retrieved from the index, enabling the LLM to store a\nlarge number of facts while maintaining low inference latency. Specialised GPU kernels written in Triton\nare", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities", "description": "Arxiv Paper titled 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities' authored by Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid", "url": "https://arxiv.org/pdf/2408.13296v1", "source": "arxiv", "id": "b3f10a87-344a-4562-ae0a-31a086304321"}, "page_content": "MoME). This system features a pre-trained transformer\nbackbone augmented by adapters that are dynamically selected from an index using cross-attention\nmechanisms. These adapters function similarly to experts in MoE architectures, and the network is\ntrained end-to-end while freezing the backbone. This setup allows for specific facts to be stored exactly\nin the selected experts.\nFigure 6.8: Diagram of the Lamini-1 Model Architecture, featuring a Massive Array of Memory Experts\n(MoME). This architecture integrates a pre-trained transformer backbone with dynamically selected\nadapters via cross-attention mechanisms. Each adapter, functioning as a memory expert, is capable of\nstoring specific factual data. (adopted from [69])\nAt inference time, only the relevant experts are retrieved from the index, enabling the LLM to store a\nlarge number of facts while maintaining low inference latency. Specialised GPU kernels written in Triton\nare used to accelerate the lookup of experts, optimising the system for quick access to stored knowledge.\nSystems Optimisations for Banishing Hallucinations\nThe MoME architecture is designed to minimise the computational demand required to memorise facts.\nDuring training, a subset of experts, such as 32 out of a million, is selected for each fact. The weights of\nthe backbone network and the cross attention used to select the expert are frozen, and gradient descent\nsteps are taken until the loss is sufficiently reduced to memorise the fact. This approach prevents the\nsame expert from being selected multiple times for different facts by first training the cross attention\n47\nselection mechanism during a generalisation training phase, then freezing its weights.\nThis method ensures that computation scales with the number of training examples, not the total\nnumber of parameters, thereby significantly reducing the computation required for memory tuning.\nThis optimised approach allows Lamini-1 to achieve near-zero loss in memory tuning on real and random\nanswers efficiently, demonstrating its efficacy in eliminating hallucinations while improving factual recall.\n6.6\nMixture of Experts\nA mixture of experts (MoE) is an architectural design for neural networks that divides the computation\nof a layer or operation (e.g., linear layers, MLPs, or attention projection) into several specialised subnet-\nworks, referred to as \u201dexperts\u201d. Each expert independently carries out its computation, and the results\nare aggregated to produce the final output of the MoE layer. MoE architectures can be categorised as\neither dense, where every expert is engaged for each input, or sparse, where only a subset of experts is\nutilised for each input.\n6.6.1\nMixtral 8x7B Architecture and Performance\nMixtral [70] 8x7B employs a Sparse Mixture of Experts (SMoE) architecture (Figure 6.9), mirroring the\nstructure of Mistral 7B but incorporating eight feedforward blocks (experts) in each layer. For every\ntoken at each layer, a router network selects two experts to process the current state and combine their\noutputs. Although each token interacts with only two experts at a time, the selected experts can vary at\neach timestep. Consequently, each token has access to 47 billion parameters but utilises only 13 billion\nactive parameters during inference. Mixtral 8x7B not only matches but often surpasses Llama 2 70B\nand GPT-3.5 across all evaluated benchmarks. Its performance is notably superior to Llama 2 70B in\nmathematics, code generation, and multilingual tasks.\nFigure 6.9: Diagram of the Mixtral 8x7B Mixture of Experts (MoE) model architecture. The model is\ncomposed of a router network that dynamically selects the most relevant experts from a pool of eight\ntransformer-based experts, each with 7 billion parameters. The experts are organised into transformer\nblocks, where the router directs data to the appropriate expert based on the input, optimising com-\nputational efficiency and model performance. This architecture allows for scalability and specialised\nprocessing within large language models. (adapted from [71])\n48\n6.7\nMixture of Agents\nDespite the numerous LLMs and their notable accomplishments, they continue to encounter fundamental\nlimitations regarding model size and training data. Scaling these models further is prohibitively expen-\nsive, often necessitating extensive retraining on multiple trillion tokens. Simultaneously, different LLMs\nexhibit distinct strengths and specialise in various aspects of tasks. A recent study has investigated\nleveraging the collective expertise of multiple LLMs to develop a more capable and robust model, a\nmethod known as Mixture of Agents (MoA) [72].\nMoA functions using a layered architecture, where each layer comprises multiple LLM agents (Figure\n6.10). This structure reveals a phenomenon known as the \u201ccollaborativeness of LLMs.\u201d The innova-\ntive MoA framework utilises the combined capabilities of several LLMs to enhance both reasoning and\nlanguage generation proficiency. Research indicates that LLMs naturally collaborate, demonstrating im-\nproved response quality when incorporating outputs from other models, even if those outputs are not\nideal.\nFigure 6.10: Illustration for Mixture of Agents (MoA) LLM configuration. The model consists of multiple\nlayers, each incorporating several agents that process the input independently before concatenating their\noutputs to form an intermediate result. The process continues across layers, refining the output at each\nstage to generate the final output based on the given prompt (adapted from [72]).\n6.7.1\nMethodology\nTo enhance collaboration among multiple LL", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities", "description": "Arxiv Paper titled 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities' authored by Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid", "url": "https://arxiv.org/pdf/2408.13296v1", "source": "arxiv", "id": "43a09e77-ec3c-4dbc-9814-5d6d7088add1"}, "page_content": "].\nMoA functions using a layered architecture, where each layer comprises multiple LLM agents (Figure\n6.10). This structure reveals a phenomenon known as the \u201ccollaborativeness of LLMs.\u201d The innova-\ntive MoA framework utilises the combined capabilities of several LLMs to enhance both reasoning and\nlanguage generation proficiency. Research indicates that LLMs naturally collaborate, demonstrating im-\nproved response quality when incorporating outputs from other models, even if those outputs are not\nideal.\nFigure 6.10: Illustration for Mixture of Agents (MoA) LLM configuration. The model consists of multiple\nlayers, each incorporating several agents that process the input independently before concatenating their\noutputs to form an intermediate result. The process continues across layers, refining the output at each\nstage to generate the final output based on the given prompt (adapted from [72]).\n6.7.1\nMethodology\nTo enhance collaboration among multiple LLMs, it is essential to understand their individual strengths\nand classify them accordingly. The classification includes:\n1. Proposers: These models excel at generating valuable reference responses for other models. While\nthey may not perform exceptionally on their own, they provide useful context and varied perspec-\ntives that improve the final output when utilised by an aggregator.\n49\n2. Aggregators: These models are adept at merging responses from various models into a single\nhigh-quality result. An effective aggregator should maintain or even enhance the quality of the\nfinal response, regardless of the quality of the individual inputs.\nThe careful selection of LLMs for each MoA layer is crucial Performance metrics, such as average win\nrates in a given layer, help assess the suitability of models for subsequent layers, ensuring the production\nof higher-quality outputs. Diversity in model outputs is vital, as varied responses from different models\ncontribute significantly more than homogeneous outputs from a single model. In MoA, given an input\nprompt, the output of the ith MoA layer yi is calculated as follows:\nyi =\nn\nM\nj=1\n[Ai,j(xi)] + x1, xi+1 = yi\n(6.1)\n6.7.2\nAnalogy with MoE\nMixture-of-Experts (MoE) is a well-established machine learning technique where multiple expert net-\nworks, each with specialised skills, collaborate to address complex problems. This approach has demon-\nstrated significant success across various applications and serves as the inspiration for the Mixture-of-\nAgents (MoA) method. In a typical MoE design, a stack of layers, known as MoE layers, consists of\nmultiple expert networks, a gating network, and residual connections to improve gradient flow. The\noutput for layer yi is calculated as follows:\nyi =\nn\nX\nj=1\nGi,j(xi)Ei,j(xi) + xi\n(6.2)\nThe MoA framework advances the MoE concept by operating at the model level through prompt-based\ninteractions rather than altering internal activations or weights. Instead of relying on specialised sub-\nnetworks within a single model, MoA utilises multiple full-fledged LLMs across different layers. In this\napproach, the gating and expert networks\u2019 functions are integrated within an LLM, leveraging its ability\nto interpret prompts and generate coherent outputs without additional coordination mechanisms.\n6.7.3\nWhat makes MoA works well?\n1. MoA\u2019s Superior Performance: MoA significantly outperforms LLM-based rankers, which select\none answer from the proposals rather than generating new responses. This suggests that MoA\u2019s\napproach of aggregating all generated responses provides more effective results than simply choosing\nfrom pre-existing options.\n2. Effective Incorporation of Proposals: The aggregator in MoA demonstrates a tendency to\nintegrate the best proposed answers. This is supported by positive correlations between aggregator\nresponses and various similarity metrics, such as BLEU scores, which measure n-gram overlaps. The\nuse of alternative similarity measures also shows a consistent positive correlation with preference\nscores, indicating that the aggregator effectively utilises the proposed responses.\n3. Influence of Model Diversity and Proposer Count: Increasing the number of proposers\nimproves output quality, highlighting the benefits of additional auxiliary information. Additionally,\nusing a diverse set of LLMs as proposers consistently yields better results compared to using a single\nLLM. This suggests that both the number and diversity of LLM agents in each MoA layer contribute\nto enhanced performance, with potential for further improvement through scaling.\n4. Model Specialisation: Analysis of model roles within the MoA ecosystem reveals that GPT-4o,\nQwen, and LLaMA-3 are effective in both assisting and aggregating tasks. In contrast, WizardLM\nexcels as a proposer but struggles with aggregating responses from other models.\n6.8\nProximal Policy Optimisation (PPO)\nPPO [73] is a widely recognised reinforcement learning algorithm used for training agents to perform tasks\nin diverse environments. This algorithm leverages policy gradient methods, where policies\u2014represented\n50\nby neural networks\u2014determine the actions taken by the agent based on the current state. PPO ef-\nfectively handles the dynamic nature of training data generated through continuous agent-environment\ninteractions, a feature that differentiates it from static datasets used in supervised learning.\nThe innovation of PPO lies in its \u201dsurrogate\u201d objective function, optimised via stochastic", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities", "description": "Arxiv Paper titled 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities' authored by Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid", "url": "https://arxiv.org/pdf/2408.13296v1", "source": "arxiv", "id": "bcd99141-7f68-4764-ba06-dd9f0f827d10"}, "page_content": " through scaling.\n4. Model Specialisation: Analysis of model roles within the MoA ecosystem reveals that GPT-4o,\nQwen, and LLaMA-3 are effective in both assisting and aggregating tasks. In contrast, WizardLM\nexcels as a proposer but struggles with aggregating responses from other models.\n6.8\nProximal Policy Optimisation (PPO)\nPPO [73] is a widely recognised reinforcement learning algorithm used for training agents to perform tasks\nin diverse environments. This algorithm leverages policy gradient methods, where policies\u2014represented\n50\nby neural networks\u2014determine the actions taken by the agent based on the current state. PPO ef-\nfectively handles the dynamic nature of training data generated through continuous agent-environment\ninteractions, a feature that differentiates it from static datasets used in supervised learning.\nThe innovation of PPO lies in its \u201dsurrogate\u201d objective function, optimised via stochastic gradient ascent.\nThis approach allows for multiple updates from the same batch of data, enhancing both training efficiency\nand stability over traditional policy gradient methods. Developed by OpenAI, PPO was designed to\nbalance ease of implementation with the robust performance characteristics of more complex algorithms\nlike Trust Region Policy Optimisation (TRPO), but without the associated computational complexity.\nPPO operates by maximising expected cumulative rewards through iterative policy adjustments that\nincrease the likelihood of actions leading to higher rewards. A key feature of PPO is its use of a clipping\nmechanism in the objective function, which limits the extent of policy updates, thus preventing drastic\nchanges and maintaining stability during training.\nFigure 6.11: Schematic of Proximal Policy Optimisation (PPO) applied in the context of Reinforcement\nLearning from Human Feedback (RLHF) for fine-tuning a Large Language Model (LLM). The process\ninvolves using a prompt dataset to train the LLM. The PPO algorithm adjusts the LLM\u2019s policy based\non rewards provided by the reward model, which is fine-tuned through human feedback. (adapted from\n[73])\nPython Library - HuggingFace Transformer Reinforcement Learning (TRL4) package supports the\nPPO Trainer5 for training language models from the preference data.\nThe PPOTrainer expects to align a generated response with a query given the rewards obtained from the\nReward model. During each step of the PPO algorithm we sample a batch of prompts from the dataset,\nwe then use these prompts to generate the a responses from the SFT model. Next, the Reward model\nis used to compute the rewards for the generated response. Finally, these rewards are used to optimise\nthe SFT model using the PPO algorithm. Therefore the dataset should contain a text column which we\ncan rename to query. Each of the other data-points required to optimise the SFT model are obtained\nduring the training loop.\n6.8.1\nBenefits of PPO\n1. Stability: Proximal Policy Optimisation (PPO) is designed to ensure stable and reliable policy\nupdates. The clipped surrogate objective function is central to this stability, as it limits policy\nupdates to prevent large, potentially destabilising changes. This results in smoother and more\nconsistent learning.\n2. Ease of Implementation: Compared to advanced algorithms TRPO, PPO is relatively straight-\nforward to implement. It avoids the need for second-order optimisation techniques, making it more\n4https://huggingface.co/docs/trl/en/index\n5https://huggingface.co/docs/trl/main/en/ppo_trainer\n51\naccessible to less experienced practitioners.\n3. Sample Efficiency: PPO achieves data efficiency through its use of the clipped surrogate objec-\ntive. This mechanism regulates policy updates, ensuring stability while effectively reusing training\ndata.\nConsequently, PPO tends to be more sample-efficient than other reinforcement learning\nalgorithms, performing well with fewer samples, which is advantageous in scenarios where data\ncollection is costly or time-consuming.\n6.8.2\nLimitations of PPO\n1. Complexity and Computational Cost: Proximal Policy Optimisation (PPO) involves intricate\npolicy and value networks, necessitating substantial computational resources for training. This\ncomplexity often results in extended training durations and increased operational expenses.\n2. Hyperparameter Sensitivity: PPO\u2019s performance is highly dependent on several hyperparame-\nters, such as the clipping range, learning rate, and discount factor. Achieving optimal performance\nrequires meticulous tuning of these parameters. Incorrect settings can lead to suboptimal policy\noutcomes or instability during the learning process.\n3. Stability and Convergence Issues: Although PPO is designed to enhance stability compared\nto earlier methods, it can still encounter convergence issues, particularly in highly dynamic or\ncomplex environments. Maintaining stable policy updates remains a significant challenge.\n4. Reward Signal Dependence: PPO\u2019s effectiveness is heavily reliant on a well-defined reward\nsignal to guide the learning process. In scenarios where designing an appropriate reward function\nis challenging or impractical, PPO may struggle to attain the desired results.\n6.8.3\nTutorial for training models using PPO technique\nThe tutorial for tuning GPT2 to generate positive movie reviews based on the IMDB dataset using PPO\ntechnique can be found here.\n6.9\nDirect Preference Optimisation (DPO)\nDirect Preference Optimisation (DPO) [74] offers a streamlined approach to aligning language models\n", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities", "description": "Arxiv Paper titled 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities' authored by Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid", "url": "https://arxiv.org/pdf/2408.13296v1", "source": "arxiv", "id": "8bf45383-b542-43a0-9b35-ff1c1c23a708"}, "page_content": "comes or instability during the learning process.\n3. Stability and Convergence Issues: Although PPO is designed to enhance stability compared\nto earlier methods, it can still encounter convergence issues, particularly in highly dynamic or\ncomplex environments. Maintaining stable policy updates remains a significant challenge.\n4. Reward Signal Dependence: PPO\u2019s effectiveness is heavily reliant on a well-defined reward\nsignal to guide the learning process. In scenarios where designing an appropriate reward function\nis challenging or impractical, PPO may struggle to attain the desired results.\n6.8.3\nTutorial for training models using PPO technique\nThe tutorial for tuning GPT2 to generate positive movie reviews based on the IMDB dataset using PPO\ntechnique can be found here.\n6.9\nDirect Preference Optimisation (DPO)\nDirect Preference Optimisation (DPO) [74] offers a streamlined approach to aligning language models\n(LMs) with human preferences, bypassing the complexity of reinforcement learning from human feedback\n(RLHF). Large-scale unsupervised LMs typically lack precise behavioural control, necessitating meth-\nods like RLHF that fine-tune models using human feedback. However, RLHF is intricate, involving the\ncreation of reward models and the fine-tuning of LMs to maximise estimated rewards, which can be\nunstable and computationally demanding. DPO addresses these challenges by directly optimising LMs\nwith a simple classification objective that aligns responses with human preferences. This approach elim-\ninates the need for explicit reward modelling and extensive hyperparameter tuning, enhancing stability\nand efficiency. DPO optimises the desired behaviours by increasing the relative likelihood of preferred\nresponses while incorporating dynamic importance weights to prevent model degeneration. Thus, DPO\nsimplifies the preference learning pipeline, making it an effective method for training LMs to adhere to\nhuman preferences.\nPython Library - HuggingFace TRL package supports the DPO Trainer6 for training language models\nfrom the preference data. The DPO training process requires a dataset formatted in a very specific\nmanner. If you are utilising the default DPODataCollatorWithPadding data collator, your final dataset\nobject must include three specific entries, which should be labelled as follows:\n\u2022 Prompt\n\u2022 Chosen\n\u2022 Rejected\nHuggingFace offers datasets compatible with DPO and can be accessed here.\n6https://huggingface.co/docs/trl/main/en/dpo_trainer\n52\nFigure 6.12: Direct Preference Optimisation (DPO) Process Flow. This figure illustrates the Direct\nPreference Optimisation (DPO) technique used in fine-tuning large language models. The process begins\nwith preference data (Yw > Yl), where Yw represents preferred outputs, and Yl represents less preferred\noutputs. Through a maximum likelihood estimation process, this preference data is used to optimise\nthe model\u2019s parameters, resulting in the final large language model (LLM). The method is designed to\nimprove the alignment of model outputs with desired user preferences, enhancing the model\u2019s effectiveness\nin specific tasks. (adapted from [74])\n6.9.1\nBenefits of DPO\n1. Direct Alignment with Human Preferences: DPO directly optimises models to generate\nresponses that align with human preferences, thereby producing more favourable outputs.\n2. Minimised Dependence on Proxy Objectives: In contrast to methods that rely on next-\nword prediction, DPO leverages explicit human preferences, resulting in responses that are more\nreflective of human behaviour.\n3. Enhanced Performance on Subjective Tasks: For tasks requiring subjective judgement, such\nas dialogue generation or creative writing, DPO excels in aligning the model with human prefer-\nences.\n6.9.2\nBest Practices for DPO\n1. High-Quality Preference Data: The performance of the model is heavily influenced by the\nquality of preference data. Ensure the dataset includes clear and consistent human preferences.\n2. Optimal Beta Value: Experiment with various beta values to manage the influence of the\nreference model. Higher beta values prioritise the reference model\u2019s preferences more strongly.\n3. Hyperparameter Tuning: optimise hyperparameters such as learning rate, batch size, and LoRA\nconfiguration to determine the best settings for your dataset and task.\n4. Evaluation on Target Tasks: Continuously assess the model\u2019s performance on the target task\nusing appropriate metrics to monitor progress and ensure the achievement of desired results.\n5. Ethical Considerations: Pay attention to potential biases in the preference data and take steps\nto mitigate them, preventing the model from adopting and amplifying these biases.\n6.9.3\nTutorial for training models using DPO technique\nThe tutorial for DPO training, including the full source code of the training scripts for SFT and DPO,\nis available here.\n6.9.4\nIs DPO Superior to PPO for LLM Alignment?\nThe recent study on DPO superior to PPO for LLM Alignment[75] investigates the efficacy of reward-\nbased and reward-free methods within RLHF. Reward-based methods, such as those developed by Ope-\nnAI, utilise a reward model constructed from preference data and apply actor-critic algorithms like\nProximal Policy Optimisation (PPO) to optimise the reward signal. Conversely, reward-free methods,\nincluding Direct Preference Optimisation (DPO), RRHF, and PRO, forego an explicit reward function,\n53\nwith DPO focusing exclusively on policy", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities", "description": "Arxiv Paper titled 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities' authored by Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid", "url": "https://arxiv.org/pdf/2408.13296v1", "source": "arxiv", "id": "52ecf79e-8276-4b6a-8a45-8f9799dedeef"}, "page_content": "ifying these biases.\n6.9.3\nTutorial for training models using DPO technique\nThe tutorial for DPO training, including the full source code of the training scripts for SFT and DPO,\nis available here.\n6.9.4\nIs DPO Superior to PPO for LLM Alignment?\nThe recent study on DPO superior to PPO for LLM Alignment[75] investigates the efficacy of reward-\nbased and reward-free methods within RLHF. Reward-based methods, such as those developed by Ope-\nnAI, utilise a reward model constructed from preference data and apply actor-critic algorithms like\nProximal Policy Optimisation (PPO) to optimise the reward signal. Conversely, reward-free methods,\nincluding Direct Preference Optimisation (DPO), RRHF, and PRO, forego an explicit reward function,\n53\nwith DPO focusing exclusively on policy optimisation through a logarithmic representation of the reward\nfunction.\nOne of the objectives of this study is to determine whether DPO is genuinely superior to PPO in the\nRLHF domain. The study combines theoretical and empirical analyses to uncover the inherent limita-\ntions of DPO and identify critical factors that enhance PPO\u2019s practical performance in RLHF.\nTheoretical findings suggest that DPO may yield biased solutions by exploiting out-of-distribution re-\nsponses. Empirical results indicate that DPO\u2019s performance is notably affected by shifts in the distri-\nbution between model outputs and the preference dataset. Furthermore, the study highlights that while\niterative DPO may offer improvements over static data training, it still fails to enhance performance\nin challenging tasks such as code generation. Ablation studies on PPO reveal essential components for\noptimal performance, including advantage normalisation, large batch sizes, and exponential moving av-\nerage updates for the reference model\u2019s parameters. These findings form the basis of practical tuning\nguidelines, demonstrating PPO\u2019s robust effectiveness across diverse tasks and its ability to achieve state-\nof-the-art results in challenging code competition tasks. Specifically, on the CodeContest dataset, the\nPPO model with 34 billion parameters surpasses AlphaCode-41B, showing a significant improvement in\nperformance metrics.\n6.10\nOptimised Routing and Pruning Operations (ORPO)\nPruning LLMs involves eliminating unnecessary or redundant components from a neural network to\nreduce its size and complexity, thereby enhancing its efficiency and performance. This process assists AI\ndevelopers and engineers in addressing the challenges associated with deploying AI models in resource-\nlimited environments, such as mobile devices, edge computing, or embedded systems. Pruning AI models\ncan be achieved through various techniques, each suited to the type and structure of the neural network,\nthe pruning objective, and the pruning criterion. The following are common approaches:\n1. Weight Pruning: Involves removing weights or connections with minimal magnitude or impact on\nthe output. This method reduces the number of parameters and operations in the model, although\nit may not necessarily decrease memory footprint or latency.\n2. Unit Pruning: Eliminates entire units or neurons with the lowest activation or contribution to\nthe output. This technique can reduce the model\u2019s memory footprint and latency but may require\nretraining or fine-tuning to maintain performance.\n3. Filter Pruning: Involves removing entire filters or channels in convolutional neural networks that\nhave the least importance or relevance to the output. This strategy also reduces memory footprint\nand latency, though it may necessitate retraining or fine-tuning to preserve performance [76].\n6.10.1\nWhen to Prune AI Models?\nPruning AI models can be conducted at various stages of the model development and deployment cycle,\ncontingent on the chosen technique and objective.\n1. Pre-Training Pruning: Leverages prior knowledge or heuristics to determine the optimal network\nstructure before training begins. This approach can save time and resources during training but\nmay necessitate careful design and experimentation to identify the best configuration.\n2. Post-Training Pruning: Involves using metrics or criteria to assess the importance or impact of\neach network component after training. This method helps maintain model performance but may\nrequire additional validation and testing to ensure quality and robustness.\n3. Dynamic Pruning: Adjusts the network structure during inference or runtime based on feedback\nor signals. This approach can optimise the model for different scenarios or tasks but may involve\nhigher computational overhead and complexity to implement and execute.\n54\n6.10.2\nBenefits of Pruning\n1. Reduced Size and Complexity: Pruning decreases the size and complexity of AI models, making\nthem easier to store, transmit, and update.\n2. Improved Efficiency and Performance: Pruned models are faster, more energy-efficient, and\nmore reliable.\n3. Enhanced generalisation and Accuracy: Pruning can make models more robust, less prone\nto overfitting, and more adaptable to new data or tasks.\n6.10.3\nChallenges of Pruning\n1. Balance Between Size Reduction and Performance: Achieving the optimal balance between\nreducing size and complexity and maintaining performance is challenging; excessive or insufficient\npruning can degrade model quality and functionality.\n2. Choosing Appropriate Techniques: Selecting the right pruning technique, criterion, and objec-\ntive for the specific neural network type and structure is crucial, as different methods can produce\nvarying effects and outcomes.\n3. Evaluation and Validation: Pruned models need thorough evaluation and validation to ensure", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities", "description": "Arxiv Paper titled 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities' authored by Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid", "url": "https://arxiv.org/pdf/2408.13296v1", "source": "arxiv", "id": "961f3951-05bc-4bd1-9f43-ee0fad0d6c92"}, "page_content": " of AI models, making\nthem easier to store, transmit, and update.\n2. Improved Efficiency and Performance: Pruned models are faster, more energy-efficient, and\nmore reliable.\n3. Enhanced generalisation and Accuracy: Pruning can make models more robust, less prone\nto overfitting, and more adaptable to new data or tasks.\n6.10.3\nChallenges of Pruning\n1. Balance Between Size Reduction and Performance: Achieving the optimal balance between\nreducing size and complexity and maintaining performance is challenging; excessive or insufficient\npruning can degrade model quality and functionality.\n2. Choosing Appropriate Techniques: Selecting the right pruning technique, criterion, and objec-\ntive for the specific neural network type and structure is crucial, as different methods can produce\nvarying effects and outcomes.\n3. Evaluation and Validation: Pruned models need thorough evaluation and validation to ensure\npruning has not introduced errors, biases, or vulnerabilities that could impact performance and\nrobustness.\n55\nChapter 7\nStage 5: Evaluation and Validation\n7.1\nSteps Involved in Evaluating and Validating Fine-Tuned\nModels\n1. Set Up Evaluation Metrics: Choose appropriate evaluation metrics, such as cross-entropy, to\nmeasure the difference between the predicted and actual distributions of the data.\n2. Interpret Training Loss Curve: Monitor and analyse the training loss curve to ensure the\nmodel is learning effectively, avoiding patterns of underfitting or overfitting.\n3. Run Validation Loops: After each training epoch, evaluate the model on the validation set to\ncompute relevant performance metrics and track the model\u2019s generalisation ability.\n4. Monitor and Interpret Results: Consistently observe the relationship between training and\nvalidation metrics to ensure stable and effective model performance.\n5. Hyperparameter Tuning and Adjustments: Adjust key hyperparameters such as learning\nrate, batch size, and number of training epochs to optimise model performance and prevent over-\nfitting.\n7.2\nSetting Up Evaluation Metrics\nCross-entropy is a key metric for evaluating LLMs during training or fine-tuning.\nOriginating from\ninformation theory, it quantifies the difference between two probability distributions.\n7.2.1\nImportance of Cross-Entropy for LLM Training and Evaluation\nCross-entropy is crucial for training and fine-tuning LLMs. It serves as a loss function, guiding the model\nto produce high-quality predictions by minimising discrepancies between the predicted and actual data.\nIn LLMs, each potential word functions as a separate class, and the model\u2019s task is to predict the next\nword given the context. This task is inherently complex, requiring the model to understand syntax,\nsemantics, and context deeply.\n7.2.2\nBeyond Cross-Entropy: Advanced LLM Evaluation Metrics\nWhile cross-entropy remains fundamental, evaluating LLMs effectively necessitates additional metrics\ntailored to various aspects of model performance. Here are some advanced metrics employed in LLM\nevaluation:\nPerplexity\nPerplexity measures how well a probability distribution or model predicts a sample. In the context of\nLLMs, it evaluates the model\u2019s uncertainty about the next word in a sequence. Lower perplexity indicates\nbetter performance, as the model is more confident in its predictions.\n56\nFactuality\nFactuality assesses the accuracy of the information produced by the LLM. It is particularly important for\napplications where misinformation could have serious consequences. Higher factuality scores correlate\nwith higher output quality.\nLLM Uncertainty\nLLM uncertainty is measured using log probability, helping to identify low-quality generations. Lower\nuncertainty indicates higher output quality. This metric leverages the log probability of each generated\ntoken, providing insights into the model\u2019s confidence in its responses.\nPrompt Perplexity\nThis metric evaluates how well the model understands the input prompt.\nLower prompt perplexity\nindicates a clear and comprehensible prompt, which is likely to yield better model performance.\nContext Relevance\nIn retrieval-augmented generation (RAG) systems, context relevance measures how pertinent the re-\ntrieved context is to the user query. Higher context relevance improves the quality of generated responses\nby ensuring that the model utilises the most relevant information.\nCompleteness\nCompleteness assesses whether the model\u2019s response fully addresses the query based on the provided\ncontext. High completeness ensures that all relevant information is included in the response, enhancing\nits utility and accuracy.\nChunk Attribution and Utilisation\nThese metrics evaluate how effectively the retrieved chunks of information contribute to the final response.\nHigher chunk attribution and utilisation scores indicate that the model is efficiently using the available\ncontext to generate accurate and relevant answers.\nData Error Potential\nThis metric quantifies the difficulty the model faces in learning from the training data. Higher data\nquality results in lower error potential, leading to better model performance.\nSafety Metrics\nSafety metrics ensure that the LLM\u2019s outputs are appropriate and non-harmful. These are included in\nthe final sections of the chapter.\nIntegrating these advanced metrics provides a holistic view of LLM performance, enabling developers to\nfine-tune and optimise models more effectively. By employing a metrics-first approach, it is possible to\nensure that LLMs not only produce accurate and high-quality outputs but also do so consistently and\nreliably across diverse applications1.\n7.3\nUnderstanding the Training Loss Curve\nThe training loss curve plots the loss value against training epochs and is essential for monitoring model\n", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities", "description": "Arxiv Paper titled 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities' authored by Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid", "url": "https://arxiv.org/pdf/2408.13296v1", "source": "arxiv", "id": "cc55a455-0483-468d-ac2f-3c03b991b120"}, "page_content": " chunk attribution and utilisation scores indicate that the model is efficiently using the available\ncontext to generate accurate and relevant answers.\nData Error Potential\nThis metric quantifies the difficulty the model faces in learning from the training data. Higher data\nquality results in lower error potential, leading to better model performance.\nSafety Metrics\nSafety metrics ensure that the LLM\u2019s outputs are appropriate and non-harmful. These are included in\nthe final sections of the chapter.\nIntegrating these advanced metrics provides a holistic view of LLM performance, enabling developers to\nfine-tune and optimise models more effectively. By employing a metrics-first approach, it is possible to\nensure that LLMs not only produce accurate and high-quality outputs but also do so consistently and\nreliably across diverse applications1.\n7.3\nUnderstanding the Training Loss Curve\nThe training loss curve plots the loss value against training epochs and is essential for monitoring model\nperformance.\n1https://www.rungalileo.io/blog/metrics-first-approach-to-llm-evaluation\n57\n7.3.1\nInterpreting Loss Curves\nAn ideal training loss curve shows a rapid decrease in loss during initial stages, followed by a gradual\ndecline and eventual plateau. Specific patterns to look for include:\n1. Underfitting: High loss value that does not decrease significantly over time, suggesting the model\ncannot learn the data.\n2. Overfitting: Decreasing training loss with increasing validation loss, indicating the model mem-\norises the training data.\n3. Fluctuations: Significant variations may indicate a high learning rate or noisy gradients.\nFigure 7.1: Example training loss curve showing the decline in loss over iterations during the fine-tuning\nof Llama2 13B on a financial Q/A dataset. The curve illustrates the effectiveness of the fine-tuning\nprocess in reducing the loss and improving model performance.\n7.3.2\nAvoiding Overfitting\nTechniques to prevent overfitting include:\n1. Regularisation: Adds a penalty term to the loss function to encourage smaller weights.\n2. Early Stopping: Stops training when validation performance no longer improves.\n3. Dropout: Randomly deactivates neurons during training to reduce sensitivity to noise.\n4. Cross-Validation: Splits data into multiple subsets for training and validation to assess model\ngeneralisation.\n5. Batch Normalisation: Normalises inputs to each layer during training to stabilise the learning\nprocess.\n6. Larger Datasets and Batch Sizes: Reduces overfitting by increasing the amount of diverse\ndata and batch sizes.\n58\n7.3.3\nSources of Noisy Gradients\nNoisy gradients are common during the training of machine learning models, including LLMs. They arise\nfrom variability in gradient estimates due to stochastic gradient descent and its variants. Strategies to\nmanage noisy gradients include:\n1. Learning Rate Scheduling: Gradually decreasing the learning rate during training can reduce\nthe impact of noisy gradients.\n2. Gradient Clipping: Setting a threshold for gradient values prevents large updates that can\ndestabilise training.\n7.4\nRunning Validation Loops\nValidation loops provide an unbiased evaluation of model performance. Typical steps include:\n1. Split Data: Divide the dataset into training and validation sets.\n2. Initialise Validation: Evaluate the model on the validation set at the end of each epoch.\n3. Calculate Metrics: Compute relevant performance metrics, such as cross-entropy loss.\n4. Record Results: Log validation metrics for each epoch.\n5. Early Stopping: Optionally stop training if validation loss does not improve for a predefined\nnumber of epochs.\n7.5\nMonitoring and Interpreting Results\nMonitoring validation results involves analysing trends in validation metrics over epochs. Key aspects\ninclude:\n1. Consistent Improvement: Indicates good model generalisation if both training and validation\nmetrics improve and plateau.\n2. Divergence: Suggests overfitting if training metrics improve while validation metrics deteriorate.\n3. Stability: Ensure validation metrics do not fluctuate significantly, indicating stable training.\n7.6\nHyperparameter Tuning and Other Adjustments\nFine-tuning involves adjusting key hyperparameters to achieve optimal performance. Important hyper-\nparameters include:\n1. Learning Rate: Determines the step size for updating model weights. A good starting point is\n2e-4, but this can vary.\n2. Batch Size: Larger batch sizes lead to more stable updates but require more memory.\n3. Number of Training Epochs: Balancing the number of epochs ensures the model learns suffi-\nciently without overfitting or underfitting.\n4. Optimiser: Optimisers like Paged ADAM optimise memory usage, advantageous for large models.\nOther tunable parameters include dropout rate, weight decay, and warmup steps.\n7.6.1\nData Size and Quality\nThe efficacy of LLMs is directly impacted by the quality of their training data. Ensuring that datasets\nare clean, relevant, and adequate is crucial. Data cleanliness refers to the absence of noise, errors, and\ninconsistencies within the labelled data. For example, having a phrase like \u201cThis article suggests... \u201d\nmultiple times in the training data can corrupt the response of LLMs and add a bias towards using this\nspecific phrase more often and in inappropriate situations.\n59\n7.7\nBenchmarking Fine-Tuned LLMs\nModern LLMs are assessed using standardised benchmarks such as GLUE,", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities", "description": "Arxiv Paper titled 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities' authored by Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid", "url": "https://arxiv.org/pdf/2408.13296v1", "source": "arxiv", "id": "7f33acb0-b283-459b-94c9-a87c9db2d102"}, "page_content": " the model learns suffi-\nciently without overfitting or underfitting.\n4. Optimiser: Optimisers like Paged ADAM optimise memory usage, advantageous for large models.\nOther tunable parameters include dropout rate, weight decay, and warmup steps.\n7.6.1\nData Size and Quality\nThe efficacy of LLMs is directly impacted by the quality of their training data. Ensuring that datasets\nare clean, relevant, and adequate is crucial. Data cleanliness refers to the absence of noise, errors, and\ninconsistencies within the labelled data. For example, having a phrase like \u201cThis article suggests... \u201d\nmultiple times in the training data can corrupt the response of LLMs and add a bias towards using this\nspecific phrase more often and in inappropriate situations.\n59\n7.7\nBenchmarking Fine-Tuned LLMs\nModern LLMs are assessed using standardised benchmarks such as GLUE, SuperGLUE, HellaSwag,\nTruthfulQA, and MMLU (See Table 7.1). These benchmarks evaluate various capabilities and provide\nan overall view of LLM performance.\nBenchmark\nDescription\nReference URL\nGLUE\nProvides a standardised set of diverse NLP tasks to\nevaluate the effectiveness of different language mod-\nels\nSource\nSuperGLUE\nCompares more challenging and diverse tasks with\nGLUE, with comprehensive human baselines\nSource\nHellaSwag\nEvaluates how well an LLM can complete a sentence\nSource\nTruthfulQA\nMeasures truthfulness of model responses\nSource\nMMLU\nEvaluates how well the LLM can multitask\nSource\nIFEval\nTests a model\u2019s ability to follow explicit instructions,\nfocusing on formatting adherence\nSource\nBBH (Big Bench Hard)\n23 challenging tasks from the BigBench dataset to\nevaluate LLMs using objective metrics\nSource\nMATH\nCompilation of high-school level competition prob-\nlems formatted using LaTeX and Asymptote\nSource\nGPQA\nChallenging\nknowledge\ndataset\nwith\nquestions\ncrafted by PhD-level domain experts\nSource\nMuSR\nDataset with complex problems requiring models to\nintegrate reasoning with long-range context parsing\nSource\nMMLU-PRO\nRefined version of MMLU with higher quality and\nmore challenging multiple-choice questions\nSource\nARC\nMeasures machine reasoning with a dataset of grade-\nschool science questions\nSource\nCOQA\nA dataset for building conversational question-\nanswering systems\nSource\nDROP\nEvaluates the ability to perform discrete reasoning\nover paragraphs of text\nSource\nSQuAD\nA reading comprehension dataset for evaluating\nmodels\u2019 ability to answer questions based on pas-\nsages of text\nSource\nTREC\nA benchmark for evaluating text retrieval method-\nologies\nSource\nWMT\nA dataset and benchmark for evaluating machine\ntranslation models\nSource\nXNLI\nA dataset for evaluating cross-lingual language un-\nderstanding\nSource\nPiQA\nA dataset for evaluating models\u2019 understanding of\nphysical interactions\nSource\nWinogrande\nA large-scale dataset for evaluating commonsense\nreasoning\nSource\nTable 7.1: Detailed Overview of Benchmark Datasets Used for Evaluating Language Model Performance.\nAs LLMs evolve, so do benchmarks, with new standards such as BigCodeBench challenging current\nbenchmarks and setting new standards in the domain. Given the diverse nature of LLMs and the tasks\nthey can perform, the choice of benchmarks depends on the specific tasks the LLM is expected to handle.\nFor generic applicability, various benchmarks for different downstream applications and reasoning should\nbe utilised. For domain/task-specific LLMs, benchmarking can be limited to relevant benchmarks like\nBigCodeBench for coding.\n60\n7.8\nEvaluating Fine-Tuned LLMs on Safety Benchmark\nThe safety aspects of Large Language Models (LLMs) are increasingly scrutinised due to their ability\nto generate harmful content when influenced by jailbreaking prompts. These prompts can bypass the\nembedded safety and ethical guidelines within the models, similar to code injection techniques used in\ntraditional computer security to circumvent safety protocols. Notably, models like ChatGPT, GPT-\n3, and InstructGPT are vulnerable to such manipulations that remove content generation restrictions,\npotentially violating OpenAI\u2019s guidelines. This underscores the necessity for robust safeguards to ensure\nLLM outputs adhere to ethical and safety standards.\nDecodingTrust [77] provides a comprehensive evaluation of the trustworthiness of LLMs, notably com-\nparing GPT-4 with GPT-3.5 (ChatGPT). This evaluation spans several critical areas:\n1. Toxicity: Optimisation algorithms and generative models are employed to create challenging\nprompts that test the model\u2019s ability to avoid generating harmful content.\n2. Stereotype Bias: An array of demographic groups and stereotype topics are utilised to assess\nmodel bias, helping to understand and mitigate prejudiced responses.\n3. Adversarial Robustness: The resilience of models against adversarial attacks is tested by chal-\nlenging them with sophisticated algorithms intended to deceive or mislead.\n4. Out-of-Distribution (OOD) Robustness: Models are evaluated on their ability to handle\ninputs that differ significantly from their training data, such as poetic or Shakespearean styles.\n5. Robustness to Adversarial Demonstrations: Demonstrations that contain misleading infor-\nmation are used to test the model\u2019s robustness across various tasks.\n6. Privacy", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities", "description": "Arxiv Paper titled 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities' authored by Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid", "url": "https://arxiv.org/pdf/2408.13296v1", "source": "arxiv", "id": "7d92e79a-8b04-4ec8-83ba-96c2c8f5af29"}, "page_content": "). This evaluation spans several critical areas:\n1. Toxicity: Optimisation algorithms and generative models are employed to create challenging\nprompts that test the model\u2019s ability to avoid generating harmful content.\n2. Stereotype Bias: An array of demographic groups and stereotype topics are utilised to assess\nmodel bias, helping to understand and mitigate prejudiced responses.\n3. Adversarial Robustness: The resilience of models against adversarial attacks is tested by chal-\nlenging them with sophisticated algorithms intended to deceive or mislead.\n4. Out-of-Distribution (OOD) Robustness: Models are evaluated on their ability to handle\ninputs that differ significantly from their training data, such as poetic or Shakespearean styles.\n5. Robustness to Adversarial Demonstrations: Demonstrations that contain misleading infor-\nmation are used to test the model\u2019s robustness across various tasks.\n6. Privacy: Various levels of privacy evaluation assess how well models safeguard sensitive informa-\ntion during interactions and understand privacy-related contexts.\n7. Hallucination Detection: Identifies instances where the model generates information not grounded\nin the provided context or factual data. Lower hallucination rates improve the reliability and trust-\nworthiness of the LLM\u2019s outputs.\n8. Tone Appropriateness: Assesses whether the model\u2019s output maintains an appropriate tone for\nthe given context. This is particularly important for applications in customer service, healthcare,\nand other sensitive areas.\n9. Machine Ethics: Ethical assessments involve testing models with scenarios that require moral\njudgments, using datasets like ETHICS and Jiminy Cricket.\n10. Fairness: The fairness of models is evaluated by generating tasks that vary protected attributes,\nensuring equitable responses across different demographic groups.\nThe dataset employed for evaluating the aforementioned eight safety dimensions can be found here.\nIn partnership with HuggingFace, the LLM Safety Leaderboard utilises DecodingTrust\u2019s framework to\nprovide a unified evaluation platform for LLM safety.\nThis allows researchers and practitioners to\nbetter understand the capabilities, limitations, and risks associated with LLMs. Users are encouraged to\nsubmit their models to HuggingFace for evaluation, ensuring they meet the evolving standards of safety\nand reliability in the field.\n7.9\nEvaluating Safety of Fine-Tuned LLM using AI Models\n7.9.1\nLlama Guard\nLlama Guard 2[78] is a safeguard model built on LLMs for managing risks in conversational AI applica-\ntions. It effectively categorises both input prompts and responses from AI agents using a detailed safety\nrisk taxonomy tailored to identify potential legal and policy risks in AI interactions. It utilises a detailed\nsafety risk taxonomy designed to identify and manage potential legal and policy risks in interactions\ninvolving conversational AI. This taxonomy enables effective classification in areas such as:\n\u2022 Violence & Hate, addressing content that could incite violent acts or discrimination.\n61\n\u2022 Sexual Content, targeting sexually explicit material or behaviour, especially involving minors.\n\u2022 Guns & Illegal Weapons, concerning the promotion or instruction of illegal armaments.\n\u2022 Regulated or Controlled Substances, covering illegal drugs and other controlled substances.\n\u2022 Suicide & Self-Harm, aimed at content that could encourage self-destructive behaviour.\n\u2022 Criminal Planning, for content that could assist in planning or executing criminal activities.\nThe core of Llama Guard 2 is its robust framework that allows for both prompt and response classifica-\ntion, supported by a high-quality dataset that enhances its ability to monitor conversational exchanges.\nOperating on a Llama2-7b model, Llama Guard 2 has been instruction-tuned to deliver strong perfor-\nmance on benchmarks like the OpenAI Moderation Evaluation dataset and ToxicChat, where it matches\nor surpasses the capabilities of existing content moderation tools.\nThe model supports multi-class classification and generates binary decision scores. Its instruction fine-\ntuning allows for extensive customisation of tasks and adaptation of output formats. This feature enables\nusers to modify taxonomy categories to align with specific use cases and supports flexible prompting\ncapabilities, including zero-shot and few-shot applications. The adaptability and effectiveness of Llama\nGuard make it a vital resource for developers and researchers. By making its model weights publicly\navailable, Llama Guard 2 encourages ongoing development and customisation to meet the evolving needs\nof AI safety within the community.\nLlama Guard 3 represents the latest advancement over Llama Guard 2, having been fine-tuned on the\nLlama 3 8b model. The key difference between the two versions is that Llama Guard 3 expands upon\nthe capabilities of Llama Guard 2 by introducing three new categories: Defamation, Elections, and\nCode Interpreter Abuse.\nPython Library: Llama Guard 3 is accessible via HuggingFace\u2019s AutoModelForCausalLM.2 A detailed\ntutorial is available at this link. Please note that access to the model requires submitting a request to\nHugging Face with the user details. Additionally, the model weights can be downloaded from the Meta\nplatform by providing user details, and the link can be found here.\nThe prompt formats for these two models also differ, with the specific formats for Llama Guard 2 available\nhere and Llama Guard 3 is accessible here.\n7.9.2\nShield Gemma\nShieldGemma [79] is an advanced content moderation model built on the Gemma2 platform, designed\nto enhance the safety and reliability of interactions between LLMs and users. It effectively filters both\nuser inputs and model outputs to mitigate key harm", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities", "description": "Arxiv Paper titled 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities' authored by Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid", "url": "https://arxiv.org/pdf/2408.13296v1", "source": "arxiv", "id": "23d7a191-708d-4976-94e7-9bab5ac76873"}, "page_content": " Guard 2 by introducing three new categories: Defamation, Elections, and\nCode Interpreter Abuse.\nPython Library: Llama Guard 3 is accessible via HuggingFace\u2019s AutoModelForCausalLM.2 A detailed\ntutorial is available at this link. Please note that access to the model requires submitting a request to\nHugging Face with the user details. Additionally, the model weights can be downloaded from the Meta\nplatform by providing user details, and the link can be found here.\nThe prompt formats for these two models also differ, with the specific formats for Llama Guard 2 available\nhere and Llama Guard 3 is accessible here.\n7.9.2\nShield Gemma\nShieldGemma [79] is an advanced content moderation model built on the Gemma2 platform, designed\nto enhance the safety and reliability of interactions between LLMs and users. It effectively filters both\nuser inputs and model outputs to mitigate key harm types, including offensive language, hate speech,\nmisinformation, and explicit content. The model\u2019s scalability, with options ranging from 2B to 27B\nparameters, allows for tailored applications that meet specific needs, such as reducing latency in online\nsafety applications or enhancing performance in complex decision-making tasks.\nA distinguishing feature of ShieldGemma is its novel approach to data curation. It leverages synthetic\ndata generation techniques to create high-quality datasets that are robust against adversarial prompts\nand fair across diverse identity groups. This reduces the need for extensive human annotation, streamlin-\ning the data preparation process while ensuring the model\u2019s effectiveness. Compared to existing content\nmoderation tools like LlamaGuard and WildGuard, which typically offer fixed-size models and limited\ncustomisation, ShieldGemma\u2019s flexible architecture and advanced data handling capabilities provide a\nmore adaptable and efficient solution.\nThese innovations position ShieldGemma as a significant ad-\nvancement in LLM-based content moderation, offering developers and researchers a versatile tool that\npromotes safer and more reliable AI interactions across various platforms.\nPython Library: The ShieldGemma series is available on HuggingFace via AutoModelForCausalLM.\nThe models can be accessed here. A tutorial for running ShieldGemma 2B on Google Colab can be found\nhere. Similar to Llama Guard series, ShieldGemma series also has guidelines for prompting and it can\nbe found here.\n7.9.3\nWILDGUARD\nWILDGUARD [80] is an innovative open-source tool developed to enhance the safety of interactions\nwith large language models (LLMs).\nThis tool addresses three critical moderation tasks: detecting\n2https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoModelForCausalLM\n62\nharmful intent in user prompts, identifying safety risks in model responses, and determining when a\nmodel appropriately refuses unsafe requests.\nCentral to its development is WILDGUARD MIX3, a\nmeticulously curated dataset comprising 92,000 labelled examples that include both benign prompts and\nadversarial attempts to bypass safety measures. The dataset is divided into WILDGUARD TRAIN, used\nfor training the model, and WILDGUARD TEST, consisting of high-quality human-annotated examples\nfor evaluation.\nThe WILDGUARD model itself is fine-tuned on the Mistral-7B language model using the WILDGUARD\nTRAIN dataset, enabling it to perform all three moderation tasks in a unified, multi-task manner. Results\nshow that WILDGUARD surpasses existing open-source moderation tools in effectiveness, particularly\nexcelling in handling adversarial prompts and accurately detecting model refusals. On many benchmarks,\nWILDGUARD\u2019s performance is on par with or exceeds that of GPT-4, a much larger, closed-source\nmodel.\nThe quick start guide and additional information on WILDGUARD are available in GitHub and it can\nbe accessed here.\n3https://huggingface.co/datasets/allenai/wildguardmix\n63\nChapter 8\nStage 6: Deployment\n8.1\nSteps Involved in Deploying the Fine-Tuned Model\n1. Model Export: Save the fine-tuned model in a suitable format (e.g., ONNX, TensorFlow Saved-\nModel, PyTorch) for deployment.\n2. Infrastructure Setup: Prepare the deployment environment, including necessary hardware, cloud\nservices, and containerisation tools.\n3. API Development: Create APIs to allow applications to interact with the model, facilitating\nprediction requests and responses.\n4. Deployment: Deploy the model to the production environment, making it accessible to end-users\nor applications.\n8.2\nCloud-Based Providers for LLM Deployment\nCloud-based large language model (LLM) inferencing frequently employs a pricing model based on the\nnumber of tokens processed. Users are charged according to the volume of text analysed or generated\nby the model. While this pricing structure can be cost-effective for sporadic or small-scale usage, it may\nnot always be economical for larger or continuous workloads.\nIn some scenarios, hosting an LLM solution in-house may offer better long-term cost savings, especially if\nthere is consistent or high-volume usage. Managing your own infrastructure provides greater control over\nresource allocation and allows for cost optimisation based on specific needs. Additionally, self-hosting\noffers advantages in terms of data privacy and security, as sensitive information remains within your own\nenvironment.\nHowever, it is", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities", "description": "Arxiv Paper titled 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities' authored by Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid", "url": "https://arxiv.org/pdf/2408.13296v1", "source": "arxiv", "id": "777671f6-ff80-4198-9318-40edd2491eba"}, "page_content": ": Deploy the model to the production environment, making it accessible to end-users\nor applications.\n8.2\nCloud-Based Providers for LLM Deployment\nCloud-based large language model (LLM) inferencing frequently employs a pricing model based on the\nnumber of tokens processed. Users are charged according to the volume of text analysed or generated\nby the model. While this pricing structure can be cost-effective for sporadic or small-scale usage, it may\nnot always be economical for larger or continuous workloads.\nIn some scenarios, hosting an LLM solution in-house may offer better long-term cost savings, especially if\nthere is consistent or high-volume usage. Managing your own infrastructure provides greater control over\nresource allocation and allows for cost optimisation based on specific needs. Additionally, self-hosting\noffers advantages in terms of data privacy and security, as sensitive information remains within your own\nenvironment.\nHowever, it is crucial to carefully evaluate the total cost of ownership when comparing cloud-based\nsolutions with self-hosted alternatives. This evaluation should consider factors such as hardware expenses,\nmaintenance, and operational overheads. Ultimately, the decision should be informed by a comprehensive\ncost-benefit analysis, considering both short-term affordability and long-term sustainability.\nSeveral companies offer deployment services for large language models (LLMs), providing a range of\ntools and platforms to efficiently implement and manage these models. Here\u2019s a detailed list of some\nprominent providers and their services:\n\u2022 Amazon Web Services (AWS)\n\u2013 Amazon Bedrock: This service offers a suite of foundation models including Amazon Ti-\ntan, which supports various NLP tasks such as summarisation and text generation. Bedrock\nintegrates seamlessly with other AWS services for scalable and secure deployment.\n\u2013 Amazon SageMaker: Provides an end-to-end machine learning service that includes tools\nfor building, training, and deploying LLMs. SageMaker JumpStart offers pre-trained models\nand step-by-step guides to simplify the deployment process.\n64\n\u2013 Tutorial: This tutorial explains the deployment of LLM Agents on Amazon Bedrock. An-\nother tutorial explains end-to-end fine-tuning and deployment of LLMs with Sagemaker Can-\nvas and Amazon Bedrock. General guidelines of Amazon Bedrock for LLM users can be found\nhere.\n\u2022 Microsoft Azure\n\u2013 Azure OpenAI Service: This service offers access to OpenAI\u2019s powerful models like GPT-\n3.5 and Codex. It provides capabilities for embedding, image generation with DALL-E, and\nspeech-to-text with Whisper. Azure\u2019s integration with OpenAI models ensures robust deploy-\nment options for various applications.\n\u2013 Azure Machine Learning: Supports the deployment of custom and pre-trained models,\noffering tools for model management, deployment, and monitoring. It integrates with Azure\u2019s\nbroader ecosystem for scalable and secure ML operations.\n\u2013 Tutorial: Here is the tutorial for creating and deploying an Azure OpenAI Service in Mi-\ncrosoft Azure platform.\n\u2022 Google Cloud Platform (GCP)\n\u2013 Vertex AI: This platform allows the deployment of large language models with tools for\ntraining, tuning, and serving models. Vertex AI supports models like BERT and GPT-3,\nproviding extensive MLOps capabilities for end-to-end management.\n\u2013 Cloud AI API: Offers APIs for NLP tasks such as translation, sentiment analysis, and\nentity recognition. These APIs are backed by Google\u2019s powerful infrastructure, ensuring high\nperformance and reliability.\n\u2013 Tutorial: This document contains a tutorial for training and deploying an LLM in GCP.\n\u2022 Hugging Face\n\u2013 Inference API: This service allows users to deploy and manage LLMs hosted on Hugging\nFace\u2019s infrastructure. It supports various models from the Transformers library and provides\nan easy-to-use API for integrating these models into applications.\n\u2013 Spaces: A collaborative environment where users can deploy and share models using Hugging\nFace\u2019s hosting platform. It supports deploying custom models and interactive demos.\n\u2013 Tutorial: This document contains a tutorial for training and deploying an LLM using Hug-\ngingFace Inference API.\n\u2022 Other Platforms\n\u2013 OpenLLM: Provides deployment solutions here.\n\u2013 Deepseed: Offers deployment solutions here.\n8.3\nTechniques for Optimising Model Performance During In-\nference\nOptimising model performance during inference is crucial for the efficient deployment of large language\nmodels (LLMs). The following advanced techniques offer various strategies to enhance performance,\nreduce latency, and manage computational resources effectively.\n8.3.1\nTraditional On-Premises GPU-Based Deployments\nThis conventional approach to deploying large language models (LLMs) involves using Graphics Process-\ning Units (GPUs) due to their parallel processing capabilities, which enable fast and efficient inference.\nHowever, this method requires upfront hardware investment and may not be suitable for applications\nwith fluctuating demand or limited budgets. GPU-based deployments face several challenges:\n1. Resource utilisation may suffer during periods of low demand due to idle servers.\n2. Scaling up or down often requires physical hardware modifications, which can be time-consuming.\n65\n3. Centralised servers can introduce single points of failure and scalability limitations.\nTo mitigate these issues, strategies such as load balancing between multiple GPUs, fallback routing, model\nparallelism, and data parallelism can be employed to achieve better results. Optimisation techniques like\ndistributed inference using PartialState from accelerate", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities", "description": "Arxiv Paper titled 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities' authored by Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid", "url": "https://arxiv.org/pdf/2408.13296v1", "source": "arxiv", "id": "ca875938-6137-4908-bd62-eb1d4a621196"}, "page_content": "\n8.3.1\nTraditional On-Premises GPU-Based Deployments\nThis conventional approach to deploying large language models (LLMs) involves using Graphics Process-\ning Units (GPUs) due to their parallel processing capabilities, which enable fast and efficient inference.\nHowever, this method requires upfront hardware investment and may not be suitable for applications\nwith fluctuating demand or limited budgets. GPU-based deployments face several challenges:\n1. Resource utilisation may suffer during periods of low demand due to idle servers.\n2. Scaling up or down often requires physical hardware modifications, which can be time-consuming.\n65\n3. Centralised servers can introduce single points of failure and scalability limitations.\nTo mitigate these issues, strategies such as load balancing between multiple GPUs, fallback routing, model\nparallelism, and data parallelism can be employed to achieve better results. Optimisation techniques like\ndistributed inference using PartialState from accelerate can further enhance efficiency.\nExample use case: Large-Scale NLP Application\nFor instance, a large e-commerce platform implemented traditional on-premises GPU-based deployment\nto handle millions of customer queries daily. By utilising load balancing and model parallelism, they\nwere able to achieve a significant reduction in latency and improved customer satisfaction.\n8.3.2\nDistributed LLM: Torrent-Style Deployment and Parallel Forward Passes\nAn innovative deployment strategy for large language models (LLMs) involves distributing them across\nmultiple GPUs in a decentralised, torrent-style manner. Libraries like Petals1 can perform this task.\nPetals functions as a decentralised pipeline designed for rapid neural network inference by partitioning\nthe model into distinct blocks or layers, which are distributed across multiple geographically dispersed\nservers. Users can connect their own GPUs to this network, acting as both contributors and clients who\ncan access and apply the model to their data.\nWhen a client request is received, the network routes it through a series of servers optimised to minimise\nthe total forward pass time. Each server dynamically selects the most optimal set of blocks, adapting to\nthe current bottlenecks in the pipeline. This framework leverages decentralisation principles to distribute\ncomputational load across diverse regions, sharing computational resources and GPUs in a way that\nreduces the financial burden on individual organisations. This collaborative approach not only optimises\nresource utilisation but also fosters a global community dedicated to shared AI goals.\nFigure 8.1: Conceptual Representation of Distributed LLM Deployment Using a Torrent-Style Approach.\nThis figure illustrates the distributed deployment of a Large Language Model (LLM) using a torrent-style\napproach, where multiple GPT model layers (stacks) are distributed across different nodes (represented\nby chefs) and perform parallel forward passes. The process mimics the flow of orders from customers\n(input data) through restaurants (intermediate processing layers) to chefs (model layers), highlighting\nthe efficiency of parallel processing and distributed computing in handling large-scale language models.\nThis approach is essential for reducing inference latency and improving the scalability of LLMs across\ndiverse computational environments. (adapted from [81])\n1https://github.com/bigscience-workshop/petals\n66\nExample use case: Global Research Collaboration\nA consortium of research institutions implemented a distributed LLM using the Petals framework to\nanalyse large datasets across different continents. By leveraging the decentralised nature of Petals, they\nachieved high efficiency in processing and collaborative model development.\n8.3.3\nWebGPU-Based Deployment of LLM\nThis deployment option for large language models (LLMs) involves utilising WebGPU, a web standard\nthat provides a low-level interface for graphics and compute applications on the web platform. With\nWebGPU, organisations can harness the power of GPUs directly within web browsers, enabling effi-\ncient inference for LLMs in web-based applications. WebGPU enables high-performance computing and\ngraphics rendering directly within the client\u2019s web browser. It allows developers to utilise the client\u2019s\nGPU for tasks such as rendering graphics, accelerating computational workloads, and performing par-\nallel processing, all without the need for plugins or additional software installations. This capability\npermits complex computations to be executed efficiently on the client\u2019s device, leading to faster and\nmore responsive web applications.\n8.3.4\nLLM on WebGPU using WebLLM\nClients can access powerful large language models and chatbots directly in their browser, leveraging\nWebGPU acceleration. This approach eliminates server dependencies, providing users with exceptional\nperformance and enhanced privacy. WebLLM facilitates the use of large language models directly in the\nclient\u2019s browser to perform tasks such as filtering out personally identifiable information (PII) or named\nentity recognition (NER) on data without transmitting it over the network.\nThis ensures enhanced\nprivacy and security by retaining sensitive information on the client side.\n67\nFigure 8.2: WebGPU-Based Deployment of LLM: This diagram illustrates the architecture of deploying\na large language model (LLM) using WebGPU technology. The CPU manages the distribution of prompt\ninferencing tasks to multiple GPUs, which then process these prompts in parallel, enhancing efficiency\nand scalability in LLM deployment across web-based platforms. (adapted from [81])\nAdditional Use Cases for WebLLM\n1. Language Translation: Enable real-time translation of text directly in the browser, allowing\nusers to communicate across language barriers without transmitting their messages over the net-\nwork.\n2. Code Autocompletion: Develop code editors that provide intelligent autocom", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities", "description": "Arxiv Paper titled 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities' authored by Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid", "url": "https://arxiv.org/pdf/2408.13296v1", "source": "arxiv", "id": "da6e5cd0-35df-4223-ac3a-5039d66e8192"}, "page_content": "client\u2019s browser to perform tasks such as filtering out personally identifiable information (PII) or named\nentity recognition (NER) on data without transmitting it over the network.\nThis ensures enhanced\nprivacy and security by retaining sensitive information on the client side.\n67\nFigure 8.2: WebGPU-Based Deployment of LLM: This diagram illustrates the architecture of deploying\na large language model (LLM) using WebGPU technology. The CPU manages the distribution of prompt\ninferencing tasks to multiple GPUs, which then process these prompts in parallel, enhancing efficiency\nand scalability in LLM deployment across web-based platforms. (adapted from [81])\nAdditional Use Cases for WebLLM\n1. Language Translation: Enable real-time translation of text directly in the browser, allowing\nusers to communicate across language barriers without transmitting their messages over the net-\nwork.\n2. Code Autocompletion: Develop code editors that provide intelligent autocompletion suggestions\nbased on context, leveraging WebLLM to understand and predict code snippets.\n3. Customer Support Chatbots: Implement chatbots on websites to provide instant customer\nsupport and answer frequently asked questions without relying on external servers.\n4. Data Analysis and Visualisation: Create browser-based tools for analysing and visualising\ndata, with WebLLM assisting in data processing, interpretation, and generating insights.\n5. Personalised Recommendations:\nDevelop recommendation engines that offer personalised\nproduct recommendations, content suggestions, or movie/music recommendations based on user\npreferences and behaviour.\n6. Privacy-Preserving Analytics: Develop analytics platforms that perform data analysis directly\nin the browser, ensuring that sensitive information remains on the client side and reducing the risk\nof data breaches.\n68\nExample use case: Privacy-Focused Web Application\nA healthcare startup deployed an LLM using WebLLM to process patient information directly within the\nbrowser, ensuring data privacy and compliance with healthcare regulations. This approach significantly\nreduced the risk of data breaches and improved user trust.\n8.3.5\nQuantised LLMs\nModel quantisation is a technique utilised to reduce the size of an AI model by representing its parameters\nwith fewer bits. In traditional machine learning models, each parameter (e.g., weights and biases in neural\nnetworks) is typically stored as a 32-bit floating-point number, necessitating significant memory and\ncomputational resources, particularly for large models. Quantisation aims to alleviate this by reducing\nthe precision of these parameters. For instance, instead of storing each parameter as a 32-bit floating-\npoint number, they may be represented using fewer bits, such as 8-bit integers.\nThis compression\nreduces the memory footprint of the model, making it more efficient to deploy and execute, especially in\nresource-constrained environments like mobile devices or edge devices. QLoRA is a popular example of\nthis quantisation for LLMs and can be used to deploy LLMs locally or host them on external servers.\nExample use case: Edge Device Deployment\nA tech company used quantised LLMs to deploy advanced NLP models on mobile devices, enabling offline\nfunctionality for applications such as voice recognition and translation. This deployment significantly\nimproved app performance and user experience by reducing latency and reliance on internet connectivity.\n8.3.6\nvLLMs\nThe vLLM2 system efficiently handles requests by employing a block-level memory management method\nand preemptive request scheduling. It utilises the PagedAttention[82] algorithm to manage the key-\nvalue (KV) cache, thereby reducing memory waste and fragmentation. By batching requests and sharing\nphysical blocks across multiple samples, vLLM optimises memory usage and enhances throughput. Per-\nformance tests indicate that vLLM surpasses other systems in various decoding scenarios. Consider a\ntransformer-based model tasked with summarising a lengthy book. Traditional transformers process the\nentire book simultaneously, which can be both computationally and memory-intensive, especially for ex-\ntended texts. With PagedAttention, the book is divided into smaller segments or pages. The model then\nfocuses on summarising one page at a time, rather than the entire book simultaneously. This approach\nreduces computational complexity and memory requirements, making it more feasible to process and\nsummarise lengthy texts efficiently.\nExample use case: High-Volume Content Generation\nA content marketing agency implemented vLLMs for generating large volumes of SEO-optimised content.\nBy leveraging the efficient memory management of vLLMs, they were able to handle multiple concurrent\nrequests, significantly increasing their content production rate while maintaining high quality.\n8.4\nKey Considerations for Deployment of LLMs\nDeploying large language models (LLMs) effectively requires careful planning and consideration of various\nfactors to ensure optimal performance, cost-efficiency, and security. Key considerations include:\n\u2022 Infrastructure Requirements:\n\u2013 Compute Resources: Ensure adequate CPU/GPU resources to handle the model\u2019s compu-\ntational demands. High-performance GPUs are typically required for efficient inference and\ntraining.\n\u2013 Memory: LLMs, especially those with billions of parameters, require substantial memory.\nMemory management techniques such as quantisation and model parallelism can be employed\nto optimise usage.\n2https://docs.vllm.ai/en/stable/\n69\n\u2022 Scalability:\n\u2013 Horizontal Scaling: Plan for horizontal scaling to distribute the load across multiple servers,\nwhich can improve performance and handle increased demand.\n\u2013 Load Balancing: Implement load balancing strategies to ensure even distribution of requests\nand prevent any single point of failure.\n\u2022 Cost Management:\n\u2013", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities", "description": "Arxiv Paper titled 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities' authored by Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid", "url": "https://arxiv.org/pdf/2408.13296v1", "source": "arxiv", "id": "2082bf19-d1a3-46d6-a68b-734e40be0f46"}, "page_content": " (LLMs) effectively requires careful planning and consideration of various\nfactors to ensure optimal performance, cost-efficiency, and security. Key considerations include:\n\u2022 Infrastructure Requirements:\n\u2013 Compute Resources: Ensure adequate CPU/GPU resources to handle the model\u2019s compu-\ntational demands. High-performance GPUs are typically required for efficient inference and\ntraining.\n\u2013 Memory: LLMs, especially those with billions of parameters, require substantial memory.\nMemory management techniques such as quantisation and model parallelism can be employed\nto optimise usage.\n2https://docs.vllm.ai/en/stable/\n69\n\u2022 Scalability:\n\u2013 Horizontal Scaling: Plan for horizontal scaling to distribute the load across multiple servers,\nwhich can improve performance and handle increased demand.\n\u2013 Load Balancing: Implement load balancing strategies to ensure even distribution of requests\nand prevent any single point of failure.\n\u2022 Cost Management:\n\u2013 Token-based Pricing: Understand the cost implications of token-based pricing models of-\nfered by cloud providers. This model charges based on the number of tokens processed, which\ncan become expensive with high usage.\n\u2013 Self-Hosting: Evaluate the costs and benefits of self-hosting versus cloud hosting.\nSelf-\nhosting might offer long-term savings for consistent, high-volume usage but requires significant\nupfront investment in hardware and ongoing maintenance.\n\u2022 Performance Optimisation:\n\u2013 Latency: Minimise latency to ensure real-time performance, particularly for applications\nrequiring instant responses like chatbots and virtual assistants.\n\u2013 Throughput: Maximise throughput to handle a high volume of requests efficiently. Tech-\nniques like batching and efficient memory management (e.g., PagedAttention) can help.\n\u2022 Security and Privacy:\n\u2013 Data Security: Implement robust security measures to protect sensitive data, including\nencryption and secure access controls.\n\u2013 Privacy: Ensure compliance with data privacy regulations by keeping sensitive data within\nyour environment if self-hosting, or ensuring cloud providers comply with relevant privacy\nstandards.\n\u2022 Maintenance and Updates:\n\u2013 Model Updates: Regularly update the model to incorporate new data and improve perfor-\nmance. Automate this process if possible to reduce manual effort.\n\u2013 System Maintenance: Plan for regular maintenance of the infrastructure to prevent down-\ntime and ensure smooth operation.\n\u2022 Flexibility and Customisation:\n\u2013 Fine-Tuning:\nAllow for model fine-tuning to adapt the LLM to specific use cases and\ndatasets. Fine-tuning can improve accuracy and relevance in responses.\n\u2013 API Integration: Ensure the deployment platform supports easy integration with existing\nsystems and workflows through APIs and SDKs.\n\u2022 User Management:\n\u2013 Access Control: Implement role-based access control to manage who can deploy, use, and\nmaintain the LLM.\n\u2013 Monitoring and Logging: Set up comprehensive monitoring and logging to track usage,\nperformance, and potential issues. This helps in proactive troubleshooting and optimisation.\n\u2022 Compliance:\n\u2013 Regulatory Compliance: Ensure that the deployment adheres to all relevant regulatory\nand legal requirements, including data protection laws like GDPR, HIPAA, etc.\n\u2013 Ethical Considerations: Implement ethical guidelines to avoid biases and ensure the re-\nsponsible use of LLMs.\n\u2022 Support and Documentation:\n\u2013 Technical Support: Choose a deployment platform that offers robust technical support and\nresources.\n\u2013 Documentation: Provide comprehensive documentation for developers and users to facili-\ntate smooth deployment and usage.\n70\nChapter 9\nStage 7: Monitoring and\nMaintenance\n9.1\nSteps Involved in Monitoring and Maintenance of Deployed\nFine-Tuned LLMs\nContinuous monitoring and maintenance of fine-tuned LLMs are essential to ensure their optimal per-\nformance, accuracy, and security over time. Below are the key steps involved in this process:\n1. Setup Initial Baselines: Establish initial performance baselines by evaluating the model on a\ncomprehensive test dataset. Record metrics such as accuracy, latency, throughput, and error rates\nto serve as reference points for future monitoring.\n2. Performance Monitoring: Implement systems to continuously track key performance metrics\nsuch as response time, server load, and token usage. Regularly compare these metrics against the\nestablished baselines to detect any deviations.\n3. Accuracy Monitoring: Continuously evaluate the model\u2019s predictions against a ground truth\ndataset. Use metrics like precision, recall, F1 score, and cross-entropy loss to ensure the model\nmaintains high accuracy levels.\n4. Error Monitoring: Track and analyse errors, including runtime errors and prediction errors.\nImplement logging mechanisms to capture detailed information about each error for troubleshooting\nand improvement.\n5. Log Analysis: Maintain comprehensive logs for each prediction request and response, including\ninput data, output predictions, response times, and encountered errors. Regularly review logs to\nidentify patterns and areas for improvement.\n6. Alerting Mechanisms: Set up automated alerting systems to notify stakeholders of any anomalies\nor deviations from expected performance metrics. Integrate alerts with communication tools like\nSlack, PagerDuty, or email for timely responses.\n7. Feedback Loop: Establish a feedback loop with end-users to gather insights on model performance\nand user satisfaction. Use this feedback to continuously refine and improve the model.\n8. Security Monitoring: Implement robust security measures to monitor for threats, including\nunauthorised access, data breaches, and adversarial attacks. Use encryption", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities", "description": "Arxiv Paper titled 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities' authored by Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid", "url": "https://arxiv.org/pdf/2408.13296v1", "source": "arxiv", "id": "6b2dbef0-cfdc-441a-902f-7b7ed196bbf3"}, "page_content": ": Track and analyse errors, including runtime errors and prediction errors.\nImplement logging mechanisms to capture detailed information about each error for troubleshooting\nand improvement.\n5. Log Analysis: Maintain comprehensive logs for each prediction request and response, including\ninput data, output predictions, response times, and encountered errors. Regularly review logs to\nidentify patterns and areas for improvement.\n6. Alerting Mechanisms: Set up automated alerting systems to notify stakeholders of any anomalies\nor deviations from expected performance metrics. Integrate alerts with communication tools like\nSlack, PagerDuty, or email for timely responses.\n7. Feedback Loop: Establish a feedback loop with end-users to gather insights on model performance\nand user satisfaction. Use this feedback to continuously refine and improve the model.\n8. Security Monitoring: Implement robust security measures to monitor for threats, including\nunauthorised access, data breaches, and adversarial attacks. Use encryption, access control, and\nregular security audits to protect the model and data.\n9. Drift Detection: Continuously monitor for data and concept drift using statistical tests and\ndrift detectors. Regularly evaluate the model on holdout datasets to detect changes in input data\ndistribution or model performance.\n10. Model Versioning: Maintain version control for different iterations of the model. Track perfor-\nmance metrics for each version to ensure that the best-performing model is in production.\n71\n11. Documentation and Reporting: Keep detailed documentation of monitoring procedures, met-\nrics, and findings. Generate regular reports to provide stakeholders with insights into the model\u2019s\nperformance and maintenance activities.\n12. Periodic Review and Update: Regularly assess and update the monitoring processes to incor-\nporate new techniques, tools, and best practices, ensuring the monitoring system remains effective\nand up-to-date.\n9.2\nContinuous Monitoring of Model Performance\nWhile large language model (LLM) applications undergo some form of evaluation, continuous monitoring\nremains inadequately implemented in most cases. This section outlines the components necessary to\nestablish an effective monitoring programme aimed at safeguarding users and preserving brand integrity.\n9.2.1\nFunctional Monitoring\nInitially, it is crucial to monitor fundamental metrics consistently. This includes tracking metrics such\nas request volume, response times, token utilisation, costs incurred, and error rates.\n9.2.2\nPrompt Monitoring\nFollowing functional metrics, attention should be directed towards monitoring user-generated prompts\nor inputs. Metrics like readability can provide valuable insights. LLM evaluators should be employed to\ndetect potential toxicity in responses. Additionally, metrics such as embedding distances from reference\nprompts prove insightful, ensuring adaptability to varying user interactions over time.\nIntroducing a new evaluation category involves identifying adversarial attempts or malicious prompt\ninjections, often overlooked in initial evaluations. Comparison against reference sets of known adversarial\nprompts helps identify and flag malicious activities. Evaluative LLMs play a crucial role in classifying\nprompts as benign or malicious.\n9.2.3\nResponse Monitoring\nMonitoring responses involves several critical checks to ensure alignment with expected outcomes. Pa-\nrameters such as relevance, coherence (hallucination), topical alignment, sentiment, and their evolution\nover time are essential. Metrics related to toxicity and harmful output require frequent monitoring due\nto their critical impact. Prompt leakage represents an adversarial tactic wherein sensitive prompt in-\nformation is illicitly extracted from the application\u2019s stored data. Monitoring responses and comparing\nthem against the database of prompt instructions can help detect such breaches. Embedding distance\nmetrics are particularly effective in this regard. Regular testing against evaluation datasets provides\nbenchmarks for accuracy and highlights any performance drift over time. Tools capable of managing\nembeddings allow exportation of underperforming output datasets for targeted improvements.\n9.2.4\nAlerting Mechanisms and Thresholds\nEffective monitoring necessitates well-calibrated alerting thresholds to avoid excessive false alarms. Im-\nplementing multivariate drift detection and alerting mechanisms can enhance accuracy. Consideration\nof false alarm rates and best practices for setting thresholds is paramount for effective monitoring sys-\ntem design. Alerting features should include integration with communication tools such as Slack and\nPagerDuty. Some systems offer automated response blocking in case of alerts triggered by problematic\nprompts. Similar mechanisms can be employed to screen responses for personal identifiable information\n(PII), toxicity, and other quality metrics before delivery to users. Custom metrics tailored to specific\napplication nuances or innovative insights from data scientists can significantly enhance monitoring ef-\nficacy. Flexibility to incorporate such metrics is essential to adapt to evolving monitoring needs and\nadvancements in the field.\n72\n9.2.5\nMonitoring User Interface (UI)\nThe monitoring system\u2019s UI is pivotal, typically featuring time-series graphs of monitored metrics. Dif-\nferentiated UIs facilitate in-depth analysis of alert trends, aiding root cause analysis.\nAdvanced UI\ncapabilities may include visualisations of embedding spaces through clustering and projections, provid-\ning insights into data patterns and relationships. Mature monitoring systems categorise data by users,\nprojects, and teams, ensuring role-based access control (RBAC) to protect sensitive information. Op-\ntimising alert analysis within the UI interface remains an area where improvements can significantly\nreduce false alarm rates and enhance operational efficiency.\n9.3\nUpdating LLM Knowledge\nTo improve the knowledge base of an LLM, continued pretraining is used to help LLM evolve with the\nlatest knowledge and information. The world and language are constantly evolving. New information\nemerges, trends shift", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities", "description": "Arxiv Paper titled 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities' authored by Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid", "url": "https://arxiv.org/pdf/2408.13296v1", "source": "arxiv", "id": "64e9a562-1eee-4d3f-9056-2b25125656c9"}, "page_content": "\n9.2.5\nMonitoring User Interface (UI)\nThe monitoring system\u2019s UI is pivotal, typically featuring time-series graphs of monitored metrics. Dif-\nferentiated UIs facilitate in-depth analysis of alert trends, aiding root cause analysis.\nAdvanced UI\ncapabilities may include visualisations of embedding spaces through clustering and projections, provid-\ning insights into data patterns and relationships. Mature monitoring systems categorise data by users,\nprojects, and teams, ensuring role-based access control (RBAC) to protect sensitive information. Op-\ntimising alert analysis within the UI interface remains an area where improvements can significantly\nreduce false alarm rates and enhance operational efficiency.\n9.3\nUpdating LLM Knowledge\nTo improve the knowledge base of an LLM, continued pretraining is used to help LLM evolve with the\nlatest knowledge and information. The world and language are constantly evolving. New information\nemerges, trends shift, and cultural references change. LLMs trained on static data can become outdated,\nleading to:\n\u2022 Factual Errors: Outdated information can cause LLMs to provide inaccurate responses.\n\u2022 Irrelevance: Models might miss the context of current events or use outdated references.\n\u2022 Bias Perpetuation: Biases present in training data can become entrenched if not addressed\nthrough updates.\n9.3.1\nRetraining Methods\n\u2022 Periodic Retraining: This involves refreshing the model\u2019s knowledge base at regular intervals\n(weekly, monthly, yearly) with new data. This is a straightforward method but requires a steady\nstream of high-quality, unbiased data.\n\u2022 Trigger-Based Retraining: This approach monitors the LLM\u2019s performance. When metrics like\naccuracy or relevance fall below a certain threshold, a retraining process is triggered. This method\nis more dynamic but requires robust monitoring systems and clear performance benchmarks.\n9.3.2\nAdditional Methods\n\u2022 Fine-Tuning: LLMs can be fine-tuned for specific tasks by training them on smaller, domain-\nspecific datasets. This allows for specialisation without complete retraining.\n\u2022 Active Learning: This approach involves selectively querying the LLM to identify areas where\nit lacks knowledge. The retrieved information is then used to update the model.\n9.3.3\nKey Considerations\n\u2022 Data Quality and Bias: New training data must be carefully curated to ensure quality and\nmitigate bias. Techniques like human annotation and fairness checks are crucial.\n\u2022 Computational Cost: Retraining LLMs can be computationally expensive, requiring significant\nresources. Optimisations like transfer learning (using pre-trained models as a starting point) can\nhelp reduce costs.\n\u2022 Downtime: Retraining often takes time, leading to LLM downtime. Strategies like rolling updates\nor deploying multiple models can minimise service disruptions.\n\u2022 Version Control: Tracking different versions of the LLM and their training data is essential for\nrollbacks in case of performance issues.\n73\n9.4\nThe Future of LLM Updates\nResearch is ongoing to develop more efficient and effective LLM update strategies. One promising area\nis continuous learning, where LLMs can continuously learn and adapt from new data streams without\nretraining from scratch. Continuous learning aims to reduce the need for frequent full-scale retraining by\nenabling models to update incrementally with new information. This approach can significantly enhance\nthe model\u2019s ability to remain current with evolving knowledge and language use, improving its long-term\nperformance and relevance.\nInnovations in transfer learning and meta-learning are also contributing to advancements in LLM updates.\nThese techniques allow models to leverage pre-existing knowledge and adapt quickly to new tasks or\ndomains with minimal additional training.\nBy integrating these advanced learning methods, future\nLLMs can become more adaptable and efficient in processing and understanding new information.\nFurthermore, ongoing improvements in hardware and computational resources will support more frequent\nand efficient updates. As processing power increases and becomes more accessible, the computational\nburden of updating large models will decrease, enabling more regular and comprehensive updates.\nCollaboration between academia and industry is vital in driving these advancements. By sharing research\nfindings and best practices, the field can collectively move towards more robust and efficient LLM update\nmethodologies, ensuring that models remain accurate, relevant, and valuable over time.\n74\nChapter 10\nIndustrial Fine-Tuning Platforms\nand Frameworks for LLMs\nThe evolution of fine-tuning techniques has been propelled by leading tech companies and platforms that\nhave introduced innovative frameworks and services. Companies like HuggingFace, Amazon Web Services\n(AWS), Microsoft Azure, and OpenAI have developed tools and platforms that simplify and democratise\nthe fine-tuning process. These advancements have not only lowered the barrier to entry for leveraging\nstate-of-the-art AI models but have also enabled a wide range of applications across various industries,\nfrom healthcare and finance to customer service and content creation. Each of these platforms offers\nunique capabilities that cater to different needs, whether it be through automated fine-tuning workflows,\nscalable cloud-based training environments, or accessible API interfaces for deploying custom models.\nHuggingFace, for example, has made significant strides with its Transformers library1 and tools like Au-\ntotrain2 and SetFit, which allow users to fine-tune models with minimal coding and data. Their platform\nprovides a robust infrastructure that supports both the research community and industry practitioners,\nfacilitating the rapid development and deployment of custom AI solutions. Similarly, AWS\u2019s SageMaker3\nand SetFit4", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities", "description": "Arxiv Paper titled 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities' authored by Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid", "url": "https://arxiv.org/pdf/2408.13296v1", "source": "arxiv", "id": "4b604f24-9341-41e3-9f4c-1af78a2fd88f"}, "page_content": " that simplify and democratise\nthe fine-tuning process. These advancements have not only lowered the barrier to entry for leveraging\nstate-of-the-art AI models but have also enabled a wide range of applications across various industries,\nfrom healthcare and finance to customer service and content creation. Each of these platforms offers\nunique capabilities that cater to different needs, whether it be through automated fine-tuning workflows,\nscalable cloud-based training environments, or accessible API interfaces for deploying custom models.\nHuggingFace, for example, has made significant strides with its Transformers library1 and tools like Au-\ntotrain2 and SetFit, which allow users to fine-tune models with minimal coding and data. Their platform\nprovides a robust infrastructure that supports both the research community and industry practitioners,\nfacilitating the rapid development and deployment of custom AI solutions. Similarly, AWS\u2019s SageMaker3\nand SetFit4 provides an extensive suite of services that cover the entire machine learning lifecycle, from\ndata preparation and training to model deployment and optimisation, making it a comprehensive solu-\ntion for enterprise-level applications.\nOn the other hand, Microsoft Azure integrates its fine-tuning capabilities with enterprise-grade tools\nand services, offering solutions like Azure Machine Learning and the Azure OpenAI Service that cater to\nlarge organisations looking to incorporate advanced AI into their operations. Azure\u2019s focus on MLOps\nand seamless integration with other Azure services ensures that fine-tuned models can be efficiently de-\nployed and maintained in production environments. Meanwhile, OpenAI has pioneered the concept of\n\u201dfine-tuning as a service\u201d allowing businesses to leverage their powerful models like GPT-4 through a\nuser-friendly API 5, enabling custom model adaptations without the need for in-house AI expertise or\ninfrastructure.\nThe collective efforts of these tech companies have not only enhanced the efficiency and scalability of\nfine-tuning but also democratised access to sophisticated AI tools. By reducing the technical barriers\nand providing comprehensive, user-friendly platforms, these innovations have enabled a wider range of\nindustries to deploy advanced AI models tailored to their specific needs. Tables 10.1 and 10.2 offer a\nquick comparison of LLM fine-tuning tools and frameworks from different providers.\n1https://huggingface.co/docs/transformers/en/index/\n2https://huggingface.co/autotrain\n3https://huggingface.co/autotrain\n4https://aws.amazon.com/sagemaker/\n5https://platform.openai.com/docs/guides/fine-tuning/fine-tuning-integrations\n75\nParameter\nNVIDIA\nNeMo\nHugging\nFace\nAutoTrain\nAPI\nAmazon\nBedrock\nAWS\nSage-\nMaker\nJump-\nStart\nHugging\nFace\nTrainer API\nPrimary Use\nCase\nCustom\nfine-\ntuning of LLMs\nwith\nadvanced\nNVIDIA GPUs.\nFine-tuning\nand deployment\nof\nLLMs\nwith\nminimal code.\nFine-tuning and\ndeploying LLMs\non AWS infras-\ntructure.\nSimplified\nfine-\ntuning and de-\nployment within\nthe AWS ecosys-\ntem.\nManual\nfine-\ntuning of LLMs\nwith\ndetailed\ncontrol\nover\ntraining\npro-\ncesses.\nModel Support\nSupports a vari-\nety of large, pre-\ntrained\nmodels,\nincluding Mega-\ntron series.\nSupports a wide\nrange\nof\npre-\ntrained\nmodels\nfrom\nthe\nHug-\nging Face model\nhub.\nSupports\nvari-\nous\nLLMs\nlike\nAmazon\nTitan\nand\nthird-party\nmodels.\nPre-trained\nmodels\nfrom\nAWS and part-\nners; integration\nwith\ncustom\nmodels.\nSupports a vast\narray of models\nfrom\nthe\nHug-\nging Face model\nhub.\nData Handling\nUsers\nprovide\ntask-specific\ndata\nfor\nfine-\ntuning,\npro-\ncessed\nusing\nNVIDIA\u2019s\nin-\nfrastructure.\nUploads\ndatasets\nvia\na\nsimple\ninter-\nface; AutoTrain\nhandles\npre-\nprocessing\nand\nmodel training.\nData is uploaded\nand\nmanaged\nwithin the AWS\nenvironment;\nintegrates\nwith\nAWS data ser-\nvices.\nUploads\nand\nprocesses\ndata\nwithin\nAWS;\nsupports various\ndata formats.\nUsers\nmanually\npreprocess data\nand\nmanage\ntraining steps.\nCustomisation\nLevel\nHigh;\nextensive\ncontrol\nover\nfine-tuning pro-\ncess and model\nparameters.\nModerate; auto-\nmated\nprocess\nwith\nsome\ncustomisation\noptions.\nHigh;\ndetailed\nconfiguration\nand\nintegration\nwith AWS ser-\nvices.\nModerate;\npre-configured\nsettings\nwith\nsome customisa-\ntion available.\nVery\nHigh;\ndetailed\ncon-\ntrol\nover\nevery\naspect\nof\nfine-\ntuning.\nScalability\nHigh;\nleverages\nNVIDIA\u2019s GPU\ncapabilities\nfor\nefficient scaling.\nHigh;\nscalable\nvia\nHugging\nFace\u2019s\ncloud", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities", "description": "Arxiv Paper titled 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities' authored by Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid", "url": "https://arxiv.org/pdf/2408.13296v1", "source": "arxiv", "id": "6667a106-49c5-47d4-a31c-893fdcac2772"}, "page_content": "processes\ndata\nwithin\nAWS;\nsupports various\ndata formats.\nUsers\nmanually\npreprocess data\nand\nmanage\ntraining steps.\nCustomisation\nLevel\nHigh;\nextensive\ncontrol\nover\nfine-tuning pro-\ncess and model\nparameters.\nModerate; auto-\nmated\nprocess\nwith\nsome\ncustomisation\noptions.\nHigh;\ndetailed\nconfiguration\nand\nintegration\nwith AWS ser-\nvices.\nModerate;\npre-configured\nsettings\nwith\nsome customisa-\ntion available.\nVery\nHigh;\ndetailed\ncon-\ntrol\nover\nevery\naspect\nof\nfine-\ntuning.\nScalability\nHigh;\nleverages\nNVIDIA\u2019s GPU\ncapabilities\nfor\nefficient scaling.\nHigh;\nscalable\nvia\nHugging\nFace\u2019s\ncloud\ninfrastructure.\nVery\nHigh;\nscalable\nacross\nAWS\u2019s extensive\ncloud infrastruc-\nture.\nHigh;\nscalable\nwithin the AWS\ncloud\necosys-\ntem.\nHigh; scalability\ndepends on the\ninfrastructure\nused (e.g., local\nvs. cloud).\nDeployment\nOptions\nOn-premises\nor\ncloud\nde-\nployment\nvia\nNVIDIA infras-\ntructure.\nDeployed\nvia\nHugging\nFace\u2019s\ncloud or can be\nexported for lo-\ncal deployment.\nIntegrated\ninto\nAWS\nservices,\neasily\ndeploy-\nable\nacross\nAWS\u2019s\nglobal\ninfrastructure.\nAWS\ncloud\ndeployment;\nintegrates\nwith\nother AWS ser-\nvices.\nDeployable\nlo-\ncally,\nin cloud,\nor\nexported\nto\nother platforms.\nIntegration with\nEcosystem\nDeep integration\nwith\nNVIDIA\ntools\n(e.g.,\nTensorRT)\nand\nGPU-based\nworkflows.\nIntegrates\nwell\nwith\nthe\nHugging\nFace\necosystem\nand\nother ML tools.\nSeamless\ninte-\ngration\nwith\nAWS\nser-\nvices\n(e.g.,\nS3,\nLambda,\nSage-\nMaker).\nStrong\nintegra-\ntion with AWS\nservices;\neasy\nto connect with\ndata\npipelines\nand analytics.\nIntegrates\nwith\nHugging\nFace\necosystem\nand\nother\nPython-\nbased ML tools.\nData Privacy\nUsers\nmust\nensure\ndata\nprivacy\ncompli-\nance;\nNVIDIA\nhandles\ndata\nduring\nprocess-\ning.\nData\nhandled\nwithin\nHugging\nFace\u2019s\nenviron-\nment;\nprivacy\ndepends\non\ndata\nhandling\npractices.\nStrong\nfocus\non data privacy\nwithin\nAWS\nenvironment;\ncompliant\nwith\nvarious\nstan-\ndards.\nStrong\nAWS\nprivacy\nand\nsecurity\nmea-\nsures; compliant\nwith\nindustry\nstandards.\nUser-managed;\ndepends\non\nwhere the mod-\nels and data are\nhosted.\nTarget Users\nEnterprises and\ndevelopers need-\ning\nadvanced\ncustomisation\nand\nperfor-\nmance in LLM\nfine-tuning.\nDevelopers\nand\nbusinesses look-\ning\nfor\neasy,\nautomated LLM\nfine-tuning solu-\ntions.\nBusinesses\nand\ndevelopers inte-\ngrated\ninto\nor\nseeking to lever-\nage AWS cloud\nservices.\nEnterprises and\ndevelopers seek-\ning\nstreamlined\nAI/ML solutions\nwithin AWS.\nResearchers,\ndevelopers,\nand\nML\nengineers\nneeding detailed\ncontrol\nover\ntraining.\nLimitations\nHigh\nresource\ndemand\nand\npotential\ncosts;\ndependency\non\nNVIDIA ecosys-\ntem.\nLess\ncontrol\nover fine-tuning\nspecifics; cloud-\nbased,\nmay\nnot suit all on-\npremises needs.\nDependency\non\nAWS;\npo-\ntential\nvendor\nlock-in,\ncost\nmanagement\ncomplexity.\nLimited\nto\nAWS\nservices;\npre-configured\noptions\nmay\nlimit deep cus-\ntomisation.\nRequires\ntech-\nnical\nexpertise;\nmore\ncomplex\nsetup and man-\nagement.\nTable 10.1: Detailed Comparison of LLM Fine-Tuning Platforms (Part I). This table provides a compre-\nhensive comparison of various fine-tuning tools for Large Language Models (LLMs), including NVIDIA\nNeMo, Hugging Face AutoTrain API, Amazon Bedrock, AWS SageMaker JumpStart, and Hugging Face\nTrainer API. It covers multiple aspects such as the primary use case, model support, data handling,\ncustomisation level, scalability, deployment options, integration with the ecosystem, data privacy, target\nusers, and limitations for each tool.\n76\nParameter\nOpenAI\nFine-\nTuning API\nGoogle Vertex AI\nStudio\nMicrosoft\nAzure\nAI Studio\nLangChain", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities", "description": "Arxiv Paper titled 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities' authored by Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid", "url": "https://arxiv.org/pdf/2408.13296v1", "source": "arxiv", "id": "ffddb8a3-7d1a-47be-91ec-8f0184f399d0"}, "page_content": "\noptions\nmay\nlimit deep cus-\ntomisation.\nRequires\ntech-\nnical\nexpertise;\nmore\ncomplex\nsetup and man-\nagement.\nTable 10.1: Detailed Comparison of LLM Fine-Tuning Platforms (Part I). This table provides a compre-\nhensive comparison of various fine-tuning tools for Large Language Models (LLMs), including NVIDIA\nNeMo, Hugging Face AutoTrain API, Amazon Bedrock, AWS SageMaker JumpStart, and Hugging Face\nTrainer API. It covers multiple aspects such as the primary use case, model support, data handling,\ncustomisation level, scalability, deployment options, integration with the ecosystem, data privacy, target\nusers, and limitations for each tool.\n76\nParameter\nOpenAI\nFine-\nTuning API\nGoogle Vertex AI\nStudio\nMicrosoft\nAzure\nAI Studio\nLangChain\nPrimary Use\nCase\nAPI-based\nfine-\ntuning\nfor\nOpenAI\nmodels with custom\ndatasets.\nEnd-to-end\nML\nmodel\ndevelopment\nand\ndeployment\nwithin Google Cloud.\nEnd-to-end AI devel-\nopment,\nfine-tuning,\nand\ndeployment\non\nAzure.\nBuilding applications\nusing\nLLMs\nwith\nmodular\nand\ncus-\ntomisable workflows.\nModel Support\nLimited\nto\nOpenAI\nmodels\nlike\nGPT-3\nand GPT-4.\nSupports\nGoogle\u2019s\npre-trained\nmodels\nand\nuser-customised\nmodels.\nSupports Microsoft\u2019s\nmodels\nand\ncustom\nmodels\nfine-tuned\nwithin Azure.\nSupports integration\nwith\nvarious\nLLMs\nand\nAI\ntools\n(e.g.,\nOpenAI, GPT-4, Co-\nhere).\nData Handling\nUsers upload datasets\nvia\nAPI;\nOpenAI\nhandles\npreprocess-\ning and fine-tuning.\nData managed within\nGoogle Cloud;\nsup-\nports\nmultiple\ndata\nformats.\nData\nintegrated\nwithin Azure ecosys-\ntem; supports various\nformats and sources.\nData handling is flex-\nible,\ndependent\non\nthe specific LLM and\nintegration used.\nCustomisation\nLevel\nModerate; focuses on\nease of use with lim-\nited deep customisa-\ntion.\nHigh;\noffers custom\nmodel\ntraining\nand\ndeployment with de-\ntailed configuration.\nHigh; extensive cus-\ntomisation\noptions\nthrough\nAzure\u2019s\nAI\ntools.\nVery High; allows de-\ntailed\ncustomisation\nof workflows, models,\nand data processing.\nScalability\nHigh;\nscalable\nthrough\nOpenAI\u2019s\ncloud infrastructure.\nVery High; leverages\nGoogle\nCloud\u2019s\nin-\nfrastructure for scal-\ning.\nVery High;\nscalable\nacross Azure\u2019s global\ninfrastructure.\nHigh; scalability de-\npends on the specific\ninfrastructure\nand\nmodels used.\nDeployment\nOptions\nDeployed via API, in-\ntegrated into applica-\ntions using OpenAI\u2019s\ncloud.\nDeployed\nwithin\nGoogle\nCloud;\nin-\ntegrates\nwith\nother\nGCP services.\nDeployed\nwithin\nAzure;\nintegrates\nwith Azure\u2019s suite of\nservices.\nDeployed\nwithin\ncustom\ninfrastruc-\nture; integrates with\nvarious\ncloud\nand\non-premises services.\nIntegration with\nEcosystem\nLimited\nto\nOpenAI\necosystem; integrates\nwell\nwith\napps\nvia\nAPI.\nSeamless\nintegration\nwith\nGoogle\nCloud\nservices\n(e.g.,\nBig-\nQuery, AutoML).\nDeep integration with\nAzure\u2019s services (e.g.,\nData Factory, Power\nBI).\nFlexible\nintegration\nwith multiple tools,\nAPIs,\nand\ndata\nsources.\nData Privacy\nManaged by OpenAI;\nusers\nmust\nconsider\ndata transfer and pri-\nvacy implications.\nStrong\nprivacy\nand\nsecurity\nmeasures\nwithin Google Cloud\nenvironment.\nStrong\nprivacy\nand\nsecurity\nmeasures\nwithin\nAzure\nenvi-\nronment.\nDependent on the in-\ntegrations and infras-\ntructure used;\nusers\nmanage privacy.\nTarget Users\nDevelopers\nand\nen-\nterprises\nlooking\nfor\nstraightforward,\nAPI-based\nLLM\nfine-tuning.\nDevelopers and busi-\nnesses integrated into\nGoogle Cloud or seek-\ning to leverage GCP.\nEnterprises\nand\nde-\nvelopers\nintegrated\ninto Azure or seeking\nto\nleverage\nAzure\u2019s\nAI tools.\nDevelopers\nneeding\nto\nbuild\ncomplex,\nmodular\nLLM-based\napplications\nwith\ncustom workflows.\nLimitations\nLimited\ncustomisa-\ntion; dependency on\nOpenAI\u2019s infrastruc-\nture; potential cost.\nLimited\nto\nGoogle\nCloud ecosystem; po-\ntential cost and ven-\ndor lock-in.\nLimited\nto\nAzure\necosystem", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities", "description": "Arxiv Paper titled 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities' authored by Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid", "url": "https://arxiv.org/pdf/2408.13296v1", "source": "arxiv", "id": "9e765bb5-cacc-4a3a-b60d-6268f6afb2df"}, "page_content": " used;\nusers\nmanage privacy.\nTarget Users\nDevelopers\nand\nen-\nterprises\nlooking\nfor\nstraightforward,\nAPI-based\nLLM\nfine-tuning.\nDevelopers and busi-\nnesses integrated into\nGoogle Cloud or seek-\ning to leverage GCP.\nEnterprises\nand\nde-\nvelopers\nintegrated\ninto Azure or seeking\nto\nleverage\nAzure\u2019s\nAI tools.\nDevelopers\nneeding\nto\nbuild\ncomplex,\nmodular\nLLM-based\napplications\nwith\ncustom workflows.\nLimitations\nLimited\ncustomisa-\ntion; dependency on\nOpenAI\u2019s infrastruc-\nture; potential cost.\nLimited\nto\nGoogle\nCloud ecosystem; po-\ntential cost and ven-\ndor lock-in.\nLimited\nto\nAzure\necosystem;\npotential\ncost\nand\nvendor\nlock-in.\nComplexity in chain-\ning multiple models\nand data sources; re-\nquires more setup.\nTable 10.2: Detailed Comparison of LLM Fine-Tuning Platforms (Part II). This table continues the\ncomparison of LLM fine-tuning tools, focusing on OpenAI Fine-Tuning API, Google Vertex AI Studio,\nMicrosoft Azure AI Studio, and LangChain.\nIt evaluates the tools based on the primary use case,\nmodel support, data handling, customisation level, scalability, deployment options, integration with the\necosystem, data privacy, target users, and limitations, offering a complete view of their capabilities and\nconstraints.\n10.1\nAutotrain\nAutotrain is HuggingFace\u2019s innovative platform that automates the fine-tuning of large language models,\nmaking it accessible even to those with limited machine learning expertise. The complexity and resource\ndemands of fine-tuning LLMs can be daunting, but Autotrain simplifies the process by handling the most\nchallenging aspects, such as data preparation, model configuration, and hyperparameter optimisation.\nThis automation is particularly valuable for small teams or individual developers who need to deploy\ncustom LLMs quickly and efficiently.\n10.1.1\nSteps Involved in Fine-Tuning Using Autotrain\nFollowing are the steps involved in fine-tuning LLMs using Autotrain. Figure 10.1 represents the visual\nworkflow.\n\u2022 Dataset Upload and Model Selection:\n77\nFigure 10.1: Overview of the Autotrain Workflow. This diagram illustrates the step-by-step process\nwithin the Autotrain system, beginning with the upload of datasets and model selection by users. The\nworkflow then moves to data preparation and model configuration, followed by automated hyperpa-\nrameter tuning to optimise model performance. The fine-tuning phase adjusts the model based on the\nprovided datasets, culminating in the deployment of the fully fine-tuned model for practical use.\n\u2013 Users begin by uploading their datasets to the Autotrain platform.\n\u2013 They then select a pre-trained model from the extensive HuggingFace Model Hub.\n\u2022 Data Preparation:\n\u2013 Autotrain automatically processes the uploaded data, including tasks like tokenization to\nconvert text into a format the LLM can understand.\n\u2022 Model Configuration:\n\u2013 The platform configures the model for fine-tuning, setting up the training environment and\nnecessary parameters.\n\u2022 Automated Hyperparameter Tuning:\n\u2013 Autotrain explores various hyperparameter configurations (such as learning rate, batch size,\nand sequence length) and selects the best-performing ones.\n\u2022 Fine-Tuning:\n\u2013 The model is fine-tuned on the prepared data with the optimised hyperparameters.\n\u2022 Deployment:\n\u2013 Once fine-tuning is complete, the model is ready for deployment in various NLP applications,\nsuch as text generation, completion, and language translation.\n78\n10.1.2\nBest Practices of Using Autotrain\n\u2022 Data Quality: Ensure high-quality, well-labelled data for better model performance.\n\u2022 Model Selection: Choose pre-trained models that are well-suited to your specific task to minimize\nfine-tuning effort.\n\u2022 Hyperparameter Optimisation: Leverage Autotrain\u2019s automated hyperparameter tuning to\nachieve optimal performance without manual intervention.\n10.1.3\nChallenges of Using Autotrain\n\u2022 Data Privacy: Ensuring the privacy and security of sensitive data during the fine-tuning process.\n\u2022 Resource Constraints: Managing computational resources effectively, especially in environments\nwith limited access to powerful hardware.\n\u2022 Model Overfitting: Avoiding overfitting by ensuring diverse and representative training data\nand using appropriate regularization techniques.\n10.1.4\nWhen to Use Autotrain\n1. Lack of Deep Technical Expertise: Ideal for individuals or small teams without extensive\nmachine learning or LLM background who need to fine-tune models quickly and effectively.\n2. Quick Prototyping and Deployment: Suitable for rapid development cycles where time is\ncritical, such as proof-of-concept projects or MVPs.\n3. Resource-Constrained Environments: Useful for scenarios with limited computational re-\nsources or where a quick turnaround is necessary.\nIn summary, Autotrain is an excellent tool for quick, user-friendly fine-tuning of LLMs for standard NLP\ntasks, especially in environments with limited resources or expertise. However, it may not be suitable\nfor highly specialised applications or those requiring significant", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities", "description": "Arxiv Paper titled 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities' authored by Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid", "url": "https://arxiv.org/pdf/2408.13296v1", "source": "arxiv", "id": "8b5cdc8a-1752-4e6b-8093-62b001d5c526"}, "page_content": " powerful hardware.\n\u2022 Model Overfitting: Avoiding overfitting by ensuring diverse and representative training data\nand using appropriate regularization techniques.\n10.1.4\nWhen to Use Autotrain\n1. Lack of Deep Technical Expertise: Ideal for individuals or small teams without extensive\nmachine learning or LLM background who need to fine-tune models quickly and effectively.\n2. Quick Prototyping and Deployment: Suitable for rapid development cycles where time is\ncritical, such as proof-of-concept projects or MVPs.\n3. Resource-Constrained Environments: Useful for scenarios with limited computational re-\nsources or where a quick turnaround is necessary.\nIn summary, Autotrain is an excellent tool for quick, user-friendly fine-tuning of LLMs for standard NLP\ntasks, especially in environments with limited resources or expertise. However, it may not be suitable\nfor highly specialised applications or those requiring significant customisation and scalability.\n10.1.5\nTutorials\n1. How To Create HuggingFace Custom AI Models Using AutoTrain\n2. Finetune models with HuggingFace AutoTrain\n10.2\nTransformers Library and Trainer API\nThe Transformers Library by HuggingFace stands out as a pivotal tool for fine-tuning large language\nmodels (LLMs) such as BERT, GPT-3, and GPT-4. This comprehensive library offers a wide array of\npre-trained models tailored for various LLM tasks, making it easier for users to adapt these models to\nspecific needs with minimal effort. Whether you\u2019re fine-tuning for tasks like sentiment analysis, text\nclassification, or generating customer support responses, the library simplifies the process by allowing\nseamless model selection from the HuggingFace Model Hub and straightforward customisation through\nits high-level APIs.\nCentral to the fine-tuning process within the Transformers Library is the Trainer API. This API includes\nthe Trainer class, which automates and manages the complexities of fine-tuning LLMs. After completing\ndata preprocessing, the Trainer class streamlines the setup for model training, including data handling,\noptimisation, and evaluation. Users only need to configure a few parameters, such as learning rate and\nbatch size, and the API takes care of the rest. However, it\u2019s crucial to note that running Trainer.train()\ncan be resource-intensive and slow on a CPU. For efficient training, a GPU or TPU is recommended.\nPlatforms like Google Colab provide free access to these resources, making it feasible for users without\nhigh-end hardware to fine-tune models effectively.\n79\nThe Trainer API also supports advanced features like distributed training and mixed precision, which\nare essential for handling the large-scale computations required by modern LLMs. Distributed training\nallows the fine-tuning process to be scaled across multiple GPUs or nodes, significantly reducing training\ntime. Mixed precision training, on the other hand, optimises memory usage and computation speed by\nusing lower precision arithmetic without compromising model performance. HuggingFace\u2019s dedication to\naccessibility is evident in the extensive documentation and community support they offer, enabling users\nof all expertise levels to fine-tune LLMs. This democratisation of advanced NLP technology empowers\ndevelopers and researchers to deploy sophisticated, fine-tuned models for a wide range of applications,\nfrom specialised language understanding to large-scale data processing.\n10.2.1\nLimitations of the Transformers Library and Trainer API\n\u2022 Limited Customisation for Advanced Users: While the Trainer API simplifies many aspects\nof training, it might not offer the deep customisation that advanced users or researchers might need\nfor novel or highly specialised applications.\n\u2022 Learning Curve: Despite the simplified API, there is still a learning curve associated with un-\nderstanding and effectively using the Transformers Library and Trainer API, particularly for those\nnew to NLP and LLM.\n\u2022 Integration Limitations: The seamless integration and ease of use are often tied to the Hug-\ngingFace ecosystem, which might not be compatible with all workflows or platforms outside their\nenvironment.\nIn summary, the Transformers Library and Trainer API provide robust, scalable solutions for fine-tuning\nLLMs across a range of applications, offering ease of use and efficient training capabilities. However, users\nmust be mindful of the resource requirements and potential limitations in customisation and complexity\nmanagement.\n10.3\nOptimum: Enhancing LLM Deployment Efficiency\nOptimum6 is HuggingFace\u2019s tool designed to optimise the deployment of large language models (LLMs)\nby enhancing their efficiency across various hardware platforms. As LLMs grow in size and complexity,\ndeploying them in a cost-effective and performant manner becomes increasingly challenging. Optimum\naddresses these challenges by applying a range of hardware-specific optimisations, such as quantisation,\npruning, and model distillation, which reduce the model\u2019s size and improve inference speed without\nsignificantly affecting accuracy. The following are the key techniques supported by Optimum:\n\u2022 Quantisation: Quantisation is one of the key techniques supported by Optimum. This process in-\nvolves converting the model\u2019s weights from high-precision floating-point numbers to lower-precision\nformats, such as int8 or float16. This reduction in precision decreases the model\u2019s memory foot-\nprint and computational requirements, enabling faster execution and lower power consumption,\nespecially on edge devices and mobile platforms. Optimum automates the quantisation process,\nmaking it accessible to users who may not have expertise", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities", "description": "Arxiv Paper titled 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities' authored by Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid", "url": "https://arxiv.org/pdf/2408.13296v1", "source": "arxiv", "id": "46579b78-2d49-4dd6-849b-6f5973792f27"}, "page_content": " hardware platforms. As LLMs grow in size and complexity,\ndeploying them in a cost-effective and performant manner becomes increasingly challenging. Optimum\naddresses these challenges by applying a range of hardware-specific optimisations, such as quantisation,\npruning, and model distillation, which reduce the model\u2019s size and improve inference speed without\nsignificantly affecting accuracy. The following are the key techniques supported by Optimum:\n\u2022 Quantisation: Quantisation is one of the key techniques supported by Optimum. This process in-\nvolves converting the model\u2019s weights from high-precision floating-point numbers to lower-precision\nformats, such as int8 or float16. This reduction in precision decreases the model\u2019s memory foot-\nprint and computational requirements, enabling faster execution and lower power consumption,\nespecially on edge devices and mobile platforms. Optimum automates the quantisation process,\nmaking it accessible to users who may not have expertise in low-level hardware optimisation.\n\u2022 Pruning: Pruning is another critical optimisation strategy offered by Optimum. It involves iden-\ntifying and removing less significant weights from the LLM, reducing its overall complexity and\nsize. This leads to faster inference times and lower storage needs, which are particularly beneficial\nfor deploying models in environments with limited computational resources. Optimum\u2019s pruning\nalgorithms carefully eliminate these redundant weights while maintaining the model\u2019s performance,\nensuring that it continues to deliver high-quality results even after optimisation.\n\u2022 Model Distillation: In addition to these techniques, Optimum supports model distillation, a\nprocess where a smaller, more efficient model is trained to replicate the behaviour of a larger, more\ncomplex model. This distilled model retains much of the knowledge and capabilities of the original\nwhile being significantly lighter and faster. Optimum provides tools to facilitate the distillation\nprocess, allowing users to create compact LLMs that are well-suited for real-time applications. By\noffering a comprehensive suite of optimisation tools, Optimum ensures that HuggingFace\u2019s LLMs\ncan be deployed effectively across a wide range of environments, from powerful cloud servers to\nresource-constrained edge devices.\n6https://huggingface.co/docs/optimum/en/index\n80\n10.3.1\nBest Practices of Using Optimum\n\u2022 Understand Hardware Requirements: Assess the target deployment environment (e.g., edge\ndevices, cloud servers) to optimise model configuration accordingly.\n\u2022 Iterative Optimisation: Experiment with different optimisation techniques (quantisation levels,\npruning thresholds) to find the optimal balance between model size, speed, and accuracy.\n\u2022 Validation and Testing: Validate optimised models thoroughly to ensure they meet performance\nand accuracy requirements across different use cases.\n\u2022 Documentation and Support: Refer to HuggingFace\u2019s resources for detailed guidance on using\nOptimum\u2019s tools effectively, and leverage community support for troubleshooting and best practices\nsharing.\n\u2022 Continuous Monitoring: Monitor deployed models post-optimisation to detect any performance\ndegradation and adjust optimisation strategies as needed to maintain optimal performance over\ntime.\n10.3.2\nTutorials\n1. An Introduction to Using Transformers and Hugging Face\n10.4\nAmazon SageMaker JumpStart\nAmazon SageMaker JumpStart is a feature within the SageMaker ecosystem designed to simplify and\nexpedite the fine-tuning of large language models (LLMs). It provides users with a rich library of pre-\nbuilt models and solutions that can be quickly customised for various use cases. This tool is particularly\nvaluable for organisations looking to deploy NLP solutions efficiently without deep expertise in machine\nlearning or the extensive computational resources typically required for training LLMs from scratch. The\narchitecture depicted in Figure 10.2 outlines a comprehensive pipeline for the fine-tuning and deployment\nof large language models (LLMs) Utilising AWS services.\n10.4.1\nSteps Involved in Using JumpStart\n\u2022 Data Preparation and Preprocessing:\n\u2013 Data Storage: Begin by securely storing raw datasets in Amazon S3, AWS\u2019s scalable object\nstorage service.\n\u2013 Preprocessing: Utilise the EMR Serverless framework with Apache Spark for efficient data\npreprocessing. This step refines and prepares the raw data for subsequent model training and\nevaluation.\n\u2013 Data Refinement: Store the processed dataset back into Amazon S3 after preprocessing,\nensuring accessibility and readiness for the next stages.\n\u2022 Model Fine-Tuning with SageMaker JumpStart:\n\u2013 Model Selection: Choose from a variety of pre-built models and solutions available through\nSageMaker JumpStart\u2019s extensive library, tailored for tasks such as sentiment analysis, text\ngeneration, or customer support automation.\n\u2013 Fine-Tuning Execution: Utilise Amazon SageMaker\u2019s capabilities, integrated with Sage-\nMaker JumpStart, to fine-tune the selected model. This involves adjusting parameters and\nconfigurations to optimise the model\u2019s performance for specific use cases.\n\u2013 Workflow Simplification: Leverage pre-built algorithms and model templates provided by\nSageMaker JumpStart to streamline the fine-tuning workflow, reducing the time and effort\nrequired for deployment.\n\u2022 Model Deployment and Hosting:\n81\nFigure 10.2: A step-by-step workflow illustrating the Amazon SageMaker JumpStart process, starting\nfrom data preprocessing using EMR Serverless Spark to the fine-tuning of LLMs, and ending with model\ndeployment on Amazon Sage", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities", "description": "Arxiv Paper titled 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities' authored by Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid", "url": "https://arxiv.org/pdf/2408.13296v1", "source": "arxiv", "id": "d46d6f2e-dfd3-4f9f-95fb-2b359e4e6da7"}, "page_content": " JumpStart\u2019s extensive library, tailored for tasks such as sentiment analysis, text\ngeneration, or customer support automation.\n\u2013 Fine-Tuning Execution: Utilise Amazon SageMaker\u2019s capabilities, integrated with Sage-\nMaker JumpStart, to fine-tune the selected model. This involves adjusting parameters and\nconfigurations to optimise the model\u2019s performance for specific use cases.\n\u2013 Workflow Simplification: Leverage pre-built algorithms and model templates provided by\nSageMaker JumpStart to streamline the fine-tuning workflow, reducing the time and effort\nrequired for deployment.\n\u2022 Model Deployment and Hosting:\n81\nFigure 10.2: A step-by-step workflow illustrating the Amazon SageMaker JumpStart process, starting\nfrom data preprocessing using EMR Serverless Spark to the fine-tuning of LLMs, and ending with model\ndeployment on Amazon SageMaker Endpoints. (adapted from [83])\n\u2013 Deployment Setup: Deploy the fine-tuned model using Amazon SageMaker\u2019s endpoint\ndeployment capabilities. This setup ensures that the model is hosted in a scalable environment\ncapable of handling real-time predictions efficiently.\n\u2013 Scalability: Benefit from AWS\u2019s infrastructure scalability, allowing seamless scaling of re-\nsources to accommodate varying workloads and operational demands.\n\u2013 Efficiency and Accessibility: Ensure that the deployed model is accessible via SageMaker\nendpoints, enabling efficient integration into production applications for real-time inference\ntasks.\n10.4.2\nBest Practices for Using JumpStart\n\u2022 Robust Data Management: Maintain secure and organised data storage practices in Amazon\nS3, facilitating efficient data access and management throughout the pipeline.\n\u2022 Cost-Effective Processing: Utilise serverless computing frameworks like EMR Serverless with\nApache Spark for cost-effective and scalable data preprocessing.\n\u2022 Optimised Fine-Tuning: Capitalise on SageMaker JumpStart\u2019s pre-built models and algorithms\nto expedite and optimise the fine-tuning process, ensuring optimal model performance without\n82\nextensive manual configuration.\n\u2022 Continuous Monitoring and Optimisation: Implement robust monitoring mechanisms post-\ndeployment to track model performance metrics. This allows for timely optimisations and adjust-\nments to maintain accuracy and efficiency over time.\n\u2022 Integration with AWS Services: Leverage AWS\u2019s comprehensive suite of services and inte-\ngration capabilities to create end-to-end pipelines that ensure reliable and scalable deployment of\nlarge-scale language models across diverse operational environments.\n10.4.3\nLimitations of Using JumpStart\n\u2022 Limited Customisation: While JumpStart simplifies the process for common use cases, it may\noffer limited flexibility for highly specialised or complex applications that require significant cus-\ntomisation beyond the provided templates and workflows.\n\u2022 Dependency on AWS Ecosystem: JumpStart is tightly integrated with AWS services, which\nmay pose challenges for users who prefer or need to operate in multi-cloud environments or those\nwith existing infrastructure outside of AWS.\n\u2022 Resource Costs: Utilising SageMaker\u2019s scalable resources for fine-tuning LLMs, especially large\nmodels, can incur substantial costs, which might be a barrier for smaller organisations or those\nwith limited budgets.\n10.4.4\nTutorials\n1. Fine-Tuning LLaMA 2 with Amazon SageMaker JumpStart\n2. LLM Agents Using AWS SageMaker JumpStart Foundation Models\n10.5\nAmazon Bedrock\nAmazon Bedrock7 is a fully managed service designed to simplify access to high-performing foundation\nmodels (FMs) from top AI innovators like AI21 Labs, Anthropic, Cohere, Meta, Mistral AI, Stability\nAI, and Amazon. It provides a unified API that integrates these models and offers extensive capabilities\nfor developing secure, private, and responsible generative AI applications. With Amazon Bedrock, users\ncan effortlessly experiment with and assess leading FMs tailored to their specific needs. The service sup-\nports private customisation of models through fine-tuning and Retrieval Augmented Generation (RAG),\nenabling the creation of intelligent agents that leverage enterprise data and systems. Amazon Bedrock\u2019s\nserverless architecture allows for quick deployment, seamless integration, and secure customisation of\nFMs without the burden of infrastructure management, Utilising AWS tools to deploy these models into\napplications efficiently and securely.\n10.5.1\nSteps Involved in Using Amazon Bedrock\nAmazon Bedrock offers a streamlined workflow for deploying and fine-tuning LLMs, making it an ideal\nchoice for businesses looking to quickly integrate advanced AI capabilities into their operations. Here\u2019s\na high-level overview of how Bedrock operates:\n\u2022 Model Selection: Users start by choosing from a curated selection of foundation models available\nthrough Bedrock. These include models from AWS (like Amazon Titan) and third-party providers\n(such as Anthropic Claude and Stability AI).\n\u2022 Fine-Tuning:\n\u2013 Once a model is selected, users can fine-tune it to better fit their specific needs. This involves\nfeeding the model with domain-specific data or task-specific instructions to tailor its outputs.\n7https://aws.amazon.com/bedrock/\n83\n\u2013 The fine-tuning process is handled via simple API calls, eliminating the need for extensive\nsetup or detailed configuration. Users provide their custom data, and Bedrock manages the\ntraining process in the background.\n\u2022 Deployment:\n\u2013 After", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities", "description": "Arxiv Paper titled 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities' authored by Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid", "url": "https://arxiv.org/pdf/2408.13296v1", "source": "arxiv", "id": "339a000a-54fd-451e-9a32-bdaa9e1dcef3"}, "page_content": " it an ideal\nchoice for businesses looking to quickly integrate advanced AI capabilities into their operations. Here\u2019s\na high-level overview of how Bedrock operates:\n\u2022 Model Selection: Users start by choosing from a curated selection of foundation models available\nthrough Bedrock. These include models from AWS (like Amazon Titan) and third-party providers\n(such as Anthropic Claude and Stability AI).\n\u2022 Fine-Tuning:\n\u2013 Once a model is selected, users can fine-tune it to better fit their specific needs. This involves\nfeeding the model with domain-specific data or task-specific instructions to tailor its outputs.\n7https://aws.amazon.com/bedrock/\n83\n\u2013 The fine-tuning process is handled via simple API calls, eliminating the need for extensive\nsetup or detailed configuration. Users provide their custom data, and Bedrock manages the\ntraining process in the background.\n\u2022 Deployment:\n\u2013 After fine-tuning, Bedrock takes care of deploying the model in a scalable and efficient manner.\nThis means that users can quickly integrate the fine-tuned model into their applications or\nservices.\n\u2013 Bedrock ensures that the model scales according to demand and handles performance optimi-\nsation, providing a seamless user experience.\n\u2022 Integration and Monitoring:\n\u2013 Bedrock integrates smoothly with other AWS services, allowing users to embed AI capabilities\ndirectly into their existing AWS ecosystem.\n\u2013 Users can monitor and manage the performance of their deployed models through AWS\u2019s\ncomprehensive monitoring tools, ensuring that the models continue to perform optimally.\n10.5.2\nLimitations of Using Amazon Bedrock\nWhile Amazon Bedrock offers a robust suite of tools and services for addressing certain AI challenges,\nit is not a comprehensive solution for all AI needs. One key limitation is that it does not eliminate the\nrequirement for human expertise. Organisations still need skilled professionals who understand the in-\ntricacies of AI technology to effectively develop, fine-tune, and optimise the models provided by Bedrock.\nAdditionally, Amazon Bedrock is not designed to function as a standalone service. It relies on integration\nwith other AWS services, such as Amazon S3 for data storage, AWS Lambda for serverless computing,\nand AWS SageMaker for machine learning model development. Therefore, businesses leveraging Amazon\nBedrock will also need to use these complementary AWS services to fully realise its potential. This\ninterconnectedness means that while Amazon Bedrock enhances the AI capabilities within an AWS\necosystem, it may present a steep learning curve and require significant infrastructure management for\nthose new to AWS.\n10.5.3\nTutorials\n1. Finetuning LLMs on Amazon Bedrock\n2. Amazon Bedrock for Generative AI\n10.6\nOpenAI\u2019s Fine-Tuning API\nOpenAI\u2019s Fine-Tuning API is a comprehensive platform that facilitates the customisation of OpenAI\u2019s\npre-trained LLMs to cater to specific tasks and domains. This service is designed to be user-friendly,\nenabling a broad range of users, from businesses to individual developers, to harness the power of\nadvanced AI without the complexities typically associated with model training and deployment.\n10.6.1\nSteps Involved in Using OpenAI\u2019s Fine-Tuning API\n\u2022 Model Selection:\n\u2013 Choosing a Pre-Trained Model: Users begin by selecting a base model from OpenAI\u2019s\nextensive lineup. This includes powerful models like GPT-4, which offer a robust starting\npoint for a wide range of language processing tasks.\n\u2013 Customisable Base: These models come pre-trained with vast amounts of data, providing\na solid foundation that can be further refined to suit specific requirements.\n\u2022 Data Preparation and Upload:\n84\n\u2013 Curating Relevant Data: Users need to gather and prepare a dataset that reflects the\nspecific task or domain they wish to fine-tune the model for. This data is crucial for teaching\nthe model to perform the desired function more effectively.\n\u2013 Uploading Data to the API: The Fine-Tuning API facilitates easy data upload. Users\ncan feed their curated datasets into the API through straightforward commands, making the\nprocess accessible even to those with limited technical backgrounds.\n\u2022 Initiating Fine-Tuning:\n\u2013 Automated Process: Once the data is uploaded, OpenAI\u2019s infrastructure handles the fine-\ntuning process. The API adjusts the model\u2019s parameters based on the new data to improve\nperformance on the specified tasks.\n\u2022 Deploying the Fine-Tuned Model:\n\u2013 API Integration: The fine-tuned model can be accessed and deployed via OpenAI\u2019s API.\nThis allows for seamless integration into various applications, such as chatbots, automated\ncontent creation tools, or specialised customer service systems.\n10.6.2\nLimitations of OpenAI\u2019s Fine-Tuning API\n\u2022 Pricing Models: Fine-tuning and using OpenAI\u2019s models through the API can be costly, espe-\ncially for large-scale deployments or continuous usage. This can be a significant consideration for\nsmaller organisations or budget-constrained projects.\n\u2022 Data Privacy and Security: Users must upload their data to OpenAI\u2019s servers for the fine-\ntuning process. This raises potential concerns about data privacy and the security of sensitive or\nproprietary information.\n\u2022 Dependency on OpenAI Infrastructure: The reliance on OpenAI\u2019s infrastructure for model\nhosting and API access can lead to vendor lock-in, limiting flexibility and control over the deploy-\nment environment.", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities", "description": "Arxiv Paper titled 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities' authored by Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid", "url": "https://arxiv.org/pdf/2408.13296v1", "source": "arxiv", "id": "d3ae941c-26af-483c-b1bd-0d059569085b"}, "page_content": "s API.\nThis allows for seamless integration into various applications, such as chatbots, automated\ncontent creation tools, or specialised customer service systems.\n10.6.2\nLimitations of OpenAI\u2019s Fine-Tuning API\n\u2022 Pricing Models: Fine-tuning and using OpenAI\u2019s models through the API can be costly, espe-\ncially for large-scale deployments or continuous usage. This can be a significant consideration for\nsmaller organisations or budget-constrained projects.\n\u2022 Data Privacy and Security: Users must upload their data to OpenAI\u2019s servers for the fine-\ntuning process. This raises potential concerns about data privacy and the security of sensitive or\nproprietary information.\n\u2022 Dependency on OpenAI Infrastructure: The reliance on OpenAI\u2019s infrastructure for model\nhosting and API access can lead to vendor lock-in, limiting flexibility and control over the deploy-\nment environment.\n\u2022 Limited Control Over Training Process: The fine-tuning process is largely automated and\nmanaged by OpenAI, offering limited visibility and control over the specific adjustments made to\nthe model.\n10.6.3\nTutorials\n1. Fine-Tuning GPT-3 Using the OpenAI API\n10.7\nNVIDIA NeMo Customizer\nNVIDIA NeMo Customiser8 is part of the NeMo framework, a suite of tools and models designed by\nNVIDIA to facilitate the development and fine-tuning of LLM models. The Customiser focuses specifi-\ncally on making it easier to fine-tune large language models (LLMs) for specialised tasks and domains.\nLike other fine-tuning tools, NeMo Customiser is geared toward users who want to adapt pre-trained\nmodels for specific applications, such as conversational AI, translation, or domain-specific text gener-\nation. It delivers enterprise-ready models by offering accurate data curation, extensive customisation\noptions, retrieval-augmented generation (RAG), and improved performance features. The platform sup-\nports training and deploying generative AI models across diverse environments, including cloud, data\ncenter, and edge locations. It provides a comprehensive package with support, security, and reliable APIs\nas part of the NVIDIA AI Enterprise.\n8https://developer.nvidia.com/blog/fine-tune-and-align-llms-easily-with-nvidia-nemo-customizer/\n85\n10.7.1\nKey Features of NVIDIA NeMo\nNVIDIA NeMo is designed to enhance AI projects with several standout features.[84]\n\u2022 State-of-the-Art Training Techniques NeMo employs GPU-accelerated tools like NeMo Cu-\nrator for preparing large-scale, high-quality datasets. These tools facilitate efficient pretraining of\ngenerative AI models by leveraging thousands of compute cores, which significantly reduces training\ntime and enhances the accuracy of large language models (LLMs).\n\u2022 Advanced Customisation for LLMs The NeMo Customiser microservice allows for precise fine-\ntuning and alignment of LLMs for specific domains. It uses model parallelism to speed up training\nand supports scaling across multiple GPUs and nodes, enabling the fine-tuning of larger models.\n\u2022 Optimised AI Inference with NVIDIA Triton NeMo includes NVIDIA Triton Inference Server\nto streamline AI inference at scale. This integration accelerates generative AI inference, ensuring\nconfident deployment of AI applications both on-premises and in the cloud.\n\u2022 User-Friendly Tools for Generative AI NeMo features a modular, reusable architecture that\nsimplifies the development of conversational AI models. It supports comprehensive workflows from\ndata processing to deployment and includes pre-trained models for automatic speech recognition\n(ASR), natural language processing (NLP), and text-to-speech (TTS), which can be fine-tuned or\nused as-is.\n\u2022 Best-in-Class Pretrained Models NeMo Collections offer a variety of pre-trained models and\ntraining scripts, facilitating rapid application development or fine-tuning for specific tasks. Cur-\nrently, NeMo supports models like Llama 2, Stable Diffusion, and NVIDIA\u2019s Nemotron-3 8B family.\n\u2022 Optimised Retrieval-Augmented Generation NeMo Retriever delivers high-performance, low-\nlatency information retrieval, enhancing generative AI applications with enterprise-grade retrieval-\naugmented generation (RAG) capabilities. This feature supports real-time business insights and\ndata Utilisation.\n10.7.2\nComponents of NVIDIA NeMo\n\u2022 NeMo Core Provides essential elements like the Neural Module Factory for training and inference,\nstreamlining the development of conversational AI models.\n\u2022 NeMo Collections Offers specialised modules and models for ASR, NLP, and TTS, including\npre-trained models and training scripts, making the platform versatile.\n\u2022 Neural Modules Serve as the building blocks of NeMo, defining trainable components such as\nencoders and decoders, which can be connected to create comprehensive models.\n\u2022 Application Scripts Simplify the deployment of conversational AI models with ready-to-use\nscripts, enabling quick training or fine-tuning on specific datasets for various AI applications.\n10.7.3\nCustomising Large Language Models (LLMs)\nWhile general-purpose LLMs, enhanced with prompt engineering or light fine-tuning, have enabled organ-\nisations to achieve successful proof-of-concept projects, transitioning to production presents", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities", "description": "Arxiv Paper titled 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities' authored by Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid", "url": "https://arxiv.org/pdf/2408.13296v1", "source": "arxiv", "id": "649c13fa-36cf-4f3e-bd63-568607e20903"}, "page_content": "\u2022 NeMo Core Provides essential elements like the Neural Module Factory for training and inference,\nstreamlining the development of conversational AI models.\n\u2022 NeMo Collections Offers specialised modules and models for ASR, NLP, and TTS, including\npre-trained models and training scripts, making the platform versatile.\n\u2022 Neural Modules Serve as the building blocks of NeMo, defining trainable components such as\nencoders and decoders, which can be connected to create comprehensive models.\n\u2022 Application Scripts Simplify the deployment of conversational AI models with ready-to-use\nscripts, enabling quick training or fine-tuning on specific datasets for various AI applications.\n10.7.3\nCustomising Large Language Models (LLMs)\nWhile general-purpose LLMs, enhanced with prompt engineering or light fine-tuning, have enabled organ-\nisations to achieve successful proof-of-concept projects, transitioning to production presents additional\nchallenges.\nFigure 10.3 illustrates NVIDIA\u2019s detailed LLM customisation lifecycle, offering valuable\nguidance for organisations that are preparing to deploy customised models in a production environment\n[85].\n1. Model Selection or Development\nNVIDIA provides a range of pre-trained models, from 8B to 43B parameters, and supports the\nintegration of other open-source models of any size. Alternatively, users can develop their own\nmodels, starting with data curation, which includes selecting, labeling, cleansing, validating, and\nintegrating data. This process, better termed data engineering, involves additional analysis, de-\nsigning storage, evaluating model training results, and incorporating reinforcement learning with\nhuman feedback (RLHF). While building a custom foundation model is often costly, complex, and\ntime-consuming, most enterprises opt to start with a pre-trained model and focus on customisation.\n86\nFigure 10.3: Nvidia NeMo Framework for Customising and Deploying LLMs. The Nvidia NeMo frame-\nwork is designed for end-to-end customisation and deployment of large language models (LLMs). This\ndiagram illustrates the process from data curation and distributed training of foundation models, through\nmodel customisation, to accelerated inference with guardrails. The platform enables AI developers to\nintegrate in-domain, secure, and cited responses into enterprise applications, ensuring that LLMs are\neffectively tailored for specific tasks and industries. The NeMo framework, supported by Nvidia AI En-\nterprise, also offers robust support for various pre-trained foundation models like OpenAI\u2019s GPT family,\nensuring scalability and reliability in AI deployments. (adapted from [85])\n2. Model Customisation\nModel customisation involves optimising performance with task-specific datasets and adjusting\nmodel weights. NeMo offers recipes for customisation, and enterprises can choose models already\ntailored to specific tasks and then fine-tune them with proprietary data.\n3. Inference\nInference refers to running models based on user queries. This phase involves considering hardware,\narchitecture, and performance factors that significantly impact usability and cost in production.\n4. Guardrails\nNVIDIA employs guardrails as intermediary services between models and applications.\nThese\nservices review incoming prompts for policy compliance, execute arbitration or orchestration steps,\nand ensure model responses adhere to policies. Guardrails help maintain relevance, accuracy, safety,\nprivacy, and security.\n5. Applications\nNVIDIA\u2019s framework presents enterprise applications as LLM-ready, though this is not always\nthe case.\nExisting applications may be connected to LLMs to enable new features.\nHowever,\ncreating assistants for knowledge access or task execution often involves designing new applications\nspecifically for natural language interfaces.\n10.7.4\nTutorials\n1. Introduction to NVIDIA NeMo \u2014 Tutorial and Example\n2. How to fine-tune a Riva NMT Bilingual model with Nvidia NeMo\n87\nChapter 11\nMultimodal LLMs and their\nFine-tuning\nA multimodal model is a machine learning model that can process information from various modalities,\nsuch as images, videos, and text. For instance, Google\u2019s multimodal model, Gemini[86], can analyse a\nphoto of a plate of cookies and produce a written recipe in response, and it can perform the reverse as well.\nThe difference between Generative AI and Multimodal AI is that generative AI refers to the use of\nmachine learning models to create new content, such as text, images, music, audio, and videos, typically\nfrom a single type of input. Multimodal AI extends these generative capabilities by processing informa-\ntion from multiple modalities, including images, videos, and text. This enables the AI to understand\nand interpret different sensory modes, allowing users to input various types of data and receive a diverse\nrange of content types in return.\nFigure 11.1: Timeline of Multimodal Model Developments \u2014 This figure illustrates the progression\nof significant multimodal models, highlighting key releases from major tech companies and research\ninstitutions from December 2023 to March 2024. The timeline showcases models like Google\u2019s TinyGPT-\nV and Gemini Nano, along with other innovations such as MoE-LLAVA, DeepSeek-VL, and LLAVA-\nGemma, indicating the rapid advancement in multimodal AI technologies (adapted from [87]).\n88\n11.1\nVision Language Model (VLMs)\nVision language models encompass multimodal models capable of learning from both images and text\ninputs. They belong to the", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities", "description": "Arxiv Paper titled 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities' authored by Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid", "url": "https://arxiv.org/pdf/2408.13296v1", "source": "arxiv", "id": "1772be25-ae3c-436f-a978-0753709d096a"}, "page_content": "\ntion from multiple modalities, including images, videos, and text. This enables the AI to understand\nand interpret different sensory modes, allowing users to input various types of data and receive a diverse\nrange of content types in return.\nFigure 11.1: Timeline of Multimodal Model Developments \u2014 This figure illustrates the progression\nof significant multimodal models, highlighting key releases from major tech companies and research\ninstitutions from December 2023 to March 2024. The timeline showcases models like Google\u2019s TinyGPT-\nV and Gemini Nano, along with other innovations such as MoE-LLAVA, DeepSeek-VL, and LLAVA-\nGemma, indicating the rapid advancement in multimodal AI technologies (adapted from [87]).\n88\n11.1\nVision Language Model (VLMs)\nVision language models encompass multimodal models capable of learning from both images and text\ninputs. They belong to the category of generative models that utilise image and text data to produce\ntextual outputs. These models, especially at larger scales, demonstrate strong zero-shot capabilities,\nexhibit robust generalisation across various tasks, and effectively handle diverse types of visual data such\nas documents and web pages. Typical applications include conversational interactions involving images,\nimage interpretation based on textual instructions, answering questions related to visual content, under-\nstanding documents, generating captions for images, and more. Certain advanced vision language models\ncan also understand spatial attributes within images. They can generate bounding boxes or segmentation\nmasks upon request to identify or isolate specific subjects, localise entities within images, or respond to\nqueries regarding their relative or absolute positions. The landscape of large vision language models is\ncharacterised by considerable diversity in training data, image encoding techniques, and consequently,\ntheir functional capabilities.\n11.1.1\nArchitecture\nVision-language models adeptly integrate both visual and textual information, leveraging three funda-\nmental components:\n\u2022 Image Encoder: This component translates visual data (images) into a format that the model\ncan process.\n\u2022 Text Encoder: Similar to the image encoder, this component converts textual data (words and\nsentences) into a format the model can understand.\n\u2022 Fusion Strategy: This component combines the information from both the image and text en-\ncoders, merging the two data types into a unified representation.\nThese elements work collaboratively, with the model\u2019s learning process (loss functions) specifically tai-\nlored to the architecture and learning strategy employed. Although the concept of vision-language mod-\nels is not new, their construction has evolved significantly. Early models used manually crafted image\ndescriptions and pre-trained word vectors. Modern models, however, utilise transformers\u2014an advanced\nneural network architecture\u2014for both image and text encoding. These encoders can learn features either\nindependently or jointly.\nA crucial aspect of these models is pre-training. Before being applied to specific tasks, the models are\ntrained on extensive datasets using carefully selected objectives. This pre-training equips them with the\nfoundational knowledge required to excel in various downstream applications. Following is one of the\nexample architectures of VLMs.\n11.1.2\nContrastive Learning\nContrastive learning is a technique that focuses on understanding the differences between data points. It\ncomputes a similarity score between instances and aims to minimise contrastive loss, making it particu-\nlarly useful in semi-supervised learning where a limited number of labelled samples guide the optimisation\nprocess to classify unseen data points.\nHow it works\nFor instance, to recognise a cat, contrastive learning compares a cat image with a similar cat image and\na dog image. The model learns to distinguish between a cat and a dog by identifying features such as\nfacial structure, body size, and fur. By determining which image is closer to the \u201danchor\u201d image, the\nmodel predicts its class.\nCLIP is a model that utilises contrastive learning to compute similarity between text and image embed-\ndings through textual and visual encoders. It follows a three-step process for zero-shot predictions:\n\u2022 Pre-training: Trains a text and image encoder to learn image-text pairs.\n\u2022 Caption Conversion: Converts training dataset classes into captions.\n\u2022 Zero-Shot Prediction: Estimates the best caption for a given input image based on learned\nsimilarities.\n89\nFigure 11.2: Workflow of Contrastive Pre-Training for Multimodal Models. This figure illustrates the\nprocess of contrastive pre-training where text and image encoders are trained to align representations\nfrom both modalities. Step 1 involves contrastive pre-training by pairing text and image data, while\nStep 2 showcases the creation of a dataset classifier using label text encoded by the text encoder. Step\n3 demonstrates the model\u2019s application for zero-shot prediction by leveraging the pre-trained text and\nimage encoders. This method enables the model to generalise across various tasks without requiring\ntask-specific fine-tuning (adopted from [88]).\n11.2\nFine-tuning of multimodal models\nFor fine-tuning a Multimodal Large Language Model (MLLM), PEFT techniques such as LoRA and\nQLoRA can be utilised. The process of fine-tuning for multimodal applications is analogous to that for\nlarge language models, with the primary difference being the nature of the input data. In addition to\nLoRA, which employs matrix factorisation techniques to reduce the number of parameters, other tools\nsuch as LLM-Adapters and (IA", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities", "description": "Arxiv Paper titled 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities' authored by Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid", "url": "https://arxiv.org/pdf/2408.13296v1", "source": "arxiv", "id": "2c9e108c-7b50-4310-b776-f94de858b692"}, "page_content": " by pairing text and image data, while\nStep 2 showcases the creation of a dataset classifier using label text encoded by the text encoder. Step\n3 demonstrates the model\u2019s application for zero-shot prediction by leveraging the pre-trained text and\nimage encoders. This method enables the model to generalise across various tasks without requiring\ntask-specific fine-tuning (adopted from [88]).\n11.2\nFine-tuning of multimodal models\nFor fine-tuning a Multimodal Large Language Model (MLLM), PEFT techniques such as LoRA and\nQLoRA can be utilised. The process of fine-tuning for multimodal applications is analogous to that for\nlarge language models, with the primary difference being the nature of the input data. In addition to\nLoRA, which employs matrix factorisation techniques to reduce the number of parameters, other tools\nsuch as LLM-Adapters and (IA)\u00b3[89] can be effectively used. LLM-Adapters integrate various adapter\nmodules into the pre-trained model\u2019s architecture, enabling parameter-efficient fine-tuning for diverse\ntasks by updating only the adapter parameters while keeping the base model parameters fixed. (IA)\u00b3,\nor Infused Adapters by Inhibiting and Amplifying Inner Activations, enhances performance by learn-\ning vectors to weight model parameters through activation multiplications, supporting robust few-shot\nperformance and task mixing without manual adjustments. Moreover, dynamic adaptation techniques\nlike DyLoRA[90] allow for the training of low-rank adaptation blocks across different ranks, optimising\nthe learning process by sorting the representations during training. LoRA-FA[91], a variant of LoRA,\noptimises the fine-tuning process by freezing the first low-rank matrix after initialisation and using it as a\nrandom projection while training the other, thereby reducing the number of parameters by half without\ncompromising performance.\nThe Efficient Attention Skipping (EAS)[92] module introduces a novel parameter and computation-\nefficient tuning method for MLLMs, aiming to maintain high performance while reducing parameter and\ncomputation costs for downstream tasks. However, MemVP[93] critiques this approach, noting that it\nstill increases the input length of language models. To address this, MemVP integrates visual prompts\nwith the weights of Feed Forward Networks, thereby injecting visual knowledge to decrease training time\nand inference latency, ultimately outperforming previous PEFT methods.\n11.2.1\nFull-parameter Fine-Tuning\nMethods such as those introduced by LOMO[94] and MeZO[95] provide alternative solutions by focusing\non memory efficiency.\nLOMO utilises a low-memory optimisation technique derived from Stochastic\nGradient Descent (SGD), reducing memory consumption typically associated with the ADAM optimiser.\nMeZO, on the other hand, offers a memory-efficient optimiser that requires only two forward passes\nto compute gradients, enabling comprehensive fine-tuning of large models with a memory footprint\nequivalent to inference [87].\n90\n11.2.2\nCase study of fine-tuning MLLMs for Medical domain\nThe following section provides a case study on fine-tuning MLLMs for the Visual Question Answering\n(VQA) task. In this example, we present a PEFT for fine-tuning MLLM specifically designed for Med-\nVQA applications. To ensure accurate performance measurement, human evaluations were conducted,\ndemonstrating that the model achieves an overall accuracy of 81.9% and surpasses the GPT-4v model\nby a substantial margin of 26% in absolute accuracy on closed-ended questions.\nThe model consists of three components: the vision encoder, a pre-trained Large Language Model (LLM)\nfor handling multimodal inputs and generating responses, and a single linear layer for projecting embed-\ndings from the visual encoding space to the LLM space, as shown in figure 11.3.\nThe Vision Transformer (ViT) type backbone, EVA, encodes image tokens into visual embeddings,\nwith model weights remaining frozen during the fine-tuning process. The technique from MiniGPT-v2\nis utilised, grouping four consecutive tokens into one visual embedding to efficiently reduce resource\nconsumption by concatenating on the embedding dimension.\nThese grouped visual tokens are then processed through the projection layer, resulting in embeddings\n(length 4096) in the LLM space. A multimodal prompt template integrates both visual and question\ninformation, which is input into the pre-trained LLM, LLaMA2-chat(7B), for answer generation. The\nlow-rank adaptation (LoRA) technique is applied for efficient fine-tuning, keeping the rest of the LLM\nfrozen during downstream fine-tuning. A beam search with a width of 1 is utilised.\nFigure 11.3: Overview of Med VQA architecture integrating LoRA and a pre-trained LLM with a Vision\nEncoder for medical visual question answering tasks. The architecture includes stages for processing\nimages and generating contextually relevant responses, demonstrating the integration of vision and lan-\nguage models in a medical setting (adopted from [96]).\nThe multimodal prompt includes input images, questions, and a specific token for VQA tasks, following\nthe MiniGPT-v2 template. In Figure 11.3, the image features derived from linear projection are labelled\nas ImageFeature, with the corresponding questions serving as text instructions. The special token [VQA]\nis used as the task identifier, forming the complete multimodal instructional template:\n91\n[INST]<img><ImageFeature></img", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities", "description": "Arxiv Paper titled 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities' authored by Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid", "url": "https://arxiv.org/pdf/2408.13296v1", "source": "arxiv", "id": "4dc07de4-2f58-483f-b886-b0bddba30761"}, "page_content": "uning, keeping the rest of the LLM\nfrozen during downstream fine-tuning. A beam search with a width of 1 is utilised.\nFigure 11.3: Overview of Med VQA architecture integrating LoRA and a pre-trained LLM with a Vision\nEncoder for medical visual question answering tasks. The architecture includes stages for processing\nimages and generating contextually relevant responses, demonstrating the integration of vision and lan-\nguage models in a medical setting (adopted from [96]).\nThe multimodal prompt includes input images, questions, and a specific token for VQA tasks, following\nthe MiniGPT-v2 template. In Figure 11.3, the image features derived from linear projection are labelled\nas ImageFeature, with the corresponding questions serving as text instructions. The special token [VQA]\nis used as the task identifier, forming the complete multimodal instructional template:\n91\n[INST]<img><ImageFeature></img>[VQA] Instruction [/INST].\nModel Training\nWeights from MiniGPT-v2, pre-trained on general domain datasets, are further fine-tuned using multi-\nmodal medical datasets in two stages. The LoRA technique is employed for efficient fine-tuning, updating\nonly a small portion of the entire model, as detailed below:\n\u2022 Fine-tuning with image captioning: During this stage, the model is fine-tuned using the ROCO\nmedical image-caption dataset, which contains medical image-caption pairs of varying lengths. The\nprompt template used is <Img><ImageHere></Img>[caption] <instruction>, with the instruc-\ntion prompt randomly selected from a pool of four candidates, such as \u201cBriefly describe this image.\u201d\nDuring training, only the linear projection layer and the LoRA layer in the LLM are fine-tuned,\nwhile other parts of the model remain frozen.\n\u2022 Fine-tuning on VQA: In the second stage, the model is fine-tuned on the Med-VQA dataset,\nVQA-RAD, which contains triplets of images, questions, and answers. Following the instruction\ntemplate proposed in MiniGPT-v2, the template used is: \u201c[INST] <img><ImageFeature></img>[VQA]\nInstruction [/INST]\u201d, where the instruction prompt is: \u201cBased on the image, respond to this\nquestion with a short answer: question,\u201d with question signifying the question corresponding to\nthe given medical image. The motivation for generating short answers is to validate against the\nexisting labelled data in VQA-RAD, where the answers are typically short in both open-ended and\nclosed-ended QA pairs. Similar to the first stage, the vision encoder and the LLM remain frozen\nwhile only the linear projection and LoRA layers in the LLM are updated.\n11.3\nApplications of Multimodal models\n1. Gesture Recognition - These models interpret and recognise human gestures, which is crucial\nfor sign language translation. Multimodal models facilitate inclusive communication by processing\ngestures and converting them into text or speech.\n2. Video Summarisation - Multimodal models can summarise lengthy videos by extracting key vi-\nsual and audio elements. This capability streamlines content consumption, enables efficient content\nbrowsing, and enhances video content management platforms.\n3. DALL-E is a notable example of multimodal AI that generates images from textual descriptions.\nThis technology expands creative possibilities in content creation and visual storytelling, with\napplications in art, design, advertising, and more.\n4. Educational Tools - Multimodal models enhance learning experiences by providing interactive\neducational content that responds to both visual and verbal cues from students. They are integral\nto adaptive learning platforms that adjust content and difficulty based on student performance and\nfeedback.\n5. Virtual Assistants - Multimodal models power virtual assistants by understanding and respond-\ning to voice commands while processing visual data for comprehensive user interaction. They are\nessential for smart home automation, voice-controlled devices, and digital personal assistants.\n11.4\nAudio or Speech LLMs Or Large Audio Models\nAudio or speech LLMs are models designed to understand and generate human language based on audio\ninputs. They have applications in speech recognition, text-to-speech conversion, and natural language\nunderstanding tasks. These models are typically pre-trained on large datasets to learn generic language\npatterns, which are then fine-tuned on specific tasks or domains to enhance performance.\nAudio and Speech Large Language Models (LLMs) represent a significant advancement in the integration\nof language processing with audio signals. These models leverage a robust Large Language Model as a\nfoundational backbone, which is enhanced to handle multimodal data through the inclusion of custom\naudio tokens. This transformation allows the models to learn and operate within a shared multimodal\nspace, where both text and audio signals can be effectively processed.\n92\nUnlike text, which is inherently discrete, audio signals are continuous and need to be discretized into\nmanageable audio tokens. Techniques like HuBERT[97] and wav2vec[98] are employed for this purpose,\nconverting audio into a tokenized format that the LLM can process alongside text. The model, typically\nautoregressive and decoder-based, is pre-trained using a combination of self-supervised tasks, such as\npredicting masked tokens in interleaved text and audio, and supervised fine-tuning for specific tasks like\ntranscription or sentiment analysis. This capability to handle and generate audio and text simultane-\nously allows for a wide range of applications, from audio question answering to speech-based sentiment\ndetection, making Audio", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities", "description": "Arxiv Paper titled 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities' authored by Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid", "url": "https://arxiv.org/pdf/2408.13296v1", "source": "arxiv", "id": "1f2cd210-90a6-44e9-bb1f-0dd56f147bc5"}, "page_content": " data through the inclusion of custom\naudio tokens. This transformation allows the models to learn and operate within a shared multimodal\nspace, where both text and audio signals can be effectively processed.\n92\nUnlike text, which is inherently discrete, audio signals are continuous and need to be discretized into\nmanageable audio tokens. Techniques like HuBERT[97] and wav2vec[98] are employed for this purpose,\nconverting audio into a tokenized format that the LLM can process alongside text. The model, typically\nautoregressive and decoder-based, is pre-trained using a combination of self-supervised tasks, such as\npredicting masked tokens in interleaved text and audio, and supervised fine-tuning for specific tasks like\ntranscription or sentiment analysis. This capability to handle and generate audio and text simultane-\nously allows for a wide range of applications, from audio question answering to speech-based sentiment\ndetection, making Audio and Speech LLMs a versatile tool in multimodal AI. The figure 11.4 illustrates\nan example of a multimodal Audio LM architecture. In this setup, a prompt provides instructions in\nboth text and audio formats. The audio is tokenized using an audio tokenizer. The multimodal model\nthen combines these text and audio tokens and generates spoken speech through a vocoder (also known\nas a voice decoder).\nFigure 11.4: Multimodal Audio-Text Language Model architecture that integrates text and audio in-\nputs for advanced multimodal processing.\nThe architecture utilises text tokenizers and audio en-\ncoders/tokenizers to convert inputs into tokens, which are then processed by the audio-text LM. This\nmodel supports both discrete and continuous speech processing and enables tasks such as sentiment anal-\nysis and response generation in natural language. The audio tokens are further refined using a vocoder,\nwhile text tokens are detokenized to produce coherent text outputs (adapted from [99]).\n93\nAudio and speech LLMs like AudioPaLM[100], AudioLM[101], and various adaptations of models like\nWhisper and LLaMA, integrate capabilities for understanding and generating audio data, including\nspeech-to-text (STT), text-to-speech (TTS), and speech-to-speech (STS) translation.\nThese models\nhave shown that LLMs, initially designed for text, can be effectively adapted for audio tasks through\nsophisticated tokenization and fine-tuning techniques.\n11.4.1\nTokenization and Preprocessing\nA key aspect of adapting LLMs for audio is the tokenization of audio data into discrete representations\nthat the model can process. For instance, AudioLM and AudioPaLM utilise a combination of acoustic\nand semantic tokens. Acoustic tokens capture the high-quality audio synthesis aspect, while semantic\ntokens help maintain long-term structural coherence in the generated audio. This dual-token approach\nallows the models to handle both the intricacies of audio waveforms and the semantic content of speech.\n11.4.2\nFine-Tuning Techniques\nFine-tuning audio and speech LLMs typically involve several key strategies:\n\u2022 Full Parameter Fine-Tuning: This involves updating all the model\u2019s parameters during fine-\ntuning. For instance, LauraGPT and SpeechGPT fine-tune all parameters to adapt pre-trained\ntext LLMs to various audio tasks, although this can be computationally expensive.\n\u2022 Layer-Specific Fine-Tuning: Techniques like LoRA (Low-Rank Adaptation) update only spe-\ncific layers or modules of the model. This method significantly reduces computational requirements\nwhile still allowing effective adaptation. Models like Qwen-Audio leverage LoRA to fine-tune pre-\ntrained components for enhanced performance on speech recognition tasks.\n\u2022 Component-Based Fine-Tuning: Recent models, such as those integrating the Whisper en-\ncoder, freeze certain parts of the model (like the speech encoder) and only fine-tune a linear\nprojector or specific adapters to align the speech and text modalities. This approach simplifies the\ntraining process and enhances efficiency[102].\n\u2022 Multi-Stage Fine-Tuning: Models like AudioPaLM perform multi-stage fine-tuning, starting\nwith a text-based pre-training phase, followed by fine-tuning on a mixture of tasks that include\nboth text and audio data. This staged approach leverages the strengths of pre-trained text models\nwhile adapting them for multimodal tasks.\n11.4.3\nFine-Tuning Whisper for Automatic Speech Recognition (ASR)\nWhisper1 is an advanced Automatic Speech Recognition (ASR) model developed by OpenAI, designed\nto convert spoken language into text. Built upon the powerful Transformer architecture, Whisper excels\nat capturing and transcribing diverse speech patterns across various languages and accents.\nUnlike\ntraditional ASR models that require extensive labelled data, Whisper leverages a vast dataset and self-\nsupervised learning, enabling it to perform robustly in noisy environments and handle a wide range of\nspeech variations. Its versatility and high accuracy make it an ideal choice for applications such as voice\nassistants, transcription services, and multilingual speech recognition systems.\nWhy Fine-Tune Whisper?\nFine-tuning Whisper for specific ASR tasks can significantly enhance its performance in specialised\ndomains. Although Whisper is pre-trained on a large and diverse dataset, it might not fully capture\nthe nuances of specific vocabularies or accents present in niche applications. Fine-tuning allows Whisper\nto adapt to particular", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities", "description": "Arxiv Paper titled 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities' authored by Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid", "url": "https://arxiv.org/pdf/2408.13296v1", "source": "arxiv", "id": "07159f78-e718-44fe-9cc1-93182de14cdf"}, "page_content": " designed\nto convert spoken language into text. Built upon the powerful Transformer architecture, Whisper excels\nat capturing and transcribing diverse speech patterns across various languages and accents.\nUnlike\ntraditional ASR models that require extensive labelled data, Whisper leverages a vast dataset and self-\nsupervised learning, enabling it to perform robustly in noisy environments and handle a wide range of\nspeech variations. Its versatility and high accuracy make it an ideal choice for applications such as voice\nassistants, transcription services, and multilingual speech recognition systems.\nWhy Fine-Tune Whisper?\nFine-tuning Whisper for specific ASR tasks can significantly enhance its performance in specialised\ndomains. Although Whisper is pre-trained on a large and diverse dataset, it might not fully capture\nthe nuances of specific vocabularies or accents present in niche applications. Fine-tuning allows Whisper\nto adapt to particular audio characteristics and terminologies, leading to more accurate and reliable\ntranscriptions. This process is especially beneficial in industries with domain-specific jargon, like medical,\nlegal, or technical fields, where the generic model might struggle with specialised vocabulary.\n1https://openai.com/index/whisper/\n94\nSteps to Fine-Tune Whisper\n\u2022 Data Collection and Preparation: Gather a sizable dataset that matches the target domain or\ntask. Ensure the dataset includes diverse examples with clear transcriptions. Clean and preprocess\nthe audio files and transcripts, ensuring they are in a consistent format and aligned correctly. Tools\nlike FFmpeg2 can help standardise audio formats and sample rates.\n\u2022 Data Augmentation: To improve robustness, augment the dataset with variations such as dif-\nferent noise levels, accents, or speeds. Techniques like adding background noise, altering pitch, or\nchanging the tempo can help the model generalise better to real-world conditions.\n\u2022 Preprocessing: Convert the audio files into a format suitable for Whisper, typically into mel\nspectrograms or another time-frequency representation. This transformation is crucial as Whisper\nrelies on such representations to learn and transcribe speech effectively.\n\u2022 Model Configuration: Initialise the Whisper model with pre-trained weights. Configure the\nmodel to accommodate the target language or domain-specific adjustments. This includes setting\nappropriate hyperparameters, like learning rate and batch size, tailored to the dataset\u2019s size and\ncomplexity.\n\u2022 Training: Fine-tune the Whisper model on the prepared dataset using a framework like PyTorch\nor TensorFlow. Ensure to monitor the model\u2019s performance on a validation set to avoid overfitting.\nTechniques like gradient clipping, learning rate scheduling, and early stopping can help maintain\ntraining stability and efficiency.\n\u2022 Evaluation and Testing: After training, evaluate the model\u2019s performance on a separate test\nset to assess its accuracy and generalisability. Metrics like Word Error Rate (WER) or Character\nError Rate (CER) provide insights into how well the model transcribes audio compared to ground\ntruth transcriptions.\n11.4.4\nCase Studies and Applications\n1. Medical Transcription: Fine-tuning speech LLMs on medical data has led to significant im-\nprovements in transcribing doctor-patient interactions. Models like Whisper have been fine-tuned\non medical terminologies, resulting in more accurate and reliable transcriptions.\n2. Legal Document Processing: Legal firms have employed fine-tuned audio LLMs to transcribe\ncourt proceedings and legal discussions.\nDomain-specific fine-tuning has enhanced the models\u2019\nability to recognise and accurately transcribe legal jargon.\n3. Customer Service Automation: Companies are using fine-tuned speech models to automate\ncustomer service interactions. These models are trained on customer support data to understand\nand respond to queries more effectively, providing a more seamless user experience.\n2https://ffmpeg.org/ffmpeg.html\n95\nChapter 12\nOpen Challenges and Research\nDirections\n12.1\nScalability Issues\nThe fine-tuning of Large Language Models (LLMs) such as GPT-4, PaLM1, and T52 has become a critical\narea of research, presenting several significant challenges and opening up new avenues for exploration,\nparticularly in scaling these processes efficiently. This discussion focuses on the two main aspects: the\nchallenges in scaling fine-tuning processes and potential research directions for scalable solutions.\n12.1.1\nChallenges in Scaling Fine-Tuning Processes\n1. Computational Resources: Large-scale models such as GPT-3 and PaLM require enormous\ncomputational resources for fine-tuning. For instance, fine-tuning a 175-billion parameter model\nlike GPT-3 necessitates high-performance GPUs or TPUs capable of handling vast amounts of data\nand complex operations. The sheer volume of parameters translates to extensive computational\ndemands. Even a relatively smaller model, such as BERT-large with 340 million parameters, can\nbe computationally intensive to fine-tune.\n2. Memory Requirements: The memory footprint for fine-tuning LLMs is staggering. Each pa-\nrameter in the model requires storage, and during training, additional memory is needed to store\nintermediate computations, gradients, and optimiser states. For example, loading a 7 billion pa-\nrameter model (e.g., LLaMA 2) in FP32 (4 bytes per parameter) requires approximately 28 GB\nof GPU memory, while fine-tuning demands around 112", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities", "description": "Arxiv Paper titled 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities' authored by Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid", "url": "https://arxiv.org/pdf/2408.13296v1", "source": "arxiv", "id": "6256b440-f39f-4500-ba2b-9470633772b5"}, "page_content": " PaLM require enormous\ncomputational resources for fine-tuning. For instance, fine-tuning a 175-billion parameter model\nlike GPT-3 necessitates high-performance GPUs or TPUs capable of handling vast amounts of data\nand complex operations. The sheer volume of parameters translates to extensive computational\ndemands. Even a relatively smaller model, such as BERT-large with 340 million parameters, can\nbe computationally intensive to fine-tune.\n2. Memory Requirements: The memory footprint for fine-tuning LLMs is staggering. Each pa-\nrameter in the model requires storage, and during training, additional memory is needed to store\nintermediate computations, gradients, and optimiser states. For example, loading a 7 billion pa-\nrameter model (e.g., LLaMA 2) in FP32 (4 bytes per parameter) requires approximately 28 GB\nof GPU memory, while fine-tuning demands around 112 GB of GPU memory[103]. This memory\ndemand is beyond the capability of most consumer-grade hardware, making fine-tuning accessible\nprimarily to well-funded organisations or research institutions.\n3. Data Volume: LLMs typically require vast amounts of training data to achieve state-of-the-art\nperformance during fine-tuning. This data needs to be loaded, preprocessed, and fed into the model\nat high speeds to maintain efficient training. Managing large datasets can become a bottleneck,\nespecially if the data is stored in a distributed fashion across multiple systems or if it needs to be\nfetched from remote storage.\n4. Throughput and Bottlenecks: High throughput is essential to keep GPUs or TPUs fully\nutilised. However, data pipelines can become bottlenecks if not properly optimised. For exam-\nple, shuffling large datasets or loading them into memory quickly enough to keep up with the\ntraining process can be challenging. Techniques like data packing, where multiple small examples\nare combined into larger batches, help improve throughput but add complexity to data handling\nroutines.[104]\n5. Efficient Use of Resources: The financial and environmental costs of fine-tuning large models\nare significant. Large-scale fine-tuning involves not just the direct cost of computational resources\nbut also the indirect costs associated with energy consumption and infrastructure maintenance.\n1https://ai.google/discover/palm2/\n2https://huggingface.co/docs/transformers/en/model_doc/t5\n96\nTechniques such as mixed-precision training and gradient checkpointing can reduce these costs by\noptimising memory and computational efficiency.\nThe challenges in scaling the fine-tuning processes of LLMs are multifaceted and complex, involving sig-\nnificant computational, memory, and data handling constraints. Innovations in PEFT, data throughput\noptimisation, and resource-efficient training methods are critical for overcoming these challenges. As\nLLMs continue to grow in size and capability, addressing these challenges will be essential for making\nadvanced AI accessible and practical for a wider range of applications.\n12.1.2\nResearch Directions for Scalable Solutions\nAdvanced PEFT Techniques and Sparse Fine-Tuning\nRecent advancements in PEFT techniques, like LoRA and its variant, Quantised LoRA, are revolu-\ntionising the scalability of LLMs. LoRA reduces the computational burden by updating only a low-rank\napproximation of the parameters, significantly lowering memory and processing requirements. Quantised\nLoRA further optimises resource usage by applying quantisation to these low-rank matrices, maintaining\nhigh model performance while minimising the need for extensive hardware. This has enabled efficient\nfine-tuning of massive models, such as in Meta\u2019s LLaMA project, where adapting a smaller set of influ-\nential parameters allowed the models to perform robustly across various tasks with less computational\nstrain.\nSparse fine-tuning techniques, such as SpIEL [105] complement these efforts by selectively updating\nonly the most impactful parameters. SpIEL fine-tunes models by only changing a small portion of the\nparameters, which it tracks with an index. The process includes updating the parameters, removing the\nleast important ones, and adding new ones based on their gradients or estimated momentum using an\nefficient optimiser.\nData Efficient Fine-Tuning (DEFT)\nTo address the scalability challenges, recently the concept of DEFT has emerged. This novel approach\nintroduces data pruning as a mechanism to optimise the fine-tuning process by focusing on the most\ncritical data samples.\nDEFT aims to enhance the efficiency and effectiveness of fine-tuning LLMs by selectively pruning the\ntraining data to identify the most influential and representative samples. This method leverages few-shot\nlearning principles, enabling LLMs to adapt to new data with minimal samples while maintaining or even\nexceeding performance levels achieved with full datasets [106].\nKey Components of DEFT\nHigh Accuracy Through Influence Score: DEFT introduces the concept of an influence score to\nevaluate and rank the importance of each data sample in the context of LLM fine-tuning. The influence\nscore estimates how removing a specific sample would impact the overall performance of the model. This\napproach allows for the selection of a small subset of data that is highly representative and influential,\nthereby enabling the model to maintain high accuracy with significantly fewer samples.\nHigh Efficiency Through Effort Score and Surrogate Models: To address the cost and complexity\nof evaluating large datasets, DEFT employs a surrogate model\u2014a smaller, computationally less intensive\nmodel\u2014to approximate the influence scores. This", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities", "description": "Arxiv Paper titled 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities' authored by Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid", "url": "https://arxiv.org/pdf/2408.13296v1", "source": "arxiv", "id": "010958e2-5ab4-47e6-949e-d85ceaa1b389"}, "page_content": "\ntraining data to identify the most influential and representative samples. This method leverages few-shot\nlearning principles, enabling LLMs to adapt to new data with minimal samples while maintaining or even\nexceeding performance levels achieved with full datasets [106].\nKey Components of DEFT\nHigh Accuracy Through Influence Score: DEFT introduces the concept of an influence score to\nevaluate and rank the importance of each data sample in the context of LLM fine-tuning. The influence\nscore estimates how removing a specific sample would impact the overall performance of the model. This\napproach allows for the selection of a small subset of data that is highly representative and influential,\nthereby enabling the model to maintain high accuracy with significantly fewer samples.\nHigh Efficiency Through Effort Score and Surrogate Models: To address the cost and complexity\nof evaluating large datasets, DEFT employs a surrogate model\u2014a smaller, computationally less intensive\nmodel\u2014to approximate the influence scores. This surrogate model helps estimate the impact of each\nsample without the heavy computational burden associated with directly using the LLM. Additionally,\nDEFT introduces an effort score to identify and prioritise more challenging samples that may require\nspecial attention from the LLM. This dual-score system ensures that the fine-tuning process remains\nboth efficient and effective.\nPractical Implications and Use Cases\n\u2022 Few-Shot Fine-Tuning for Rapid Adaptation: DEFT is particularly beneficial for applica-\ntions where models need to quickly adapt to new data with minimal samples. In scenarios such as\n97\npersonalised recommendations or adapting to sudden changes in user behaviour, DEFT allows for\nrapid fine-tuning, maintaining high performance with a fraction of the data typically required.\n\u2022 Reducing Computational Costs in Large-Scale Deployments: By focusing on the most\ninfluential data samples and using surrogate models, DEFT significantly reduces the computational\nresources needed for fine-tuning. This makes it feasible to maintain high-performing LLMs even in\nlarge-scale deployments where data volumes are substantial.\nFuture Directions\nThe DEFT introduces a data pruning task for fine-tuning large language models (LLMs), setting the\nstage for new research into efficient LLM-based recommendation systems and presenting numerous op-\nportunities for future exploration. Key areas for further investigation include:\n\u2022 Applying the proposed DEALRec[107] approach to a broader range of LLM-based recommender\nmodels across diverse cross-domain datasets, thereby enhancing fine-tuning performance within\nresource constraints.\n\u2022 Addressing the limited context window of LLMs by selectively focusing on the most informative\nitems in user interaction sequences for fine-tuning purposes.\n12.1.3\nHardware and Algorithm Co-Design\nCo-designing hardware and algorithms tailored for LLMs can lead to significant improvements in the\nefficiency of fine-tuning processes. Custom hardware accelerators optimised for specific tasks or types of\ncomputation can drastically reduce the energy and time required for model training and fine-tuning.\n\u2022 Custom Accelerators: Developing hardware accelerators specifically for the sparse and low-\nprecision computations often used in LLM fine-tuning can enhance performance. These accelerators\nare designed to efficiently handle the unique requirements of LLMs, such as the high memory\nbandwidth and extensive matrix multiplications involved in transformer architectures.\n\u2022 Algorithmic Optimisation: Combining hardware innovations with algorithmic optimisation\ntechniques, such as those that minimise data movement or leverage hardware-specific features\n(e.g., tensor cores for mixed-precision calculations), can further enhance the efficiency of fine-tuning\nprocesses.\n\u2022 Example: NVIDIA\u2019s TensorRT3 is an example of hardware and algorithm co-design in action.\nIt optimises deep learning models for inference by leveraging NVIDIA GPUs\u2019 capabilities, signifi-\ncantly speeding up the process while reducing the resource requirements. TensorRT\u2019s optimisations\ninclude support for mixed-precision and sparse tensor operations, making it highly suitable for fine-\ntuning large models.\nAs the scale of language models continues to grow, addressing the challenges of fine-tuning them efficiently\nbecomes increasingly critical. Innovations in PEFT, sparse fine-tuning, data handling, and the integration\nof advanced hardware and algorithmic solutions present promising directions for future research. These\nscalable solutions are essential not only to make the deployment of LLMs feasible for a broader range of\napplications but also to push the boundaries of what these models can achieve.\n12.2\nEthical Considerations in Fine-Tuning LLMs\n12.2.1\nBias and Fairness\nWhen fine-tuning LLMs, the goal is often to optimise their performance for specific tasks or datasets.\nHowever, these datasets may inherently carry biases that get transferred to the model during the fine-\ntuning process. Biases can arise from various sources, including historical data, imbalanced training\nsamples, and cultural prejudices embedded in language. For instance, an LLM fine-tuned on a dataset\nprimarily sourced from English-speaking countries might underperform or make biased predictions when\n3https://docs.nvidia.com/tensorrt/index.html\n98\napplied to text from other linguistic or cultural backgrounds. Google AI\u2019s Fairness Indicators tool4 is a\npractical solution that allows developers to evaluate the fairness of their models by analysing performance\nmetrics across different demographic groups. This tool can be integrated into the fine-tuning pipeline to\nmonitor and address bias in real-time.\nAddressing Bias and Fairness\n\u2022 Diverse and", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities", "description": "Arxiv Paper titled 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities' authored by Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid", "url": "https://arxiv.org/pdf/2408.13296v1", "source": "arxiv", "id": "01fe320d-f6e9-4318-9a6d-b5d89d854e5a"}, "page_content": ", the goal is often to optimise their performance for specific tasks or datasets.\nHowever, these datasets may inherently carry biases that get transferred to the model during the fine-\ntuning process. Biases can arise from various sources, including historical data, imbalanced training\nsamples, and cultural prejudices embedded in language. For instance, an LLM fine-tuned on a dataset\nprimarily sourced from English-speaking countries might underperform or make biased predictions when\n3https://docs.nvidia.com/tensorrt/index.html\n98\napplied to text from other linguistic or cultural backgrounds. Google AI\u2019s Fairness Indicators tool4 is a\npractical solution that allows developers to evaluate the fairness of their models by analysing performance\nmetrics across different demographic groups. This tool can be integrated into the fine-tuning pipeline to\nmonitor and address bias in real-time.\nAddressing Bias and Fairness\n\u2022 Diverse and Representative Data: Ensuring that fine-tuning datasets are diverse and repre-\nsentative of all user demographics can help mitigate bias.\n\u2022 Fairness Constraints: Incorporating fairness constraints, as suggested by the FairBERTa frame-\nwork5, ensures that fine-tuned models maintain equitable performance across different groups.\n\u2022 Example Application: In healthcare, an LLM fine-tuned to assist in diagnosing conditions might\ninitially be trained on data from predominantly white patients. Such a model could produce less\naccurate diagnoses for patients from other racial backgrounds. By using fairness-aware fine-tuning\ntechniques, healthcare providers can develop models that perform more equitably across diverse\npatient populations.\n12.2.2\nPrivacy Concerns\nFine-tuning often involves using sensitive or proprietary datasets, which poses significant privacy risks. If\nnot properly managed, fine-tuned models can inadvertently leak private information from their training\ndata. This issue is especially critical in domains like healthcare or finance, where data confidentiality is\nparamount.\nEnsuring Privacy During Fine-Tuning\n\u2022 Differential Privacy6: Implementing differential privacy techniques during fine-tuning can pre-\nvent models from leaking sensitive information.\n\u2022 Federated Learning7: Utilising federated learning frameworks allows models to be fine-tuned\nacross decentralised data sources, which enhances privacy by keeping data localised.\n\u2022 Example Application: In customer service applications, companies might fine-tune LLMs using\ncustomer interaction data. Employing differential privacy ensures that the model learns from these\ninteractions without memorising and potentially leaking personal information, thus maintaining\ncustomer confidentiality.\n12.2.3\nSecurity Risks\n\u2022 Security Vulnerabilities in Fine-Tuned Models: Fine-tuned LLMs are susceptible to secu-\nrity vulnerabilities, particularly from adversarial attacks. These attacks involve inputs designed to\nexploit model weaknesses, causing them to produce erroneous or harmful outputs. Such vulnera-\nbilities can be more pronounced in fine-tuned models due to their specialised training data, which\nmay not cover all possible input scenarios.\n\u2022 Recent Research and Industry Practices: Microsoft\u2019s Adversarial ML Threat Matrix pro-\nvides a comprehensive framework for identifying and mitigating adversarial threats during model\ndevelopment and fine-tuning. This matrix helps developers understand the potential attack vectors\nand implement defensive strategies accordingly.\n\u2022 Enhancing Security in Fine-Tuning:\n\u2013 Adversarial Training: Exposing models to adversarial examples during fine-tuning can\nenhance their robustness against attacks.\n\u2013 Security Audits: Regularly conducting security audits on fine-tuned models can help iden-\ntify and address potential vulnerabilities.\n4https://research.google/blog/fairness-indicators-scalable-infrastructure-for-fair-ml-systems/\n5https://huggingface.co/facebook/FairBERTa\n6https://privacytools.seas.harvard.edu/differential-privacy\n7https://research.ibm.com/blog/what-is-federated-learning\n99\n12.3\nAccountability and Transparency\n12.3.1\nThe Need for Accountability and Transparency\nFine-tuning can significantly alter an LLM\u2019s behaviour, making it crucial to document and understand\nthe changes and their impacts.\nThis transparency is essential for stakeholders to trust the model\u2019s\noutputs and for developers to be accountable for its performance and ethical implications.\n12.3.2\nRecent Research and Industry Practices\nMeta\u2019s Responsible AI framework8 underscores the importance of documenting the fine-tuning process\nand its effects on model behaviour. This includes maintaining detailed records of the data used, the\nchanges made during fine-tuning, and the evaluation metrics applied.\n12.3.3\nPromoting Accountability and Transparency\n\u2022 Comprehensive Documentation: Creating detailed documentation of the fine-tuning process\nand its impact on model performance and behaviour.\n\u2022 Transparent Reporting: Utilising frameworks like Model Cards9 to report on the ethical and\noperational characteristics of fine-tuned models.\n\u2022 Example Application: In content moderation systems, LLMs fine-tuned to identify and filter\nharmful content need clear documentation and reporting. This ensures that platform users and\nregulators understand how the model operates and can trust its moderation decisions.\n12.3.4\nProposed frameworks/techniques for Ethical Fine-Tuning\nFrameworks for Mitigating Bias\nBias-aware fine-tuning frameworks aim to incorporate fairness into the model training process. Fair-\nBERTa, introduced by Facebook,", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities", "description": "Arxiv Paper titled 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities' authored by Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid", "url": "https://arxiv.org/pdf/2408.13296v1", "source": "arxiv", "id": "52b068c0-c874-4485-bcf1-8bd9141315b7"}, "page_content": " of the data used, the\nchanges made during fine-tuning, and the evaluation metrics applied.\n12.3.3\nPromoting Accountability and Transparency\n\u2022 Comprehensive Documentation: Creating detailed documentation of the fine-tuning process\nand its impact on model performance and behaviour.\n\u2022 Transparent Reporting: Utilising frameworks like Model Cards9 to report on the ethical and\noperational characteristics of fine-tuned models.\n\u2022 Example Application: In content moderation systems, LLMs fine-tuned to identify and filter\nharmful content need clear documentation and reporting. This ensures that platform users and\nregulators understand how the model operates and can trust its moderation decisions.\n12.3.4\nProposed frameworks/techniques for Ethical Fine-Tuning\nFrameworks for Mitigating Bias\nBias-aware fine-tuning frameworks aim to incorporate fairness into the model training process. Fair-\nBERTa, introduced by Facebook, is an example of such a framework that integrates fairness constraints\ndirectly into the model\u2019s objective function during fine-tuning. This approach ensures that the model\u2019s\nperformance is balanced across different demographic groups.\nOrganisations can adopt fairness-aware frameworks to develop more equitable AI systems. For instance,\nsocial media platforms can use these frameworks to fine-tune models that detect and mitigate hate speech\nwhile ensuring fair treatment across various user demographics.\nTechniques for Privacy Preservation\nDifferential privacy and federated learning are key techniques for preserving privacy during fine-tuning.\nTensorFlow Privacy10, developed by Google, provides built-in support for differential privacy, allowing\ndevelopers to fine-tune models securely without compromising data confidentiality.\nLLMs are highly effective but face challenges when applied in sensitive areas where data privacy is cru-\ncial. To address this, researchers focus on enhancing Small Language Models (SLMs) tailored to specific\ndomains. Existing methods often use LLMs to generate additional data or transfer knowledge to SLMs,\nbut these approaches struggle due to differences between LLM-generated data and private client data. In\nresponse, a new Federated Domain-specific Knowledge Transfer (FDKT)[108] framework is introduced.\nFDKT leverages LLMs to create synthetic samples that mimic clients\u2019 private data distribution using\ndifferential privacy. This approach significantly boosts SLMs\u2019 performance by approximately 5% while\nmaintaining data privacy with a minimal privacy budget, outperforming traditional methods relying\nsolely on local private data.\nIn healthcare, federated fine-tuning can allow hospitals to collaboratively train models on patient data\nwithout transferring sensitive information. This approach ensures data privacy while enabling the de-\nvelopment of robust, generalisable AI systems.\n8https://ai.meta.com/responsible-ai/\n9https://huggingface.co/docs/hub/en/model-cards\n10https://www.tensorflow.org/responsible_ai/privacy/guide\n100\nFrameworks for Enhancing Security\nAdversarial training and robust security measures[109] are essential for protecting fine-tuned models\nagainst attacks. The adversarial training approach involves training models with adversarial examples\nto improve their resilience against malicious inputs. Microsoft Azure\u2019s adversarial training tools provide\npractical solutions for integrating these techniques into the fine-tuning process, helping developers create\nmore secure and reliable models.\nIn cybersecurity, fine-tuned LLMs used for threat detection can benefit from adversarial training to\nenhance their ability to identify and respond to sophisticated attacks, thereby improving organisational\nsecurity.\nFrameworks for Ensuring Transparency\nTransparency and accountability frameworks, such as Model Cards and AI FactSheets11, provide struc-\ntured ways to document and report on the fine-tuning process and the resulting model behaviours. These\nframeworks promote understanding and trust among stakeholders by clearly outlining the model\u2019s capa-\nbilities, limitations, and ethical considerations.\nIn government applications, where AI systems might be used for decision-making or public services,\nmaintaining transparent documentation through frameworks like AI FactSheets ensures that these sys-\ntems are accountable and their decisions can be audited and trusted by the public.\nFine-tuning LLMs introduces several ethical challenges, including bias, privacy risks, security vulnera-\nbilities, and accountability concerns. Addressing these requires a multifaceted approach that integrates\nfairness-aware frameworks, privacy-preserving techniques, robust security measures, and transparency\nand accountability mechanisms.\nBy leveraging recent advancements in these areas, researchers and\npractitioners can develop and deploy LLMs that are not only powerful but also ethically sound and\ntrustworthy.\n12.4\nIntegration with Emerging Technologies\nIntegrating LLMs with emerging technologies such as IoT (Internet of Things) and edge computing\npresents numerous opportunities and challenges, reflecting advancements and insights from recent re-\nsearch and industry developments.\n12.4.1\nOpportunities\n\u2022 Enhanced Decision-Making and Automation: LLMs have the capability to analyse and derive\ninsights from vast amounts of unstructured data generated by IoT devices. This data can range\nfrom sensor readings in manufacturing plants to environmental data in smart cities. By processing\nthis data in real-time, LLMs can optimise decision-making processes and automate tasks that\ntraditionally required human intervention. For example:\n\u2013 Industrial Applications: Predictive maintenance can be enhanced by LLMs analysing sen-\nsor data to predict equipment failures before they occur, thereby reducing downtime and\nmaintenance costs.\n\u2013 Smart Cities: LLMs can analyse traffic patterns and environmental data from IoT sensors\nto optim", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities", "description": "Arxiv Paper titled 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities' authored by Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid", "url": "https://arxiv.org/pdf/2408.13296v1", "source": "arxiv", "id": "94653761-6c91-4dae-9990-578e5d4135cc"}, "page_content": " Technologies\nIntegrating LLMs with emerging technologies such as IoT (Internet of Things) and edge computing\npresents numerous opportunities and challenges, reflecting advancements and insights from recent re-\nsearch and industry developments.\n12.4.1\nOpportunities\n\u2022 Enhanced Decision-Making and Automation: LLMs have the capability to analyse and derive\ninsights from vast amounts of unstructured data generated by IoT devices. This data can range\nfrom sensor readings in manufacturing plants to environmental data in smart cities. By processing\nthis data in real-time, LLMs can optimise decision-making processes and automate tasks that\ntraditionally required human intervention. For example:\n\u2013 Industrial Applications: Predictive maintenance can be enhanced by LLMs analysing sen-\nsor data to predict equipment failures before they occur, thereby reducing downtime and\nmaintenance costs.\n\u2013 Smart Cities: LLMs can analyse traffic patterns and environmental data from IoT sensors\nto optimise city infrastructure and improve urban planning decisions.\n\u2022 Personalised User Experiences: Integration with edge computing allows LLMs to process\ndata locally on devices rather than relying solely on cloud-based servers. This enables LLMs to\ndeliver highly personalised services based on real-time data and user preferences, enhancing user\nexperiences across various domains:\n\u2013 Healthcare: LLMs can provide personalised healthcare recommendations by analysing data\nfrom wearable devices and integrating it with medical records securely stored on edge devices.\n11https://aifs360.res.ibm.com/\n101\n\u2022 Improved Natural Language Understanding: IoT data integration enriches LLMs\u2019 ability to\nunderstand context and respond more intelligently to natural language queries. This can signifi-\ncantly improve user interactions with smart environments:\n\u2013 Smart Homes: LLMs integrated with IoT devices can understand and respond to voice\ncommands more accurately, adjusting smart home settings based on real-time sensor data\n(e.g., adjusting lighting and temperature based on occupancy and environmental conditions).\n12.4.2\nChallenges\n\u2022 Data Complexity and Integration: Integrating data from diverse IoT devices poses challenges\nrelated to data quality, interoperability, and scalability.\nLLMs need to effectively process and\ninterpret this heterogeneous data to derive meaningful insights:\n\u2013 Data Integration: Ensuring seamless integration of data streams from different IoT plat-\nforms and devices without compromising data integrity or performance.\n\u2013 Data Preprocessing: Cleaning and preprocessing IoT data to ensure consistency and reli-\nability before feeding it into LLMs for analysis.\n\u2022 Privacy and Security: Edge computing involves processing sensitive data locally on devices,\nraising concerns about data privacy and security:\n\u2013 Data Privacy: Implementing robust encryption techniques and access control mechanisms\nto protect sensitive data processed by LLMs on edge devices.\n\u2013 Secure Communication: Ensuring secure communication channels between IoT devices\nand LLMs to prevent data breaches or unauthorised access.\n\u2022 Real-Time Processing and Reliability: LLMs deployed in edge computing environments must\noperate with low latency and high reliability to support real-time applications:\n\u2013 Latency: Optimising algorithms and processing capabilities of LLMs to handle real-time\ndata streams efficiently without delays.\n\u2013 Reliability: Ensuring the accuracy and consistency of insights generated by LLMs in dynamic\nand unpredictable IoT environments.\n12.5\nFuture Research Areas\n\u2022 Federated Learning and Edge Computing: Exploring federated learning techniques where\nLLMs can be trained collaboratively across edge devices without centralised data aggregation.\nThis approach addresses privacy concerns and reduces communication overhead.\n\u2022 Real-Time Decision Support Systems: Developing LLM-based systems capable of real-time\ndecision-making by integrating with edge computing infrastructure. This includes optimising algo-\nrithms for low-latency processing and ensuring reliability under dynamic environmental conditions.\n\u2022 Ethical and Regulatory Implications: Investigating the ethical implications of integrating\nLLMs with IoT and edge computing, particularly regarding data ownership, transparency, and\nfairness. This area requires frameworks for ethical AI deployment and governance.\n102\nGlossary\nLLM Large Language Model \u2013 A type of AI model, typically with billions of parameters, trained on vast\namounts of text data to understand and generate human-like text. They are primarily designed\nfor tasks in natural language processing (NLP).\nNLP Natural Language Processing \u2013 A field of artificial intelligence that focuses on the interaction\nbetween computers and humans through natural language, including tasks like language generation,\ntranslation, and sentiment analysis.\nLoRA Low-Rank Adaptation \u2013 A parameter-efficient fine-tuning technique that adjusts only small low-\nrank matrices to adapt pre-trained models to specific tasks, thus preserving most of the original\nmodel\u2019s parameters.\nDoRA Weight-Decomposed Low-Rank Adaptation \u2013 A technique that decomposes model weights into\nmagnitude and direction components, facilitating fine-tuning while maintaining inference efficiency.\nQLoRA Quantised Low-Rank Adaptation \u2013 A variation of LoRA, specifically designed for quantised\nmodels, allowing for efficient fine-tuning in resource-constrained environments.\nPPO Proximal Policy Optimisation \u2013 A reinforcement learning algorithm that adjusts policies by bal-\nancing the exploration of new actions and exploitation of known rewards, designed for stability and\nefficiency in training.\nDPO Direct Preference Optimisation \u2013 A method that directly aligns language models with human\npreferences through preference optimisation, bypassing reinforcement learning models like PPO.\nMoE Mixture of Experts \u2013 A model architecture that employs multiple specialised subnetworks, called\nexperts, which are selectively activated based on the input to improve model", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities", "description": "Arxiv Paper titled 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities' authored by Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid", "url": "https://arxiv.org/pdf/2408.13296v1", "source": "arxiv", "id": "6fa7f4d3-cb6a-436f-ac4a-4d1cc6069e9d"}, "page_content": "\nmodel\u2019s parameters.\nDoRA Weight-Decomposed Low-Rank Adaptation \u2013 A technique that decomposes model weights into\nmagnitude and direction components, facilitating fine-tuning while maintaining inference efficiency.\nQLoRA Quantised Low-Rank Adaptation \u2013 A variation of LoRA, specifically designed for quantised\nmodels, allowing for efficient fine-tuning in resource-constrained environments.\nPPO Proximal Policy Optimisation \u2013 A reinforcement learning algorithm that adjusts policies by bal-\nancing the exploration of new actions and exploitation of known rewards, designed for stability and\nefficiency in training.\nDPO Direct Preference Optimisation \u2013 A method that directly aligns language models with human\npreferences through preference optimisation, bypassing reinforcement learning models like PPO.\nMoE Mixture of Experts \u2013 A model architecture that employs multiple specialised subnetworks, called\nexperts, which are selectively activated based on the input to improve model performance and\nefficiency.\nMoA Mixture of Agents \u2013 A multi-agent framework where several agents collaborate during training\nand inference, leveraging the strengths of each agent to improve overall model performance.\nPEFT Parameter-Efficient Fine-Tuning \u2013 A fine-tuning approach for large models that involves adjust-\ning only a subset of model parameters, improving efficiency in scenarios with limited computational\nresources. This includes techniques like LoRA, QLoRA, and adapters.\nAdapters Small, trainable modules introduced into the layers of pre-trained language models, allowing\nefficient task-specific fine-tuning without modifying the core parameters of the original model.\nTechniques such as **AdapterFusion** and **AdapterSoup** fall under this category, facilitating\nthe combination of multiple adapters for complex multitasking.\nSoft Prompt Tuning (SPT) A fine-tuning technique where a set of trainable prompt tokens are added\nto the input sequence to guide a pre-trained model towards task-specific performance without\nmodifying internal model weights.\nPrefix-Tuning A variation of soft prompt tuning where a fixed sequence of trainable vectors is prepended\nto the input layer at every layer of the model, enhancing task-specific adaptation.\nQuantisation The process of reducing the precision of model weights and activations, often from 32-bit\nto lower-bit representations like 8-bit or 4-bit, to reduce memory usage and improve computational\nefficiency.\n103\nQuantised LLMs Large Language Models that have undergone quantisation, a process that reduces\nthe precision of model weights and activations, often from 32-bit to 8-bit or lower, to enhance\nmemory and computational efficiency.\nPruning A model optimisation technique that reduces the complexity of large language models by\nremoving less significant parameters, enabling faster inference and lower memory usage.\nHalf Fine-Tuning (HFT) A fine-tuning method where half of the model\u2019s parameters are kept frozen\nwhile the other half are updated, helping to maintain pre-trained knowledge while adapting the\nmodel to new tasks.\nStructured Masking A technique that masks entire layers, heads, or other structural components of\na model to reduce complexity while fine-tuning for specific tasks.\nUnstructured Masking A technique where certain parameters of the model are masked out randomly\nor based on a pattern during fine-tuning, allowing for the identification of the most important\nmodel weights.\nGLUE General Language Understanding Evaluation \u2013 A benchmark used to evaluate the performance\nof NLP models across a variety of language understanding tasks, such as sentiment analysis and\nnatural language inference.\nSuperGLUE Super General Language Understanding Evaluation \u2013 A more challenging extension of\nGLUE, consisting of harder tasks designed to test the robustness and adaptability of NLP models.\nTruthfulQA A benchmark designed to measure the truthfulness of a language model\u2019s output, focusing\non factual accuracy and resistance to hallucination.\nIFEval Instruction Following Evaluation \u2013 A benchmark that assesses a model\u2019s ability to follow explicit\ninstructions across tasks, usually in the context of fine-tuning large models for adherence to specific\ninstructions.\nBBH Big Bench Hard \u2013 A subset of the Big Bench dataset, which consists of particularly difficult tasks\naimed at evaluating the advanced reasoning abilities of large language models.\nMATH A dataset created to evaluate a model\u2019s ability to solve high-school level mathematical problems,\npresented in formal formats like LaTeX.\nGPQA General-Purpose Question Answering \u2013 A challenging dataset that features knowledge-based\nquestions crafted by experts to assess deep reasoning and factual recall.\nMuSR Multimodal Structured Reasoning \u2013 A dataset that involves complex problems requiring lan-\nguage models to integrate reasoning across modalities, often combining text with other forms of\ndata such as images or graphs.\nMMLU Massive Multitask Language Understanding \u2013 A benchmark that evaluates a language model\u2019s\nability to perform various tasks across diverse domains, such as humanities, STEM, social sciences,\nand others, typically requiring high-level reasoning.\nMMLU-PRO A refined version of the MMLU dataset with a focus on more challenging, multi-choice\nproblems, typically requiring the model to parse long-range context.\nARC AI2 Reasoning Challenge \u2013 A benchmark for evaluating a language model\u2019s reasoning capabilities\nusing a dataset of multiple-choice science questions.\nCOQA Conversational Question Answering \u2013 A benchmark that evaluates how well a language model\ncan understand and engage in back-and-forth conversation, especially in a question-answer format.\nDROP Discrete Reasoning Over Paragraphs \u2013 A benchmark that tests a model\u2019s ability", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities", "description": "Arxiv Paper titled 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities' authored by Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid", "url": "https://arxiv.org/pdf/2408.13296v1", "source": "arxiv", "id": "a947288f-5239-4d2b-a70a-36a636b63977"}, "page_content": ", often combining text with other forms of\ndata such as images or graphs.\nMMLU Massive Multitask Language Understanding \u2013 A benchmark that evaluates a language model\u2019s\nability to perform various tasks across diverse domains, such as humanities, STEM, social sciences,\nand others, typically requiring high-level reasoning.\nMMLU-PRO A refined version of the MMLU dataset with a focus on more challenging, multi-choice\nproblems, typically requiring the model to parse long-range context.\nARC AI2 Reasoning Challenge \u2013 A benchmark for evaluating a language model\u2019s reasoning capabilities\nusing a dataset of multiple-choice science questions.\nCOQA Conversational Question Answering \u2013 A benchmark that evaluates how well a language model\ncan understand and engage in back-and-forth conversation, especially in a question-answer format.\nDROP Discrete Reasoning Over Paragraphs \u2013 A benchmark that tests a model\u2019s ability to perform\ndiscrete reasoning over text, especially in scenarios requiring arithmetic, comparison, or logical\nreasoning.\nSQuAD Stanford Question Answering Dataset \u2013 A popular dataset for evaluating a model\u2019s ability to\nunderstand and answer questions based on passages of text.\n104\nTREC Text REtrieval Conference \u2013 A benchmark that evaluates models on various text retrieval tasks,\noften focusing on information retrieval and document search.\nWMT Workshop on Machine Translation \u2013 A dataset and benchmark for evaluating the performance\nof machine translation systems across different language pairs.\nXNLI Cross-lingual Natural Language Inference \u2013 A dataset designed to evaluate a model\u2019s ability to\nunderstand and infer meaning across multiple languages.\nPiQA Physical Interaction Question Answering \u2013 A dataset that measures a model\u2019s understanding of\nphysical interactions and everyday tasks.\nWinogrande A large-scale dataset aimed at evaluating a language model\u2019s ability to handle common-\nsense reasoning, typically through tasks that involve resolving ambiguous pronouns in sentences.\nRLHF Reinforcement Learning from Human Feedback \u2013 A method where language models are fine-\ntuned based on human-provided feedback, often used to guide models towards preferred behaviours\nor outputs.\nRAFT Retrieval-Augmented Fine-Tuning \u2013 A method combining retrieval techniques with fine-tuning\nto enhance the performance of language models by allowing them to access external information\nduring training or inference.\n105[SEP]", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "A Survey on LoRA of Large Language Models", "description": "Arxiv Paper titled 'A Survey on LoRA of Large Language Models' authored by Yuren Mao, Yuhang Ge, Yijiang Fan, Wenyi Xu, Yu Mi, Zhonghao Hu, Yunjun Gao", "url": "https://arxiv.org/pdf/2407.11046", "source": "arxiv", "id": "b6992480-62b7-4813-b2cd-e6552a030451"}, "page_content": "[CLS]Low-Rank Adaptation (LoRA), which\nupdates the dense neural network layers with plug-\ngable low-rank matrices, is one of the best per-\nformed parameter efficient fine-tuning paradigms.\nFurthermore, it has significant advantages in cross-\ntask generalization and privacy-preserving. Hence,\nLoRA has gained much attention recently, and the\nnumber of related literature demonstrates exponen-\ntial growth. It is necessary to conduct a compre-\nhensive overview of the current progress on LoRA.\nThis survey categorizes and reviews the progress\nfrom the perspectives of (1) downstream adaptation\nimproving variants that improve LoRA\u2019s perfor-\nmance on downstream tasks; (2) cross-task gener-\nalization methods that mix multiple LoRA plugins\nto achieve cross-task generalization; (3) efficiency-\nimproving methods that boost the computation-\nefficiency of LoRA; (4) data privacy-preserving\nmethods that use LoRA in federated learning; (5)\napplication. Besides, this survey also discusses the\nfuture directions in this field. At last, we provide a\nGithub page 1 for readers to check the updates and\ninitiate discussions on this survey paper.\n1https://github.com/ZJU-LLMs/Awesome-LoRAs.git\nKeywords\nLow-Rank Adaptation, LoRA, Large\nLanguage Models, LLMs\n1\nIntroduction\nRapidly increasing parameter scales of pre-training\nlanguage models improves their generalization\nability and brings emergent abilities. In the last\nfew years, the parameter scales of pre-training lan-\nguages models have increased by thousands of\ntimes (e.g., from 330M parameter BERT [1] to\n540B parameter PaLM [2]).\nThese pre-training\nlanguage models having large parameter scales are\ntermed Large language models (LLMs).\nNever-\ntheless, due to the knowledge boundaries of the\nLLMs, their abilities on some downstream tasks\nare still limited. To expand the knowledge bound-\naries, it remains necessary to fine-tune LLMs on\nthe downstream tasks.\nHowever, fine-tuning the full parameters of an\nLLM, namely full fine-tuning, is extremely compu-\ntationally expensive, for example, full fine-tuning\nReceived month dd, yyyy; accepted month dd, yyyy\nE-mail: gaoyj@zju.edu.cn\narXiv:2407.11046v4  [cs.LG]  24 Oct 2024\n2\nFront. Comput. Sci., 2024, 0(0): 1\u201330\nof a LLaMA2-7B\n[3] model requires approxi-\nmately 60GB of memory, which exceeds the ca-\npacity of common consumer GPUs [4]. To reduce\nthe computational cost, various parameter-efficient\nfine-tuning (PEFT) methods have been proposed\n[5].\nThey adapt LLMs to downstream tasks by\nonly fine-tuning a small number of (extra) model\nparameters. From the perspective of whether ex-\ntra parameters are involved, PEFT methods can\nbe divided into two categories: extra-parameter\nmethods and intra-parameter methods. The extra-\nparameter methods freeze all of the original param-\neters of an LLM and insert a set of learnable param-\neters to optimize the model input or model layers\nsuch as adapter tuning [6] and prompt tuning [7].\nBy contrast, intra-parameter methods freeze most\nof the original parameters of an LLM and only tune\na small number of parameters of the LLM such as\nBitFit [8], LISA [4] and LoRA [9].\nWhen we do not have access to modify the model\narchitecture, intra-parameter methods are desir-\nable. Among the intra-parameter methods, LoRA\nis the most widely used one, because it can achieve\na comparable or better downstream adaptation per-\nformance to the full fine-tuning on a range of\ndownstream tasks [9] and is easy to implement.\nBesides, there are many variants have been pro-\nposed to further improve the downstream adapta-\ntion ability of LoRA on more challenging down-\nstream tasks.\nLoRA achieves parameter efficiency by updat-\ning the dense neural network layers of an LLM\nwith pluggable low-rank matrices. These matrices\n(a.k.a, LoRA plugins) are independent of the LLM,\nwhich can be stored and reused in other related\ndownstream tasks. Furthermore, these LoRA plu-\ngins can be combined to achieve cross-task gener-\nalization, which can facilitate multi-task learning,\ndomain adaptation, and continual learning.\nAs the LoRA modules accumulate, the computa-\ntion cost of managing LoRA modules is increas-\ning. Although LoRA is computation-efficient, the\ncomputational cost of managing a larger number\nof LoRA modules is unignorable. It is necessary\nto further improve the computation efficiency of\nLoRA. The improvement can come from reducing\nthe computation cost of single LoRA modules and\naccelerating the scalable serving of multiple mod-\nules. It can boost the application of LoRA in real-\nworld use cases, such as Generative-as-a-Service\n(GaaS) cloud products.\nIn some cases, the training data are privately owned\nby multiple clients and cannot be centralized. To\nadapt LLMs with the distributed training", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "A Survey on LoRA of Large Language Models", "description": "Arxiv Paper titled 'A Survey on LoRA of Large Language Models' authored by Yuren Mao, Yuhang Ge, Yijiang Fan, Wenyi Xu, Yu Mi, Zhonghao Hu, Yunjun Gao", "url": "https://arxiv.org/pdf/2407.11046", "source": "arxiv", "id": "1e964e3b-9738-4009-a2be-d6f508217ba7"}, "page_content": "RA plu-\ngins can be combined to achieve cross-task gener-\nalization, which can facilitate multi-task learning,\ndomain adaptation, and continual learning.\nAs the LoRA modules accumulate, the computa-\ntion cost of managing LoRA modules is increas-\ning. Although LoRA is computation-efficient, the\ncomputational cost of managing a larger number\nof LoRA modules is unignorable. It is necessary\nto further improve the computation efficiency of\nLoRA. The improvement can come from reducing\nthe computation cost of single LoRA modules and\naccelerating the scalable serving of multiple mod-\nules. It can boost the application of LoRA in real-\nworld use cases, such as Generative-as-a-Service\n(GaaS) cloud products.\nIn some cases, the training data are privately owned\nby multiple clients and cannot be centralized. To\nadapt LLMs with the distributed training data, we\ncan adopt federated learning to protect the data\nprivacy of each client. However, federated learn-\ning suffers expensive communication and compu-\ntation costs. To reduce costs, LoRA is a natural\nchoice. Its parameter-efficient nature helps to re-\nduce the computation cost of each client and the\ncommunication cost of sharing parameters across\nclients.\nFurthermore, the pluggable feature of\nLoRA, which supports the localization or encryp-\ntion of personalized parameters, enhances privacy\nprotection within federated learning.\nTherefore,\nLoRA has a great potential for privacy-preserving.\nWhile some previous surveys have mentioned\nLoRA [5, 10, 11], they mainly focus on PEFT and\nonly introduce a small number of LoRA-related\nworks, lacking systematic treatment and compre-\nhensive overview on LoRA and its variants.\nIn\nthis survey, we give a comprehensive overview\nof the current progress on LoRA for methods (1)\nimproving downstream adaption performance of\nYuren MAO et al. A Survey on LoRA of Large Language Models\n3\nLoRA; (2) mixing LoRA modules to achieve cross-\ntask generalization; (3) boosting the computation-\nefficiency of LoRA; (4) adopting LoRA in feder-\nated learning. Besides, the application of LoRA\nis briefly introduced.\nThis taxonomy of LoRA-\nrelated methods is illustrated in Figure 1. This sur-\nvey is expected to give comprehensive background\nknowledge, research trends and technical insights\nfor LoRA.\nThe rest of this survey is organized as follows.\nSection 2 introduces the background knowledge of\nLoRA, and Section 3 introduces the LoRA\u2019s vari-\nants that aim to improve the downstream adapta-\ntion performance.\nIn Section 4, we review the\nLoRA mixture methods that mix LoRA modules\nto achieve cross-task generalization. The LoRA-\ndriven federated learning methods are introduced\nin Section 6. Section 7 reports the applications of\nLoRA. We conclude this survey and discuss the fu-\nture directions in Section 8.\n2\nLow-Rank Adaptation (LoRA)\nThe Low-dimensional intrinsic dimensionality hy-\npothesis [189] presents that over-parameterized\nmodels reside on a low intrinsic dimension, which\ndemonstrates that we can achieve proper learn-\ning performance by only updating parameters re-\nlated to the intrinsic rank. Based on this hypothe-\nsis, LoRA [9] proposes to update dense layers in\na model with low-rank matrices.\nIt can achieve\nboth parameter- and computational- efficiency. In\nthis section, we first introduce the details of LoRA\nand then introduce existing works that focus on\nthe theoretical analysis of LoRA. Furthermore, we\ndemonstrate LoRA\u2019s efficiency in practice. At last,\nthis section presents that LoRA can be used in other\nuse cases except fine-tuning.\n2.1\nLoRA\nGiven a dense neural network layer parameterized\nby W0 \u2208Rd\u00d7k, to adapt it to a downstream task, we\nupdate it with \u2206W \u2208Rd\u00d7k and obtain an updated\nlayer parameterized by W = W0 + \u2206W. For full\nfine-tuning, \u2206W is computed based on gradients of\nall the d \u00d7k parameters for the layer, which is com-\nputationally expensive and requires a large amount\nof GPU memory for LLMs. To improve the com-\nputational efficiency, LoRA decomposes \u2206W into\ntwo small matrices B \u2208Rd\u00d7r and A \u2208Rr\u00d7k, i.e.,\nW = W0 + \u03b1BA\n(1)\nwhere r \u226amin{d, k}, B and A are initialized with\na random Gaussian distribution and zero respec-\ntively, \u03b1 represents the scaling factor that controls\nthe strength of updates. The parameter number of\nLoRA is r \u00d7 (d + k), which is significantly less than\nd \u00d7 k. Figure 2 (a) and (b) compare the structures\nof full fine-tuning and LoRA.\nLoRA is highly parameter efficient for it up-\ndates only a small subset of model parameters,\nwhich reduces the memory and computational re-\nquirements for fine-tuning without increasing in-\nference latency [190]. Furthermore, The parame-\nter efficiency can be further improved by extend-\ning from the low-rank matrix to low-rank ten-\nsor [191] or combining with the Kronecker de-\ncomposition [192,", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "A Survey on LoRA of Large Language Models", "description": "Arxiv Paper titled 'A Survey on LoRA of Large Language Models' authored by Yuren Mao, Yuhang Ge, Yijiang Fan, Wenyi Xu, Yu Mi, Zhonghao Hu, Yunjun Gao", "url": "https://arxiv.org/pdf/2407.11046", "source": "arxiv", "id": "d50a8971-0844-414e-80ad-a24cf9aafc1f"}, "page_content": "W = W0 + \u03b1BA\n(1)\nwhere r \u226amin{d, k}, B and A are initialized with\na random Gaussian distribution and zero respec-\ntively, \u03b1 represents the scaling factor that controls\nthe strength of updates. The parameter number of\nLoRA is r \u00d7 (d + k), which is significantly less than\nd \u00d7 k. Figure 2 (a) and (b) compare the structures\nof full fine-tuning and LoRA.\nLoRA is highly parameter efficient for it up-\ndates only a small subset of model parameters,\nwhich reduces the memory and computational re-\nquirements for fine-tuning without increasing in-\nference latency [190]. Furthermore, The parame-\nter efficiency can be further improved by extend-\ning from the low-rank matrix to low-rank ten-\nsor [191] or combining with the Kronecker de-\ncomposition [192, 193]. Except for parameter ef-\nficiency, LoRA is also pluggable for the LoRA\nparameters that can be separated from the model\nafter training. The pluggable character of LoRA\nenables it to be shared and reused by multiple\nusers [98].\nWhen we have LoRA modules for\nmultiple tasks, we can combine these modules and\nexpect a proper cross-task generalization perfor-\n4\nFront. Comput. Sci., 2024, 0(0): 1\u201330\nLow-Rank Adaptation of Large Language Models\nLow-Rank Adaptation(\u00a72)\nTheoretical Analysis(\u00a72.2)\nMalladi et al. [12], Koubbi et al. [13], Jang et al. [14],\nZhu et al. [15], Zeng et al. [16]\nBeyond Fine-tuning(\u00a72.4)\nReLoRA [17], MoRA [18], LTE [19], InfLoRA [20],\nGS-LoRA [21], I-LoRA [22], LongLoRA [3],\nSinkLoRA [23]\nDownstream Adaptation Improving(\u00a73)\nBreaking the Low-rank Bottleneck(\u00a73.1)\nStacking LoRAs along Fine-tuning(\u00a73.1.1)\nReLoRA [17], COLA [24], MELoRA [25]\nUpdating as gradient compressor(\u00a73.1.2)\nFLoRA [26]\nCo-learning LLM and LoRA(\u00a73.1.3)\nDelta-LoRA [27]\nDynamic Rank Allocation(\u00a73.2)\nSVD-Based Methods(\u00a73.2.1)\nAdaLoRA [28], SaLoRA [29], IncreLoRA [30]\nSRD-based Methods(\u00a73.2.2)\nDoRA (Dynamic Low-Rank Adaptation) [31],\nAutoLoRA [32], SoRA [33], ALoRA [34]\nRank Sampling-based Methods(\u00a73.2.3)\nDyLoRA [35]\nOptimizing the Learning Procedure(\u00a73.3)\nInitialization Improvement(\u00a73.3.1)\nHayou et al. [36], PiSSA [37], MiLoRA [38]\nGradient Update Optimization(\u00a73.3.2)\nZhang et al. [39], LoRA+ [40], ResLoRA [41],\nSIBO [42], Jin et al. [43], DoRA [44]\nOverfitting Mitigation(\u00a73.3.3)\nBiLoRA [45], Lin et al. [46], HiddenKey [47]\nCombining with other Learning Paradigms(\u00a73.4)\nLaplace-LoRA [48], PILLOW [49], STAR [50]\nCross-task Generalization(\u00a74)\nMixture with Manually Designed Weights(\u00a74.1)\nWang et al. [51], Zhao et al. [52], Smith et al. [53],\nControlPE [54], Zhang et al. [55], Chitale et al. [56],\nToken-level Adaptation [57], BYOM [58]\nMixture with Learnt Weights(\u00a74.2)\nAsadi et al. [59], LoRAHub [60], ComPEFT [61],\nL-LoRA [62], MixLoRA [63], X-LoRA [64]\nMixture of LoRA Experts(\u00a74.3)\nMoRAL [65], LoRAMoE [66], MoCLE [67],\nMOELoRA [68], Mixture-of-LoRAs [69],\nMultiLoRA [70], MLoRE [71], MTLoRA [72],\nMoLA [73], LLaVA-MoLE [74], SiRA [75],\nOctavius [76], Fast LoRA [77], MoSLoRA [78]\nEfficiency Improving(\u00a75)\nParameter Reduction(\u00a75.1)\nParameter Freezing(\u00a75.1.1)\nLoRA-SP [79], LoRA-FA [80], AFLoRA [81],\nDropBP [82], LoRA-XS [83], BYOM-LoRA [58]\nParameter Pruning(\u00a75.1.2)\nLoRA-drop [84], LoRAprune [85],\nLoRAshear [86], Zhu et al. [87]", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "A Survey on LoRA of Large Language Models", "description": "Arxiv Paper titled 'A Survey on LoRA of Large Language Models' authored by Yuren Mao, Yuhang Ge, Yijiang Fan, Wenyi Xu, Yu Mi, Zhonghao Hu, Yunjun Gao", "url": "https://arxiv.org/pdf/2407.11046", "source": "arxiv", "id": "95d6a3dd-7367-4447-833a-9f0ed292b0ed"}, "page_content": "], MoCLE [67],\nMOELoRA [68], Mixture-of-LoRAs [69],\nMultiLoRA [70], MLoRE [71], MTLoRA [72],\nMoLA [73], LLaVA-MoLE [74], SiRA [75],\nOctavius [76], Fast LoRA [77], MoSLoRA [78]\nEfficiency Improving(\u00a75)\nParameter Reduction(\u00a75.1)\nParameter Freezing(\u00a75.1.1)\nLoRA-SP [79], LoRA-FA [80], AFLoRA [81],\nDropBP [82], LoRA-XS [83], BYOM-LoRA [58]\nParameter Pruning(\u00a75.1.2)\nLoRA-drop [84], LoRAprune [85],\nLoRAshear [86], Zhu et al. [87]\nParameter Sharing (\u00a75.1.3)\nVeRA [88], VB-LoRA [89], FourierFT [90]\nParameter Quantization(\u00a75.2)\nPTQ-based methods (\u00a75.2.1)\nQLoRA [91], QA-LoRA [92]\nQAT-base (\u00a75.2.2)\nLoftQ [93], ApiQ [94], L4Q [95]\nParallel LoRA Computing Frameworks(\u00a75.3)\nParallel Fine-tuning(\u00a75.3.1)\nASPEN [96]\nParallel Inference(\u00a75.3.2)\nPunica [97], S-LoRA [98], CARASERVE [99]\nLoRA for Federate Learning(\u00a76)\nData Heterogeneity(\u00a76.1)\nSLoRA [100], FeDeRA [101], FFA-LoRA [102]\nDevice Heterogeneity(\u00a76.2)\nFedMS [103], FlexLoRA [104], HETLORA [105]\nModel Heterogeneity(\u00a76.3)\npFedLoRA [106]\nParameter Privacy(\u00a76.4)\nHuang et al. [107], PrivateLoRA [108]\nApplications of LoRA(\u00a77)\nLanguage Tasks(\u00a77.1)\nTraditional NLP Task [109\u2013117], Code Task [118\u2013123],\nModel Alignment Task [124\u2013131],\nVertical Domain Task [132\u2013143]\nVision Task(\u00a77.2)\nImage Generation Tasks [144\u2013172],\nImage Segmentation Task [173\u2013181]\nMultimodal Tasks(\u00a77.2)\nAudio-Text [182], Image-Text [183\u2013185],\nVideo-Text [186\u2013188]\nFig. 1\nThe taxonomy of this paper.\nYuren MAO et al. A Survey on LoRA of Large Language Models\n5\n(a) Full Fine-Tuning\nOutput\nInput\nPre-trained\u00a0\nWeights\n(b) LoRA\nPre-trained\u00a0\nWeights\nUpdate\nWeights\n(c)\u00a0Breaking the Low-rank Bottleneck\n(d)\u00a0Dynamic Rank Allocation\nLow-Rank\nHigh-Rank\n...\n...\nDifferent Layers/Modules\nFrozen\nTrainable\nOutput\nInput\nFig. 2\nAn illustration of full fine-tuning (a), LoRA (b) and its variants for improving downstream adaptation, which includes\nbreaking the low-rank bottleneck (c) and dynamic rank allocation (d).\nmance [60]. Besides, the low-rank mechanism of\nLoRA is compatible with other parameter-efficient\nmethods, such as adapter\n[194, 195].\nBesides,\nLoRA can achieve proper downstream adapta-\ntion performance on various downstream tasks.\nFor example, on MMLU [196] benchmark, com-\nparing with full fine-tuning, fine-tuning with LoRA\ncan achieve comparable or even better performance\nacross 57 tasks [4].\nIn practice, for a Transformer-based LLM, the\ndense layers typically consist of two types of\nweight matrices: the projection matrices in atten-\ntion modules and feed-forward neural (FFN) mod-\nules. The experiments mentioned above are con-\nducted based on the original LoRA settings, apply-\ning it to the query and value weight matrices in the\nattention modules. It is worth mentioning that sub-\nsequent work shows that applying it to the FFN lay-\ners can further improve model performance [197].\n2.2\nTheoretical Analysis\nTo understand why LoRA is effective and how\nLoRA can be more effective, several works have\nprovided theoretical analyses from various aspects.\nTo answer the question that why LoRA is effec-\ntive, Malladi et al. [12] analyze the fine-tuning dy-\nnamics of LoRA from the kernel view and demon-\nstrate that in the lazy regime, LoRA fine-tuning\nis nearly equivalent to full fine-tuning. Besides,\nZeng et al. [16] provides a theoretical analysis of\nthe LoRA\u2019s expressive power for both fully con-\nnected neural networks (FNNs) and Transformer\nnetworks (TFNs).\nThey proved that LoRA can\nadapt any model f to accurately represent any\nsmaller target model \u00aff if LoRA-rank \u2265(width of\nf) \u00d7 depth o f \u00aff\ndepth o", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "A Survey on LoRA of Large Language Models", "description": "Arxiv Paper titled 'A Survey on LoRA of Large Language Models' authored by Yuren Mao, Yuhang Ge, Yijiang Fan, Wenyi Xu, Yu Mi, Zhonghao Hu, Yunjun Gao", "url": "https://arxiv.org/pdf/2407.11046", "source": "arxiv", "id": "4804aeec-1f90-4403-9611-9fa62709af86"}, "page_content": "Theoretical Analysis\nTo understand why LoRA is effective and how\nLoRA can be more effective, several works have\nprovided theoretical analyses from various aspects.\nTo answer the question that why LoRA is effec-\ntive, Malladi et al. [12] analyze the fine-tuning dy-\nnamics of LoRA from the kernel view and demon-\nstrate that in the lazy regime, LoRA fine-tuning\nis nearly equivalent to full fine-tuning. Besides,\nZeng et al. [16] provides a theoretical analysis of\nthe LoRA\u2019s expressive power for both fully con-\nnected neural networks (FNNs) and Transformer\nnetworks (TFNs).\nThey proved that LoRA can\nadapt any model f to accurately represent any\nsmaller target model \u00aff if LoRA-rank \u2265(width of\nf) \u00d7 depth o f \u00aff\ndepth o f f under a mild assumption, where the\ndepth and width are the number of layers and the\nnumber of neurons of the layer having the largest\nnumber of neurons, respectively. Moreover, they\nquantify the approximation error when the LoRA-\nrank falls below this threshold. Regarding TFNs,\nthey showed that any model can be adapted to\na target model of equivalent size using a rank-\n\u0010 embedding size\n2\n\u0011\nfor LoRA. Additionally, Koubbi et\nal. [13] utilize the mathematical framework for\nTransformers established by [198\u2013200] to inves-\ntigate how variations in attention parameters and\ninitial token values impact the structural dynamics\nof token clusters.\nAs to the question that how LoRA can be\nmore effective, Jang et al. [14] analyze the fine-\ntuning of LoRA within the neural tangent kernel\n(NTK) [201] framework when N data points are\navailable. They demonstrate that employing a rank\n6\nFront. Comput. Sci., 2024, 0(0): 1\u201330\nr \u2273\n\u221a\nN in LoRA helps to avoid spurious lo-\ncal minima and facilitates the discovery of low-\nrank solutions that exhibit good generalization. Be-\nsides, Zhu et al. [15] observe that the project-down\nmatrix A is utilized for extracting features from\nthe input, while the project-up matrix B employs\nthese features to create the desired output. Based\non this observation, they demonstrate that freezing\nthe project-down matrix A while tuning only the\nproject-up matrix B leads to better generalization\ncompared to tuning both matrices, in addition to\nachieving a 2\u00d7 reduction in parameters.\n2.3\nEfficiency in Practice\nThe computational efficiency of LoRA is signifi-\ncantly higher than that for full fine-tuning. Tak-\ning fine-tuning the dense weight matrix of the first\nFFN layer in LLaMA2-7B as an example, full\nfine-tuning needs to fine-tune 11, 008 \u00d7 4, 096 =\n45, 088, 768 parameters while LoRA only needs to\ntune (11, 008 \u00d7 4) + (4 \u00d7 4, 096) = 60, 416 parame-\nters when r = 4. For this layer, LoRA only adjusts\nnearly one-thousandth of the parameters compared\nto full fine-tuning.\nLoRA can significantly decrease the memory us-\nage of fine-tuning an LLM, which can be divided\ninto four parts: (1) Model Memory: the memory\nrequired to store the model weights; (2) Activa-\ntion Memory: the memory occupied by interme-\ndiate activations during forward propagation.\nIt\nmainly depends on factors such as batch size and\nsequence length; (3) Gradient Memory: the mem-\nory required to store gradients during backpropaga-\ntion. The gradients are only calculated for trainable\nparameters; (4) Optimization Memory: the mem-\nory used to store optimizer states. For example,\nthe Adam optimizer stores the \u201cfirst moment\u201d and\n\u201csecond moment\u201d of trainable parameters.\nPan et al. [4] provides a comprehensive em-\npirical comparison between full fine-tuning and\nLoRA fine-tuning on an LLaMA2-7B model with\nbatch size 1, utilizing a single NVIDIA RTX4090\n(24GB) GPU. According to this study, full fine-\ntuning requires approximately 60GB of memory,\nwhich exceeds the capacity of an RTX4090 GPU;\nby contrast, LoRA fine-tuning only needs about\n23GB of memory.\nLoRA significantly reduces\nmemory usage and makes fine-tuning LLaMA2-\n7B feasible on a single NVIDIA RTX4090 (24GB)\nGPU. Specifically, due to fewer trainable parame-\nters, both optimization memory and gradient mem-\nory decrease significantly by approximately 25GB\nand 14GB respectively. On the other hand, while\nLoRA introduces additional \u201cincremental param-\neters\u201d resulting in slight increases in activation\nmemory and weight memory (totaling about 2GB),\nthis increase is negligible when considering the\noverall reduction in memory. Moreover, reducing\nmemory brings an acceleration of forward propa-\ngation. LoRA is 1.9\u00d7 times faster compared to full\nfine-tuning.\n2.4\nBeyond Fine-tuning\nBesides fine-tuning, LoRA can be applied to other\nlearning", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "A Survey on LoRA of Large Language Models", "description": "Arxiv Paper titled 'A Survey on LoRA of Large Language Models' authored by Yuren Mao, Yuhang Ge, Yijiang Fan, Wenyi Xu, Yu Mi, Zhonghao Hu, Yunjun Gao", "url": "https://arxiv.org/pdf/2407.11046", "source": "arxiv", "id": "47f53bb0-7407-451b-b907-0f318dac9599"}, "page_content": " fine-tuning only needs about\n23GB of memory.\nLoRA significantly reduces\nmemory usage and makes fine-tuning LLaMA2-\n7B feasible on a single NVIDIA RTX4090 (24GB)\nGPU. Specifically, due to fewer trainable parame-\nters, both optimization memory and gradient mem-\nory decrease significantly by approximately 25GB\nand 14GB respectively. On the other hand, while\nLoRA introduces additional \u201cincremental param-\neters\u201d resulting in slight increases in activation\nmemory and weight memory (totaling about 2GB),\nthis increase is negligible when considering the\noverall reduction in memory. Moreover, reducing\nmemory brings an acceleration of forward propa-\ngation. LoRA is 1.9\u00d7 times faster compared to full\nfine-tuning.\n2.4\nBeyond Fine-tuning\nBesides fine-tuning, LoRA can be applied to other\nlearning paradigms, such as pre-training [17, 19]\nand continual training [20].\nFor pre-training,\nReLoRA [17] and MoRA [18] are proposed to\nuse low-rank updates to train high-rank networks;\nmoreover, LTE [19] is proposed to perform par-\nallel training of multiple low-rank heads across\ncomputing nodes to minimize the need for fre-\nquent synchronization, which facilitates the utiliza-\ntion of LoRA in pre-training.\nAs for continual\ntraining, there are several methods have been pro-\nYuren MAO et al. A Survey on LoRA of Large Language Models\n7\nposed to address the catastrophic forgetting prob-\nlem. InfLoRA [20] addresses catastrophic forget-\nting by reparameterizing pre-trained weights with\na minimal set of parameters in a subspace. GS-\nLoRA [21] uses group sparse regularization to au-\ntomatically select specific LoRA groups while ze-\nroing out others to mitigate catastrophic forgetting\neffects. I-LoRA [202] leverages dual-memory ex-\nperience replay combined with LoRA parameter\ninterpolation to combat catastrophic forgetting.\nFurthermore, LoRA can be used to overcome the\nlimited context size for LLMs [3,23]. For instance,\nLongLoRA [3] successfully computaitional effi-\nciently extends the context window of LLaMA2-\n7B [203] from 4k to 100k tokens by combining\nLoRA with shifted sparse attention. However, Lon-\ngLoRA does not match the efficiency of vanilla at-\ntention due to chaotic attention head structures and\nunnecessary information exchange between token\ngroups. To address these issues, SinkLoRA [23]\nintroduces Sink Fixed Attention (SF-Attn) to pro-\nportionally returns cyclically shifted groups of at-\ntention heads to their un-shifted state and achieves\nproper performance.\n3\nDownstream Adaptation Improving\nAlthough LoRA can achieve proper adaptation per-\nformance on some downstream tasks, there is still\na performance gap between LoRA and full fine-\ntuning on many downstream tasks, such as math-\nematical reasoning [5, 204, 205]. To fill this gap,\nmany methods are proposed to further improve\nthe downstream adaptation performance of LoRA.\nTypically, existing methods improve the down-\nstream adaptation performance from the follow-\ning perspectives: (1) breaking the low-rank bot-\ntleneck, refer to Figure 2 (c); (2) adaptively allo-\ncating the ranks of different LoRA modules, refer\nto Figure 2 (d); (3) optimizing the learning proce-\ndure of LoRA; (4) combining with other learning\nparadigms. In this section, we introduce these four\ntypes of methods respectively.\n3.1\nBreaking the Low-rank Bottleneck\nThe low-rank updates enable LoRA to be param-\neter efficient; however, it restricts LLMs\u2019 ability\nto memorize downstream knowledge and general-\nization on downstream tasks [18, 204\u2013207]. This\nlow-rank limitation causes inferior performance of\nLoRA in knowledge- and skill-intensive domains\ncomparing to full-fine tuning, such as code and\nmath. Experimental study [205] demonstrates that\nthe rank for full fine-tuning is significant\n(10-\n100 \u00d7) higher than that for LoRA, and increas-\ning the rank of LoRA updation can narrow the\nperformance gap between LoRA and full fine-\ntuning. To increase the rank of LoRA and improve\nits performance, several methods have been pro-\nposed [17,24,27,208], which typically increase the\nrank through (1) stacking LoRAs along learning it-\nerations; (2) updating as gradient compressors; (3)\nco-updating LLM and LoRA modules during fine-\ntuning.\n3.1.1\nStacking LoRAs along Fine-tuning\nMatrix rank is subadditive, i.e., rank(M1 + M2) \u2264\nrank(M1) + rank(M2) for matrices M1 and M2 that\nhave the same size.\nBased on the subadditiv-\nity, we can aggregate multiple LoRA modules to-\ngether to increase the rank and break the low-rank\nbottleneck.\nFollowing this idea, ReLoRA [17]\nproposes a merge-and-reinit procedure for LoRA,\nwhich periodically", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "A Survey on LoRA of Large Language Models", "description": "Arxiv Paper titled 'A Survey on LoRA of Large Language Models' authored by Yuren Mao, Yuhang Ge, Yijiang Fan, Wenyi Xu, Yu Mi, Zhonghao Hu, Yunjun Gao", "url": "https://arxiv.org/pdf/2407.11046", "source": "arxiv", "id": "94fd8adf-4f6d-4937-92ec-08b0b17d983b"}, "page_content": " several methods have been pro-\nposed [17,24,27,208], which typically increase the\nrank through (1) stacking LoRAs along learning it-\nerations; (2) updating as gradient compressors; (3)\nco-updating LLM and LoRA modules during fine-\ntuning.\n3.1.1\nStacking LoRAs along Fine-tuning\nMatrix rank is subadditive, i.e., rank(M1 + M2) \u2264\nrank(M1) + rank(M2) for matrices M1 and M2 that\nhave the same size.\nBased on the subadditiv-\nity, we can aggregate multiple LoRA modules to-\ngether to increase the rank and break the low-rank\nbottleneck.\nFollowing this idea, ReLoRA [17]\nproposes a merge-and-reinit procedure for LoRA,\nwhich periodically merges the LoRA modules to\nthe LLM and then reinitializes the LoRA mod-\nules during fine-tuning.\nIt equals stacking mul-\n8\nFront. Comput. Sci., 2024, 0(0): 1\u201330\ntiple LoRA modules along with fine-tuning and\ncan increase the rank of the overall updates.\nSimilarly, COLA [24] proposes another merge-\nand-reinit method based on Frank-Wolfe algo-\nrithm [209]. However, MELoRA [25] points out\nthat the merge-and-reinit procedure does not nec-\nessarily guarantee an increase in rank, because\nthere can be overlap between the series of LoRA\nmodules along fine-tuning. To solve this problem,\nMELoRA proposes to decompose the LoRA mod-\nules into smaller mini LoRAs and then parallelly\nstack these mini LoRAs, whose effectiveness in in-\ncreasing the rank is theoretically verified.\n3.1.2\nUpdating as Gradient Compressor\nThe above methods break the low-rank bottle-\nneck in the parameter space.\nAs a supplement,\nFLoRA [26] finds that LoRA performs a fixed\nrandom projection to compress gradients and re-\nstricts the total weight matrix change to low-rank.\nTo overcome this low-rank bottleneck in gradient\nspace, FLoRA proposes to resample the random\nprojection, which is demonstrated to largely re-\ncover the performance of full-matrix SGD.\n3.1.3\nCo-updating LLM and LoRA\nThe above two kinds of methods focus on improv-\ning the representation ability of LoRA itself. Dif-\nferent from them, Delta-LoRA [27] proposes to\njointly update the LLM and LoRA modules, which\ndirectly updates the high-rank LLM and can gain\nbetter representation capable than updating LoRA\nindependently. It updates the LLM based on the\ndifference between two LoRA modules of two con-\nsecutive iterations, which enables it to update the\nLLM without any extra memory.\n3.2\nDynamic Rank Allocation\nFor the rank of LoRA, higher is not always better.\nThe abundant LoRA ranks may cause degeneration\nin both performance and efficiency. Furthermore,\nthe importance of weights can vary across different\nlayers of a Transformer model during fine-tuning,\nrequiring different ranks for each layer. [28, 31,\n33, 35].\nTherefore, assigning the same rank to\nLoRA modules of different layers is not the op-\ntimal choice.\nIt is better to adaptively allocate\nranks to LoRA modules of different layers. Ex-\nisting methods adaptively allocate ranks for LoRA\nmodules from the perspectives of (1) singular value\ndecomposition (SVD); (2) single-rank decomposi-\ntion (SRD); (3) rank sampling.\n3.2.1\nSVD-based Methods\nDecomposing a matrix with singular value decom-\nposition (SVD) and selectively truncating its sin-\ngular values is an effective way to control the rank\nof the matrix. Inspire by SVD, we can decompose\nthe LoRA parameter matrix BA into an SVD form,\ni.e, P\u039bQ where P and Q are orthogonal and \u039b\nis a non-negative diagonal matrix. By controlling\nthe elements in \u039b, we can control the rank of BA\nand allocate ranks for LoRA modules. Following\nthis idea, several rank allocation methods approx-\nimate the SVD decomposition for BA and allocate\nthe ranks by filtering the diagonal matrix. For in-\nstance, AdaLoRA [28] approximates the SVD de-\ncomposition by regularizing the orthogonality of P\nand Q. Then, it drops unimportant singular val-\nues based on novel importance scoring methods.\nSimilarly, SaLoRA [29] also introduces an orthog-\nonality regularization for P and Q; by contrast, it\ndrops unimportant singular values based on the L0\nnorm. However, the above methods are not effi-\nYuren MAO et al. A Survey on LoRA of Large Language Models\n9\ncient enough for they start with a high rank and\nthen reduce the rank iteratively, which brings a pre-\ndefined budget [30]. To solve this problem, In-\ncreLoRA [30] proposes to start from a single rank\nand then automatically increase the rank based on\na heuristic importance score, where the orthogo-\nn", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "A Survey on LoRA of Large Language Models", "description": "Arxiv Paper titled 'A Survey on LoRA of Large Language Models' authored by Yuren Mao, Yuhang Ge, Yijiang Fan, Wenyi Xu, Yu Mi, Zhonghao Hu, Yunjun Gao", "url": "https://arxiv.org/pdf/2407.11046", "source": "arxiv", "id": "4c8d3090-3e97-4851-94dc-71f273adb34c"}, "page_content": " diagonal matrix. For in-\nstance, AdaLoRA [28] approximates the SVD de-\ncomposition by regularizing the orthogonality of P\nand Q. Then, it drops unimportant singular val-\nues based on novel importance scoring methods.\nSimilarly, SaLoRA [29] also introduces an orthog-\nonality regularization for P and Q; by contrast, it\ndrops unimportant singular values based on the L0\nnorm. However, the above methods are not effi-\nYuren MAO et al. A Survey on LoRA of Large Language Models\n9\ncient enough for they start with a high rank and\nthen reduce the rank iteratively, which brings a pre-\ndefined budget [30]. To solve this problem, In-\ncreLoRA [30] proposes to start from a single rank\nand then automatically increase the rank based on\na heuristic importance score, where the orthogo-\nnality regularization is also involved while the ele-\nments in \u039b is not required to be non-negative.\n3.2.2\nSRD-based Methods\nHowever, the orthogonality regularization brings\nunignorable computational costs for LoRA and\ndegenerates its efficiency.\nTo address this prob-\nlem, several methods omit the orthogonality re-\nquirement of SVD and directly decompose BA\ninto single-rank components.\nThen, they allo-\ncate the ranks by selecting the proper components.\nDoRA (Dynamic Low-Rank Adaptation) [31]\nproposes to decompose the LoRA parameter ma-\ntrix BA into single-rank components and prunes\nthe components based on a heuristic importance\nscore. Similarly, AutoLoRA [32] also decomposes\nthe LoRA parameter matrix BA into single-rank\ncomponents, but it prunes the components based\non meta-learning. SoRA [33] eliminates the or-\nthogonality regularization and filters columns and\nrows of P and Q (their combination can be regarded\nas single-rank components) by directly controlling\nthe diagonal matrix. It controls the diagonal matrix\nby formulating them as a set of learnable gating\nunits which are updated in the fine-tuning proce-\ndure. ALoRA [34] also filters the components by\nusing gating units; by contrast, it learns the gating\nunits based on neural architecture search [210].\n3.2.3\nRank Sampling-based Methods\nIn the SVD parameterization- and component-wise\ndecomposition-based methods, we need to spend\nthe extra computational costs to search proper\nranks. To avoid the extra cost, DyLoRA [35] points\nout that we can allocate ranks directly by random\nsampling. In each training step, it samples a value\nb from a pre-defined discrete distribution and allo-\ncates b as the rank. Then, the matrices A and B are\ntruncated to rank-b. In the fine-tuning procedure,\nonly the parameters on the b-th row of A and b-\nth column of B are tunable while other parameters\nare frozen. Besides, the distribution can be defined\nbased on users\u2019 preferences.\n3.3\nOptimizing the Learning Procedure\nIn practice, LoRA converges more slowly than\nfull fine-tuning.\nMoreover, it is also sensitive\nto hyperparameters and suffers from overfitting.\nThese issues affect LoRA\u2019s efficiency and hinder\nits downstream adaptation performance.\nTo ad-\ndress these issues, researchers have developed sev-\neral approaches to optimize the learning procedure\nof LoRA, which can be categorized into the fol-\nlowing three types: (1) Initialization Improvement;\n(2) Gradient Update Optimization; (3) Overfitting\nMitigation.\n3.3.1\nInitialization Improvement\nLoRA usually initializes its parameter matrices A\nand B using Gaussian noise and zeros respectively.\nThere are two simple schemes: Init[A], which sets\nmatrix B to zero and randomly initializes matrix\nA, and Init[B], which does the reverse. Literature\n[36] compares these two schemes and concludes\nthat Init[A] is better through theoretical analysis. It\nreveals that Init[A] allows using a larger learning\nrate without causing instability, making the learn-\ning process more efficient.\nHowever, even with\ninit[A], this random initialization method still re-\nsults in small initial gradients, leading to slower\n10\nFront. Comput. Sci., 2024, 0(0): 1\u201330\nconvergence.\nTo solve this, PiSSA [37] initial-\nizes LoRA with the principal singular components\nof the pre-trained matrix. Since principal singular\ncomponents represent the most significant direc-\ntions in the matrix, aligning the initial weights with\nthese components can accelerate convergence and\nimprove performance. In contrast, MiLoRA [38]\ninitializes LoRA with the minor singular compo-\nnents. Given that random initialization of low-rank\nmatrices can interfere with the important features\nlearned in the pre-trained matrix, it reduces this\ninterference to improve overall performance while\nadapting to new tasks.\n3.3.2\nGradient Update Optimization\nTo further enhance the convergence and reliability\nof LoRA, several studies have proposed improve-\nments from the perspective of gradient updates.\n[39] introduces a scaled gradient method based on\nRiemannian optimization, which incorporates an\nr\u00d7r preconditioner item in the gradient update step\n", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "A Survey on LoRA of Large Language Models", "description": "Arxiv Paper titled 'A Survey on LoRA of Large Language Models' authored by Yuren Mao, Yuhang Ge, Yijiang Fan, Wenyi Xu, Yu Mi, Zhonghao Hu, Yunjun Gao", "url": "https://arxiv.org/pdf/2407.11046", "source": "arxiv", "id": "269d55a4-095a-46e3-b66f-727e5c33eb3f"}, "page_content": "37] initial-\nizes LoRA with the principal singular components\nof the pre-trained matrix. Since principal singular\ncomponents represent the most significant direc-\ntions in the matrix, aligning the initial weights with\nthese components can accelerate convergence and\nimprove performance. In contrast, MiLoRA [38]\ninitializes LoRA with the minor singular compo-\nnents. Given that random initialization of low-rank\nmatrices can interfere with the important features\nlearned in the pre-trained matrix, it reduces this\ninterference to improve overall performance while\nadapting to new tasks.\n3.3.2\nGradient Update Optimization\nTo further enhance the convergence and reliability\nof LoRA, several studies have proposed improve-\nments from the perspective of gradient updates.\n[39] introduces a scaled gradient method based on\nRiemannian optimization, which incorporates an\nr\u00d7r preconditioner item in the gradient update step\nto improve the convergence and hyperparameter\nrobustness of LoRA. Through theoretical analysis,\nLoRA+ [40] discovered the necessity of setting a\nproportional learning rate for matrices A and B to\nachieve stable feature learning and accelerate con-\nvergence. ResLoRA [41] introduced residual con-\nnections into LoRA to optimize the gradient prop-\nagation path, speeding up training convergence\nand enhancing model performance.\nSimilarly,\nSIBO [42] mitigate over-smoothing by injecting\nresidual connections of initial token representa-\ntions into LoRA\u2019s input. Additionally, to further re-\nduce computational resources, literature [43] em-\nploys gradient-free optimization methods such as\nCMA-ES and FWA to optimize LoRA, demon-\nstrating competitive performance in few-shot NLU\ntasks. Besides, DoRA (Weight-Decomposed Low-\nRank Adaptation) [44] constrains the gradient up-\ndate, focusing on the directional change of the pa-\nrameter. It decomposes pre-trained weight into two\ncomponents, direction and magnitude, and applies\nLoRA only to the direction component to enhance\ntraining stability.\n3.3.3\nOverfitting Mitigation\nAlthough LoRA effectively reduces the number\nof trainable parameters compared to full fine-\ntuning, some studies have shown that LoRA is also\nprone to overfitting [47], which contradicts previ-\nous views.\nTo address this issue, BiLoRA [45]\nadopts a bi-level optimization strategy.\nIt alter-\nnately trains the singular vectors and singular val-\nues of the low-rank increment matrix on differ-\nent subsets of the training data.\nThis approach\navoids the simultaneous optimization of parame-\nters at different levels on a single dataset, thus mit-\nigating overfitting. In addition, literature [46] ap-\nplies dropout to LoRA parameters to reduce over-\nfitting, while HiddenKey [47] employs column-\nwise dropout for attention layers and element-wise\ndropout for feedforward layers.\n3.4\nCombining with other Learning Paradigms\nLoRA\nis\ncompatible\nwith\nother\nlearning\nparadigms,\nsuch\nas\nBayesian\nLearning,\nIn-\ncontext Learning and Active Learning. Combining\nLoRA with these learning paradigms can address\nseveral problems that hurt the downstream adap-\ntation performance.\nFor example, combining\nwith Bayesian Learning,\nLaplace-LoRA [48]\ncan relieve the overconfidence phenomenon that\nhappened in downstream adaptation. Combining\nwith In-context Learning, PILLOW [49] aims\nto solve the low-resource dilemmas existing in\nsome downstream tasks. Combining with Active\nYuren MAO et al. A Survey on LoRA of Large Language Models\n11\nTable 1\nPerformance of LoRA and its variants for RoBERTa-base model on the GLUE benchmark. We report Matthew\u2019s\ncorrelation for CoLA, Pearson correlation for STS-B, and accuracy for the other datasets. The results are reported according\nto the results reported in literature [9,32,45,89,90].\nMethod\n# Params\nSST-2\nMPRC\nCoLA\nQNLI\nRTE\nSTS-B\nTied-LoRA [213]\n0.043M\n94.4\n88.5\n61.9\n92.0\n76.2\n89.8\nAutoLoRA [32]\n0.3M\n94.9\n89.4\n61.3\n92.9\n77.0\n90.8\nDyLoRA [35]\n0.3M\n94.3\n89.5\n61.1\n92.2\n78.7\n91.1\nAdaLoRA [28]\n0.3M\n94.5\n88.7\n62.0\n93.1\n81.0\n90.5\nFourierFT [90]\n0.024M\n94.2\n90.0\n63.8\n92.2\n79.1\n90.8\nVeRA [88]\n0.043M\n94.6\n89.5\n65.6\n91.8\n78.7\n90.7\nFull Fine-tuning [9]\n125M\n94.8\n90.2\n63.6\n92.8\n78.7\n91.2\nLoRA [9]\n0.3M\n95.", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "A Survey on LoRA of Large Language Models", "description": "Arxiv Paper titled 'A Survey on LoRA of Large Language Models' authored by Yuren Mao, Yuhang Ge, Yijiang Fan, Wenyi Xu, Yu Mi, Zhonghao Hu, Yunjun Gao", "url": "https://arxiv.org/pdf/2407.11046", "source": "arxiv", "id": "c581f02e-e76f-4230-a99e-77686142f723"}, "page_content": ".0\n90.8\nDyLoRA [35]\n0.3M\n94.3\n89.5\n61.1\n92.2\n78.7\n91.1\nAdaLoRA [28]\n0.3M\n94.5\n88.7\n62.0\n93.1\n81.0\n90.5\nFourierFT [90]\n0.024M\n94.2\n90.0\n63.8\n92.2\n79.1\n90.8\nVeRA [88]\n0.043M\n94.6\n89.5\n65.6\n91.8\n78.7\n90.7\nFull Fine-tuning [9]\n125M\n94.8\n90.2\n63.6\n92.8\n78.7\n91.2\nLoRA [9]\n0.3M\n95.1\n89.7\n63.4\n93.3\n78.4\n91.5\nVB-LoRA [89]\n0.023M\n94.4\n89.5\n63.3\n92.2\n82.3\n90.8\nBiLoRA [45]\n0.3M\n95.1\n91.7\n64.8\n93.3\n87.2\n91.7\nLearning, STAR [50] can effectively improve the\ndata efficiency.\nAt last, to illustrate the performance difference be-\ntween LoRA and some of its variants, we report\ntheir performance for RoBERTa-base [211] model\non the GLUE benchmark [212] in Table 1. These\nresults are derived from previous studies [9,16,32,\n45,90].\n4\nCross-task Generalization\nLoRA\u2019s pluggable nature enables users to accu-\nmulate LoRA plugins for different tasks. For ex-\nample, on Hugging Face platform, there are more\nthan 20,000 LoRA plugins compatible with vari-\nous LLMs for different tasks. These accumulated\nLoRA plugins can not only be utilized indepen-\ndently but also be mixed to achieve cross-task gen-\neralization [60]. Mixing multiple LoRA plugins\ntogether, namely LoRA mixture, has been widely\napplied in areas requiring cross-task generaliza-\ntion, such as multi-task learning, domain adapta-\ntion, and continual learning. Existing LoRA mix-\nture methods can be categorized into (1) mixture\nwith manually designed weights; (2) mixture with\nlearnt weights; (3) mixture of LoRA experts. This\nsection introduces each category of methods re-\nspectively, as shown in Fig. 3.\n4.1\nMixture with Manually Designed Weights\nEarly LoRA mixture methods attempt to linearly\ncombine different LoRA modules with manually\ndesigned weights.\nSome research demonstrates\nthat we can achieve proper cross-task generaliza-\ntion ability by simply averaging LoRA modules or\ntheir related outputs [51\u201353]. Furthermore, several\nmethods have been proposed to further improve\nthe performance of the LoRA mixture via adopting\nmanually designed weights.\nFor example, Con-\ntrolPE [54], [55] and [56] set the weight factors\nas hyperparameters, and ControlPE uses hyperpa-\nrameter search to determine the optimal combina-\ntion of two LoRA modules. Additionally, Token-\nlevel Adaptation [57] utilizes cosine similarity be-\ntween the input feature and the adapter dataset cen-\nter as weight factors, while BYOM [58] applies ba-\nsic model fusion methods such as Task Arithmetic,\nFisher-Merging, and RegMean.\nMixture with manually designed weights can\nquickly mix multiple LoRAs without extra train-\n12\nFront. Comput. Sci., 2024, 0(0): 1\u201330\n(a) Mixture with Manually\nDesigned Weights\n\u2026\nTask 1\nTask 2\nTask n\n\u2026\nOutput\nPretrained\nWeights\nInput\nManually\nSpecified\n\u2026\nTask 1\nTask 2\nTask n\n\u2026\nOutput\nPretrained\nWeights\nInput\n(b) Mixture with Learnt Weights\nGating\nMixture Weights\nMixture Weights\n\u2026\nTask 1\nTask 2\nTask n\n\u2026\nOutput\nPretrained\nWeights\nInput\n(c) Mixture of LoRA Experts\nGating\nFrozen\nTrainable\nFig. 3\nAn illustration of LoRA mixture methods.\ning, which demonstrates simplicity and computa-\ntional efficiency.\nHowever, it often fails to find\nthe optimal weights, leading to unstable perfor-\nmance and limited generalization. Subsequently,\nresearchers have explored using learning-based\nmethods to achieve more precise and adaptive mix-\ntures.\n4.2\nMixture with Learnt Weights\nTo learn the optimal mixture weights, several meth-\nods have been proposed at task level, instance level\nand token level to meet different needs. Task-level\nmethods focus on enhancing task transferability,\nwhich can be either gradient-based, such as [59],\nor gradient-free, as seen in LoRAHub [60]. Lo-\nRAHub employs a black-box algorithm named\nCMA-ES [214] to optimize weight factors for\nLoRA modules, simplifying the training process.\nLater, ComPEFT [61] and L", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "A Survey on LoRA of Large Language Models", "description": "Arxiv Paper titled 'A Survey on LoRA of Large Language Models' authored by Yuren Mao, Yuhang Ge, Yijiang Fan, Wenyi Xu, Yu Mi, Zhonghao Hu, Yunjun Gao", "url": "https://arxiv.org/pdf/2407.11046", "source": "arxiv", "id": "4912a8de-f9c9-4e41-8ef5-5929dc853fe4"}, "page_content": " mixture methods.\ning, which demonstrates simplicity and computa-\ntional efficiency.\nHowever, it often fails to find\nthe optimal weights, leading to unstable perfor-\nmance and limited generalization. Subsequently,\nresearchers have explored using learning-based\nmethods to achieve more precise and adaptive mix-\ntures.\n4.2\nMixture with Learnt Weights\nTo learn the optimal mixture weights, several meth-\nods have been proposed at task level, instance level\nand token level to meet different needs. Task-level\nmethods focus on enhancing task transferability,\nwhich can be either gradient-based, such as [59],\nor gradient-free, as seen in LoRAHub [60]. Lo-\nRAHub employs a black-box algorithm named\nCMA-ES [214] to optimize weight factors for\nLoRA modules, simplifying the training process.\nLater, ComPEFT [61] and L-LoRA [62] use Lo-\nRAHub to mix quantized LoRA modules, further\nimproving computational efficiency.\nCompared to task-level methods, instance-level\nand token-level methods can provide flexibility\nand precision for complex inputs. For multimodal\ninstruction tuning, MixLoRA [63] dynamically\nchooses appropriate low-rank decomposition vec-\ntors based on the input instance, which are then in-\ntegrated into LoRA matrices for training. To con-\nduct protein mechanics analysis and design tasks,\nX-LoRA [64] develops a dynamic gating mecha-\nnism to assign weights for LoRA modules at the\ntoken level and layer granularity. These approaches\ndemonstrate better performance in specific tasks or\napplication scenarios.\n4.3\nMixture of LoRA Experts\nWhen the LoRA modules are trainable, we can\njointly learn the mixture weights and the LoRA\nmodules, which can further improve the perfor-\nmance of the LoRA mixture.\nTo jointly learn\nthe mixture weights and LoRA modules, Mix-\nture of LoRA Experts (LoRA MoE) is a natural\nchoice, where each LoRA module acts as an ex-\npert, while a router network typically assigns the\nmixture weights. LoRA MoE has been proven to\nbe effective in many tasks, such as continual learn-\ning [65, 66], vision-language tasks [67] and multi-\ntask medical applications [68].\nExisting methods improve the performance of\nLoRA MoE from the perspectives of initialization,\nYuren MAO et al. A Survey on LoRA of Large Language Models\n13\ntask relationship management and efficiency. For\ninitialization, Mixture-of-LoRAs [69] first trains\nmultiple LoRAs separately as initialization and\nthen optimizes the router and LoRAs jointly. Mul-\ntiLoRA [70] proposes refining the initialization\nto reduce parameter dependency, which can yield\nmore balanced unitary subspaces. As for task bal-\nance, MLoRE [71] adds a low-rank convolution\npath in the MoE structure to capture global task\nrelationships.\nMTLoRA [72] adopts both task-\nagnostic and task-specific LoRA modules to ad-\ndress task conflicts.\nFor efficiency, MoLA [73]\nadaptively allocates different numbers of LoRA ex-\nperts to different layers of the Transformer model\nto save the number of LoRA modules. LLaVA-\nMoLE [74] and SiRA [75] leverage sparse com-\nputation to reduce computational cost. Addition-\nally, Octavius [76] sparsely activates independent\nLoRA experts with instance-level instructions to\nmitigate task interference and improve efficiency.\nFast LoRA [77] allows each sample in a minibatch\nto have its unique low-rank adapters, enabling effi-\ncient batching.\nBesides, some methods are not explicitly based\non MoE but follow MoE ideas. For example, I-\nLoRA [202] uses two LoRAs to manage long-term\nand short-term memory for continual learning, re-\nspectively.\n5\nEfficiency Improving\nWith the popularization of LLMs, the demand\nfor training and running LoRA modules increases\nrapidly. This increasing demand brings an unig-\nnorable computational burden; thus, for LoRA, the\nsmaller, the faster, the better. To meet this demand,\nexisting methods improve the computational effi-\nciency of LoRA from the perspectives of (1) pa-\nrameter reduction; (2) parameter quantization; (3)\nparallel LoRA computing frameworks. This sec-\ntion introduces each category of methods, as illus-\ntrated in Fig. 4.\n5.1\nParameter Reduction\nLoRA significantly reduces the number of tunable\nparameters for fine-tuning LLMs. However, it still\nrequires expensive activation memory to update\nlow-rank matrices. To further reduce the memory\ncost, existing methods reduce the number of tun-\nable parameters of LoRA via parameter freezing,\nparameter pruning, and parameter sharing.\n5.1.1\nParameter Freezing\nParameter freezing methods reduce the number of\ntunable parameters for LoRA via freezing some\nof its parameters. They can be divided into two\ncategories:\nintra-parameter methods and extra-\nparameter methods.\nThe intra-parameter methods tune a subset of\nparameters of LoRA while freezing the others.\nLoRA-SP [79] randomly selects half", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "A Survey on LoRA of Large Language Models", "description": "Arxiv Paper titled 'A Survey on LoRA of Large Language Models' authored by Yuren Mao, Yuhang Ge, Yijiang Fan, Wenyi Xu, Yu Mi, Zhonghao Hu, Yunjun Gao", "url": "https://arxiv.org/pdf/2407.11046", "source": "arxiv", "id": "04d0caa2-c4cd-4b48-8b79-c8284a74f0d6"}, "page_content": "; (3)\nparallel LoRA computing frameworks. This sec-\ntion introduces each category of methods, as illus-\ntrated in Fig. 4.\n5.1\nParameter Reduction\nLoRA significantly reduces the number of tunable\nparameters for fine-tuning LLMs. However, it still\nrequires expensive activation memory to update\nlow-rank matrices. To further reduce the memory\ncost, existing methods reduce the number of tun-\nable parameters of LoRA via parameter freezing,\nparameter pruning, and parameter sharing.\n5.1.1\nParameter Freezing\nParameter freezing methods reduce the number of\ntunable parameters for LoRA via freezing some\nof its parameters. They can be divided into two\ncategories:\nintra-parameter methods and extra-\nparameter methods.\nThe intra-parameter methods tune a subset of\nparameters of LoRA while freezing the others.\nLoRA-SP [79] randomly selects half of the LoRA\nparameters to freeze during fine-tuning.\nLoRA-\nFA [80]freezes the down-projection weights and\nupdates the up-projection weights in each layer of\nLoRA. AFLoRA [81] constructs a low-rank train-\nable path and gradually freezes parameters during\ntraining LoRA. Additionally, DropBP [82] accel-\nerates the training process by randomly dropping\nsome LoRA gradient calculations during back-\npropagation.\nBy contrast, the extra-parameter methods intro-\nduce and tune a set of extra parameters while\nfreezing the original parameters of LoRA. Most of\nthem are proposed based on Singular Value De-\ncomposition(SVD). LoRA-XS [83] adds a small\n14\nFront. Comput. Sci., 2024, 0(0): 1\u201330\nBFP\nA\nInput 1\n......\nPre-trained\u00a0\nWeights\nOutput\nInput 2\nInput 3\nScheduler\nLLM\nKV-Cache\nLoRA\nLoRA\n...\nGPU N\u00a0\nLLM\nKV-cache\nLoRA\nLoRA\n...\nGPU 0\u00a0\n......\nOutput 1\nOutput 2\nOutput 3\n......\nB\n0.21\n2.43\n1.92\n-0.81\n3.2\n-0.4\n1.2\n0.02\n1.47\n27\n142\n112\n1\n159\n13\n41\n19\n83\nWFP\nWINT\nAFP\nPre-trained\u00a0Weights\nQuantization\nA\nd\nb\nB\nA\nB\nParameter\u00a0\nFreezing\nParameter\u00a0\nPuring\nParameter\u00a0\nSharing\n(a) Parameter Reduction\n(b) Parameter Quantization\n(c) Parallel LoRA Computing Frameworks\nFrozen\nTrainable\nInput\nOutput\nInput\nFig. 4\nAn illustration of efficiency improving methods.\nr \u00d7 r weight matrix between frozen LoRA matri-\nces, which are constructed using the SVD of the\noriginal weight matrix; then it tunes only the r \u00d7 r\nweight matrices in fine-tuning. Similarly, BYOM-\nLoRA [58] adopts SVD to compress LoRA matri-\nces for multi-task models.\n5.1.2\nParameter Pruning\nParameter pruning methods aim to remove unim-\nportant LoRA parameters during training and in-\nference.\nThey prune parameters by either prun-\ning LoRA independently or jointly pruning LoRA\nand the LLM. LoRA-drop [84] uses the output\nof LoRA at each layer to evaluate the importance\nof parameters and prune the unimportant parame-\nters. By contrast, LoRAPrune [85] jointly pruning\nLoRA matrices and the LLM parameters based on\nLoRA\u2019s gradients. Besides, we can also use LoRA\nto support parameters pruning for LLMs [86,87].\n5.1.3\nParameter Sharing\nParameter-sharing methods reduce the number of\nparameters by sharing parameters across differ-\nent layers or modules of LLMs. VeRA [88] and\nVB-LoRA [89] are two representative parameter-\nsharing methods for LoRA. Specifically, VeRA\nproposes to share a pair of frozen random matri-\nces across all layers and conduct layer-wise adapta-\ntion with \u201cscaling vectors\u201d. By contrast, VB-LoRA\nproposes a \u201cdivide-and-share\u201d paradigm, which di-\nvides LoRA\u2019s low-rank decomposition by a rank-\none decomposition and achieves global sharing\nbased on an admixture model. Instead of sharing\nparameters in the original parameter space, Fouri-\nerFT [90] converts the incremental matrix \u2206W\ninto the spatial domain using Fourier transform. It\nshares spectral entries across all layers and only\nlearns its sparse spectral coefficients for each layer,\nthus reducing the number of trainable parameters.\n5.2\nParameter Quantization\nQuantization, which reduces the bit width of pa-\nrameters (e.g., from 32-bit floats to 4-bit integers),\ncan be used to reduce the memory and computa-\ntional cost of LoRA. Existing quantization-aware\nLoRA methods consist of post-training quantiza-\ntion (PTQ)-based methods and quantization-aware\ntraining (QAT)-based methods [95].\n5.2.1\nPTQ-based methods\nIn PTQ-based methods, we first quantize an LLM\n", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "A Survey on LoRA of Large Language Models", "description": "Arxiv Paper titled 'A Survey on LoRA of Large Language Models' authored by Yuren Mao, Yuhang Ge, Yijiang Fan, Wenyi Xu, Yu Mi, Zhonghao Hu, Yunjun Gao", "url": "https://arxiv.org/pdf/2407.11046", "source": "arxiv", "id": "20f210fe-bfe6-45d8-8348-f7bbe00527fa"}, "page_content": " admixture model. Instead of sharing\nparameters in the original parameter space, Fouri-\nerFT [90] converts the incremental matrix \u2206W\ninto the spatial domain using Fourier transform. It\nshares spectral entries across all layers and only\nlearns its sparse spectral coefficients for each layer,\nthus reducing the number of trainable parameters.\n5.2\nParameter Quantization\nQuantization, which reduces the bit width of pa-\nrameters (e.g., from 32-bit floats to 4-bit integers),\ncan be used to reduce the memory and computa-\ntional cost of LoRA. Existing quantization-aware\nLoRA methods consist of post-training quantiza-\ntion (PTQ)-based methods and quantization-aware\ntraining (QAT)-based methods [95].\n5.2.1\nPTQ-based methods\nIn PTQ-based methods, we first quantize an LLM\nand then fine-tune the quantized model, namely\nquantization and fine-tuning are sequentially con-\nducted.\nQLoRA [91] is the first PTQ-based\nYuren MAO et al. A Survey on LoRA of Large Language Models\n15\nquantization-aware LoRA method.\nIn the fine-\ntuning stage, it first quantizes an LLM to 4 bits\nand then fine-tunes a LoRA module on it with a\nhigher precision, such as BFloat16 or Float16. In\nthe inference stage, it dequantizes the LLM to the\nsame precision as LoRA and then adds the LoRA\nupdates to the LLM.\nAlthough QLoRA can significantly reduce mem-\nory cost for fine-tuning, it does not bring bene-\nfits for inference, because it requires dequantiz-\ning the LLM to high precision again.\nTo solve\nthis problem, QA-LoRA [92] is proposed to re-\nduce memory cost for both the fine-tuning and in-\nference stages. QA-LoRA uses group-wise opera-\ntors to balance the degrees of freedom of the LLM\nquantization and fine-tuning, which enables it to\nobtain a LoRA module having identical precision\nwith the quantized LLM. Thus, it can perform in-\nference without dequantization.\n5.2.2\nQAT-based methods\nIn QAT-based methods, we jointly quantize and\nfine-tune an LLM, namely quantization and fine-\ntuning are simultaneously conducted. These meth-\nods can alleviate the quantization discrepancies ob-\nserved in PTQ-based methods.\nTo address the\nquantization discrepancy of QLoRA, LoftQ [93]\nalternatively applies quantization and low-rank ap-\nproximation during fine-tuning to minimize the\nquantization error. However, ApiQ [94] points out\nthat LoftQ ignores the error propagation across lay-\ners and proposes activation-preserved initialization\nto avoid error propagation. Besides, L4Q [95] is\nanother QAT-based method that has an advanced\nlayer design.\n5.3\nParallel LoRA Computing Frameworks\nLoRA\u2019s parameter-efficient nature enables us to\nfine-tune or infer multiple modules on a single\nGPU or a GPU cluster, which can save compu-\ntational resources and improve the efficiency of\nLoRA. This section introduces the parallel fine-\ntuning and parallel inference frameworks, respec-\ntively.\n5.3.1\nParallel Fine-tuning\nParallelly fine-tuning multiple LoRA modules on\na single GPU can reduce GPU memory usage and\nimprove computation efficiency. ASPEN [96] pro-\nposes a high-throughput parallel finetuning frame-\nwork for LoRA, which consists of a BatchFu-\nsion approach and an adaptive job scheduling algo-\nrithm. Specifically, the BatchFusion approach sup-\nports parallelly fine-tuning multiple LoRA mod-\nules on a shared LLM by fusing multiple input\nbatches into a single batch, while the adaptive\njob scheduling algorithm allocates computation re-\nsources to the fine-tuning jobs.\n5.3.2\nParallel Inference\nParallel inference framework for LoRA can not\nonly improve the computational efficiency but also\nsupport the needs of multi-tenant service. Punica\n[97] uses a new CUDA kernel design to batch GPU\noperations for different LoRA modules. Based on\nPunica, S-LoRA [98] further optimizes the par-\nallel inference framework by introducing a uni-\nfied paging mechanism and a new tensor paral-\nlelism strategy, which enables the service of thou-\nsands of concurrent LoRA modules. Then, based\non Punica and S-LoRA, CARASERVE [99] re-\nduces the cold-start overhead and further improves\nthe service efficiency and SLO (service-level ob-\n16\nFront. Comput. Sci., 2024, 0(0): 1\u201330\njective) attainment rates by CPU-GPU cooperation\nand rank-aware scheduling.\n6\nLoRA for Federated Learning\nWhen adapting LLMs to vertical domains such as\nmedicine and finance, the available training data\ncan be privately owned by multiple clients. In this\nscenario, the training data is not centralized, and\nwe have to fine-tune LLMs while keeping the data\nlocal", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "A Survey on LoRA of Large Language Models", "description": "Arxiv Paper titled 'A Survey on LoRA of Large Language Models' authored by Yuren Mao, Yuhang Ge, Yijiang Fan, Wenyi Xu, Yu Mi, Zhonghao Hu, Yunjun Gao", "url": "https://arxiv.org/pdf/2407.11046", "source": "arxiv", "id": "29d93d85-561d-4eb6-bbc2-faab823d1271"}, "page_content": "] further optimizes the par-\nallel inference framework by introducing a uni-\nfied paging mechanism and a new tensor paral-\nlelism strategy, which enables the service of thou-\nsands of concurrent LoRA modules. Then, based\non Punica and S-LoRA, CARASERVE [99] re-\nduces the cold-start overhead and further improves\nthe service efficiency and SLO (service-level ob-\n16\nFront. Comput. Sci., 2024, 0(0): 1\u201330\njective) attainment rates by CPU-GPU cooperation\nand rank-aware scheduling.\n6\nLoRA for Federated Learning\nWhen adapting LLMs to vertical domains such as\nmedicine and finance, the available training data\ncan be privately owned by multiple clients. In this\nscenario, the training data is not centralized, and\nwe have to fine-tune LLMs while keeping the data\nlocalized, namely federated learning. In federated\nlearning, the clients typically compute weight up-\ndates locally and then share these updates with oth-\ners to globally update the LLM. It brings both com-\nmunication and computation costs for the clients.\nFortunately, LoRA is parameter efficient and plug-\ngable, which can reduce communication costs and\nlower computational resource requirements. LoRA\ncan enhance the overall efficiency and scalability of\nfederated learning.\nHowever, adopting LoRA in federated learning is\nnot trivial for federated learning faces challenges\nsuch as data heterogeneity, device heterogeneity,\nand model heterogeneity. To address these issues,\nrecent studies have designed various methods for\nLoRA to meet the diverse needs of federated learn-\ning, as shown in Fig. 5. Additionally, as a localized\nparameter component, LoRA\u2019s pluggable nature al-\nlows it to support parameter privacy protection in\nfederated learning.\n6.1\nData Heterogeneity\nData heterogeneity refers to differences in data dis-\ntribution across clients. In federated learning, dif-\nferent clients usually have different data distribu-\ntions. The inconsistency in data distribution affects\nthe overall performance of the model. Research\nreveals that in federated learning, as user data be-\ncomes more diverse, the performance gap between\nLoRA and full fine-tuning widens [100]. To ad-\ndress this issue, researchers have proposed several\nimprovement methods.\nSLoRA [100] introduces a data-driven initializa-\ntion method for LoRA. It first performs sparse fed-\nerated fine-tuning before applying LoRA and then\nperforms SVD to decompose the accumulated gra-\ndient updates into low-rank matrices for LoRA ini-\ntialization. The goal is to enable the LoRA mod-\nules to better adapt to the data distribution of each\nclient, thereby integrating these heterogeneous data\ncharacteristics into the global model more effec-\ntively.\nFeDeRA [101] uses a simpler initializa-\ntion method. It directly applies SVD to pre-trained\nweights to initialize LoRA. Retaining the principal\ncomponents of the pre-trained weights aligns the\ndirection and magnitude of weight updates across\ndifferent clients to handle data heterogeneity. Ad-\nditionally, FFA-LoRA [102] freezes one low-rank\nmatrix and fine-tunes only the other. This reduces\ninconsistency during server aggregation of LoRA\ngradients, alleviating the optimization instability\ncaused by non-IID data.\n6.2\nDevice Heterogeneity\nDevice heterogeneity refers to the differences in\nhardware capabilities, and network connectivity\namong clients participating in federated learning.\nTraditional federated learning methods often en-\ncounter the \u201cbuckets effect\u201d, implying that the sys-\ntem\u2019s overall performance is limited by the capabil-\nity of the least powerful client. Specifically, these\nmethods use the smallest LoRA rank to accommo-\ndate all clients, which prevents many resource-rich\nclients from fully utilizing their potential.\nYuren MAO et al. A Survey on LoRA of Large Language Models\n17\nServer\nClient 2\nClient 3\nClient 1\n(a) Data Heterogeneity\nServer\nClient 2\nClient 3\nClient 1\n(c) Model Heterogeneity\nPrivate Params\nPrivate Data\nServer\nClient\nShared Params\nTEE\n(d) Parameter Privacy\nServer\nClient 2\nClient 3\nClient 1\n(b) Device Heterogeneity\nFig. 5\nAn illustration of LoRA for federated learning.\nTo address this issue, a dynamic parameter allo-\ncation strategy can be adopted. FedMS [103] dy-\nnamically adjusts the number of activated LoRA\nmatrices based on the real-time computational re-\nsources of clients.\nFlexLoRA [104] uses a dy-\nnamic parameter allocation strategy. It adjusts the\nLoRA rank and redistributes the SVD components\nof the global LoRA weights based on resource con-\nstraints. Similarly, HETLORA [105] assigns dif-\nferent ranks for different clients. However, it per-\nforms weighted aggregation according to the spar-\nsity of the updates from different clients, balancing\nupdate information better than simple aggregation.\n6.3\nModel Heterogeneity\nModel heterogeneity indicates differences in model\nstructures among clients. In traditional federated\nlearning, clients use local models with the same\narchitecture, allowing their parameters to be ag-\ngregated into a global model on the server.", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "A Survey on LoRA of Large Language Models", "description": "Arxiv Paper titled 'A Survey on LoRA of Large Language Models' authored by Yuren Mao, Yuhang Ge, Yijiang Fan, Wenyi Xu, Yu Mi, Zhonghao Hu, Yunjun Gao", "url": "https://arxiv.org/pdf/2407.11046", "source": "arxiv", "id": "a01d6167-949d-4bf4-849c-e327f52e59aa"}, "page_content": "cation strategy can be adopted. FedMS [103] dy-\nnamically adjusts the number of activated LoRA\nmatrices based on the real-time computational re-\nsources of clients.\nFlexLoRA [104] uses a dy-\nnamic parameter allocation strategy. It adjusts the\nLoRA rank and redistributes the SVD components\nof the global LoRA weights based on resource con-\nstraints. Similarly, HETLORA [105] assigns dif-\nferent ranks for different clients. However, it per-\nforms weighted aggregation according to the spar-\nsity of the updates from different clients, balancing\nupdate information better than simple aggregation.\n6.3\nModel Heterogeneity\nModel heterogeneity indicates differences in model\nstructures among clients. In traditional federated\nlearning, clients use local models with the same\narchitecture, allowing their parameters to be ag-\ngregated into a global model on the server. How-\never, in practice, clients may prefer unique local\nmodel architectures due to personal needs and of-\nten do not want to disclose model details. Thus,\nit is necessary to transfer knowledge between het-\nerogeneous models without sharing private data or\nrevealing local model structures [215].\nPrevious work has used knowledge distillation,\nmodel ensembling, and mutual learning to ad-\ndress model heterogeneity. However, these meth-\nods have limitations, such as reliance on public\ndatasets, additional communication costs and poor\nlocal model performance. To avoid these limita-\ntions, pFedLoRA [106] uses LoRA as a carrier\nof both global and local knowledge. It adopts an\niterative training strategy to facilitate knowledge\ntransfer and integration, enabling knowledge shar-\ning among heterogeneous models across different\nclients.\n6.4\nParameter Privacy\nIn federated learning, protecting client-specific pa-\nrameters is crucial because ensuring the privacy of\nthese parameters also indirectly safeguards client\ndata privacy.\nAs a modular approach to adjust-\ning personalized parameters, LoRA can be effec-\ntively integrated into federated learning systems to\nachieve parameter privacy protection.\nLiterature [107] proposes a secure distributed lan-\nguage model training framework based on model\nslicing.\nThey deploy LoRA in a Trusted Exe-\ncution Environment (TEE) and use OTP encryp-\ntion to transmit features between the GPU and\nTEE, protecting model parameter privacy.\nPri-\nvateLoRA [108] introduces a distributed system\nbased on LoRA. It adds a square matrix M between\nlow-rank matrices A and B. The non-trainable ma-\n18\nFront. Comput. Sci., 2024, 0(0): 1\u201330\ntrices A and B, along with most of the pre-trained\nweights, are deployed on the global server to en-\nhance computation. Meanwhile, the trainable ma-\ntrix M is stored on the client as personalized pa-\nrameters, thus ensuring parameter privacy protec-\ntion.\nFurthermore, recent works have integrated differ-\nential privacy (DP) techniques with LoRA in fed-\nerated learning to enhance data privacy.\nDP-\nLoRA [216] ensures differential privacy by adding\nGaussian noise to LoRA\u2019s weight updates during\nthe update process. This approach maintains pri-\nvacy and improves communication efficiency. To\nsolve the noise amplification when applying differ-\nential privacy in LoRA, FFA-LoRA [102] fixes the\nmatrix A, avoiding the local semi-quadratic struc-\nture and enhancing robustness and performance.\n7\nApplications of LoRA\nIn the rapidly evolving field of deep learning,\nLoRA has become widely used due to its unique\nadvantages.\nResearchers utilize LoRA to fine-\ntune pre-trained models for various downstream\ntasks, reducing computational resource require-\nments while enhancing performance.\nLoRA\u2019s\nstrong adaptability and efficiency have signifi-\ncantly improved various applications. In this sec-\ntion, we will introduce LoRA\u2019s applications in the\nfollowing scenarios: (1) language tasks; (2) vision\ntasks; (3) multimodal tasks.\n7.1\nLanguage Tasks\nRecently, the rapid development of pre-trained lan-\nguage models, especially LLMs, is revolutionizing\nthe approach to language tasks due to their out-\nstanding performance. However, these pre-trained\nmodels are trained on a large amount of general\ndata and still require further fine-tuning on task-\nspecific data to adapt to downstream tasks. There-\nfore, it is natural to use LoRA to fine-tune these\npre-trained language models, as it reduces compu-\ntational resource requirements. We mainly focus\non some representative downstream tasks, which\ninclude traditional NLP tasks, code tasks, model\nalignment and vertical domain tasks.\n7.1.1\nTraditional NLP Tasks\nGiven the strong instruction-following and con-\ntextual understanding abilities of LLMs, some re-\nsearches apply LoRA to fine-tune these models\nfor traditional NLP tasks. For example, LoRA is\nwidely adopted in LLaMA for various tasks, such\nas emotion recognition [109], text classification\n[110] and role recognition [111]. AutoRE [112]\napplies QLoRA to three document-level relation\nextraction tasks, achieving great performance on\ndifferent LLMs.\nSome studies [113\u2013115] lever-\n", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "A Survey on LoRA of Large Language Models", "description": "Arxiv Paper titled 'A Survey on LoRA of Large Language Models' authored by Yuren Mao, Yuhang Ge, Yijiang Fan, Wenyi Xu, Yu Mi, Zhonghao Hu, Yunjun Gao", "url": "https://arxiv.org/pdf/2407.11046", "source": "arxiv", "id": "2a797502-1c74-4d8e-b2a9-b774017b60ba"}, "page_content": "\nfore, it is natural to use LoRA to fine-tune these\npre-trained language models, as it reduces compu-\ntational resource requirements. We mainly focus\non some representative downstream tasks, which\ninclude traditional NLP tasks, code tasks, model\nalignment and vertical domain tasks.\n7.1.1\nTraditional NLP Tasks\nGiven the strong instruction-following and con-\ntextual understanding abilities of LLMs, some re-\nsearches apply LoRA to fine-tune these models\nfor traditional NLP tasks. For example, LoRA is\nwidely adopted in LLaMA for various tasks, such\nas emotion recognition [109], text classification\n[110] and role recognition [111]. AutoRE [112]\napplies QLoRA to three document-level relation\nextraction tasks, achieving great performance on\ndifferent LLMs.\nSome studies [113\u2013115] lever-\nage LoRA from different perspectives to enhance\nthe model\u2019s capability in machine translation tasks.\nAdditionally, LoRA can also improve the perfor-\nmance of models like BERT and T5 for text under-\nstanding tasks [116,117].\n7.1.2\nCode Tasks\nSome researchs apply LoRA to improve model per-\nformance in various code-related tasks. For exam-\nple, BERT-style models fine-tuned with LoRA are\nsuitable for code-change-related tasks, specifically\nin Just-In-Time defect prediction (JIT-DP) [118,\n119].\nSimilarly, training CodeT5 and PLBART\nwith LoRA can enhance their adaptability for code\nsummarization and code clone detection [120].\nAs for the decoder-only model, RepairLLaMA\n[121] uses LoRA to fine-tune Llama for automated\nprogram repair (APR), while WizardCoder-15B is\nYuren MAO et al. A Survey on LoRA of Large Language Models\n19\nfine-tuned with LoRA for Text-to-SQL task [122].\nAdditionally, SteloCoder [123], a fine-tuned ver-\nsion of StarCoder, is designed for multi-language\nto Python code translation.\n7.1.3\nModel Alignment Tasks\nModel alignment tasks focus on adjusting a ma-\nchine learning model to align with human val-\nues and intentions, often using techniques like\nReinforcement Learning from Human Feedback\n(RLHF). To reduce memory requirements of\nRLHF, some studies use LoRA to fine-tune the re-\nward model and policy model [124\u2013126]. Further-\nmore, other works improve reward models by in-\ntegrating multiple LoRA adapters. For example,\nDMoERM [127] combines MoE with LoRA, rout-\ning model inputs to multiple LoRA experts while\nanother work [128] proposes a LoRA-based en-\nsemble method as well. The integration can also\nbenefit the quantification of uncertainty in reward\nmodels [129].\nBesides, literature [130] applies\nLaplace-LoRA [131] to train Bayesian reward\nmodels, which mitigates reward overoptimization\nin best-of-n sampling.\n7.1.4\nVertical Domain Tasks\nLLMs often perform suboptimally in vertical do-\nmains, requiring fine-tuning with domain-specific\nexpertise. Some works apply LoRA to improve the\nperformance of LLMs on domain-specific tasks.\nFor example, some studies fine-tune LLMs on\nmedical datasets with LoRA to adapt them to the\nmedical domain [132\u2013134].\nAdditionally, other\nstudies improve medical tasks like clinical dialogue\nsummarization [135], assertion detection [136] and\nmedical QA tasks [137, 138].\nSimilarly, several\nstudies fine-tune LLMs with LoRA on financial\ndata to solve tasks such as financial news analytics\nand sentiment classification [139\u2013142]. Besides,\nLoRA can also be used to enhance the performance\nin database tasks like query rewrite and index tun-\ning [143].\n7.2\nVision Tasks\nIn vision tasks, LoRA is primarily applied to im-\nage generation and image segmentation, signifi-\ncantly improving training efficiency and optimiz-\ning model performance.\n7.2.1\nImage Generation\nImage generation tasks hold significant importance\nin the field of computer vision. In recent years, dif-\nfusion model have demonstrated exceptional per-\nformance in image generation tasks.\nLoRA is\nwidely used in diffusion models to address vari-\nous image generation tasks while reducing com-\nputational resources.\nSome works use LoRA to\nfine-tune diffusion models for image style transfer\n[144\u2013148], while others apply it to text-to-image\ngeneration [149\u2013153].\nFurthermore, researchers have designed several\nLoRA-based methods to improve image gener-\nation quality.\nFor instance, Smooth Diffusion\n[154] uses LoRA to achieve smoothness in the la-\ntent space, leading to better performance in var-\nious image generation and editing tasks.\nRe-\nsAdapter [155] employs LoRA to learn resolu-\ntion priors, adjusting the receptive fields of con-\nvolutional layers to dynamical resolution. Addi-\ntionally, to specifically enhance text-to-image qual-\nity, STAMINA [156] uses LoRA to fine-", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "A Survey on LoRA of Large Language Models", "description": "Arxiv Paper titled 'A Survey on LoRA of Large Language Models' authored by Yuren Mao, Yuhang Ge, Yijiang Fan, Wenyi Xu, Yu Mi, Zhonghao Hu, Yunjun Gao", "url": "https://arxiv.org/pdf/2407.11046", "source": "arxiv", "id": "9ad10a1a-6563-4e76-a14c-2e64d664318e"}, "page_content": "widely used in diffusion models to address vari-\nous image generation tasks while reducing com-\nputational resources.\nSome works use LoRA to\nfine-tune diffusion models for image style transfer\n[144\u2013148], while others apply it to text-to-image\ngeneration [149\u2013153].\nFurthermore, researchers have designed several\nLoRA-based methods to improve image gener-\nation quality.\nFor instance, Smooth Diffusion\n[154] uses LoRA to achieve smoothness in the la-\ntent space, leading to better performance in var-\nious image generation and editing tasks.\nRe-\nsAdapter [155] employs LoRA to learn resolu-\ntion priors, adjusting the receptive fields of con-\nvolutional layers to dynamical resolution. Addi-\ntionally, to specifically enhance text-to-image qual-\nity, STAMINA [156] uses LoRA to fine-tune diffu-\nsion models for longer concept sequences. Dream-\nSync [157] and StyleAdapter [158] use LoRA\nto improve text fidelity and image quality. Mix-\nof-Show [159] captures out-of-domain informa-\n20\nFront. Comput. Sci., 2024, 0(0): 1\u201330\ntion with LoRA weights to combine multiple cus-\ntomized concepts with high fidelity, reducing con-\ncept conflicts. Other studies combine LoRA with\nmodel distillation to accelerate image generation\n[160, 161]. Moreover, LoRA can also be applied\nto video generation [162\u2013167] and 3D generation\ntasks [168\u2013172].\n7.2.2\nImage Segmentation\nImage segmentation is a significant challenge in\ncomputer vision, aiming to divide an image into\nmultiple meaningful regions or objects. To address\nthis, SAM has been proposed as a foundational\nmodel for image segmentation and demonstrated\nsuperior generalization ability. To further enhance\nits performance in specific vertical domains, many\nstudies utilize LoRA to fine-tune it. For instance,\nin license plate detection, SamLP [173] utilizes\nLoRA to adapt SAM for efficient segmentation of\nlicense plates. In structural damage detection, liter-\nature [174] fine-tunes SAM\u2019s encoder using LoRA\nfor instance segmentation task. In the medical do-\nmain, many studies also apply LoRA to fine-tune\nSAM for a variety of tasks, including nuclei seg-\nmentation [175], OCTA image segmentation [176],\nbrain tumor segmentation [177], organ segmen-\ntation [178], and surgical instrument segmenta-\ntion [179]. Additionally, some studies use LoRA\nto fine-tune Vision Transformer (ViT) for visual\ntracking [180] and face forgery detection [181].\n7.3\nMultimodal Tasks\nMultimodal Large Language Models (MLLMs)\naim to integrate text with various modalities such\nas audio, image and video, which enable cross-\nmodal understanding and reasoning through a uni-\nfied embedding space. The success of LoRA in\nboth NLP and vision tasks has sparked consider-\nable interest in applying them to MLLMs.\nIn MLLMs, LoRA can not only improve train-\ning efficiency but also facilitate effective modal-\nity alignment. In audio-text tasks, SALM [182]\ncomprises LoRA layers, a frozen text-based LLM,\nan audio encoder and a modality adapter to han-\ndle speech inputs and corresponding task in-\nstructions.\nFor image-text tasks, InternLM-\nXComposer2 [183] achieves modality alignment\nby applying LoRA to image tokens, mPLUG-Owl\n[184] freezes the visual module while jointly fine-\ntuning LoRA and abstractor of the text module,\nand CoLLaVO [185] employs QLoRA to preserve\nobject-level image understanding. In the realm of\nvideo-text tasks, VSP-LLM [186] fine-tunes the\ntext module with QLoRA for visual speech pro-\ncessing, MolCA [187] uses LoRA to understand\n2D molecular graphs and text, while TPLLM\n[188] employs LoRA for efficient traffic prediction\nby integrating sequence and spatial features. These\napplications demonstrate the versatility and power\nof LoRA in MLLMs tasks.\n8\nConclusion and Future Direction\nIn this survey, the recent progress of LoRA have\nbeen systematically reviewed from the perspec-\ntive of downstream adaptation improving, cross-\ntask generalization, efficiency improving, feder-\nated learning and applications. From this review,\nwe can find that LoRA is parameter efficient, plug-\ngable, compatible and easy to achieve cross-task\ngeneralization, which enables it to be one of the\nmost important technology for LLMs applications.\nRecent progress further boosts the generalization\nand efficiency of LoRA, and stimulate its potential\nto be used in more scenarios. Here, we list three fu-\nture directions where LoRA will be indispensable.\nYuren MAO et al. A Survey on LoRA of Large Language Models\n21\n8.1\nLoRA for GaaS\nIn Generative-as-a-Service (GaaS), cloud-based\nplatforms provide users with generative artificial\nintelligence (AGI) services.\nGaaS enables users\nenjoy AGI without deploying local computational\nresources. For the users\u2019 needs are diverse, it is\nnecessary to provides various functions for GaaS.", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "A Survey on LoRA of Large Language Models", "description": "Arxiv Paper titled 'A Survey on LoRA of Large Language Models' authored by Yuren Mao, Yuhang Ge, Yijiang Fan, Wenyi Xu, Yu Mi, Zhonghao Hu, Yunjun Gao", "url": "https://arxiv.org/pdf/2407.11046", "source": "arxiv", "id": "f771a167-6d11-4b7e-aa9f-0730be2013b1"}, "page_content": ". From this review,\nwe can find that LoRA is parameter efficient, plug-\ngable, compatible and easy to achieve cross-task\ngeneralization, which enables it to be one of the\nmost important technology for LLMs applications.\nRecent progress further boosts the generalization\nand efficiency of LoRA, and stimulate its potential\nto be used in more scenarios. Here, we list three fu-\nture directions where LoRA will be indispensable.\nYuren MAO et al. A Survey on LoRA of Large Language Models\n21\n8.1\nLoRA for GaaS\nIn Generative-as-a-Service (GaaS), cloud-based\nplatforms provide users with generative artificial\nintelligence (AGI) services.\nGaaS enables users\nenjoy AGI without deploying local computational\nresources. For the users\u2019 needs are diverse, it is\nnecessary to provides various functions for GaaS.\nTo implement the various functions, we can con-\nstruct a LoRA module for each function. The pra-\nmameter efficiency and plugability of LoRA can\nfacilitate efficient functions\u2019 construction and ex-\necution. Besides, the services on GaaS platforms\ncan change rapidly alonging time. To follow the\nchanges, we can train new LoRA modules that ini-\ntialized by combination of previous LoRA mod-\nules. The cross-task generalization ability of LoRA\ncan facilitate fast adaption to service updations.\n8.2\nLoRA for Continued Pre-training\nIn continued pre-training, a foundation model is\ncontinuely trained with unlabeled user data to adapt\nthe model to specific domains. Typically, the self-\nsupervised training objective is same with that for\npre-training, and the learning rate is much smaller\nthan than for pre-training. Continued pre-training\nis a important stage for constructing vertical do-\nmain LLMs. However, it is highly computational\nexpensive, which impedes the development of ver-\ntical domain LLMs, especailly for the organiza-\ntions with limited computational resources.\nEn-\nhancing LoRA for continued pre-training and re-\nducing its computational cost is worth to explored.\n8.3\nLoRA for Autonomous Agents\nIn LLM-based autonomous agents, the agents are\nassigned with specific roles.\nBased the roles\nand environment, agents make actions to response\nusers\u2019 or other agents\u2019 request. The actions can\nbe made based on self-knowledge or tools that de-\nsigned for domain-specific tasks. The request and\nthe actions are stored in memory to support the fu-\nture requests.\nIn the current agents, the roles are typically as-\nsigned by prompts; however, prompt may cannot\ngive a comprehensive discription of the role when\nthe role is complex and the number of related data\nis large. Assiging roles with LoRA modules train-\ning from data related to the roles can be a better\nchoice.\nFurthermore, the tools for agent can be\nLoRA modules. Besides, the memory usually aug-\nments the agents with retrieval augmented genera-\ntion (RAG); however, due to the input token lim-\nitation and the short-comings of in-context learn-\ning, the RAG-based support may be less effec-\ntive. By contrast, we can use LoRA-based contin-\nual learning to construct memory modules, which\ncan solve the problem of RAG. Therefore, LoRA-\ndriven agents are worth to explore.\nAcknowledgements\nThis work was supported in part\nby the NSFC under Grants No.\n(62025206, 62302436,\nU23A20296), Zhejiang Province\u2019s \u201dLingyan\u201d R&D Project\nunder Grant No.\n2024C01259, and Ningbo Science and\nTechnology Special Projects under Grant No.\n2023Z212.\nYunjun Gao is the corresponding author of this work.[SEP]", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey", "description": "Arxiv Paper titled 'Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey' authored by Zeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang, Sai Qian Zhang", "url": "https://arxiv.org/pdf/2403.14608", "source": "arxiv", "id": "ebea3d72-55fb-4c05-941d-a7cdc65e7009"}, "page_content": "[CLS]\u2014Large models represent a groundbreaking advance-\nment in multiple application fields, enabling remarkable achieve-\nments across various tasks. However, their unprecedented scale\ncomes with significant computational costs. These models, often\nconsisting of billions of parameters, require vast amounts of\ncomputational resources for execution. Especially, the expansive\nscale and computational demands pose considerable challenges\nwhen customizing them for particular downstream tasks, particu-\nlarly over the hardware platforms constrained by computational\ncapabilities.\nParameter Efficient Fine-Tuning (PEFT) provides a practical\nsolution by efficiently adjusting the large models over the various\ndownstream tasks. In particular, PEFT refers to the process of\nadjusting the parameters of a pre-trained large model to adapt\nit to a specific task or domain while minimizing the number\nof additional parameters introduced or computational resources\nrequired. This approach is particularly important when dealing\nwith large-scale language models with high parameter counts,\nas fine-tuning these models from scratch can be computationally\nexpensive and resource-intensive, posing considerable challenges\nin the supporting system platform design.\nIn this survey, we present comprehensive studies of various\nPEFT algorithms, examining their performance and computa-\ntional overhead. Moreover, we provide an overview of applica-\ntions developed using different PEFT algorithms and discuss\ncommon techniques employed to mitigate computation costs\nfor PEFT. In addition to providing an extensive survey from\nan algorithmic standpoint, we also examine various real-world\nsystem designs to investigate the implementation costs associated\nwith different PEFT approaches. This survey serves as a valuable\nresource for researchers aiming to understand both the PEFT al-\ngorithm and its system implementation, offering detailed insights\ninto recent advancements and practical applications.\nIndex Terms\u2014Large Language Model, Parameter-Efficient\nFine-tuning, Computer System, Distributed System.\nI. INTRODUCTION\nLarge Models (LMs) have recently captured considerable\npublic interest. Their ability to understand context and nuances\nenables them to proficiently handle diverse tasks across mul-\ntiple domains, including natural language processing (NLP),\ncomputer vision (CV), etc. In the field of NLP, Large Lan-\nguage Models (LLMs) have achieved significant advance-\nments across various tasks including text generation [1], [2],\ntranslation [3], [4], personalized chat-bots [5], [6], [7], and\nsummarization [8], demonstrating remarkable proficiency.\n* Corresponding author\nEarlier studies [1] have suggested that LLMs exhibit high\nlevels of generalization, enabling them to apply their acquired\nknowledge to new tasks not included in their original training.\nThis capability is commonly known as zero-shot learning.\nNevertheless, fine-tuning remains essential to further enhance\nLLMs for optimal performance on new user datasets and tasks.\nDue to its scale, a widely adopted strategy for fine-tuning\nLLMs involves adjusting a limited number of LLM parame-\nters while keeping the remainder unchanged. This technique,\ntermed Parameter-Efficient-Fine-Tuning (PEFT), involves se-\nlectively adjusting a small proportion of their parameters while\nkeeping the rest unaltered. Furthermore, the application of\nPEFT extends beyond the realm of NLP and quickly attracts\ninterest in the CV community for handling fine-tuning vision\nmodels with large parameters, such as Vision Transformers\n(ViT) and diffusion models, as well as disciplinary models\nsuch as vision-language models.\nIn this survey, we systematically review and categorize\nrecent advancements in PEFT algorithms as well as the system\nimplementation costs associated with various PEFT algorithms\nacross diverse scenarios. Figure 1 presents the overview con-\ntent for this survey. In section II, we present some fundamental\nconcepts for LLM and PEFT, including computational flow\nfor LLM, basic knowledge of PEFT, commonly used datasets\nand tasks, and evaluation benchmarks. We categorize all\ntypes of PEFT algorithms in Section III according to their\ncomputational flow. In Section III-A, we detail additive algo-\nrithms that either introduce new weight parameters or modify\nactivations. Algorithms that only require fine-tuning of existing\nparameters are categorized as selective approaches, which are\nintroduced in Section III-B. In Section III-C, we explore\nreparameterized PEFT, which constructs a (low- dimensional)\nreparameterization of original model parameters for training\nwhile transforming the weights back to maintain the inference\nspeed. Additionally, there exist algorithms that combine the\nabove techniques, and we have classified these as hybrid\napproaches, elaborating on them in Section III-D. We also\ninvestigate strategies for further reducing the computational\ncomplexity of different PEFT algorithms, including KV-cache\nmanagement, pruning, quantization, and memory optimization,\nin Section IV.\nIn Section V, we expand the scope of this survey beyond\nthe computational perspective to involve various potential\napplication scenarios. Specifically, we explore innovations that\narXiv:2403.14608v7  [cs.LG]  16 Sep 2024\n2\nBackground\nComputational \nflow for LLM\nPEFT \nTaxonomy\nSelective\nPEFT\nAdditive\nPEFT\nSystem Design\nChallenge\nSystem Design \nfor PEFT\nCentralized PEFT \nServing System\nPEFT for \nLLMs\nDistributed PEFT \nTraining System\nHybrid\nPEFT\nPEFT\noverview\nReparameterized \nPEFT\nEfficient PEFT\nDesign\nKV-cache \nManagement for", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey", "description": "Arxiv Paper titled 'Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey' authored by Zeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang, Sai Qian Zhang", "url": "https://arxiv.org/pdf/2403.14608", "source": "arxiv", "id": "125b716e-e8a6-4f87-a584-2f2301925dd8"}, "page_content": "investigate strategies for further reducing the computational\ncomplexity of different PEFT algorithms, including KV-cache\nmanagement, pruning, quantization, and memory optimization,\nin Section IV.\nIn Section V, we expand the scope of this survey beyond\nthe computational perspective to involve various potential\napplication scenarios. Specifically, we explore innovations that\narXiv:2403.14608v7  [cs.LG]  16 Sep 2024\n2\nBackground\nComputational \nflow for LLM\nPEFT \nTaxonomy\nSelective\nPEFT\nAdditive\nPEFT\nSystem Design\nChallenge\nSystem Design \nfor PEFT\nCentralized PEFT \nServing System\nPEFT for \nLLMs\nDistributed PEFT \nTraining System\nHybrid\nPEFT\nPEFT\noverview\nReparameterized \nPEFT\nEfficient PEFT\nDesign\nKV-cache \nManagement for \nPEFT Efficiency\nPEFT Pruning\nPEFT \nQuantization\nMemory-efficient \nPEFT\nParallel PEFT \nTraining System\nApply PEFT for \nother Applications\nPEFT for \nViTs\nPEFT for \nVLAs\nPEFT for \nDiffusion Models\nDownstream\ntasks\nSection 2\nSection 3\nSection 4\nSection 5\nSection 6\n2.1\n2.2\n2.3\n3.1\n3.2\n3.3\n3.4\n4.1\n4.2\n4.3\n4.4\n5.1\n5.2\n5.3\n5.4\n6.1\n6.2\n6.3\n6.4\nFig. 1: A content overview covered in the survey.\napplying PEFT techniques to different model architecture,\nincluding LLMs (Section V-A), Vision Transformer (Sec-\ntion V-B), Vision-Language alignment models (Section V-C),\nand Diffusion models (Section V-D), for varied downstream\ntasks, underscoring PEFT\u2019s versatility and applicability in a\nrange of scenarios. After that, in Section VI, we explore the\nsystem design challenge for PEFT methods. The discussion\nincludes three advanced system solutions for practical PEFT\ndeployment: PEFT query serving (Section VI-B), distributed\ntuning (Section VI-C), and concurrent PEFT tuning (Sec-\ntion VI-D). Finally, in Section VII, we summarize our survey\nand propose several potential future directions from both\nalgorithmic and systemic perspectives, aiming to offer valuable\ninsights for further research and development in the field.\nII. BACKGROUND\nIn this section, we first discussed the computation flow of\nLLM, including its fundamental components, computational\ncomplexity, and the flow of computations it involves as a case\nstudy. We then provide a brief overview of different PEFT\nalgorithms in section II-B.\nA. Computation flow for LLaMA\nIn order to gain a deeper understanding of LLM and other\nTransformer-based models, we employ LLaMA-7B, a cutting-\nedge open-source LLM model, to scrutinize the architecture\nof LLM as well as Transformer. As shown in Figure 2 (a),\nLLaMA consists of three major components: an embedding\nblock, a stack of decoder blocks, and a head block which\nconsists of linear and softmax layer. The embedding layer\u2019s\nprimary role is to transform unstructured textual information,\ninto chunks of discrete numerical vectors (tokens) to facilitate\nsubsequent processing. The embedded tokens are then deliv-\nered to the decoder layers for further processing. Each LLaMA\ndecoder is composed of two fundamental components: Multi-\nhead Self-Attention (MSA) and Feedforward Network (FFN).\nIn the MSA module, each of the tokens will be clustered by\nan attention map obtained by a dot production between two\nlinear mappings of the input tokens. Then the grouped tokens\nwill be further processed by a Feedforward Neural network.\nAdditionally, Root Mean Square Layer Normalization (RM-\nSNorm) [9] is adopted in LLaMA as a replacement for Layer\nNormalization to ensure efficient training.\nLLM distinguishes itself from other deep neural network\n(DNN) models such as convolutional neural networks (CNN)\nin two significant ways. Firstly, LLM exhibits an inherent\nautoregressive nature, necessitating multiple iterations to com-\nplete the generation task. Moreover, LLM incorporates an\nattention mechanism, a component with computational com-\nplexity that scales quadratically with the length of the inputs.\nOn the other hand, the inherent computation characteristic of\nLLM lies in the attention blocks inside each decoder layer.\nFigure 2 (c) depicts the high-level overview of the computation\nflow in the attention block.\nDuring the inference process, each decoder takes a three-\ndimensional tensor x P Rb\u02c6l\u02c6d as the input tokens. The\ninput tokens are first multiplied with three weight matrices\nWQ, WK, and WV, producing the output referred to as\nquery(Q), key(K) and value(V ). Given the MSA module\u2019s\ninability to recognize positional data and the inherent auto-\nregressive nature of LLMs, the query and key will undergo\na process using Rotary Positional Embedding [10] (RoPE,\ndenoted as Rp.q in Eq 1) to encode the position information.\nSubsequently, the key and value will be combined with prior\n", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey", "description": "Arxiv Paper titled 'Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey' authored by Zeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang, Sai Qian Zhang", "url": "https://arxiv.org/pdf/2403.14608", "source": "arxiv", "id": "6fb70d1a-78aa-4cbc-ae67-0132fe3f1a0d"}, "page_content": "On the other hand, the inherent computation characteristic of\nLLM lies in the attention blocks inside each decoder layer.\nFigure 2 (c) depicts the high-level overview of the computation\nflow in the attention block.\nDuring the inference process, each decoder takes a three-\ndimensional tensor x P Rb\u02c6l\u02c6d as the input tokens. The\ninput tokens are first multiplied with three weight matrices\nWQ, WK, and WV, producing the output referred to as\nquery(Q), key(K) and value(V ). Given the MSA module\u2019s\ninability to recognize positional data and the inherent auto-\nregressive nature of LLMs, the query and key will undergo\na process using Rotary Positional Embedding [10] (RoPE,\ndenoted as Rp.q in Eq 1) to encode the position information.\nSubsequently, the key and value will be combined with prior\ntokens.\nAfter the positional embedding, the intermediate activation\nwill then undergo a series of multiplication, softmax, and\nresidual addition to generate MSA output as described in Eq 9.\nTo be noted here, dk in the equation refers to the number of\nfeature dimensions in the multi-head attention mechanism.\nQ, K, V \u201c RpWqxq, RpWkxq, Wvx\n(1)\nSApxq \u201c Softmaxp QKT\n?dhead\nqV\n(2)\nMSApxq \u201c rSA1pxq; SA2pxq;... ; SAkpxqsWo\n(3)\nThe SA output will then be forwarded to the FFN blocks\nfor further processing. The FFN block will have another three\n3\n<BOS>\nFFN\nSA\nLLaMA\nML\nML\nis\nLLaMA\nLLaMA\nis\nawesome\nLLaMA\nawesome\n<EOS>\nDecoder\nDecoder\nDecoder\nLinear &\nSoftmax\n\u2026\nQ\nK\nV\nLoRA\nFC\nFC\nReLU\nAdapter\nDecoder\nSA\nFFN\nWdown\nWup\nInput tokens\nPrompt\n(c)\nEmbedding\n(b)\n(a)\nFig. 2: (a) LLaMA architecture. (b) LLaMA auto-regressive pattern. (c) Three common PEFT operations. All the learnable\ncomponents are highlighted in red, while the frozen components are highlighted in grey. LoRA is applied on all the Query, Key,\nand Value blocks. The adapter targets the FFN module. Soft-Prompt focused on tuning the input activation of each decoder.\nWe only show one decoder for illustration simplicity.\nmatrices Wup, Wdown, and Wgate and the computation can be\nillustrated by:\nFFNLLaMapxq \u201c WuppSiLUpWgatexq d pWdownxqq ` x,\n(4)\nwhere x denotes the input of the FFN layer, and SiLU\nis the nonlinear function used in LLaMA. In the original\nTransformer, the FFN block can be demonstrated by:\nFFNT ransfomerpxq \u201c WuppReLUpWdownxqq ` x.\n(5)\nThe output of the last decoder layer will be sent to a\nlinear layer, which then generates a probability distribution\nspanning the complete vocabulary to predict the next token in\nthe sequence. The produced token will then be concatenated\nwith the previous tokens and used as the input for the next\nround of processing. This generating process repeats in an\nauto-regressive manner until a full sequence of tokens, referred\nto as a completion, is produced (Figure 2 (b)). For training, the\ncomputation flow is similar to that for inference, except that\nthe generated sentences are directly compared to the ground\ntruth output and generate the training loss. Gradients will then\nbe computed across the LLM weights to minimize this training\nloss.\nTo analyze the computation cost and memory overhead\nin LLM, we also set a series of parameters used in later\nsection III. Table I shows the parameter size and computation\ndimension in the LLaMA-7B model as a starting example.\nLLM models generate tokens (words) one for each round,\ndepicted in Fig 2, based on the previous prompt (input) and\npreviously generated sequence. This process will be repeated\nuntil the model outputs hits and termination token. To accel-\nerate the inference process in LLM models, people take the\nstrategy of storing the previous Keys and Values in the Key-\nValue cache (KV-cache), so they don\u2019t need to recalculate\nthem for each new token. Mathematically, we can represent\nthe total decoders\u2019 KV-cache memory cost in equation 6. In\nthe equation, l and b are the context length and batch size\nand L refers to the number of layers. The dhead is the head\ndimension and nhead is the number of heads.\nSize \u201c L \u02c6 2 \u02c6 b \u02c6 l \u02c6 dhead \u02c6 nhead\n(6)\nB. Overview on Parameter Efficient Fine Tuning\nFine-tuning remains essential to enhance LLM performance\non unseen user datasets and tasks. With the size of the model\ngrowing (e.g. 1.5B in GPT-2 to 175B in GPT-3), standard\nfull fine-tuning paradigm requires thousands of GPU work\nin parallel, which is highly inefficient and uns", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey", "description": "Arxiv Paper titled 'Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey' authored by Zeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang, Sai Qian Zhang", "url": "https://arxiv.org/pdf/2403.14608", "source": "arxiv", "id": "dbfffff0-cd1a-4677-97bb-c85a7438d792"}, "page_content": "V-cache), so they don\u2019t need to recalculate\nthem for each new token. Mathematically, we can represent\nthe total decoders\u2019 KV-cache memory cost in equation 6. In\nthe equation, l and b are the context length and batch size\nand L refers to the number of layers. The dhead is the head\ndimension and nhead is the number of heads.\nSize \u201c L \u02c6 2 \u02c6 b \u02c6 l \u02c6 dhead \u02c6 nhead\n(6)\nB. Overview on Parameter Efficient Fine Tuning\nFine-tuning remains essential to enhance LLM performance\non unseen user datasets and tasks. With the size of the model\ngrowing (e.g. 1.5B in GPT-2 to 175B in GPT-3), standard\nfull fine-tuning paradigm requires thousands of GPU work\nin parallel, which is highly inefficient and unsustainable. A\ntype of algorithm has been raised namely Parameter-efficient\nfine-tuning (PEFT) which aims to tune minimal parameters\nto achieve better performance over full tuning on downstream\ntasks.\nIn parallel developments, large-scale pre-trained models in\nvision and multimodal domains have also demonstrated their\neffective representational learning capabilities, enabling adap-\ntation from large datasets to smaller ones or across various data\nmodalities through fine-tuning. Consequently, this capability\nhas made PEFT increasingly attractive to the wider research\ncommunity.\nWe categorized the PEFT algorithms into additive, selec-\ntive, reparameterized, and hybrid fine-tuning based on their\noperations. As Figure 3 depicts, three major additive fine-\ntuning algorithms are normally used: (1) Adapter; (2) Soft\nPrompt; (3) Others. They differ from each other in terms of the\ndifferent additional tunable modules or parameters. Selective\nfine-tuning, on the other hand, doesn\u2019t require any additional\nparameters, it selects a small subset of parameters from the\nbackbone model and only makes them tunable while keeping\nthe majority of parameters untouched during fine-tuning on\ndownstream tasks. We categorized selective fine-tuning based\n4\nTABLE I: Configuration parameters and computation operation for LLaMA-7B architecture\nOperation\nWeights Symbol\nWeights Dimension\nInput Tensor Dimension\nComplexity\nEq. 1\nWQ, WK, WV\nd \u02c6 k \u02c6 d\nk\nb \u02c6 l \u02c6 d\nOplq\nEq. 2\n-\n-\nb \u02c6 l \u02c6 3 \u02c6 k \u02c6 d\nk\nOpl2q\nEq. 3\nWo\nd \u02c6 d\nb \u02c6 l \u02c6 d\nOplq\nEq. 4\nWup, Wdown, Wgate\nd \u02c6 4d\nb \u02c6 l \u02c6 d OR l \u02c6 b \u02c6 4d\nOplq\non the grouping of chosen parameters: (1) Unstructural Mask-\ning; and (2) Structural Masking. Reparametrization represents\ntransforming model parameters between two equivalent forms.\nSpecifically, reparametrized fine-tuning introduces addi-\ntional low-rank trainable parameters during training, which\nare then integrated with the original model for inference. This\napproach is categorized into two main strategies: (1) Low-\nrank Decomposition, and (2) LoRA Derivatives. Hybrid fine-\ntuning explores the design spaces of different PEFT methods\nand combines their advantages.\nC. Downstream Tasks for LLM Evaluation\nTwo types of tasks have been widely used for LLM eval-\nuation, the first type is the General Language Understand-\ning Evaluation (GLUE) [11] benchmark, which integrates\nnine sentence or sentence-pair language understanding tasks\n(CoLA, SST-2, MRPC, STS-B, QQP, MNLI, QNLI, RTE, and\nWNLI), chosen for their diversity in dataset sizes, text genres,\nand difficulty levels, and is based on established existing\ndatasets. It also includes a diagnostic dataset specifically\ndesigned to evaluate and analyze model performance across\nvarious linguistic phenomena inherent in natural language.\nAdditionally, it features a public leaderboard to track perfor-\nmance on the benchmark and a dashboard to visualize model\nperformance on the diagnostic set.\nThe other type of dataset that has been used in recent\nLLM papers is common sense reasoning which integrated\ninto our study caters to a variety of research facets: (1)\nOpenBookQA [12] is curated to foster research in advanced\nquestion-answering, delving into a profound understanding\nof both the subject matter and the language in which it\nis articulated. (2) PIQA [13] primarily emphasizes everyday\nscenarios, demonstrating a predilection for unconventional\nsolutions. (3) Social IQA [14] emerges as a novel question-\nanswering benchmark tailored for gauging social common-\nsense intelligence. (4) HellaSwag [15] serves as a dataset, the\nessence of which is to ascertain the capability of machines in\naptly concluding sentences. (5) BoolQ [16] is a dataset dedi-\ncated to question-answering, particularly for binary responses\n(yes/no queries). (6) WinoGrande [17] is introduced as a fresh\ncompilation, encompassing", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey", "description": "Arxiv Paper titled 'Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey' authored by Zeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang, Sai Qian Zhang", "url": "https://arxiv.org/pdf/2403.14608", "source": "arxiv", "id": "1849343e-3f7a-40ce-8a78-11c765706172"}, "page_content": " variety of research facets: (1)\nOpenBookQA [12] is curated to foster research in advanced\nquestion-answering, delving into a profound understanding\nof both the subject matter and the language in which it\nis articulated. (2) PIQA [13] primarily emphasizes everyday\nscenarios, demonstrating a predilection for unconventional\nsolutions. (3) Social IQA [14] emerges as a novel question-\nanswering benchmark tailored for gauging social common-\nsense intelligence. (4) HellaSwag [15] serves as a dataset, the\nessence of which is to ascertain the capability of machines in\naptly concluding sentences. (5) BoolQ [16] is a dataset dedi-\ncated to question-answering, particularly for binary responses\n(yes/no queries). (6) WinoGrande [17] is introduced as a fresh\ncompilation, encompassing a substantial 44,000 problems. (7)\nARC-easy [18] presents itself as a novel dataset constituting\ngenuine grade-school level multiple-choice science questions,\ndesigned to invigorate research in intricate question-answering.\n(8) ARC-challenges [18], distinctively, encompasses solely\nthose questions that were inaccurately addressed by both a\nretrieval-based algorithm and a word co-occurrence algorithm.\nImage recognition is the primary benchmark and application\nfor vision models, exemplified by benchmarks such as fine-\ngrained visual categorization (FGVC) and visual task adapta-\ntion benchmark (VTAB). Beyond image classification, video\naction recognition is another key application area, involving\ndatasets like Kinetics-400 [19], SSv2 [20], and HMDB51 [21].\nAdditionally, PEFT has been utilized for dense prediction\ntasks, using datasets like MSCOCO [22], ADE20K [23], and\nPASCAL VOC [24].\nD. Evaluation Benchmarks for PEFT\nTo help readers evaluate the performance differences be-\ntween various PEFT methods under a unified standard, a com-\nprehensive benchmark is essential. Next, we discuss several\ncommonly used benchmarks.\nFrom the algorithmic perspective, [25] benchmarks the\nperformance of several PEFT algorithms across more than\n100 NLP tasks and conducts systematic experiments based on\ncriteria such as performance, convergence, efficiency, combin-\nability, scalability, and transferability. Similarly, [26] and [27]\nhave also established targeted benchmarks to evaluate different\nPEFT algorithms.\nFrom the system perspective, three commonly used bench-\nmarks are outlined below to evaluate system performance.\nThe first benchmark is the ShareGPT dataset [28], which\nincludes real-world interactions with OpenAI\u2019s ChatGPT. It\nencompasses a broad spectrum of conversational queries and\nresponses that are representative of typical user interactions\nwith large language models (LLMs). This dataset is vital\nfor evaluating the system\u2019s ability to manage diverse and\nrealistic conversational requirements, focusing on the accuracy\nof responses and efficiency in handling requests.\nThe second benchmark involves the Microsoft Azure Func-\ntion Trace from the years 2019 and 2021 [29], containing\nlogs from serverless computing activities via Azure Functions.\nWhile these logs are from a general serverless computing con-\ntext rather than LLM-specific applications, they offer insights\ninto the computational demands driven by events. These traces\nsimulate the arrival patterns and workload intensities that LLM\nsystems might face, including irregular and peak demands,\nthus acting as practical proxies for LLM inference tasks.\nThe third benchmark is based on the Gamma process [30],\na prevalent approach in simulations to model the timing of\nincoming requests in queueing and service systems. This\nmethod facilitates the creation of workloads with varied arrival\nrates and patterns, producing synthetic, yet realistic request\n5\nPEFT Methods for PLMs\nAdditive\nFine-tuning\nAdapter-based\nFine-tuning\nAdapter Design\nSerial Adapter [31], Parallel Adapter [32], CIAT [33], CoDA [34]\nMulti-task Adaptation\nAdapterFusion [35], AdaMix [36], PHA [37], AdapterSoup [38], MerA [39], Hyperformer [40]\nSoft Prompt-based\nFine-tuning\nSoft Prompt Design\nPrefix-tuning [41], Prefix-Propagation [42], p-tuning v2 [43], APT [44], p-tuning [45],\nprompt-tuning [46], Xprompt [47], IDPG [48], LPT [49], SPT [50], APrompt [51]\nTraining Speedup\nSPoT [52], TPT [53], InfoPrompt [54], PTP [55], IPT [56], SMoP [57], DePT [58]\nOthers\n(IA)3 [59], MoV [60], SSF [61], IPA [62]\nSelective\nFine-tuning\nUnstructural\nMasking\nU-Diff pruning [63], U-BitFit [64], PaFi [65], FishMask [66], Fish-Dip [67], LT-SFT [68], SAM [69], Child-tuning [70]\nStructural Masking\nS-Diff pruning [63], S-BitFit [64], FAR [71], Bitfit [72], Xattn Tuning [73], SPT [74]\nReparameterized\n", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey", "description": "Arxiv Paper titled 'Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey' authored by Zeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang, Sai Qian Zhang", "url": "https://arxiv.org/pdf/2403.14608", "source": "arxiv", "id": "397d6084-3748-4c4b-a25d-38404868974d"}, "page_content": "48], LPT [49], SPT [50], APrompt [51]\nTraining Speedup\nSPoT [52], TPT [53], InfoPrompt [54], PTP [55], IPT [56], SMoP [57], DePT [58]\nOthers\n(IA)3 [59], MoV [60], SSF [61], IPA [62]\nSelective\nFine-tuning\nUnstructural\nMasking\nU-Diff pruning [63], U-BitFit [64], PaFi [65], FishMask [66], Fish-Dip [67], LT-SFT [68], SAM [69], Child-tuning [70]\nStructural Masking\nS-Diff pruning [63], S-BitFit [64], FAR [71], Bitfit [72], Xattn Tuning [73], SPT [74]\nReparameterized\nFine-tuning\nLow-rank\nDecomposition\nIntrinsic SAID [75], LoRA [76], Compacter [77], KronA [78], KAdaptation [79], HiWi [65], VeRA [80], DoRA [81]\nLoRA Derivatives\nDynamic Rank\nDyLoRA [82], AdaLoRA [83], SoRA [84], CapaBoost [85], AutoLoRA [86]\nLoRA Improvement\nLaplace-LoRA [87], LoRA Dropout [88], PeriodicLoRA [89], LoRA+ [90], MoSLoRA [91]\nMultiple LoRA\nLoRAHub [92], MOELoRA [93], MoLORA [60], MoA [94], MoLE [95], MixLoRA [96]\nHybrid\nFine-tuning\nUniPELT [97], S4 [98], MAM Adapter [32], NOAH [99], AUTOPEFT [100], LLM-Adapters [101], S3PET [102]\nFig. 3: Taxonomy of Parameter-Efficient Fine-Tuning Methods for Large Models.\nOutput\nCombine\nFrozen\nLearnable\nInput\nOutput\nInput\n(c) Reparameterization PEFT\n(a) Additive PEFT\n(b) Selective PEFT\nMerge\nOutput\nInput\nInput (train)\nFig. 4: Different types of PEFT algorithms.\nscenarios that a system could encounter during actual opera-\ntions. Such synthetic workloads are crucial for testing system\nperformance under controlled conditions that resemble real-\nworld user activity.\nIII. PEFT TAXONOMY\nThe PEFT strategies can be broadly classified into four\ncategories: additive PEFT (Section III-A), which modifies\nthe model architecture by injecting new trainable modules\nor parameters; selective PEFT (Section III-B), which makes\na subset of parameters trainable during fine-tuning; repa-\nrameterized PEFT (Section III-C), which constructs a (low-\ndimensional) reparameterization of the original model param-\neters for training, then equivalently transforms it back for\ninference; and hybrid PEFT (Section III-D), which combines\nadvantages from different PEFT methods to build a unified\nPEFT model. An overview of different types of PEFT algo-\nrithms is depicted in Figure 4.\nA. Additive PEFT\nStandard full fine-tuning entails substantial computational\nexpenses and could also potentially harm the model\u2019s gener-\nalization ability. To mitigate this problem, a widely employed\napproach is to maintain the pre-trained backbone unchanged\nand introduce only a minimal number of trainable parameters\nthat are strategically positioned within the model architecture.\nWhile fine-tuning for a specific downstream task, only the\nweights of these additional modules or parameters are updated,\nwhich results in a substantial reduction in storage, memory,\nand computational resource requirements. Due to their char-\nacteristic of adding parameters, these techniques can be termed\nas Additive Tuning, as shown in Figure 4 (a). Next, we discuss\nseveral popular Additive PEFT algorithms.\n1) Adapters: Adapter approaches involve the insertion of\nsmall adapter layers within Transformer blocks. Typically, an\nadapter layer consists of a down-projection matrix Wdown P\nRr\u02c6d, followed by a non-linear activation function \u03c3p\u00a8q, and\nan up-projection matrix Wup P Rd\u02c6r. In this context, d\nrepresents the dimension of the hidden layer, and r serves\nas the bottleneck dimension, which is a hyperparameter used\nin configuring the adapters. Denote hin as the input to the\nadapter, the computation within the adapter module (with\nresidual) can be summarized as follows:\nAdapterpxq \u201c Wup\u03c3pWdownxq ` x.\n(7)\nThe concept of adapters in the field of NLP was initially\nintroduced by Serial Adapter [31] as shown in Figure 5\n(a). In their approach, each Transformer block is enhanced\nby adding two adapter modules, with one positioned after\nthe self-attention layer and the other after the FFN layer,\nrespectively. Subsequent research has aimed to address the\nadditional computational cost associated with adapter layers.\nA modified framework AdapterFusion [35] was proposed,\nwhere adapter layers are inserted only after the \u2019Add & Norm\u2019\nstep following the FFN layer to enhance the computational\nefficiency. The adapters mentioned above follow a sequen", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey", "description": "Arxiv Paper titled 'Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey' authored by Zeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang, Sai Qian Zhang", "url": "https://arxiv.org/pdf/2403.14608", "source": "arxiv", "id": "1ec7a271-2b2d-4094-bb93-fe334f69191c"}, "page_content": " a hyperparameter used\nin configuring the adapters. Denote hin as the input to the\nadapter, the computation within the adapter module (with\nresidual) can be summarized as follows:\nAdapterpxq \u201c Wup\u03c3pWdownxq ` x.\n(7)\nThe concept of adapters in the field of NLP was initially\nintroduced by Serial Adapter [31] as shown in Figure 5\n(a). In their approach, each Transformer block is enhanced\nby adding two adapter modules, with one positioned after\nthe self-attention layer and the other after the FFN layer,\nrespectively. Subsequent research has aimed to address the\nadditional computational cost associated with adapter layers.\nA modified framework AdapterFusion [35] was proposed,\nwhere adapter layers are inserted only after the \u2019Add & Norm\u2019\nstep following the FFN layer to enhance the computational\nefficiency. The adapters mentioned above follow a sequen-\ntial design, placing adapter layers as bottlenecks within the\nTransformer blocks. This approach may potentially reduce the\nmodel\u2019s parallelism and require a trade-off between inference\nefficiency and accuracy. In contrast, [32] introduced a parallel\n6\nTransformer \nModule\nAdapter\nTransformer \nModule\n(a) Serial Adapter \n(b) Parallel Adapter \nTransformer \nModule\n(c) CoDA\nall tokens\nall tokens\ntop-k tokens\nall tokens\n(d) Adapter Layer\nWdown\nWup\nReLU\nAdapter\nAdapter\nFig. 5: Illustration of three representative adapter-based fine-tuning algorithms. Blue represents frozen, while yellow represents\ntrainable.\nadapter (PA) approach as depicted in Figure 5 (b), which\nreorganizes the traditionally sequential adapter layers into a\nparallel side-network that runs alongside each Transformer\nsublayer. Similarly, CIAT [33], CoDA [34] and KronA [78]\nalso adopts a parallel adapter design. Except for the parallel\ndesign, CoDA employs a sparse activation mechanism to\nimprove the inference efficiency as shown in Figure 5 (c).\nSpecifically, CoDA uses a soft top-k selection process that\nidentifies k important tokens in each layer, which will be\nprocessed by both the frozen pre-trained Transformer layer and\nthe adapter branch to maintain model accuracy. In contrast,\nthose unimportant tokens are only processed by the adapter\nbranch while skipping the heavy pre-trained layer, therefore\noptimizing for inference efficiency without compromising\noverall performance.\nTo enhance the performance and generalization of adapters,\nvarious studies have implemented multi-task learning strate-\ngies, such as AdapterFusion [35], AdaMix [36], PHA [37],\nAdapterSoup [38], MerA [39], and Hyperformer [40].\nAdapterFusion keeps all pre-trained adapters in the model\nand employs a fusion module to merge the multi-task in-\nformation. Unlike AdapterFusion, MerA merges pretrained\nadapters into a single one through optimal transport based\non weights and activations. This approach avoids introduc-\ning any additional trainable parameters, thereby enhancing\ncomputational efficiency. Hyperformer stores the multi-task\ninformation in a shared hypernetwork, which generates task\nand layer-specific adapter parameters conditioned on task and\nlayer ID embeddings. Given a new task, only an additional\ntask embedding needs to be learned, therefore reducing the\nnumber of trained parameters.\n2) Soft Prompt: Alternatively, prompt tuning presents an\nadditional approach for refining the model to achieve improved\nperformance through fine-tuning. Instead of optimizing dis-\ncrete token representations through in-context learning, there\nis a prevailing belief that the continuous embedding space\nof soft prompts inherently contains more information [103].\nDrawing inspiration from this concept, researchers directly\nprepend adjustable vectors, referred to as soft prompts, to the\nstart of the input sequence. This can be represented as follows:\nXplq \u201c rsplq\n1,..., splq\nNS, xplq\n1,..., xplq\nNXs\n(8)\nwhere Xplq is the sequence of input tokens for layer l,\nincluding soft prompt tokens splq\ni\nfollowed by the original input\ntokens xplq\ni. NS is the number of soft prompt tokens, and NX\nis the number of original input tokens.\nPrefix-tuning [41] introduces learnable vectors that are\nprepended to keys k and values v across all Transformer layers.\nTo ensure stability during the optimization process, Prefix-\ntuning adopts a reparameterization strategy, which utilizes\nan MLP layer to generate these prefix vectors rather than\noptimizing them directly. After fine-tuning, only the prefix\nvectors are saved for inference. This technique has been\nadapted and improved in several studies [42], [43], [44]. For\ninstance, p-tuning v2 [43] removes reparameterization and\nexpands its usage to broader model scales and NLP tasks.\nAPT (Adaptive Prefix Tuning) [44] enhances Prefix-tuning by\nintroducing an adaptive gate mechanism to control the prefix\nimportance in each layer. Concurrent work p-tuning [45]\nand prompt-tuning [46] apply learnable vectors only at the\ninitial word embedding layer rather than all layers to enhance\ntraining and inference efficiency. It\u2019s important to highlight\nthat prompt-tuning demonstrates its effectiveness", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey", "description": "Arxiv Paper titled 'Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey' authored by Zeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang, Sai Qian Zhang", "url": "https://arxiv.org/pdf/2403.14608", "source": "arxiv", "id": "aa18aa93-9451-4b40-a525-3baa84bc5a36"}, "page_content": ", Prefix-\ntuning adopts a reparameterization strategy, which utilizes\nan MLP layer to generate these prefix vectors rather than\noptimizing them directly. After fine-tuning, only the prefix\nvectors are saved for inference. This technique has been\nadapted and improved in several studies [42], [43], [44]. For\ninstance, p-tuning v2 [43] removes reparameterization and\nexpands its usage to broader model scales and NLP tasks.\nAPT (Adaptive Prefix Tuning) [44] enhances Prefix-tuning by\nintroducing an adaptive gate mechanism to control the prefix\nimportance in each layer. Concurrent work p-tuning [45]\nand prompt-tuning [46] apply learnable vectors only at the\ninitial word embedding layer rather than all layers to enhance\ntraining and inference efficiency. It\u2019s important to highlight\nthat prompt-tuning demonstrates its effectiveness primarily in\nthe context of large models, specifically those with over 11\nbillion parameters [46]. Complementing this, Xprompt [47]\neliminates the negative prompt tokens through a hierarchi-\ncally structured pruning, which closes the performance gap at\nsmaller model scales. [104] provides some theoretical analysis\ntowards prompt tuning, demonstrating its universality and\nlimitations in limited-depth Transformers. IDPG (Instance-\nDependent Prompt Generation) [48] improves prompt tuning\nby generating prompts based on each input sentence with\na lightweight prompt generator. In a related approach, LPT\n(Late Prompt Tuning) [49] also leverages a prompt generator\nto obtain instance-aware prompt. Unlike previous work, LPT\nadds these prompts only after an intermediate layer, rather than\nat the initial or all layers. This strategic placement eliminates\nthe gradient calculation below the intermediate layer, thereby\nsignificantly accelerating the training speed. Simultaneously,\nLPT can improve the overall performance due to the shorter\nbackpropagation path preserves more task-related information.\nInspired by LPT, SPT (Selective Prompt Tuning) [50] delves\ndeeper into the importance of prompt inserting strategies.\nIt introduces a learnable probabilistic gate in each layer to\ndetermine whether to use the prompt propagated from the pre-\nvious layer or inject a newly generated prompt. APrompt [51]\nemploys another prompt inserting strategy. In addition to input\nprompts inserted at the beginning of the input sequence for\neach Transformer layer, APrompt also prepends additional\nlearnable prompts to the respective query, key, and value\n7\nV\nK\nQ\n\u2299\nlk\nlv\nlff\n\u2299\nsoftmax\nWdown\n\ud835\uded4\n\u2299\nWup\n(a) (IA)3\n\u2299\n\u2295\nOperation 1\nOperation 2\n(b) SSF\nscale\nshift\nFig. 6: Illustration of (IA)3 and SSF. Blue represents frozen,\nwhile yellow represents trainable.\nmatrices in the self-attention blocks to learn new attention\npatterns. Besides, APrompt incorporates the learning of a task-\nspecific head.\nThe concept of soft prompts has been employed for various\ndownstream tasks [105], [106], although their training can\nbe prone to instability and slow convergence. To address\nthis, SPoT [52] uses a source prompt learned from one or\nmultiple tasks to initialize prompts for new tasks. Similarly,\nthe transfer of soft prompts from one task to initialize another\nis proposed in TPT (transferable prompt tuning) [53], which\ndemonstrates that a better prompt initialization results in a\nlarge training convergence speedup. InfoPrompt [54] develops\ntwo mutual information-based loss functions, i.e., head loss\nand representation loss, to find better prompt initialization\nand learn sufficient task-relevant information, thereby also\nexpediting convergence. PTP [55] delves into the root causes\nof training instability. It identifies the steep nature of the loss\nlandscape in conventional prompt tuning, where minor varia-\ntions in input data can lead to significant loss fluctuations. To\nmitigate this, PTP introduces perturbation-based regularizers\nto smooth the loss landscape and consequently stabilize the\ntraining process. DePT [58] decomposes the soft prompt into\na shorter soft prompt with a pair of low-rank matrices, which\nare optimized with two distinct learning rates. This strategy\nnot only improves performance but also enhances training and\ninference efficiency. SMoP (Sparse Mixture-of-Prompts) [57]\nreduce the training and inference cost by utilizing short soft\nprompts. During training, multiple short soft prompts are\ntrained, each tailored to specific subsets of the dataset. During\ninference, SMoP integrates a gating mechanism that routes\neach input instance to an appropriate short prompt. This\ntechnique not only increases efficiency in both training and\ninference stages but also retains performance comparable to\nthose achieved with longer soft prompts. To further cut down\nthe number of soft prompt parameters, IPT (Intrinsic Prompt\nTuning) [56] identifies an intrinsic task subspace by training\nan auto-encoder on multiple tasks. Tuning on new tasks then\nrequires adjusting only a few parameters within this subspace,\nsignificantly reducing the number of training parameters.\n3) Other Additive Methods: Apart from the methods men-\ntioned above, there appear other approaches that strategi-\ncally incorporate additional parameters during the fine-tuning\nprocess. For", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey", "description": "Arxiv Paper titled 'Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey' authored by Zeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang, Sai Qian Zhang", "url": "https://arxiv.org/pdf/2403.14608", "source": "arxiv", "id": "5fd3b2ff-b895-46bb-af89-c272d2386902"}, "page_content": " the training and inference cost by utilizing short soft\nprompts. During training, multiple short soft prompts are\ntrained, each tailored to specific subsets of the dataset. During\ninference, SMoP integrates a gating mechanism that routes\neach input instance to an appropriate short prompt. This\ntechnique not only increases efficiency in both training and\ninference stages but also retains performance comparable to\nthose achieved with longer soft prompts. To further cut down\nthe number of soft prompt parameters, IPT (Intrinsic Prompt\nTuning) [56] identifies an intrinsic task subspace by training\nan auto-encoder on multiple tasks. Tuning on new tasks then\nrequires adjusting only a few parameters within this subspace,\nsignificantly reducing the number of training parameters.\n3) Other Additive Methods: Apart from the methods men-\ntioned above, there appear other approaches that strategi-\ncally incorporate additional parameters during the fine-tuning\nprocess. For example, (IA)3 [59] introduces three learnable\nrescaling vectors: lk P Rdk, lv P Rdv, and lff P Rdff,\nto rescale the key, value, and FFN activations, respectively,\nas depicted in Figure 6 (a). The operations within the self-\nattention block can be described as follows:\nSApxq \u201c SoftmaxpQplk d KT q\n?dhead\nqpplv d V q.\n(9)\nIn FFN, the rescaling can be denoted as:\nFFNT ransfomerpxq \u201c Wupplff d \u03c3pWdownxqq,\n(10)\nwhere d is Hadamard product. Furthermore, the scale vectors\nlk and lv can be seamlessly integrated into the weight matrices\nof AQ and AW. This integration effectively eliminates the ex-\ntra computational costs during inference. A similar technique\nSSF [61] also performs linear transformation to the model\nactivations, as illustrated in Figure 6 (b). Specifically, after\neach operation (i.e., MSA, FFN, and layer normalization) in\nthe pre-trained model, an SSF-ADA layer is injected, which\nperforms scaling and shifting to the features generated from\nthe operation. During fine-tuning, only those SSF-ADA layers\ncan be updated, while during inference, similar to (IA)3, these\nSSF-ADA layers can be merged into model weights, so no ad-\nditional inference overhead would be incurred. IPA (Inference-\nTime Policy Adapters) [62] offers a novel approach to align\nLLMs, such as GPT-4, with user-specific requirements without\nmodifying the base model\u2019s parameters. This is particularly\nsignificant when dealing with models whose parameters are\nextremely large and often not directly accessible. IPA achieves\nthis by combining (through multiplication and normalization)\nthe output distribution of a base LLM (base policy) with that\nof a smaller-sized model (adapter policy) during the decoding\nphase. During training, the policy adapter\u2019s parameters are\nfine-tuned using reinforcement learning, while the base pol-\nicy\u2019s parameters remain fixed. During inference, IPA decodes\nwith the combined distribution of the base model and the\ntrained policy adapter, tailoring it to fulfill specific user-defined\ncriteria.\nB. Selective PEFT\nRather than additive PEFT, which increases the model\ncomplexity by adding more parameters, selective PEFT fine-\ntunes a subset of the existing parameters to enhance model\nperformance over downstream tasks, as depicted in Figure 4\n(b).\nSpecifically,\ngiven\na\nmodel\nwith\nparameters\n\u03b8\n\u201c\nt\u03b81, \u03b82,..., \u03b8nu where each \u03b8i denotes an individual model\nparameter and n represents the total count of these parameters,\nthe process of selective PEFT is represented by applying a\nbinary mask M \u201c tm1, m2,..., mnu to these parameters. Each\nmi in M is either 0 or 1, indicating whether the corresponding\nparameter \u03b8i is selected (1) or not selected (0) for fine-tuning.\nThe updated parameter set \u03b81 after fine-tuning is given by:\n\u03b81\ni \u201c \u03b8i \u00b4 \u03b7 \u00a8 mi \u00a8 BL\nB\u03b8i\n(11)\nwhere \u03b7 represents the learning rate, and\nBL\nB\u03b8i is the gradient\nof the loss function with respect to the parameter \u03b8i. In this\nformulation, only the parameters that are selected (i.e., mi \u201c\n1) are updated during backpropagation.\n8\n \n(a) Unstructural \nMasking\n(b) Structural \nMasking\nFrozen\nLearnable\nFig. 7: Illustration of two parameter masking methods.\nDiff pruning [63] is a representative work that applies\na learnable binary mask to the model weights during fine-\ntuning. To achieve parameter efficiency, the mask is reg-\nularized by a differentiable approximation of the L0-norm\npenalty. PaFi [65] simply select model parameters with the\nsmallest absolute magnitude as trainable. FishMask [66] de-\ntermines parameter importance using the approximate Fisher\ninformation. It then selects the top k parameters based on this\ninformation to form the mask M. Similarly, Fish-Dip [67]\nalso uses Fisher information to calculate M, but the mask\nwill be re-calculated dynamically in each train period. LT-\nSFT [68] introduces another technique to determine parameter\nimportance inspired by", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey", "description": "Arxiv Paper titled 'Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey' authored by Zeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang, Sai Qian Zhang", "url": "https://arxiv.org/pdf/2403.14608", "source": "arxiv", "id": "9d96045e-582f-450a-a04a-a956eba7c082"}, "page_content": " \nMasking\n(b) Structural \nMasking\nFrozen\nLearnable\nFig. 7: Illustration of two parameter masking methods.\nDiff pruning [63] is a representative work that applies\na learnable binary mask to the model weights during fine-\ntuning. To achieve parameter efficiency, the mask is reg-\nularized by a differentiable approximation of the L0-norm\npenalty. PaFi [65] simply select model parameters with the\nsmallest absolute magnitude as trainable. FishMask [66] de-\ntermines parameter importance using the approximate Fisher\ninformation. It then selects the top k parameters based on this\ninformation to form the mask M. Similarly, Fish-Dip [67]\nalso uses Fisher information to calculate M, but the mask\nwill be re-calculated dynamically in each train period. LT-\nSFT [68] introduces another technique to determine parameter\nimportance inspired by the Lottery Ticket Hypothesis [107],\n[108], where the subset of parameters that change the most\nduring an initial fine-tuning stage is selected to form the mask\nM. SAM [69] proposes a second-order approximation method,\nwhich approximates the original problem with an analytically\nsolvable optimization function, to help decide the parameter\nmask. Child-tuning [70] proposes two approaches to select a\nchild network during each training iteration, where only the\nparameters within this child network can be updated.\nHowever, the above unstructured parameter masking results\nin an uneven distribution of non-zero masks and diminished\nhardware efficiency when implementing PEFT. As shown in\nFigure 7, the structured mask organizes parameter masking\nin regular patterns, unlike unstructured ones that apply it\nrandomly, thus enhancing computational and hardware effi-\nciency during training. Therefore, various structured selective\nPEFT techniques have undergone extensive investigation. Diff\npruning proposes a structured pruning strategy by partitioning\nthe weight parameters into local groups and strategically elim-\ninating them together. Similarly, FAR [71] fine-tunes BERT\nmodels by grouping weights of the FFN in Transformer blocks\ninto nodes, then ranking and selecting the learner nodes using\nL1 norm. To further reduce the memory access frequency,\nthey also reconfigure the FFN by grouping the learner nodes.\nBitfit [72] is proposed to only fine-tune the bias parameters\nof each DNN layer, and achieves competitive results for small\nmodels. However, this method fails to handle large models.\n[64] applies NAS to Bitfit, where S-BitFit keeps the structural\nnature in Bitfit that restricts NAS algorithm must choose\nwhether \u03b4b \u201c 0 or not for each bias module. Similar to\nBitfit that fine-tunes a specific module in Transformer, Xattn\nTuning [73] fine-tunes only the cross-attention layers. SPT\n(sensitivity-aware visual parameter-efficient fine-tuning) [74]\nfirst identifies the sensitive parameters measured by the loss\nreduction when being tuned. This sensitivity is calculated\nusing a first-order Taylor expansion, derived from a single\nforward and backward pass before fine-tuning in one shot.\nNext, SPT finds the weight matrices whose number of sensitive\nparameters exceeds a pre-defined threshold and then applies\na selected PEFT technique (e.g., LoRA and Adapter) to these\ntargeted weights to achieve structural tuning.\nC. Reparameterized PEFT\nReparameterization stands for equivalently transforming a\nmodel\u2019s architecture from one to another via transforming\nits parameters. In the context of PEFT, this often means\nconstructing a low-rank parameterization to achieve the goal of\nparameter efficiency during training. For inference, the model\ncan be converted to its original weight parameterization, en-\nsuring unchanged inference speed. This procedure is depicted\nin Figure 4 (c).\nEarlier research studies [75] have shown that common\npre-trained models exhibit an exceptionally low intrinsic di-\nmensionality. In other words, it is possible to find a low-\ndimensional reparameterization that is effective for fine-tuning\nas the entire parameter space. Intrinsic SAID [75] is the pi-\noneering work in investigating the intrinsic dimension feature\nduring the fine-tuning of LLMs. However, the most widely\nrecognized reparameterization technique is LoRA (Low-Rank\nAdaptation) [76], [109], as shown in Figure 8 (a). For a given\npre-trained weight matrix W0 P Rd\u02c6k, LoRA introduces two\ntrainable weight matrices, Wup P Rd\u02c6r and Wdown P Rr\u02c6k\nwhere the rank r! minpd, kq, operating in parallel to W0.\nLet hin represent the input. Under normal conditions, the\noutput through W0 is hout \u201c W0hin. Instead, LoRA modifies\nthis output by introducing an incremental update \u2206W that\nencapsulates task-specific knowledge:\nhout \u201c W0hin ` \u03b1\nr \u2206Whin \u201c W0hin ` \u03b1\nr WupWdownhin,\n(12)\nwhere \u03b1 denotes a scaling factor. At the onset of training,\nWdown is initialized using a random Gaussian distribution,\nwhile Wup is initialized to zero, ensuring that \u2206W initially\nholds a value of zero. LoRA is straightforward to implement\nand has been evaluated on models with up to 175 billion\nparameters. Fig 8 (c) used a single decoder as an example,\nthe frozen and learn", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey", "description": "Arxiv Paper titled 'Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey' authored by Zeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang, Sai Qian Zhang", "url": "https://arxiv.org/pdf/2403.14608", "source": "arxiv", "id": "b09b4f07-6b1c-464f-b0fa-336213687572"}, "page_content": " and Wdown P Rr\u02c6k\nwhere the rank r! minpd, kq, operating in parallel to W0.\nLet hin represent the input. Under normal conditions, the\noutput through W0 is hout \u201c W0hin. Instead, LoRA modifies\nthis output by introducing an incremental update \u2206W that\nencapsulates task-specific knowledge:\nhout \u201c W0hin ` \u03b1\nr \u2206Whin \u201c W0hin ` \u03b1\nr WupWdownhin,\n(12)\nwhere \u03b1 denotes a scaling factor. At the onset of training,\nWdown is initialized using a random Gaussian distribution,\nwhile Wup is initialized to zero, ensuring that \u2206W initially\nholds a value of zero. LoRA is straightforward to implement\nand has been evaluated on models with up to 175 billion\nparameters. Fig 8 (c) used a single decoder as an example,\nthe frozen and learnable components are highlighted in grey\nand red, respectively. Once fine-tuning is complete, LoRA\u2019s\nadaptive weights seamlessly integrate with the pre-trained\nbackbone weights. This integration ensures that LoRA main-\ntains the model\u2019s efficiency, adding no extra burden during\ninference.\nIn LoRA training, selecting an appropriate rank has always\nbeen a challenging issue. To address this, DyLoRA [82], as\ndepicted in Figure 8 (b), trains the LoRA module on a range of\nranks within a predefined training budget, rather than adhering\nto a single, fixed rank. Specifically, for a given rank range R \u201c\ntrmin, rmin`1,..., rmaxu, DyLoRA dynamically chooses a rank\nr P R at each iteration of the training process. Consequently,\nthe matrices Wdown and Wup are tailored for the selected rank\nr, resulting in truncated versions Wdown\u00d3r \u201c Wdownr1 : r, :s\nand Wup\u00d3r \u201c Wupr:, 1 : rs, and the subsequent forward\nand backward pass during this iteration will be restricted\n9\nPre-trained\nWeights\nW0 \u220a Rd\u00d7k\nWdown \u220a Rr\u00d7k\nWup \u220a Rd\u00d7r\n(a) LoRA\n\u00d7\nd\nd\nrmax\nr\nr\nrmax\nWdown\u2193r\nWup\u2193r\nWup\nWdown\n\u00d7\n(b) DyLoRA\nPre-trained\nWeights\nDecompose\nW0 \u220a Rd\u00d7k\nPre-trained\nWeights\n                 \n  \nMagnitude\nDirection\nm\u220a R1\u00d7k\n1/\u0965V+\u0394V\u0965c\n\u00d7\nV \u220a Rd\u00d7k\nWup \u220a Rd\u00d7r\nWdown \u220a Rr\u00d7k\n\u00d7\n(c) DoRA\nFig. 8: Illustration of three representative reparameterized PEFT algorithms. Blue represents frozen, while yellow represents\ntrainable.\non Wdown\u00d3r and Wup\u00d3r instead of Wdown and Wup. With\nthis dynamic and search-free approach, DyLoRA significantly\nreduces the training time required to find an optimal and fixed\nLoRA rank for specific tasks. AdaLoRA [83] reformulates\nthe \u2206W with a singular value decomposition (SVD), denoted\nas \u2206W\n\u201c P\u039bQ, where P\nP Rd\u02c6r and Q P Rr\u02c6k are\northometric, \u039b is a diagonal matrix containing singular values\nt\u03bbiu1\u010fi\u010fr. All three weight matrices are made learnable.\nDuring training, the singular values are pruned iteratively\nbased on their importance scores, which are constructed from\nthe moving average of the magnitude of the gradient-weight\nproduct. To ensure the orthogonality between P and Q, i.e.,\nP T P \u201c QQT \u201c I, an additional regularizer term is included\nin the loss:\nRpP, Qq \u201c\n\u203a\u203aP T P \u00b4 I\n\u203a\u203a2\nF `\n\u203a\u203aQQT \u00b4 I\n\u203a\u203a2\nF.\n(13)\nThis adaptive approach enables the model to dynamically ad-\njust the rank within each LoRA module, effectively managing\nits parameter counts based on the significance of the weight\nmatrices. However, according to SoRA [84], the importance\nscores used in AdaLoRA are heuristically constructed, which\nlacks rigorous theoretical motivation. Additionally, both mov-\ning average operation and calculation of Eq. 13 introduce\nextra computation costs during training. To address this, SoRA\neliminates the orthogonality premise of P and Q. Instead, a\ngating unit g P Rr between Wup and Wdown is directly applied\nand optimized:\nhout \u201c Wuppg d pWdownhinqq,\n(14)\nwhere d is Hadamard product. The gate g is updated using a\nvariation of proximal gradient iteration for l1 loss [110], [111],\nwhich has a clear mathematical meaning and does not need\nthe heuristic premise. After training, the zeroed-out gate units\nare pruned by removing the corresponding columns and rows\nin Wdown and Wup.\nSeveral subsequent studies have aimed to improve LoRA\u2019s\nperformance in various aspects. For instance, Laplace-\nLoRA [87] notices that fine-tuned LLMs often exhibit over-\nconfidence. To enhance the calibration", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey", "description": "Arxiv Paper titled 'Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey' authored by Zeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang, Sai Qian Zhang", "url": "https://arxiv.org/pdf/2403.14608", "source": "arxiv", "id": "a8642388-e358-4b33-9e55-7127778f7a47"}, "page_content": " of Eq. 13 introduce\nextra computation costs during training. To address this, SoRA\neliminates the orthogonality premise of P and Q. Instead, a\ngating unit g P Rr between Wup and Wdown is directly applied\nand optimized:\nhout \u201c Wuppg d pWdownhinqq,\n(14)\nwhere d is Hadamard product. The gate g is updated using a\nvariation of proximal gradient iteration for l1 loss [110], [111],\nwhich has a clear mathematical meaning and does not need\nthe heuristic premise. After training, the zeroed-out gate units\nare pruned by removing the corresponding columns and rows\nin Wdown and Wup.\nSeveral subsequent studies have aimed to improve LoRA\u2019s\nperformance in various aspects. For instance, Laplace-\nLoRA [87] notices that fine-tuned LLMs often exhibit over-\nconfidence. To enhance the calibration of fine-tuned LLMs,\nLaplace-LoRA utilizes a Bayesian approach, specifically a\npost-hoc Laplace approximation [112], [113], to the posterior\nover the LoRA parameters. LoRA Dropout [88] introduces\nrandom noises to the learnable low-rank matrices and in-\ncreases parameter sparsity to reduce the risk of overfitting.\nLoRA+ [90] proposes to set different learning rates for the\nLoRA matrices Wdown and Wup, such that \u03b7up \u201c \u03bb\u03b7down with\n\u03bb \u0105 1 fixed and tune \u03b7down. MoSLoRA (Mixture-of-Subspaces\nLoRA) [91] decomposes LoRA into subspaces via structural\nreparameterization, then employs a learnable mixer, trained\njointly with the original LoRA weights, to fuse the subspaces.\nSimilarly to LoRA, MoSLoRA can also be merged into the\noriginal weights.\nThanks to the modular design of LoRA, many studies\nincorporate multiple LoRA modules in their frameworks to\nenhance performance. For example, LoRAHub aggregates\nvarious LoRA modules trained on different tasks. Given\na handful of examples from a new task, LoRAHub can\nautonomously compose compatible LoRA modules without\nhuman intervention via a gradient-free method Shiwa [114].\nMOELoRA employs a Mixture-of-Experts (MOE) approach\nto train LoRA in a multi-task setting, resulting in multiple\nexpert LoRA modules. To retrieve parameters for certain tasks,\nMOELoRA utilizes a task-motivated gate function that assigns\ncontribution weights to each expert based on the task ID, and\nthe final parameters are calculated through a weighted sum of\nall experts.\nIn addition to LoRA, several other reparameterization tech-\nniques are emerging with significant potential. For instance,\nCompacter [77] introduces a light-weight adapter modules\nby parameterizing the Wdown and Wup as W \u201c \u0159n\ni\u201c1 Ai bBi,\nwhere Ai P Rn\u02c6n, Bi P R\nr\nn \u02c6 d\nn, and b denotes the Kronecker\nproduct. They further decrease the parameter count by desig-\nnating Ai as shared parameters and reparameterizing Bi using\nthe product of two low-rank matrices, effectively reducing the\nparameter complexity from Oprdq to Opr`dq. Related studies,\nsuch as KronA [78] and KAdaptation [79], also employ the\nKronecker product to reparameterize adapter weights, aiming\nto achieve parameter reduction. HiWi [65] proposes an adapter\nfine-tuning method that applies an adapter directly to pre-\ntrained parameters instead of hidden representations as:\nW 1 \u201c W ` \u03c3pWWdownqWup,\n(15)\nwhere W denotes the weights or biases within the Transformer\nblock\u2019s feed-forward layer. Notably, during inference, this\nmethod computes W 1 in advance, ensuring that the model\u2019s\n10\ninference latency remains on par with that of traditional\nfull fine-tuning. VeRA (Vector-based Random Matrix Adapta-\ntion) [80] employs a single pair of frozen low-rank matrices\nWup and Wdown that are shared across all layers, and adapts\nthese matrices by learning small, trainable scaling vectors\nrepresented as b and d (formally denoted by diagonal matrices\n\u039bb and \u039bd). Specifically, the reparameterization is given by:\nhout \u201c W0hin ` \u039bbWup\u039bdWdownhin,\n(16)\nwhere both Wup and Wdown are initialized using a random\nGaussian distribution. Similar to LoRA, the scaling vector\nb is initialized to zeros to ensure that the weight matrix is\nunaffected during the first forward pass. This method signif-\nicantly reduces the number of trainable parameters compared\nto LoRA yet maintains the same performance, enabling the\nfine-tuning of larger models on a single GPU. DoRA (Weight-\nDecomposed Low-Rank Adaptation) [81] presents a novel\napproach as illustrated in Figure 8 (c) by decomposing model\nweights W0 P Rd\u02c6k into magnitude and direction as follows:\nW0 \u201c m V\n}V }c\n\u201c }W0}c\nW0\n}W0}c\n,\n(17)\nwhere m P R1\u02c6k is the magnitude vector, V\nP Rd\u02c6k is\nthe directional matrix, with } \u00a8 }c being the vector-wise norm", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey", "description": "Arxiv Paper titled 'Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey' authored by Zeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang, Sai Qian Zhang", "url": "https://arxiv.org/pdf/2403.14608", "source": "arxiv", "id": "7b2049b9-5156-4ce7-9a06-28788ff9a51d"}, "page_content": " random\nGaussian distribution. Similar to LoRA, the scaling vector\nb is initialized to zeros to ensure that the weight matrix is\nunaffected during the first forward pass. This method signif-\nicantly reduces the number of trainable parameters compared\nto LoRA yet maintains the same performance, enabling the\nfine-tuning of larger models on a single GPU. DoRA (Weight-\nDecomposed Low-Rank Adaptation) [81] presents a novel\napproach as illustrated in Figure 8 (c) by decomposing model\nweights W0 P Rd\u02c6k into magnitude and direction as follows:\nW0 \u201c m V\n}V }c\n\u201c }W0}c\nW0\n}W0}c\n,\n(17)\nwhere m P R1\u02c6k is the magnitude vector, V\nP Rd\u02c6k is\nthe directional matrix, with } \u00a8 }c being the vector-wise norm\nof a matrix across each column. Subsequently, DoRA adopts\na unique fine-tuning strategy for m and V. While both are\ntunable, only V undergoes LoRA reparameterization, defined\nas:\nW 1 \u201c m V ` \u2206V\n}V ` \u2206V }c\n\u201c m\nW0 ` WupWdown\n}W0 ` WupWdown}c\n,\n(18)\nwhere \u2206V is the incremental directional update learned by\nLoRA, and the underlined parameters denote the trainable\nparameters. Through this methodology, DoRA consistently\noutperforms LoRA across various tasks and models, demon-\nstrating its superiority.\nD. Hybrid PEFT\nThe efficacy of various PEFT methods can significantly\ndiffer across different tasks. As a result, numerous studies aim\nto either combine the advantages of diverse PEFT approaches\nor seek to establish a unified perspective by analyzing the\nsimilarities among these methods. For instance, UniPELT [97]\nintegrates LoRA, prefix-tuning, and adapters into each Trans-\nformer block. To control which PEFT submodules should\nbe activated, they also introduce a gating mechanism. This\nmechanism consists of three small FFNs that each produce\na scalar value G P p0, 1q, which is then applied to the\nLoRA, prefix, and adapter matrices, respectively. Across var-\nious setups, UniPELT has consistently shown improvements\nin accuracy ranging from 1% to 4%. S4 [98] explores design\nspaces for several PEFT methods (i.e., Adapter (A), Prefix\n(P), BitFit (B), and LoRA (L)) to uncover underlying design\npatterns. After a series of experiments, their findings include:\n(1) Applying the spindle grouping partitioning for Transformer\nlayers, which results in four layer groups Gi for i P t1... 4u.\nLayers in one group have similar behaviors together, which\nmeans should apply similar PEFT strategies. (2) Allocating\nthe number of trainable parameters to layers uniformly. (3)\nTuning all the groups. (4) Assigning different PEFT strategies\nin different groups. The resulting design space that has the\nbest performance is:\nG1 : pA, Lq, G2 : pA, Pq, G3 : pA, P, Bq, G4 : pP, B, Lq\nMAM Adapter[32] explores the intrinsic similarity between\nthree additive PEFT methods: adapters, prefix-tuning, and\nLoRA, which leads to the development of three variants:\nParallel Adapter, which places adapter layers alongside spe-\ncific layers (SA or FFN) instead of after them; Multi-head\nParallel Adapter, which divides the parallel adapter into\nmultiple heads, each affecting the head attention output in\nSA; and Scaled Parallel Adapter, which adds a scaling term\nafter the parallel adapter layer, similar to LoRA. Extensive\nexperimentation revealed that the most effective configura-\ntion involves using prefix-tuning in the SA layer and the\nscaled parallel adapter in the FFN layer, which is called the\nMAM Adapter. LLM-Adapters [101] builds an easy-to-use\nframework that incorporates various PEFT techniques into\nLLMs. Through comprehensive benchmarking across multiple\ndatasets, the study reveals several key insights: (1) The most\neffective locations for series adapters, parallel adapters, and\nLoRA are after the MLP layers, alongside the MLP layers, and\nsimultaneously following the Attention layers and MLP layers,\nrespectively. (2) Smaller LLMs utilizing PEFT can achieve\ncompetitive or even superior results on certain tasks when\ncompared to their larger counterparts. (3) With appropriate\nin-distribution fine-tuning data, smaller models are capable of\nsurpassing larger models in task-specific performance.\nSeveral studies leverage neural architecture search (NAS)\nto find better PEFT combination approaches. For example,\nNOAH [99] discovers that different PEFT configurations are\nspecifically tailored for different tasks. To address this issue,\nNOAH employs NAS to identify the most effective PEFT con-\nfigurations for each dataset. Specifically, NOAH\u2019s searching\nspace encompasses three PEFT methods: Adapter, LoRA, and\nVisual Prompt Tuning (VPT). It utilizes AutoFormer [115], a\none-shot NAS algorithm, for the efficient discovery of optimal\nprompt modules. In a related vein, AUT", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey", "description": "Arxiv Paper titled 'Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey' authored by Zeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang, Sai Qian Zhang", "url": "https://arxiv.org/pdf/2403.14608", "source": "arxiv", "id": "1d6f5905-237f-4dd1-a94c-e98bd4889114"}, "page_content": " layers,\nrespectively. (2) Smaller LLMs utilizing PEFT can achieve\ncompetitive or even superior results on certain tasks when\ncompared to their larger counterparts. (3) With appropriate\nin-distribution fine-tuning data, smaller models are capable of\nsurpassing larger models in task-specific performance.\nSeveral studies leverage neural architecture search (NAS)\nto find better PEFT combination approaches. For example,\nNOAH [99] discovers that different PEFT configurations are\nspecifically tailored for different tasks. To address this issue,\nNOAH employs NAS to identify the most effective PEFT con-\nfigurations for each dataset. Specifically, NOAH\u2019s searching\nspace encompasses three PEFT methods: Adapter, LoRA, and\nVisual Prompt Tuning (VPT). It utilizes AutoFormer [115], a\none-shot NAS algorithm, for the efficient discovery of optimal\nprompt modules. In a related vein, AUTOPEFT [100] first\nestablishes a searching space that includes serial adapters,\nparallel adapters, and prefix tuning. After that, they propose\nan effective NAS method based on a high-dimensional multi-\ndimensional Bayesian optimisation [116]. Both NOAH and\nAUTOPEFT demonstrate the capability of NAS in enhancing\nPEFT configurations across a variety of tasks.\nIV. EFFICIENT PEFT DESIGN\nProcessing latency and peak memory overhead are pivotal\nfactors to consider from a computational standpoint. This\nsection introduces a key characteristic in LLMs aimed at\nbalancing between latency and memory usage (Section IV-A).\nFollowing this, we explore strategies for developing efficient\nPEFT methods to address computational challenges, including\nPEFT pruning (Section IV-B), PEFT quantization (Sec-\ntion IV-C), and memory-efficient PEFT techniques (Sec-\ntion IV-D), each designed to enhance model performance\n11\nEfficient PEFT Design\nPEFT Pruning\nAdapterDrop [117], SparseAdapter [118], SPLoRA [119], LoRAPruning [120], ProPETL [121]\nPEFT\nQuantization\nBI-Adapter [122], PEQA [123], QLoRA [124], LoftQ [125], LQ-LoRA [126], QA-LoRA [127], INT2.1 [128], QDyLoRA [129],\nBitDelta [130]\nMemory-efficient\nPEFT\nSide-Tuning [131], LST [132], Res-Tuning [133], MEFT [134], LoRA-FA [135], HyperTuning [136], PEFT Plug-in [137],\nMeZO [138], GaLore [139]\nFig. 9: Taxonomy of Efficient PEFT Design.\nwhile minimizing resource consumption. It is noteworthy that\nquantization inherently addresses memory overhead concerns.\nHowever, given its distinct characteristics, we address these\nquantization methods separately rather than incorporating\nthem under the memory-efficient PEFT section.\nA. KV-cache Management for PEFT Efficiency\nThe core of the LLMs model lies in an auto-regressive\nTransformer model. When we consider the auto-regression\ncharacteristic, it becomes a major challenge in designing an\ninference system, because every time a new token is generated,\nthe entire LLM model has to transfer all the weights from\ndifferent memories to the memory of the graphics processor,\nwhich is very unfriendly to single-user task scheduling or\nmulti-user workload balance. The challenging part of serving\nthe auto-regressive paradigm is that all previous sequences\nhave to be cached and saved for the next proceeding iteration;\nthe cached activation generated from the previous sequences\nis stored as the Key-Value Cache (KV-cache). To effectively\nmanage these challenges, S-LoRA [140] employs a Unified\nPaging mechanism within a unified memory pool that dynam-\nically allocates and manages memory in a paged fashion. This\nsophisticated approach minimizes memory fragmentation and\nenhances the efficiency of KV-cache storage by allowing for\nflexible and efficient memory access patterns. These pages are\nmanaged such that the KV-cache associated with each adapter\nis segmented into manageable blocks, streamlining access and\nreducing the overhead associated with variable cache sizes. By\ndynamically adjusting to different KV-cache requirements, S-\nLoRA maintains high throughput and performance, ensuring\nthat the system remains responsive and efficient even as it\nscales to serve thousands of adapters simultaneously. This\nefficient handling of KV-cache is crucial for supporting the\nauto-regressive nature of LLMs in high-demand environments,\noptimizing both single-user and multi-user workload balanc-\ning.\nB. Pruning Strategies for PEFT\nThe inclusion of pruning can substantially enhance the\nefficiency of PEFT methods. In particular, AdapterDrop [117]\nexplores the removal of adapters from lower transformer\nlayers and multi-task adapters in AdapterFusion [35], which\nshows that the pruning can improve the training and in-\nference efficiency with minimal decrease in performance.\nSparseAdapter [118] investigates different pruning methods\nand finds that high sparsity ratio (80%) can outperform stan-\ndard adapters. Additionally, the Large-Sparse configuration,\nwhich increases the bottleneck dimension while maintaining\na constant parameter budget (e.g., doubling dimensions with\na 50% sparsity), substantially enhances the model\u2019s capacity,\nresulting in improved performance. SPLoRA [119] adopts\nchannel", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey", "description": "Arxiv Paper titled 'Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey' authored by Zeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang, Sai Qian Zhang", "url": "https://arxiv.org/pdf/2403.14608", "source": "arxiv", "id": "c8de9796-0b99-4aad-a55d-97af83621c15"}, "page_content": " single-user and multi-user workload balanc-\ning.\nB. Pruning Strategies for PEFT\nThe inclusion of pruning can substantially enhance the\nefficiency of PEFT methods. In particular, AdapterDrop [117]\nexplores the removal of adapters from lower transformer\nlayers and multi-task adapters in AdapterFusion [35], which\nshows that the pruning can improve the training and in-\nference efficiency with minimal decrease in performance.\nSparseAdapter [118] investigates different pruning methods\nand finds that high sparsity ratio (80%) can outperform stan-\ndard adapters. Additionally, the Large-Sparse configuration,\nwhich increases the bottleneck dimension while maintaining\na constant parameter budget (e.g., doubling dimensions with\na 50% sparsity), substantially enhances the model\u2019s capacity,\nresulting in improved performance. SPLoRA [119] adopts\nchannel-based pruning to the LoRA weights Wdown and Wup.\nThis pruning affects not only the source weights W0, but\nalso the LoRA parameters Wup and Wdown. Similarly, Lo-\nRAPruning [120] adopts structured pruning not only to the\npretrained model weights but also to the LoRA weights.\nIn contrast to unstructured LoRA pruning methods, which\nprimarily focus on sparsifying model weights while leaving\nLoRA weights dense, thus making weight merging challenging\nto achieve, LoRAPruning enables the weights to be merged\neasily. Additionally, this work also introduces a novel criterion\nthat utilizes LoRA\u2019s gradients as an approximation of the\ngradients for the pre-trained weights, enabling the estimation\nof weight importance. ProPETL [121] constructs a single\nshared prototype (e.g., adapter, prefix, or LoRA) across layers\nand tasks. In addition, ProPETL learns binary masks to prune\ndifferent sub-networks in different layers and tasks. As a result,\nthe parameters can be reused across layers and tasks, largely\nincreasing the parameter efficiency.\nC. Quantization Strategies for PEFT\nQuantization serves as another popular technique for im-\nproving computational efficiency and reducing memory us-\nage. For example, by investigating the loss landscape of\nadapters, BI-Adapter [122] finds that adapters are resistant\nto noise in parameter space. Building on this insight, the\nauthors introduce a clustering-based quantization approach.\nRemarkably, they demonstrate that a 1-bit quantization of\nadapters not only minimizes storage requirements but also\nachieves superior performance among all precision settings.\nPEQA (Parameter-Efficient and Quantization-aware Adapta-\ntion) [123] uses a two-stage pipeline to achieve parameter-\nefficient and quantization-aware fine-tuning. In the first stage,\nthe pre-trained FFN weight matrix W P Rn\u02c6m is quantized\nto W \u201c s \u00a8 W, where s P Rn\u02c61 represents per-channel\nscales and W denotes the quantized weight. In the second\nstage, W remains fixed, and fine-tuning is only conducted\non s. This approach not only ensures memory efficiency but\nalso facilitates parameter efficiency. QLoRA [124] proposes\nseveral novel techniques, including a 4-bit NormalFloat, a\nDouble Quantization, and a Paged Optimizers, to backprop-\nagate a 4-bit quantized pretrained language model into LoRA.\n12\nThese techniques enable the fine-tuning for a 65B language\nmodel on a single 48GB GPU while maintaining similar\nperformance to the full 16-bit fine-tuning. Similar to the\noriginal implementation [76], QLoRA attaches the fixed zero-\ninitialized LoRA weights to the quantized pre-trained model\nas the training start point. However, when applying the ex-\ntreme low-bit (e.g., 2-bit) quantization, the huge quantization\nerror can adversely impact the initialization of LoRA fine-\ntuning, i.e., quantizationpW0q ` WdownWup \u2030 W0 where\nWdown \u201c 0, which will harm the fine-tuning performance as\nshown in the work by [134]. To solve this, several quanti-\nzation strategies are proposed to eliminate the quantization\nerror. For example, LoftQ (LoRA-Fine-Tuning-aware Quanti-\nzation) [125] presents an innovative framework that provides\na superior initialization point of quantized backbone weights\nand LoRA weights for subsequent LoRA fine-tuning. This\napproach addresses the discrepancies caused by quantization\nthrough the optimization of a Frobenius norm objective during\nnetwork initialization, which takes both the LoRA weights\nand the quantized pre-trained backbone into consideration.\nLoftQ exhibits superior performance in 2-bit quantization over\nQLoRA, as well as greater generalization for downstream\ntasks. LQ-LoRA [126] uses an iterative algorithm inspired\nby robust principal components analysis [141], [142] which\ndecomposes the weight W0 such that W0 \u00ab Q ` L1L2\nto resolve the inaccuracy caused by the quantization error,\nwhere Q is the quantized component which remains fixed\nand L1L2 is the trainable low-rank component. Moreover, this\napproach leverages integer linear programming to determine\na mixed quantization strategy, enabling dynamic quantization\nconfigurations for each weight matrix while adhering to a\npredetermined total bit rate limit. QA-LoRA [127] address\n", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey", "description": "Arxiv Paper titled 'Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey' authored by Zeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang, Sai Qian Zhang", "url": "https://arxiv.org/pdf/2403.14608", "source": "arxiv", "id": "1335b3ab-54f3-402f-8efa-43ba7223de82"}, "page_content": " by quantization\nthrough the optimization of a Frobenius norm objective during\nnetwork initialization, which takes both the LoRA weights\nand the quantized pre-trained backbone into consideration.\nLoftQ exhibits superior performance in 2-bit quantization over\nQLoRA, as well as greater generalization for downstream\ntasks. LQ-LoRA [126] uses an iterative algorithm inspired\nby robust principal components analysis [141], [142] which\ndecomposes the weight W0 such that W0 \u00ab Q ` L1L2\nto resolve the inaccuracy caused by the quantization error,\nwhere Q is the quantized component which remains fixed\nand L1L2 is the trainable low-rank component. Moreover, this\napproach leverages integer linear programming to determine\na mixed quantization strategy, enabling dynamic quantization\nconfigurations for each weight matrix while adhering to a\npredetermined total bit rate limit. QA-LoRA [127] address\nanother limitation of QLoRA, which struggles to preserve its\nquantized property post-fine-tuning. In QLoRA, the quantized\npre-trained weight (NF4) has to be recovered to FP16 to match\nthe LoRA weight precision (FP16) during weight merging.\nInstead, QA-LoRA uses INT4 quantization and introduces\ngroup-wise operators to enable quantization during the infer-\nence stage, therefore improving the efficiency and accuracy\ncompared with QLoRA. BitDelta [130] introduces a novel 1-\nbit post-training quantization method that acts on the weight\ndelta between a fine-tuned model and its underlying pre-\ntrained model. Specifically, given the weight matrices Wfine\nand Wbase from the fine-tuned and base models respectively,\nthe weight delta \u2206\u201c Wfine \u00b4 Wbase is binarized as \u02c6\u2206\u201c \u03b1 d\nSignp\u2206q. Here, \u03b1, a high-precision scalar, is initialized based\non the mean absolute delta value \u03b1 \u201c\n1\nnm\n\u0159\nij |Wij|, with\nSignp\u00a8q indicating the sign of \u2206. BitDelta further calibrates\nthe scaling factors via distillation on a compact calibration\ndataset, while the binary matrices remain unchanged. This\napproach notably streamlines the deployment of multiple fine-\ntuned models on shared servers by utilizing a singular full-\nprecision base model alongside efficiently batched 1-bit deltas.\nD. Memory-efficient PEFT Methods\nFine-tuning the full LLMs necessitates substantial training\nmemory owing to their considerable size. While most PEFT\nmethods primarily target parameter efficiency, they still in-\ncur a significant memory overhead during training because\ngradient computation and backpropagation are still necessary\nfor these methods. For example, prevalent PEFT techniques\nsuch as adapters and LoRA can only reduce memory usage\nto approximately 70% compared to full model fine-tuning ac-\ncording to some literature [132], [137]. From a computational\nperspective, memory efficiency also remains a critical factor\nthat cannot be overlooked.\nTo improve memory efficiency, various techniques have\nbeen developed to minimize the need for caching gradi-\nents for the entire LLM during fine-tuning, thereby reducing\nmemory usage. For example, both Side-Tuning [131] and\nLST (Ladder-Side Tuning) [132] introduce a learnable net-\nwork branch parallel to the backbone model. By channeling\nthe backpropagation exclusively through this parallel branch,\nit circumvents the need to store gradient information for\nthe main model\u2019s weights, thus markedly reducing memory\nrequirements during training. Similarly, Res-Tuning [133]\ndisentangles the PEFT tuners (e.g., prompt tuning, adapter)\nfrom the backbone model. On top of the disentanglement, a\nmemory-efficient fine-tuning framework named Res-Tuning-\nBypass is proposed, which generates a bypass network in\nparallel with the backbone model by removing the data flow\nfrom the decoupled tuners to the backbone. This eliminates the\nrequirement for gradient caching within the backbone model\nduring backpropagation. MEFT [134] (memory-efficient fine-\ntuning) is an approach inspired by the reversible model [143].\nDuring the training of a reversible model, intermediate ac-\ntivations are not required to be cached in the forward pass.\nDuring backpropagation, they can be recalculated from the\nfinal output. To save the memory during fine-tuning, MEFT\ninvestigates how to transform an LLM to its reversible counter-\nparts without additional pre-training. A critical aspect of this\ntransformation is the careful initialization of newly introduced\nparameters in the pre-trained models. MEFT demonstrates the\nimportance of parameter initialization and suggests that these\nparameters must be initialized in a manner that preserves the\npre-trained model\u2019s starting point, ensuring that the fine-tuning\nof the modified model achieves performance on par with full\nfine-tuning methods. With this key consideration, MEFT intro-\nduces three distinct methods, each significantly curtailing the\nmemory demands traditionally required for storing activations.\nLoRA-FA [135] addresses a limitation about memory over-\nhead in LoRA fine-tuning. During training, LoRA modules\nstill require high activation memory consumption. This is be-\ncause, during backpropagation, large input activations must be\nstored during the forward pass to compute gradients. LoRA-FA\nresolves this issue by freezing both the pre-trained weights W0\nand", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey", "description": "Arxiv Paper titled 'Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey' authored by Zeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang, Sai Qian Zhang", "url": "https://arxiv.org/pdf/2403.14608", "source": "arxiv", "id": "5414c72f-c3c1-4fc5-af61-bef3009e80e3"}, "page_content": " this\ntransformation is the careful initialization of newly introduced\nparameters in the pre-trained models. MEFT demonstrates the\nimportance of parameter initialization and suggests that these\nparameters must be initialized in a manner that preserves the\npre-trained model\u2019s starting point, ensuring that the fine-tuning\nof the modified model achieves performance on par with full\nfine-tuning methods. With this key consideration, MEFT intro-\nduces three distinct methods, each significantly curtailing the\nmemory demands traditionally required for storing activations.\nLoRA-FA [135] addresses a limitation about memory over-\nhead in LoRA fine-tuning. During training, LoRA modules\nstill require high activation memory consumption. This is be-\ncause, during backpropagation, large input activations must be\nstored during the forward pass to compute gradients. LoRA-FA\nresolves this issue by freezing both the pre-trained weights W0\nand the projection-down weights Wdown, and only updating the\nprojection-up weights Wup. Consequently, the input activation\nhin no longer needs to be stored, as the intermediate activation\nWdownhin is adequate for gradient computation for Wup. Given\nthat r! d, the memory requirement for activations in LoRA-\nFA can be significantly reduced.\nTo further reduce memory usage during fine-tuning, some\nmethods attempt to circumvent backpropagation within LLMs\nto address this issue. HyperTuning [136] employs a Hyper-\n13\nModel to generate PEFT parameters using only fewshot exam-\nples. This approach demonstrates results comparable to those\nobtained through full model fine-tuning. PEFT Plug-in [137]\nfirst trains PEFT modules on small language models, which\nis more memory efficient compared to training on large ones.\nSubsequently, the research introduces a suite of techniques for\nseamlessly integrating these trained PEFT modules into LLMs\nduring inference. This strategy effectively circumvents the\nnecessity of gradient-based optimization directly on the larger\nmodels, resulting in substantial memory savings. However, it is\nimportant to note that both HyperModel and PEFT Plug-in still\nrequire additional model training, and this training cost cannot\nbe entirely overlooked. MeZO [138] introduces a memory-\nefficient zeroth-order (ZO) optimizer for LLMs. Unlike con-\nventional PEFT techniques, which rely on backpropagation to\ncompute gradients for updating model parameters, MeZO fine-\ntunes LLMs through only forward passes. It accomplishes this\nby employing a ZO gradient estimator to calculate the gradient.\nNotably, MeZO implements an in-place solution for the classic\nZO gradient estimator, effectively mitigating memory con-\nsumption during inference execution. This innovative approach\nallows for efficient fine-tuning of LLMs containing 30 billion\nparameters on a single GPU with 80GB of memory, all while\nmaintaining performance that is comparable to fine-tuning\nusing backpropagation. Furthermore, it can substantially de-\ncrease storage demands in comparison to the traditional PEFT\nmethods such as LoRA and Adapter.\nV. PEFT FOR DNNS OF OTHER APPLICATIONS\nIn Section III, we outlined four categories of PEFT methods\nalong with their improvements. Nonetheless, our discussion\ndid not fully extend to the utilization or adaptation of PEFT\ntechniques beyond traditional architectures (e.g., LLMs) or\nstandard benchmarks (e.g., the GLUE dataset), where the ma-\njority of the discussed PEFT methods are applied. Therefore,\nin this section, we will highlight and discuss several most\nrepresentative works that leverage PEFT strategies for various\ndownstream tasks. We do not aim to cover all PEFT applica-\ntion scenarios in this section. Our objective is to showcase the\nsignificant influence of PEFT within various research domains\nand demonstrate how to optimize and tailor general-purpose\nPEFT methods to achieve enhanced performance in specific\nmodels or tasks.\nTypically, fine-tuning happens when adapting a pre-trained\nbackbone model to specialized downstream tasks. To this end,\nthis section organizes the discussion around various model\narchitectures, which include: LLM, Vision Transformer (ViT),\nVision-Language Alignment Model (VLA), and Diffusion\nmodel. Within each architectural category, the discussion is\nfurther classified based on different downstream tasks.\nA. PEFT for LLMs \u2013 Beyond the Basics\nInstead of common tasks in NLP such as NLU and NLG,\nPEFT techniques boast a wide array of applications across\ndiverse scenarios. PEFT has been successfully implemented in\ncommonsense question answering [144], [145], multi-level im-\nplicit discourse relation recognition [146], out-of-distribution\ndetection [147], privacy protection [148], [149], federated\nlearning [150], and social biases mitigation [151]. In this\nsection, we pay more focus on three representative downstream\ntasks: visual instruction following, continual learning, and\ncontext window extension.\n1) Visual Instruct Following: Several studies, including\nVL-BART [152], MiniGPT-4 [153], and LLaVA [154], have\nsuccessfully extended the capabilities of LLMs, initially de-\nsigned for pure text, to comprehend and generate responses to\nvisual inputs. These enhanced models, namely visual instruct-\nfollowing LLMs, can process both images and text to produce\ntextual responses, which can be benchmarked on tasks such as\nimage captioning [155], [156], [157], [158] and visual question\nanswering", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey", "description": "Arxiv Paper titled 'Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey' authored by Zeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang, Sai Qian Zhang", "url": "https://arxiv.org/pdf/2403.14608", "source": "arxiv", "id": "b2db9bb6-236c-43de-996a-c832edeffebe"}, "page_content": " [145], multi-level im-\nplicit discourse relation recognition [146], out-of-distribution\ndetection [147], privacy protection [148], [149], federated\nlearning [150], and social biases mitigation [151]. In this\nsection, we pay more focus on three representative downstream\ntasks: visual instruction following, continual learning, and\ncontext window extension.\n1) Visual Instruct Following: Several studies, including\nVL-BART [152], MiniGPT-4 [153], and LLaVA [154], have\nsuccessfully extended the capabilities of LLMs, initially de-\nsigned for pure text, to comprehend and generate responses to\nvisual inputs. These enhanced models, namely visual instruct-\nfollowing LLMs, can process both images and text to produce\ntextual responses, which can be benchmarked on tasks such as\nimage captioning [155], [156], [157], [158] and visual question\nanswering (VQA) [159], [160], [161]. However, these methods\nfine-tune the entire LLM to learn the visual representations,\nwhich can be inefficient in both time and memory. Therefore, it\nis natural to apply PEFT techniques in the fine-tuning of visual\ninstruct-following LLMs. An earlier work VL-Adapter [162]\ndirectly applies several PEFT methods (Adapter [31], Hy-\nperformer [40] and Compacter [77]) on VL-BART [152]\nthen benchmarks them on several image-text and video-text\ntasks. Results show that vanilla adapters are the best among\nthem, which can achieve performance on par with full fine-\ntuning. However, considering the functionality gap between\nthe encoders and decoders in VL-BART, directly assigning\nidentical modular modifications will lead to suboptimal perfor-\nmance. Therefore, VL-PET [163] selectively integrates PEFT\nmodules into different components of the encoder and decoder.\nThey also introduce a granularity-controlled mechanism for\nfiner-grained control.\nTo adapt the recently prevalent LLaMA model, LLaMA-\nAdapter [164] prepends a set of learnable prompts (similar\nto prefix tuning) to the input tokens in LLaMA\u2019s higher trans-\nformer layers. To avoid the unstable fine-tuning with large loss\nvalues at early training stages, instead of the randomly initial-\nized weights of other PEFT methods, LLaMA-Adapter adopts\na zero-initialized attention mechanism, which learns a zero-\ninitialized gating factor to adaptively control the contribution\nof adaptation prompts to the word tokens. This can maintain\nthe fine-tuning starting point the same as the original model\nand progressively inject new knowledge into the model, where\na similar idea can be found in MEFT [134] and LoftQ [125]\ndiscussed earlier. To represent visual information, LLaMA-\nAdapter extracts multi-scale global image features using a\nCLIP image encoder and then projects them to linguistic em-\nbedding space. After that, the feature is element-wisely added\nonto the adaptation prompts at all inserted transformer layers.\nLLaMA-Adapter only introduces 1.2M learnable parameters\nin LLaMA-7B and costs less than one hour for fine-tuning on\n8 A100 GPUs. A following work LLaMA-Adapter V2 [165]\ndemonstrates that the simple multimodal fusion in LLaMA-\nAdapter cannot generalize to more challenging open-ended\nmultimodal reasoning tasks, where the visual cues tend to\ndominate the adaptation prompts than the language instruction\ndata. To address this, LLaMA-Adapter V2 decouples the learn-\ning of instruction-following ability (to generate long language\nresponses) and vision-language alignment to avoid interfer-\nence between visual and language fine-tuning. Specifically,\n14\nLLaMA-Adapter V2 sets disjoint parameter groups which\nare respectively learned from image-text pairs and language\ninstruction data. The visual adaptation prompts are inserted in\nthe early stage of LLM, while the language adaptation prompts\nremain at the higher transformer layers similar to the LLaMA-\nAdapter. Additionally, LLaMA-Adapter V2 introduces more\nlearnable parameters and several expert systems (e.g., caption-\ning, detection, and OCR) to enhance multimodal performance.\nLayerNorm Tuning [166] adjust only the weights of the\nLayerNorm within each attention block. This straightforward\ntechnique can achieve comparable or even better performance\nthan the finetuning, while offering about 10\u00d7 more parameter\nefficiency than LoRA.\n2) Continual Learning: Continual Learning (CL) aims to\nlearn a sequence of new tasks over time within one single\nmodel, which has broad application in scenarios such as\ndialogue systems [167], information extraction systems [168],\nand question answering systems [169]. The main challenge in\nCL is catastrophic forgetting [170]. A popular practice, called\narchitecture-based methods, tackles the CL by maintaining\ntask-specific parameters in the model for each new task. There-\nfore, it\u2019s natural to leverage PEFT methods for CL tasks [171],\n[172], [173], [174]. For example, AdapterCL [171] pa-\nrameterizes each new task using residual adapters. During\ntesting, since the task-id is not provided, AdapterCL uses\nan entropy-based classifier to select which adapter to use\nfor accomplishing a specific task. CPT (Continual Prompt\nT", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey", "description": "Arxiv Paper titled 'Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey' authored by Zeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang, Sai Qian Zhang", "url": "https://arxiv.org/pdf/2403.14608", "source": "arxiv", "id": "52f966e0-f20b-4991-91e9-f9aa210fe91d"}, "page_content": "\n2) Continual Learning: Continual Learning (CL) aims to\nlearn a sequence of new tasks over time within one single\nmodel, which has broad application in scenarios such as\ndialogue systems [167], information extraction systems [168],\nand question answering systems [169]. The main challenge in\nCL is catastrophic forgetting [170]. A popular practice, called\narchitecture-based methods, tackles the CL by maintaining\ntask-specific parameters in the model for each new task. There-\nfore, it\u2019s natural to leverage PEFT methods for CL tasks [171],\n[172], [173], [174]. For example, AdapterCL [171] pa-\nrameterizes each new task using residual adapters. During\ntesting, since the task-id is not provided, AdapterCL uses\nan entropy-based classifier to select which adapter to use\nfor accomplishing a specific task. CPT (Continual Prompt\nTuning) [172] trains a soft prompt for each task. Instead of\ntraining soft prompts from scratch, CPT proposes a series\nof techniques (continual prompt initialization, query fusion,\nmemory replay, and a memory-guided technique) to achieve\nknowledge transfer from preceding and subsequent tasks.\nO-LoRA (orthogonal low-rank adaptation) [175] employs a\nstrategy of learning distinct tasks within separate low-rank\nvector subspaces that are kept orthogonal to each other in order\nto minimize interference. This approach can effectively reduce\ncatastrophic forgetting during the acquisition of new tasks.\n3) Context Window Extension: LLMs are typically trained\nwith a pre-defined context size. For example, LLaMA and\nLLaMA2 have pre-defined context sizes of 2048 and 4096\ntokens, respectively. The positional encoding RoPE has weak\nextrapolation properties [176], which means the performance\ndrops obviously given an input length exceeds the pre-defined\ncontext length. To solve this, a naive solution is to fine-\ntune a pre-trained LLM to a longer context. However, this\nescalates computational costs quadratically with context size,\nstraining memory and processing resources. To address this,\nLongLoRA [177] proposes to fine-tune a pre-trained LLM\nusing LoRA to enlarge the context size. To reduce the\nperplexity gap between LoRA tuning and full fine-tuning,\nLongLoRA also opens embedding and normalization layers\nfor training. In order to further improve training efficiency in\na long context scenario, LongLoRA further introduces a novel\nshifted sparse attention (S2-Attn) as an efficient substitute for\nstandard self-attention during training. A subsequent study\nLongQLoRA [178] combines the advantages of LongLoRA\nwith QLoRA and Position Interpolation [10] to save GPU\nmemory. This work successfully extends the context length\nof LLaMA2-13B from 4096 to 8192 on a single V100 with\n32GB memory. LLoCO [179] introduces a pipeline that\nlearns contexts offline through the combination of context\ncompression and LoRA. The process begins by compressing\ndocuments into compact contexts, then fine-tuning LLM us-\ning LoRA on the compacted context to improve the LLM\u2019s\nability to accurately extract and utilize information from these\ncompressed representations. During model serving, a standard\nRAG retriever selects both the compressed document and the\nmost relevant LoRA module, and applies them to the LLM\nfor inference. This approach effectively extends the context\nwindow of a 4k token LLaMA2-7B model to handle up to\n128k tokens.\nIn addition to limited training-stage sequence length, real-\nworld system memory constraints introduce another critical\nbottleneck to the context window. Specifically, the capacity\nof the KV-cache is curtailed by available system memory. For\nexample, a 30B parameter LLM operating with an input length\nof 1024 and a batch size of 128 might necessitate up to 180GB\nfor the KV-cache [180], thereby restricting the feasible size of\nthe context window. In response to this, some strategies have\nresorted to quantizing the KV cache [181], [182], but quanti-\nzation will certainly compromise performance. To effectively\ncounteract this issue without significant loss, GEAR [183]\npresents a novel approach by employing a low-rank matrix\nto capture the majority of coherent bases of quantization\nerror, complemented by a sparse matrix that addresses errors\nfrom outlier entries, thus efficiently minimizing approximation\nerrors.\nB. PEFT for ViTs\nViT [184] has emerged as a powerful backbone model in the\nrecent computer vision community. In the ViT model, images\nare treated as sequences of fixed-size patches analogous to how\nLLM uses discrete tokens. These patches undergo linear em-\nbedding and then receive positional encodings. Subsequently,\nthey are processed through standard Transformer encoders.\nThe training of ViT can be supervised [184], [185] or self-\nsupervised [186], [187], and ViT can achieve superior perfor-\nmance when training with more data and using larger model\nsize [188]. However, such scaling up inevitably escalates\ntraining and storage costs. Therefore, similar to LLMs, PEFT\nis widely implemented in various downstream tasks, such as\ndense prediction [189], continual learning [190], [191], deep\nmetric learning [192]. Here, we focus on two typical tasks to\nshowcase the involvement of PEFT: image", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey", "description": "Arxiv Paper titled 'Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey' authored by Zeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang, Sai Qian Zhang", "url": "https://arxiv.org/pdf/2403.14608", "source": "arxiv", "id": "c1f76d77-6ed0-4c3f-be15-042697aa97f5"}, "page_content": "184] has emerged as a powerful backbone model in the\nrecent computer vision community. In the ViT model, images\nare treated as sequences of fixed-size patches analogous to how\nLLM uses discrete tokens. These patches undergo linear em-\nbedding and then receive positional encodings. Subsequently,\nthey are processed through standard Transformer encoders.\nThe training of ViT can be supervised [184], [185] or self-\nsupervised [186], [187], and ViT can achieve superior perfor-\nmance when training with more data and using larger model\nsize [188]. However, such scaling up inevitably escalates\ntraining and storage costs. Therefore, similar to LLMs, PEFT\nis widely implemented in various downstream tasks, such as\ndense prediction [189], continual learning [190], [191], deep\nmetric learning [192]. Here, we focus on two typical tasks to\nshowcase the involvement of PEFT: image classification and\nvideo recognition.\n1) Image Classification: Image classification on targeted\nvisual datasets is a very common demand and has extensive\napplications, while pre-train then fine-tuning paradigm serves\nas a widespread strategy. A variety of methods leverage PEFT\ntechniques to achieve efficient model tuning [193], [189],\n[194], [195]. For instance, AdaptFormer [194] inserts adapter\nmodules in parallel to the FFN of the original ViT model for\nvisual recognition tasks. VPT (Visual Prompt Tuning) [193]\nprepends a small amount of task-specific parameters into the\ninput sequence of each Transformer layer. When applying\n15\nViT to downstream tasks, only these added parameters and\nthe classification head are set to trainable. [196] notices that\ncompared with supervised ViT, VPT often underperforms with\nself-supervised ViT. Further analysis demonstrates that differ-\nent pre-trained methods and downstream tasks have varying\ndegrees of dependency on transformer blocks at different lo-\ncations. To tackle this issue, the research introduces adaptable\ngates for ViT blocks. These gates dynamically modulate the\ncontribution of prompt tokens to ViT blocks, allowing for a\nmore targeted adaptation of the model to the task at hand.\n2) Video Recognition: Several works consider the more\nchallenging adaptation problem that transfers ViT to down-\nstream tasks that have a much larger domain gap. For example,\nST-Adapter (Spatio-Temporal Adapter) [197] and AIM [198]\nboth insert adapters layers into pre-trained ViT blocks. Their\nprimary goal is to model spatial-temporal information, thereby\nenabling efficient adaptation of ViTs from image models\nto video tasks. Notably, both methodologies have exhibited\nperformance that surpasses traditional full-model fine-tuning\napproaches.\nC. PEFT for VLAs\nVision-language\nalignment\nmodels\n(VLA),\nsuch\nas\nCLIP [199], ALIGN [200], DeCLIP [201], and FLAVA [202],\nare designed to learn a good image and text features which can\nbe aligned within a unified representation space. Each VLA\ntypically consists of separate image and text encoders that\nextract respective features. Contrastive learning is leveraged in\nthese models to effectively align the image and text features.\nFine-tuning is leveraged to improve the performance of VLA\nin specific datasets or tasks, but fine-tuning the full model\nis computationally intensive. For instance, fine-tuning CLIP\nRN50x64 requires a batch size of 32,768 and 18 days of\ntraining on 592 V100 GPUs [199]. Moreover, full fine-tuning\non smaller datasets often leads to catastrophic forgetting [170].\nIn response to these challenges, and drawing inspiration from\nthe success of PEFT techniques in NLP, a range of PEFT\nstrategies have been proposed and implemented in VLA\nmodels, such as semantic segmentation [203], [204], [205],\npoint cloud understanding [206], [207], [208], [209], video\nunderstanding [210], [211], [212], visual reasoning [213],\n[214], temporal action detection [215], to name a few. This\nsection will focus on one common task that uses VLAs:\nopen-vocabulary image classification.\n1) Open-vocabulary\nImage\nClassification:\nIn\nopen-\nvocabulary\nimage\nclassification,\nearlier\nworks\ndesign\nclass-specific prompts, e.g., a photo of a [CLASS], for each\ncategory, and rank images based on their similarity to these\ntextual descriptions. CoOp (Context Optimization) [216]\nreplaces the handcrafted text prompt with learnable vectors,\nwhile keeping the entire VLA fixes during training. CoCoOp\n(Conditional Context Optimization) [217] builds on this by\ntackling CoOp\u2019s limitations in generalizing to unseen classes.\nIt introduces a lightweight neural network that generates an\ninput-specific context token, dynamically adapting the prompt\nbased on each image, thereby enhancing generalizability,\nbut at the cost of increased computational demands due to\nthe instance-aware operation. ProGrad [218] addresses the\nover-fitting risk in CoOp in a few-shot setting by regularizing\nthe soft prompt updates whose gradient is aligned to the\ngeneral knowledge only updates the prompt whose gradient is\naligned (or non-conflicting) to the general knowledge offered\nby the original prompt. MaPLe [219] notes that existing\n", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey", "description": "Arxiv Paper titled 'Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey' authored by Zeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang, Sai Qian Zhang", "url": "https://arxiv.org/pdf/2403.14608", "source": "arxiv", "id": "c0850559-8666-4715-8f2c-480115bbd84e"}, "page_content": "\ntextual descriptions. CoOp (Context Optimization) [216]\nreplaces the handcrafted text prompt with learnable vectors,\nwhile keeping the entire VLA fixes during training. CoCoOp\n(Conditional Context Optimization) [217] builds on this by\ntackling CoOp\u2019s limitations in generalizing to unseen classes.\nIt introduces a lightweight neural network that generates an\ninput-specific context token, dynamically adapting the prompt\nbased on each image, thereby enhancing generalizability,\nbut at the cost of increased computational demands due to\nthe instance-aware operation. ProGrad [218] addresses the\nover-fitting risk in CoOp in a few-shot setting by regularizing\nthe soft prompt updates whose gradient is aligned to the\ngeneral knowledge only updates the prompt whose gradient is\naligned (or non-conflicting) to the general knowledge offered\nby the original prompt. MaPLe [219] notes that existing\nmethods learn prompts either in the language or in the vision\nbranch of CLIP, which is not efficient in leveraging the\nmultimodal nature of VLAs. To address this, MaPLe proposes\nbranch-aware hierarchical prompts that simultaneously adapt\nboth language and vision branches, and achieves superior\nperformance. TPT (test-time prompt tuning) [220] studies\nprompt tuning on the fly without additional training samples.\nSpecifically, during inference, TPT first augments the input\nimage into various views, which are then utilized to tune\nthe learnable prompts. The primary training objective is to\nensure the VLA can generate consistent responses when faced\nwith these differing views. A following work DiffTPT [221]\nfurther enhances the data diversity of test samples through\ndiffusion models.\nIn another direction, several studies explore the usage of\nadapters in VLA. For example, CLIP-Adapter [222] inte-\ngrates residual-style adapters after CLIP\u2019s text and visual en-\ncoders. Therefore, unlike CoOp and CoCoOp, CLIP-Adapter\navoids the gradient backpropagation through CLIP\u2019s encoders,\nleading to reduced computational requirements in terms of\nboth training memory and time. Tip-Adapter [223] adopts\nthe same design with CLIP-Adapter. Different from CLIP-\nAdapter, the weights of the adapter are obtained in a training-\nfree manner from a query-key cache model [224], [225]\nconstructed from few-shot supervisions in a non-parametric\nmanner. As a result, Tip-Adapter exhibits great efficiency\ncompared to CLIP-Adapter\u2019s SGD training process.\nD. PEFT for Diffusion Models\nDiffusion models [226], [227] are a class of generative\nmodels that learn to generate data by transforming random\nnoise into a structured output by a progressive denoising\nprocess. During training, diffusion models learn to reverse\nthe noise added to training data using a denoising network,\nwhile in inference, they start from noise, using a denois-\ning network to iteratively create data that mirrors the same\ndistribution as the training examples. Diffusion models have\nvarious applications [228], [229], [230], [231], [232], while the\nmost notable is stable diffusion [233], which bridges the gap\nbetween text and image with its robust capability to generate\ncoherent and contextually relevant images directly from textual\ndescriptions. Numerous studies leverage PEFT techniques to\nadapt a pre-trained diffusion model for downstream tasks, in-\ncluding accelerating sampling speed [234], [235], text-to-video\nadaptation [236], [237], text-to-3D adaptation [238], etc. This\nsection mainly focuses on two scenarios: integrating additional\ninput modalities beyond mere text-based conditioning, and\ncustomizing content generation based on pre-trained diffusion\nmodel.\n1) Additional Input Control: To incorporate additional in-\nput modalities (e.g., layout, keypoints) while retaining the\n16\nextensive knowledge in the pre-trained model, GLIGEN\nintroduces a novel approach, which maintains the original\nmodel\u2019s weights intact and integrates new, trainable gated\nTransformer layers [239] that take in the new grounding\ninput. The resulting model can not only accurately repre-\nsent the grounding conditions but also produce high-quality\nimages. Remarkably, the model can also generalize well to\nunseen objects during inference. ControlNet [240] fine-tunes\na trainable copy of the encoding layers from Stable Diffusion\nwhile locking its pre-trained parameter weights. The fixed\noriginal model and the trainable copy are bridged through zero\nconvolution layers. These layers, starting with zero-initialized\nweights, are designed to progressively adapt during training,\nensuring that harmful noise does not affect the pre-trained\nfeatures of Stable Diffusion at the beginning of training. This\nrefined model is capable of conditioning on a variety of inputs\nsuch as Canny edges, Hough lines, user scribbles, human key\npoints, segmentation maps, shape normals, depths, etc. Con-\ncept Sliders [241] introduces a plug-and-play LoRA adaptors\nto allow precise editing of concepts (e.g., age, smiling) within\na diffusion model. T2I-Adapter [242] introduces a lightweight\nadapter model designed to align external control signals with\nthe internal knowledge of text-to-image diffusion models. This\nadapter enables precise manipulation through structural control\n(e.g., sketch, depth map, semantic segmentation map, and\nkeypose", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey", "description": "Arxiv Paper titled 'Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey' authored by Zeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang, Sai Qian Zhang", "url": "https://arxiv.org/pdf/2403.14608", "source": "arxiv", "id": "a835f366-764d-4193-a013-5daf151f9b94"}, "page_content": "\nconvolution layers. These layers, starting with zero-initialized\nweights, are designed to progressively adapt during training,\nensuring that harmful noise does not affect the pre-trained\nfeatures of Stable Diffusion at the beginning of training. This\nrefined model is capable of conditioning on a variety of inputs\nsuch as Canny edges, Hough lines, user scribbles, human key\npoints, segmentation maps, shape normals, depths, etc. Con-\ncept Sliders [241] introduces a plug-and-play LoRA adaptors\nto allow precise editing of concepts (e.g., age, smiling) within\na diffusion model. T2I-Adapter [242] introduces a lightweight\nadapter model designed to align external control signals with\nthe internal knowledge of text-to-image diffusion models. This\nadapter enables precise manipulation through structural control\n(e.g., sketch, depth map, semantic segmentation map, and\nkeypose), color control (e.g., hue and color distribution), and\nintegrating various controls by composing multiple adapters.\n2) Customized Generation: The effectiveness of text-to-\nimage diffusion models is limited by the user\u2019s ability to\narticulate the desired target through text descriptions. For\ninstance, it is difficult to describe the precise features of an\ninnovative toy car which is not encountered during large-scale\nmodel training. Consequently, the objective of customized\ngeneration is to enable the model to grasp new concepts from a\nminimal set of user-supplied images. Textual Inversion [243]\naddresses this by finding a new pseudo-word S\u02da (similar to\nsoft prompt discussed in Section III-A2) that represents new,\nspecific concepts in the textual embedding space of pre-trained\ntext-to-image diffusion models. The pseudo-word S\u02da is opti-\nmized via the original optimization goal in diffusion models\ngiven a small image set (typically 3-5 images) depicting the\nconcept, and the pre-trained model is left untouched. During\ninference, S\u02da can be treated like any other word and composed\nwith other textual queries (e.g., \u201da photo of S\u02da on the beach\u201d).\nCustom Diffusion [244] tackles a more challenging setting:\ncompositional fine-tuning of multiple concepts. It fine-tunes\nonly the Wk, Wv mapping from text to latent features in\nattention layers, which yields superior performance in multi-\nconcept learning scenarios. Additionally, during fine-tuning,\nCustom Diffusion prevents model forgetting by introducing\na small set of real images with captions akin to the target,\nalongside employing augmentation for faster convergence and\nimproved results. IP-Adapter [245] identifies limitations in\ncurrent approaches (e.g., ControlNet and T2I-Adapter) which\nproject condition signals into the cross-attention modules.\nWhen handling image conditions aiming at controlling content,\nthese methods are unable to generate images faithful to the\nprompted image. The issue stems from that merging image\nfeatures and text features within cross-attention layers loses\nimage-specific information, leading to only coarse-grained\ncontrollable generation such as image style rather than image\ncontent. To overcome this, IP-Adapter introduces a novel\ndecoupled cross-attention mechanism to distinguish between\ntext and image features. IP-Adapter adds an additional cross-\nattention layer exclusively for image features in each cross-\nattention layer, and only the parameters of the new cross-\nattention layers are trained.\nVI. SYSTEM DESIGN CHALLENGE FOR PEFT\nA. System design for PEFT\nIn this section, we begin by providing a concise overview\nof cloud-based PEFT systems and analyzing the design chal-\nlenges. These include the efficient handling of numerous task-\nspecific queries via centralized PEFT query servicing, the\nresolution of privacy and data transmission issues through\ndistributed PEFT training, and the complexities associated\nwith concurrent multi-PEFT training processes. Centralized\nsystems are required to process a substantial volume of queries\nwith minimal latency and maximal throughput. Distributed\ntraining frameworks must address privacy concerns and the\ncomputational inefficiencies that arise from data exchanges\nbetween users and cloud services. Furthermore, multi-PEFT\ntraining necessitates the optimization of memory utilization,\nthe management of simultaneous model training, and the\nformulation of system architectures capable of supporting\nmulti-tenant workloads effectively. These challenges under-\nscore the imperative for innovative approaches to improve\nscalability, safeguard privacy, and optimize resource allocation\nin PEFT system architectures. Following this, we present the\ncorresponding metrics employed for evaluating the system\nperformance. Furthermore, we delve into three prospective\nutilization scenarios to illustrate the challenges in system\ndesign.\n1) Centralized PEFT Query Serving: Cloud providers have\nrecently introduced a range of LLM services aimed at pro-\nviding user applications through application programming\ninterfaces (APIs) [246], [247]. These APIs facilitate the seam-\nless integration of many machine-learning functionalities into\napplications. When receiving one query for one specific down-\nstream task through API, the cloud-based server processes the\nquery with one featured LLM model. Under this scenario, the\nimportance of PEFT becomes apparent. Cloud providers store\nonly a single copy of the LLM and multiple PEFT modules\nfeaturing different downstream tasks. This setup allows the\nLLM to maintain various branches of PEFT modules, each\nlinked to specific API queries, i.e., PEFT queries.\nCentralized PEFT query serving solutions address scenarios\nwhere", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey", "description": "Arxiv Paper titled 'Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey' authored by Zeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang, Sai Qian Zhang", "url": "https://arxiv.org/pdf/2403.14608", "source": "arxiv", "id": "9a7b7688-084d-4ecb-ac49-8ffe3c44cc21"}, "page_content": "ve into three prospective\nutilization scenarios to illustrate the challenges in system\ndesign.\n1) Centralized PEFT Query Serving: Cloud providers have\nrecently introduced a range of LLM services aimed at pro-\nviding user applications through application programming\ninterfaces (APIs) [246], [247]. These APIs facilitate the seam-\nless integration of many machine-learning functionalities into\napplications. When receiving one query for one specific down-\nstream task through API, the cloud-based server processes the\nquery with one featured LLM model. Under this scenario, the\nimportance of PEFT becomes apparent. Cloud providers store\nonly a single copy of the LLM and multiple PEFT modules\nfeaturing different downstream tasks. This setup allows the\nLLM to maintain various branches of PEFT modules, each\nlinked to specific API queries, i.e., PEFT queries.\nCentralized PEFT query serving solutions address scenarios\nwhere multiple PEFT queries arrive in quick succession. A\ncase study of one state-of-the-art system for this purpose\nis discussed in Section VI-B. Figure 10 (b) illustrates the\ncomputation pattern for multi-query PEFT inference, wherein\npacked PEFT queries are scheduled and executed according\nto their deadlines and current system conditions.\n2) Distributed PEFT Training: In most cases, personal-\nized tasks are not fully supported with pre-trained models,\n17\nLLMs\nEdge \nDevice \nPersonal data\nCloud\nTrainable \nModules\n\ud83d\udd25\nFrozen Large Models\nScheduler\nRequest Pool\nQuery\nResponse\nExecution\nEngine\nServing System\nI like\nI enjoy\nLLM\nprogramming\n(a)\n(b)\nFig. 10: (a) Distributed-based system computation pattern; (b)\ncentralized PEFT Query inference.\nconsequently, extra fine-tuning is required to be executed\nwith the methodologies mentioned in the previous sections.\nHowever, significant concerns arise when considering the\ntransfer of datasets to cloud providers, given the issues related\nto data privacy, copyright, proprietary information, and the\ncomplexities and inefficiencies involved in data transmission.\nSection VI-C gives two approaches that address this concern.\n3) Multi-PEFT Training: Different from multiple-PEFT\nserving, tuning with multiple customized PEFTs always in-\nvolves different backbone LLMs. Therefore, simultaneously\ntuning multiple PEFTs can pose considerable challenges.\nChallenges like how to manage memory gradient and model\nweights storage, and how to design an efficient kernel for\nbatching PEFT training remain unsolved. PEFTs will be cat-\negorized based on their PEFT algorithms and backbone LLM\nmodels. The design challenge involves how to consolidate\nmultiple PEFTs with the same LLM backbone and multiple\ndifferent LLM backbones simultaneously. We present case\nstudies related to this topic in Section VI-D.\n4) Evaluation Metrics: For the proposed evaluation met-\nrics, without loss of generality, we adopt large language\nmodels as the basis for our metric definitions.\nTo evaluate the system performance of PEFT serving sys-\ntems, we propose a set of evaluation metrics:\n\u201a System throughput: Considering PEFT queries as inter\nand intra tasks, we use tokens per second to measure the\nsystem throughput.\n\u201a Memory footprint: Run-time memory consumption dur-\ning query serving, the memory utilization comes from\nboth model parameters and KV-cache as mentioned in\nSection IV-A.\n\u201a Accuracy performance: Real-world queries normally\nhave different context lengths, and performance with\nvariation length serves as a performance benchmark.\n\u201a Quality of services: Queries are associated with latency\nrequirements and deadline missing rates are considered\nas another benchmark.\nTo assess the efficacy of PEFT training systems, we also\nestablish a set of evaluative metrics:\n\u201a Accuracy performance: Performance of the fine-tuned\nmodel over the downstream tasks.\n\u201a Compute cost: The compute cost during forward and\nbackward propagation operations on cloud servers and\nedge devices.\n\u201a Communication cost: Refers to the volume of data\ninvolved during the transfer of intermediate data between\nthe edge device and the cloud.\nB. Centralized PEFT Serving Frameworks\nThe PEFT algorithm is notable for its ability to distin-\nguish between modifiable and immutable weights within a\nmodel. This characteristic inspires developers to amalgamate\ndiverse LLMs with distinct PEFT techniques into collective\nunits. PetS, as introduced in [248], advocates for a com-\nprehensive approach to managing multiple PEFT tasks by\nsuggesting a unified serving framework. The framework\u2019s\ncore advancement lies in the translation of varying PEFT\ntasks into integrated computation kernels to enhance efficiency.\nMoreover, PetS pioneers an orchestrated batching approach\nand a scheduling methodology, aiming to augment system\nthroughput and leverage task parallelism respectively.\nAs depicted in Figure 11, the PetS framework begins\nwith users registering PEFT tasks through a standardized\nApplication Programming Interface (API). Upon registration,\ndevelopers are expected to provide the Pre-Trained Model Tag\n(e.g., LLaMA), PEFT parameters in a compressed format,\nand the specific PEFT algorithms (e.g., LoRA, Adapter, Bitfit,\netc.). These tasks are then endowed with unique identifiers,\nand the inference engine takes charge of query processing.\nPetS bifurcates the primary computational workload (e.g.,\nlinear layer computations)", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey", "description": "Arxiv Paper titled 'Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey' authored by Zeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang, Sai Qian Zhang", "url": "https://arxiv.org/pdf/2403.14608", "source": "arxiv", "id": "0f7b0b88-e650-4ea8-b50c-e9b3da362897"}, "page_content": " to managing multiple PEFT tasks by\nsuggesting a unified serving framework. The framework\u2019s\ncore advancement lies in the translation of varying PEFT\ntasks into integrated computation kernels to enhance efficiency.\nMoreover, PetS pioneers an orchestrated batching approach\nand a scheduling methodology, aiming to augment system\nthroughput and leverage task parallelism respectively.\nAs depicted in Figure 11, the PetS framework begins\nwith users registering PEFT tasks through a standardized\nApplication Programming Interface (API). Upon registration,\ndevelopers are expected to provide the Pre-Trained Model Tag\n(e.g., LLaMA), PEFT parameters in a compressed format,\nand the specific PEFT algorithms (e.g., LoRA, Adapter, Bitfit,\netc.). These tasks are then endowed with unique identifiers,\nand the inference engine takes charge of query processing.\nPetS bifurcates the primary computational workload (e.g.,\nlinear layer computations) into three distinct computational\noperations: (1) Dense Matrix-Vector Multiplication (MVM)\nleveraging universally accessible, pre-trained weights. (2) Bias\nvector addition (Vadd), using either common or task-exclusive\nbiases. (3) A combination of Sparse/dense MVM operations\nemploying task-specific PET parameters. A unified pre-trained\nweight matrix W is employed across PetS, facilitating the\nbatching of initial operations, Xt \u02c6 W. However, subsequent\ntask-specific computations involving PET parameters, despite\nbeing relatively minimal in complexity, are processed individ-\nually.\nConsidering the Adapter and Bitfit tasks as an illustration,\nboth aim at the MLP component of LLMs. The Adapter\ntask integrates additional weight segments, whereas Bitfit\nadjusts bias elements. The Adapter operation is modeled as\nY \u201c Xin1 \u02c6 pW ` Wadq ` b0, where Xin1 represents the\ninput for the Adapter task, W and Wad are the original\nand adapter-specific PEFT weights respectively, and b0 is the\ninitial bias. The Bitfit operation, on the other hand, is defined\nas Y\n\u201c Xin2 \u02c6 W ` b1, with b1 symbolizing the Bitfit-\nadjustable bias. These operations are further synthesized as\ntY1, Y2u \u201c tXin1, Xin2u \u02c6 W ` tXin1 \u02c6 Wad, 0u ` tb0, b1u,\ndelineating that the tXin1, Xin2u \u02c6 W part is amenable to\nbatching through MVM, while the tb0, b1u segment pertains\nto the Vadd operation.\nFor tasks like Diff-Pruning III-B, is a little bit different\nthan Bitfit and Adapter. For Diff-Pruning, the computation\nconcerning the shared weight and \u2018difference\u2019 are conducted\nseparately. Then the results are added up, namely\nXt \u02c6 pW ` \u03b4tq \u201c Xt \u02c6 W ` Xt \u02c6 \u03b4t\n, here, the W denotes the backbone model weights while \u03b4t\ndenotes the pruned weights which can be represented as Sparse\nMVM.\n18\nPET Serving\nPET Inference Pipeline\nPre-train Model \nID\nShadow \nParameters\nPET Type\nPre-train Model \nID\nShadow \nParameters\nPET Type\nPre-trained \nModel Tag\nPET Parameters\nPET Type\nPET Parameters\nShared Model\nParameters\nRegister Tasks\nu\nTask Register\nPET Manager\nTask Manager\nParameter Repository\nv\nw\n<Task_id> \n<Input Data>\n\u2026\nQuery 0:\nQuery 1:\nInput Queries\nx\nPerformance \nModel\nBatch Scheduler\nScheduling \nPolicy\nEngine\nPET Task\nScheduler\nPET Operator\nLibrary\ny\nInput \nAnalyzing\nInput \nReformatting\nUser Inputs\n<Task_id> \n<Input Data>\nFig. 11: PetS system overview: (1) Tasks register; (2) Task\nmanager (3) Task schedule; (4) Task serving. (Image is taken\nfrom PetS [248])\nTask 0\nTask 4\nStep 1: Intra-Task Batching\nTask 1\nTask 2\nTask 3\nMini\nBatch\n\ud835\udefd\u2212Model\n\ud835\udefc\u2212Model\nPET-OPs Profiling\nBatch 1\nBatch 0\nB=2, S=34\nStep 2: Inter-Task Batching\nBatch 2\nMacro\nBatch\nShared-OPs\nProfiling\nB=4, S=34\nTask 0\nTask 1\nTask 3\nTask 2\nTask 4\nFig. 12: Coordinated Batching (CB) Strategy\nThe other challenge PetS proposed is how to schedule dif-\nferent PEFT requests to achieve high performance. PetS sched-\nuler achieves high parallelism through a two-level scheduling\npolicy: Coordinated Batching (CB) and Macro-batch Stream-\ning (MS) as Figure 12 depicts. Through CB, the input queries\nwill first be clustered based on their input length and then\ngrouped based on their shared operator. This is to make sure\nthe same sequence length of queries will be executed without\nwasting padding. MS strategy will take the grouped queries\nafter coordinated batching and the theoretical latency for\ndifferent operators as well as the system modeling parameters\nto generate the best execution order.\nThe other example design is DLo", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey", "description": "Arxiv Paper titled 'Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey' authored by Zeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang, Sai Qian Zhang", "url": "https://arxiv.org/pdf/2403.14608", "source": "arxiv", "id": "c4510959-9269-4f0d-af34-4ec8988b9534"}, "page_content": "Shared-OPs\nProfiling\nB=4, S=34\nTask 0\nTask 1\nTask 3\nTask 2\nTask 4\nFig. 12: Coordinated Batching (CB) Strategy\nThe other challenge PetS proposed is how to schedule dif-\nferent PEFT requests to achieve high performance. PetS sched-\nuler achieves high parallelism through a two-level scheduling\npolicy: Coordinated Batching (CB) and Macro-batch Stream-\ning (MS) as Figure 12 depicts. Through CB, the input queries\nwill first be clustered based on their input length and then\ngrouped based on their shared operator. This is to make sure\nthe same sequence length of queries will be executed without\nwasting padding. MS strategy will take the grouped queries\nafter coordinated batching and the theoretical latency for\ndifferent operators as well as the system modeling parameters\nto generate the best execution order.\nThe other example design is DLoRA [249], which intro-\nduces a system that improves the efficiency of serving low-\nrank adaptation (LoRA) models for large language models\n(LLMs) by dynamically managing the merging and unmerging\nof LoRA adapters and the migration of requests across worker\nreplicas. This dynamic orchestration addresses the challenges\nof high memory footprints, low GPU utilization, and load\nimbalance caused by variable input and output lengths in\ntraditional LLM serving systems. dLoRA\u2019s novel approaches,\nincluding a credit-based batching algorithm and a request-\nadapter co-migration algorithm, significantly enhance through-\nput.\nC. Distributed PEFT Training Frameworks\nWe already know that fine-tuning LLM for downstream\ntasks is challenging for two reasons: dual privacy concerns\nbetween cloud server and data owner, and issues with com-\nputational resources and efficiency. Firstly, the privacy of\nboth parties is at risk: the weights of large models are often\nproprietary and not made public. Sharing data with model\nowners for fine-tuning can lead to data privacy concerns while\nproviding model weights to data proprietors could compromise\nthe ownership of proprietary models. Secondly, even if down-\nstream users have access to pre-trained weights, the stringent\nhardware requirements make transfer learning impractical for\nmost end users.\nTo resolve these two issues, DLoRA [250] presents a\ndistributed PEFT framework. During the PEFT process, the\nbackbone LLM is executed in the cloud servers while the\nPEFT modules are trained entirely within the user devices.\nDLoRA scheme is depicted in Figure 10(a).\nSimilarly,\nOffsite-Tuning\n[251]\npresents\na\nprivacy-\npreserving and efficient transfer learning framework that\nenables foundational models to adapt to downstream tasks\nwithout the need to access the complete model weights. The\nkey insight of Offsite-Tuning is the cloud provider sends an\nadapter and an emulator to the data proprietor. Then, with the\nassistance of the emulator, the data proprietor fine-tunes the\nadapter. The fine-tuned adapter is then sent back to the cloud\nside, which integrates it into the complete model, creating a\nfine-tuned foundational model for downstream users. Offsite-\nTuning safeguards the privacy of data proprietors since they\ndo not need to share their training data directly. It also\nprotects the foundational model owners, as the complete model\nweights are not shared, and the emulator provided is lossy,\nwith significantly degraded performance. Compared to existing\nfine-tuning methods that require access to the full model\nweights, Offsite-Tuning is more resource-efficient because it\nallows for fine-tuning through a compressed emulator without\nneeding the complete model.\nD. Parallel PEFT Training Frameworks\nUnlike the PEFT query serving system, which aims to\naccommodate flexible multi-PEFT algorithms, Punica [252]\nfocuses solely on facilitating multiple-LoRA blocks for various\ntasks. Designing multiple PEFT training systems presents key\nchallenges in two main aspects:\n\u201a Efficient concurrent execution of multiple PEFT models\nwith the same LLM backbone.\n\u201a Designing an efficient system for multi-tenant serving\nwith different LLM backbones.\na) Efficient kernel design: Punica addresses the first chal-\nlenge by using existing matrix multiplication for the backbone\ncomputation and introducing a new CUDA kernel, Segmented\nGather Matrix-Vector Multiplication (SGMV), for adding the\nPEFT add-ons to the backbone computation in a batched\nmanner. This kernel parallelizes the feature-weight multipli-\ncation for different requests in the batch and groups requests\ncorresponding to the same PEFT model to increase operational\nintensity and use GPU Tensor Cores for acceleration.\n19\nThe second challenge is beyond the computational cost,\ndesigning an efficient system architecture that can effectively\nserve multi-tenant PEFT model workloads on the smallest set\nof GPUs possible while occupying the least amount of GPU\nresources is another significant challenge. Punica addresses\nthis by scheduling user requests to active GPUs that already\nserve or train PEFT models, thereby improving GPU utiliza-\ntion. For older requests, Punica periodically migrates them to\nconsolidate workloads, thus freeing up GPU resources for new\nrequests.\nb) Multi-Tenant PEFT design: Designing an efficient\nsystem for the multi-tenant PEFT model serving in the Punica\nframework focuses on addressing several key", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey", "description": "Arxiv Paper titled 'Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey' authored by Zeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang, Sai Qian Zhang", "url": "https://arxiv.org/pdf/2403.14608", "source": "arxiv", "id": "4e85e1f9-2459-4fbf-b151-dd2e0c7360bc"}, "page_content": ". This kernel parallelizes the feature-weight multipli-\ncation for different requests in the batch and groups requests\ncorresponding to the same PEFT model to increase operational\nintensity and use GPU Tensor Cores for acceleration.\n19\nThe second challenge is beyond the computational cost,\ndesigning an efficient system architecture that can effectively\nserve multi-tenant PEFT model workloads on the smallest set\nof GPUs possible while occupying the least amount of GPU\nresources is another significant challenge. Punica addresses\nthis by scheduling user requests to active GPUs that already\nserve or train PEFT models, thereby improving GPU utiliza-\ntion. For older requests, Punica periodically migrates them to\nconsolidate workloads, thus freeing up GPU resources for new\nrequests.\nb) Multi-Tenant PEFT design: Designing an efficient\nsystem for the multi-tenant PEFT model serving in the Punica\nframework focuses on addressing several key challenges to\nmaximize hardware utilization and minimize resource con-\nsumption. The system aims to consolidate multi-tenant LoRA\nserving workloads onto the smallest set of GPUs possible. This\nconsolidation is achieved through strategic scheduling of user\nrequests to active GPUs that are already serving or training\nLoRA models, thereby improving GPU utilization. For older\nrequests, Punica periodically migrates them to consolidate\nworkloads further, thus freeing up GPU resources for new\nrequests. It incorporates on-demand loading of LoRA model\nweights, which introduces only millisecond-level latency. This\nfeature provides Punica with the flexibility to dynamically\nconsolidate user requests to a small set of GPUs, without being\nconstrained by the specific LoRA models already running on\nthose GPUs. Besides that, Punica identifies that the decode\nstage is a predominant factor in the cost of model serving,\nPunica\u2019s design primarily focuses on optimizing decode stage\nperformance. Other aspects of model serving leverage straight-\nforward techniques, such as on-demand loading of LoRA\nmodel weights, to efficiently manage resource utilization.\nVII. CONCLUSION AND FUTURE DIRECTIONS\nIn the current era dominated by large models and large\ndatasets, PEFT stands out as a highly attractive method for\nefficiently adapting models to downstream tasks. This tech-\nnique gains its appeal by addressing the significant challenges\nposed by traditional full-model fine-tuning, which often places\nsubstantial computational and data demands. This survey of-\nfers a comprehensive examination of the most recent advance-\nments in PEFT, including algorithmic design, computational\nefficiency, application scenarios, and system implementation\nfor PEFT. It offers a comprehensive taxonomy and explanation\nthat serves as an excellent guidance and knowledge base,\nwhich enables readers of various levels and disciplines to\nswiftly grasp the core concepts of PEFT.\nFor further research on PEFT, we propose a series of pos-\nsible directions from both algorithm and system perspectives,\nhoping to inspire more researchers to engage in further studies\nin these areas.\nA. Simplify hyperparameter tuning\nThe effectiveness of PEFT is often sensitive to its hyperpa-\nrameters, such as the bottleneck dimension of the adapter, the\nrank of LoRA, and the arrangement of various additive PEFT\nlayers. Manually tuning these hyperparameters will cost lots\nof effort. Therefore, future efforts could focus on developing\nmethods that are less dependent on manual tuning of these\nparameters, or automatically find the optimal configuration\nsettings. Several studies [82], [83], [84], [98], [99], [100] have\nstarted to address this issue, but there\u2019s a need for more simple\nand efficient solutions optimizing these hyperparameters.\nB. Establish a unified benchmark\nDespite the existence of libraries like HuggingFace\u2019s\nPEFT [253] and AdapterHub [254], a comprehensive bench-\nmark for PEFT is still lacking. This gap hinders the ability\nto fairly compare the performance and efficiency of different\nPEFT approaches. A well-accepted, up-to-date benchmark\nakin to MMDetection [255] for object detection would enable\nresearchers to validate their methods against a standard set\nof tasks and metrics, fostering innovation and collaboration\nwithin the community.\nC. Enhance training efficiency\nThe presumed parameter efficiency of PEFT is not always\nconsistent with computational and memory savings during\ntraining. Given that trainable parameters are intertwined within\nthe pre-trained model\u2019s architecture, computing and storing\nactivations and gradients for the full model often become\nnecessary during fine-tuning. This oversight calls for a rethink-\ning of what constitutes efficiency. As outlined in Section IV,\npotential solutions lie in the integration of model compres-\nsion techniques such as pruning and quantization, alongside\ninnovations specifically designed to optimize memory during\nPEFT tuning [256]. Further research into enhancing the com-\nputational efficiency of PEFT methodologies is imperative.\nD. Explore scaling laws\nThe design and effectiveness of PEFT methods originally\ndeveloped for smaller Transformer models do not necessarily\nscale with larger models. As the size of foundation models\nincreases, identifying and adapting PEFT strategies that remain\neffective is crucial. This investigation will aid in customizing\nPEFT methodologies to suit the evolving landscape of large\nmodel architectures.\nE. Serve more models and tasks\nThe rise of large foundation models across various domains\npresents new opportunities for PEFT. Designing PEFT meth-\nods tailored to the unique characteristics of models, such as\nSora [257], Mamba [258], and LVM [259", "type": "Document"}}
{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"title": "Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey", "description": "Arxiv Paper titled 'Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey' authored by Zeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang, Sai Qian Zhang", "url": "https://arxiv.org/pdf/2403.14608", "source": "arxiv", "id": "33ae4641-f230-4752-ae4d-144217c320f6"}, "page_content": " IV,\npotential solutions lie in the integration of model compres-\nsion techniques such as pruning and quantization, alongside\ninnovations specifically designed to optimize memory during\nPEFT tuning [256]. Further research into enhancing the com-\nputational efficiency of PEFT methodologies is imperative.\nD. Explore scaling laws\nThe design and effectiveness of PEFT methods originally\ndeveloped for smaller Transformer models do not necessarily\nscale with larger models. As the size of foundation models\nincreases, identifying and adapting PEFT strategies that remain\neffective is crucial. This investigation will aid in customizing\nPEFT methodologies to suit the evolving landscape of large\nmodel architectures.\nE. Serve more models and tasks\nThe rise of large foundation models across various domains\npresents new opportunities for PEFT. Designing PEFT meth-\nods tailored to the unique characteristics of models, such as\nSora [257], Mamba [258], and LVM [259], can unlock new\napplication scenarios and opportunities.\nF. Enhancing data privacy\nTrusting centralized systems to serve or fine-tune personal-\nized PEFT modules is yet another issue for system developers.\nMultiple types of inversion attacks [260], [261] have been pro-\nposed to reconstruct user\u2019s data by hijacking the intermediate\nresults. One perspective of future trust-worthy LLM system\ndesign involves developing an encryption protocol for both\npersonal data and intermediate training and inference results.\n20\nG. PEFT with model compression\nModel compression is one of the most effective ways to\nmake LLM executable on resource-limited devices. Yet, the\nimpact of model compression techniques on the performance\nof PEFT algorithms running on hardware remains another\nsystemic challenge. Common compression techniques such\nas quantization and pruning necessitate dedicated hardware\nplatforms to expedite the process, and building such hardware\nplatforms for compressed models is yet another direction for\nfuture research.[SEP]", "type": "Document"}}