{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a47b888e",
   "metadata": {},
   "source": [
    "# Usage Example of TrainWiseBM25Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2c97e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " we developed are released under the [HF1BitLLM](https://huggingface.co/HF1BitLLM) organization. Two of these models were fine-tuned on 10B tokens with different training setup, while the third was fine-tuned on 100B tokens. Notably, our models surpass the Llama 1 7B model in MMLU benchmarks.\n",
      "\n",
      "### How to Use with Transformers\n",
      "\n",
      "To integrate the BitNet architecture into Transformers, we introduced a new quantization method called \"bitnet\" ( [PR](https://github.com/huggingface/transformers/pull/33410)). This method involves replacing the standard Linear layers with specialized BitLinear layers that are compatible with the BitNet architecture, with appropriate dynamic quantization of activations, weight unpacking, and matrix multiplication.\n",
      "\n",
      "Loading and testing the model in Transformers is incredibly straightforward, there are zero changes to the API:\n",
      "\n",
      "```python\n",
      "model = AutoModelForCausalLM.from_pretrained(\n",
      "    \"HF1BitLLM/Llama3-8B-1.58-100B-tokens\",\n",
      "    device_map=\"cuda\",\n",
      "    torch_dtype=torch.bfloat16\n",
      ")\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
      "\n",
      "input_text = \"Daniel went back to the garden. Mary travelled to the kitchen. Sandra journeyed to the kitchen. Sandra went to the hallway. John went to the bedroom. Mary went back to the garden. Where is Mary?\\nAnswer:\"\n",
      "\n",
      "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").cuda()\n",
      "output = model.generate(input_ids, max_new_tokens=10)\n",
      "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
      "print(generated_text)\n",
      "```\n",
      "\n",
      "With this code, everything is managed seamlessly behind the scenes, so there's no need to worry about additional complexities, you just need to install the latest version of transformers.\n",
      "\n",
      "For a quick test of the model, check out this [notebook](https://colab.research.google.com/drive/1ovmQUOtnYIdvcBkwEE4MzVL1HKfFHdNT?usp=sharing)\n",
      "\n",
      "## What is BitNet In More Depth?\n",
      "\n",
      "[BitNet](https://arxiv.org/abs/2402.17764) replaces traditional Linear layers in Multi-Head Attention and Feed-Forward Networks with specialized layers called BitLinear that use ternary precision (or even binary, in the initial version). The BitLinear layers we use in this project quantize the weights using ternary precision (with values of -1, 0, and 1), and we quantize the activations to 8-bit precision. We use a different implementation of BitLinear for training than we do for inference, as we'll see in the next section.\n",
      "\n",
      "The main obstacle to training in ternary precision is that the weight values are discretized (via the `round()` function) and thus non-differentiable. BitLinear solves this with a nice trick: [STE (Straight Through Estimator)](https://arxiv.org/abs/1903.05662). The STE allows gradients to flow through the non-differentiable rounding operation by approximating its gradient as 1 (treating `round()` as equivalent to the identity function). Another way to view it is that, instead of stopping the gradient at the rounding step, the STE lets the gradient pass through as if the rounding never occurred, enabling weight updates using standard gradient-based optimization techniques.\n",
      "\n",
      "![The architecture of BitNet with BitLinear layers](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/bitlinear.png)The architecture of BitNet with BitLinear layers (source: BitNet paper https://arxiv.org/pdf/2310.11453)\n",
      "\n",
      "### Training\n",
      "\n",
      "We train in full precision, but quantize the weights into ternary values as we go, using symmetric per tensor quantization. First, we compute the average of the absolute values of the weight matrix and use this as a scale. We then divide the weights by the scale, round the values, constrain them between -1 and 1, and finally rescale them to continue in full precision.\n",
      "\n",
      "scalew=11nm∑ij∣Wij∣ scale\\_w = \\\\frac{1}{\\\\frac{1}{nm} \\\\sum\\_{ij} \\|W\\_{ij}\\|} scalew​=nm1​∑ij​∣Wij​∣1​\n",
      "\n",
      "Wq=clamp\\[−1,1\\](round(W∗scale)) W\\_q = \\\\text{clamp}\\_{\\[-1,1\\]}(\\\\text{round}(W\\*scale)) Wq​=clamp\\[−1,1\\]​(round(W∗scale))\n",
      "\n",
      "Wdequantized=Wq∗scalew W\\_{dequantized} = W\\_q\\*scale\\_w Wdequantized​=Wq​∗scalew​\n",
      "\n",
      "Activations are then quantized to a specified bit-width (8-bit, in our case) using absmax per token quantization (for a comprehensive introduction to quantization methods check out this [post](https://mlabonne.github.io/blog/posts/Introduction_to_Weight_\n",
      "[\"completion\"][i]},\\\n",
      "            ]\n",
      "            output_texts.append(converted_sample)\n",
      "        return {'messages': output_texts}\n",
      "    else:\n",
      "        converted_sample = [\\\n",
      "            {\"role\": \"user\", \"content\": examples[\"prompt\"]},\\\n",
      "            {\"role\": \"assistant\", \"content\": examples[\"completion\"]},\\\n",
      "        ]\n",
      "        return {'messages': converted_sample}\n",
      "```\n",
      "\n",
      "```python\n",
      "dataset = dataset.rename_column(\"sentence\", \"prompt\")\n",
      "dataset = dataset.rename_column(\"translation_extra\", \"completion\")\n",
      "dataset = dataset.map(format_dataset)\n",
      "dataset = dataset.remove_columns(['prompt', 'completion', 'translation'])\n",
      "messages = dataset[0]['messages']\n",
      "messages\n",
      "```\n",
      "\n",
      "```\n",
      "[{'role': 'user',\\\n",
      "  'content': 'The birch canoe slid on the smooth planks.'},\\\n",
      " {'role': 'assistant',\\\n",
      "  'content': 'On the smooth planks, the birch canoe slid. Yes, hrrrm.'}]\n",
      "```\n",
      "\n",
      "### Tokenizer\n",
      "\n",
      "Before moving into the actual training, we still need to **load the tokenizer that corresponds to our model**. The tokenizer is an important part of this process, determining how to convert text into tokens in the same way used to train the model.\n",
      "\n",
      "For instruction/chat models, the tokenizer also contains its corresponding **chat template** that specifies:\n",
      "\n",
      "- Which **special tokens** should be used, and where they should be placed.\n",
      "- Where the system directives, user prompt, and model response should be placed.\n",
      "- What is the **generation prompt**, that is, the special token that triggers the model's response (more on that in the \"Querying the Model\" section)\n",
      "\n",
      "* * *\n",
      "\n",
      "**IMPORTANT UPDATE**: due to changes in the default collator used by the `SFTTrainer` class while building the dataset, the EOS token (which is, in Phi-3, the same as the PAD token) was masked in the labels too thus leading to the model not being able to properly stop token generation.\n",
      "\n",
      "In order to address this change, we can assign the UNK token to the PAD token, so the EOS token becomes unique and therefore not masked as part of the labels.\n",
      "\n",
      "* * *\n",
      "\n",
      "```python\n",
      "tokenizer = AutoTokenizer.from_pretrained(repo_id)\n",
      "tokenizer.pad_token = tokenizer.unk_token\n",
      "tokenizer.pad_token_id = tokenizer.unk_token_id\n",
      "tokenizer.chat_template\n",
      "```\n",
      "\n",
      "```\n",
      "\"{% for message in messages %}\n",
      "    {% if message['role'] ##'system' %}\n",
      "      {{'<|system|>\\n' + message['content'] + '<|end|>\\n'}}\n",
      "    {% elif message['role'] ## 'user' %}\n",
      "      {{'<|user|>\\n' + message['content'] + '<|end|>\\n'}}\n",
      "    {% elif message['role'] ## 'assistant' %}\n",
      "      {{'<|assistant|>\\n' + message['content'] + '<|end|>\\n'}}\n",
      "    {% endif %}\n",
      "{% endfor %}\n",
      "{% if add_generation_prompt %}\n",
      "  {{ '<|assistant|>\\n' }}{% else %}{{ eos_token }}\n",
      "{% endif %}\"\n",
      "```\n",
      "\n",
      "Never mind the seemingly overcomplicated template (I have added line breaks and indentation to it so it's easier to read). It simply organizes the messages into a coherent block with the appropriate tags, as shown below (`tokenize=False` ensures we get readable text back instead of a numeric sequence of token IDs):\n",
      "\n",
      "```python\n",
      "print(tokenizer.apply_chat_template(messages, tokenize=False))\n",
      "```\n",
      "\n",
      "```\n",
      "<|user|>\n",
      "The birch canoe slid on the smooth planks.<|end|>\n",
      "<|assistant|>\n",
      "On the smooth planks, the birch canoe slid. Yes, hrrrm.<|end|>\n",
      "<|endoftext|>\n",
      "```\n",
      "\n",
      "Notice that each interaction is wrapped in either `<|user|>` or `<|assistant|>` tokens at the beginning and `<|end|>` at the end. Moreover, the `<|endoftext|>` token indicates the end of the whole block.\n",
      "\n",
      "Different models will have different templates and tokens to indicate the beginning and end of sentences and blocks.\n",
      "\n",
      "We're now ready to tackle the actual fine-tuning!\n",
      "\n",
      "## Fine-Tuning with SFTTrainer\n",
      "\n",
      "**Fine-tuning a model**, whether large or otherwise, follows exactly **the same training procedure as training a model from scratch**. We could write our own training loop in pure PyTorch, or we could use Hugging Face's `Trainer` to fine-tune our model.\n",
      "\n",
      "It is much easier, however, to use `SFTTrainer` instead (which uses `Trainer` underneath, by the way), since it takes care of most of the nitty-gritty details for us, as long as we provide it with the following four arguments:\n",
      "\n",
      "- a model\n",
      "- a tokenizer\n",
      "- a dataset\n",
      "- a configuration object\n",
      "\n",
      "We've already got the first three elements; let's work on the last one.\n",
      "\n",
      "### SFTConfig\n",
      "\n",
      "There are many parameters that we can set in the configuration object. We have divided them into four groups:\n",
      "\n",
      "- **Memory usage** optimization parameters related to **gradient accumulation and checkpointing**\n",
      "webui](https://github.com/oobabooga/text-generation-webui). Since you can specify different precisions (see [my article about GGUF and llama.cpp](https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html)), we'll loop over a list to quantize it in `q2_k`, `q3_k_m`, `q4_k_m`, `q5_k_m`, `q6_k`, `q8_0` and upload these quants on Hugging Face. The [mlabonne/FineLlama-3.1-8B-GGUF](https://huggingface.co/mlabonne/FineLlama-3.1-8B-GGUF) contains all our GGUFs.\n",
      "\n",
      "```python\n",
      "quant_methods = [\"q2_k\", \"q3_k_m\", \"q4_k_m\", \"q5_k_m\", \"q6_k\", \"q8_0\"]\n",
      "for quant in quant_methods:\n",
      "    model.push_to_hub_gguf(\"mlabonne/FineLlama-3.1-8B-GGUF\", tokenizer, quant)\n",
      "```\n",
      "\n",
      "Congratulations, we fine-tuned a model from scratch and uploaded quants you can now use in your favorite inference engine. Feel free to try the final model available on [mlabonne/FineLlama-3.1-8B-GGUF](https://huggingface.co/mlabonne/FineLlama-3.1-8B-GGUF). What to do now? Here are some ideas on how to use your model:\n",
      "\n",
      "- **Evaluate** it on the [Open LLM Leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard) (you can submit it for free) or using other evals like in [LLM AutoEval](https://github.com/mlabonne/llm-autoeval).\n",
      "- **Align** it with Direct Preference Optimization using a preference dataset like [mlabonne/orpo-dpo-mix-40k](https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k) to boost performance.\n",
      "- **Quantize** it in other formats like EXL2, AWQ, GPTQ, or HQQ for faster inference or lower precision using [AutoQuant](https://colab.research.google.com/drive/1b6nqC7UZVt8bx4MksX7s656GXPM-eWw4?usp=sharing).\n",
      "- **Deploy** it on a Hugging Face Space with [ZeroChat](https://colab.research.google.com/drive/1LcVUW5wsJTO2NGmozjji5CkC--646LgC) for models that have been sufficiently trained to follow a chat template (~20k samples).\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "This article provided a comprehensive overview of supervised fine-tuning and how to apply it in practice to a Llama 3.1 8B model. By leveraging QLoRA's efficient memory usage, we managed to fine-tune an 8B LLM on a super high-quality dataset with limited GPU resources. We also provided more efficient alternatives for bigger runs and suggestions for further steps, including evaluation, preference alignment, quantization, and deployment.\n",
      "\n",
      "I hope this guide was useful. If you're interested in learning more about LLMs, I recommend checking the [LLM Course](https://github.com/mlabonne/llm-course). If you enjoyed this article, follow me on X [@maximelabonne](https://x.com/maximelabonne) and on Hugging Face [@mlabonne](https://huggingface.co/mlabonne). Good luck fine-tuning models![SEP]\n",
      " LoraConfig\n",
      "\n",
      "config = LoraConfig(use_rslora=True,...)\n",
      "```\n",
      "\n",
      "### Activated LoRA (aLoRA)\n",
      "\n",
      "Activated LoRA (aLoRA) is a low rank adapter architecture for Causal LMs that allows for reusing existing base model KV cache for more efficient inference. This approach is best suited for inference pipelines which rely on the base model for most tasks/generations, but use aLoRA adapter(s) to perform specialized task(s) within the chain. For example, checking or correcting generated outputs of the base model. In these settings, inference times can be sped up by an order of magnitude or more. For more information on aLoRA and many example use cases, see [https://huggingface.co/papers/2504.12397](https://huggingface.co/papers/2504.12397).\n",
      "\n",
      "This technique scans for the last occurence of an invocation sequence (`alora_invocation_tokens`) in each input (this can be as short as 1 token), and activates the adapter weights on tokens starting with the beginning of the invocation sequence (any inputs after the invocation sequence are also adapted, and all generated tokens will use the adapted weights). Weights on prior tokens are left un-adapted — making the cache for those tokens interchangeable with base model cache due to the causal attention mask in Causal LMs. Usage is very similar to standard LoRA, with the key difference that this invocation sequence must be specified when the adapter is created:\n",
      "\n",
      "Copied\n",
      "\n",
      "```\n",
      "from peft import LoraConfig\n",
      "\n",
      "config = LoraConfig(alora_invocation_tokens=alora_invocation_tokens, task_type=\"CAUSAL_LM\",...)\n",
      "```\n",
      "\n",
      "where `alora_invocation_tokens` is a list of integer token ids. Given a desired invocation string, this can be obtained as\n",
      "\n",
      "Copied\n",
      "\n",
      "```\n",
      "invocation_string = \"placeholder\"\n",
      "alora_invocation_tokens = tokenizer.encode(invocation_string, add_special_tokens=False).\n",
      "```\n",
      "\n",
      "where the tokenizer is the tokenizer for the base model. Note that we have `add_special_tokens=False` to avoid adding SOS/EOS tokens in our search string (which will most likely cause failure to find).\n",
      "\n",
      "**Notes**\n",
      "\n",
      "- aLoRA is only supported for `task_type=CAUSAL_LM` tasks due to its focus on cache reuse.\n",
      "- Since the weights are adapted on fewer tokens, often (not always) aLoRA requires higher rank (`r`) than LoRA. `r=32` can be a good starting point.\n",
      "- aLoRA weights cannot be merged into the base model by definition, since the adapter weights are selectively applied to a subset of tokens. Attempts to merge will throw errors.\n",
      "- Beam search is not yet supported.\n",
      "- It is generally not recommended to add new tokens to the tokenizer that are not present in the base model, as this can complicate the target use case of both the base model and adapter model operating on overlapping context. That said, there is a possible workaround by first efficiently adding [trainable tokens](https://huggingface.co/docs/peft/en/package_reference/trainable_tokens) to the base model prior to training the adapter.\n",
      "\n",
      "#### Choice of invocation sequence and SFT design\n",
      "\n",
      "Each input must have the `alora_invocation_tokens` sequence present, it is not added automatically. To maximize model performance without compromising cache reuse, it is recommended to have the adapter weights activated early, i.e. at the start of any adapter-specific prompting, but after any long inputs such as prior generations or documents. As with any model,\n",
      "formatting should be consistent between train and test.\n",
      "\n",
      "Consider the following example, where the base model has a chat template,\n",
      "and the goal is to train the adapter to generate a desired output.\n",
      "\n",
      "- Option 1: If there is no task-specific prompt, i.e. the input is a chat history with the `assistant` prompt, then the chat template’s `assistant` prompt (e.g. `<|start_of_role|>assistant<|end_of_role|>`) is a natural choice for the invocation string. See the model’s chat template to find the prompt for the model.\n",
      "- Option 2: If there is a task-specific prompt for the adapter that describes the task the adapter is learning, and that prompt is put as a `user` turn immediately prior to the generation, then the chat template’s `user` prompt (e.g. `<|start_of_role|>user<|end_of_role|>`) is a natural choice for the invocation string.\n",
      "\n",
      "Once deciding on an invocation string, get the model tokenizer and obtain `alora_invocation_tokens` as\n",
      "\n",
      "Copied\n",
      "\n",
      "```\n",
      "alora_invocation_tokens = tokenizer.encode(invocation_string, add_special_tokens=False).\n",
      "```\n",
      "\n",
      "An example inference setup is at [alora finetuning](https://github.com/huggingface/peft/blob/main/examples/alora_finetuning/alora_finetuning.py).\n",
      "\n",
      "**Note** If using custom strings for the invocation string, make sure that the start and end of the string are special tokens to avoid issues with tokenization at the boundaries.\n",
      "\n",
      "To see why, imagine that ‘a’, ‘b’,\n",
      " are added to the keys. When converting to the PEFT format, it is required to add these prefixes.\n",
      "\n",
      "> This last point is not true for prefix tuning techniques like prompt tuning. There, the extra embeddings are directly stored in the `state_dict` without any prefixes added to the keys.\n",
      "\n",
      "When inspecting the parameter names in the loaded model, you might be surprised to find that they look a bit different, e.g. `base_model.model.encoder.layer.0.attention.self.query.lora_A.default.weight`. The difference is the _`.default`_ part in the second to last segment. This part exists because PEFT generally allows the addition of multiple adapters at once (using an `nn.ModuleDict` or `nn.ParameterDict` to store them). For example, if you add another adapter called “other”, the key for that adapter would be `base_model.model.encoder.layer.0.attention.self.query.lora_A.other.weight`.\n",
      "\n",
      "When you call [save\\_pretrained()](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.PeftModel.save_pretrained), the adapter name is stripped from the keys. The reason is that the adapter name is not an important part of the model architecture; it is just an arbitrary name. When loading the adapter, you could choose a totally different name, and the model would still work the same way. This is why the adapter name is not stored in the checkpoint file.\n",
      "\n",
      "> If you call `save_pretrained(\"some/path\")` and the adapter name is not `\"default\"`, the adapter is stored in a sub-directory with the same name as the adapter. So if the name is “other”, it would be stored inside of `some/path/other`.\n",
      "\n",
      "In some circumstances, deciding which values to add to the checkpoint file can become a bit more complicated. For example, in PEFT, DoRA is implemented as a special case of LoRA. If you want to convert a DoRA model to PEFT, you should create a LoRA checkpoint with extra entries for DoRA. You can see this in the `__init__` of the previous `LoraLayer` code:\n",
      "\n",
      "Copied\n",
      "\n",
      "```\n",
      "self.lora_magnitude_vector: Optional[torch.nn.ParameterDict] = None  # for DoRA\n",
      "```\n",
      "\n",
      "This indicates that there is an optional extra parameter per layer for DoRA.\n",
      "\n",
      "### adapter\\_config\n",
      "\n",
      "All the other information needed to load a PEFT model is contained in the `adapter_config.json` file. Let’s check this file for a LoRA model applied to BERT:\n",
      "\n",
      "Copied\n",
      "\n",
      "```\n",
      "{\n",
      "  \"alpha_pattern\": {},\n",
      "  \"auto_mapping\": {\n",
      "    \"base_model_class\": \"BertModel\",\n",
      "    \"parent_library\": \"transformers.models.bert.modeling_bert\"\n",
      "  },\n",
      "  \"base_model_name_or_path\": \"bert-base-uncased\",\n",
      "  \"bias\": \"none\",\n",
      "  \"fan_in_fan_out\": false,\n",
      "  \"inference_mode\": true,\n",
      "  \"init_lora_weights\": true,\n",
      "  \"layer_replication\": null,\n",
      "  \"layers_pattern\": null,\n",
      "  \"layers_to_transform\": null,\n",
      "  \"loftq_config\": {},\n",
      "  \"lora_alpha\": 8,\n",
      "  \"lora_dropout\": 0.0,\n",
      "  \"megatron_config\": null,\n",
      "  \"megatron_core\": \"megatron.core\",\n",
      "  \"modules_to_save\": null,\n",
      "  \"peft_type\": \"LORA\",\n",
      "  \"r\": 8,\n",
      "  \"rank_pattern\": {},\n",
      "  \"revision\": null,\n",
      "  \"target_modules\": [\\\n",
      "    \"query\",\\\n",
      "    \"value\"\\\n",
      "  ],\n",
      "  \"task_type\": null,\n",
      "  \"use_dora\": false,\n",
      "  \"use_rslora\": false\n",
      "}\n",
      "```\n",
      "\n",
      "This contains a lot of entries, and at first glance, it could feel overwhelming to figure out all the right values to put in there. However, most of the entries are not necessary to load the model. This is either because they use the default values and don’t need to be added or because they only affect the initialization of the LoRA weights, which is irrelevant when it comes to loading the model. If you find that you don’t know what a specific parameter does, e.g., `\"use_rslora\",` don’t add it, and you should be fine. Also note that as more options are added, this file will get more entries in the future, but it should be backward compatible.\n",
      "\n",
      "At the minimum, you should include the following entries:\n",
      "\n",
      "Copied\n",
      "\n",
      "```\n",
      "{\n",
      "  \"target_modules\": [\"query\", \"value\"],\n",
      "  \"peft_type\": \"LORA\"\n",
      "}\n",
      "```\n",
      "\n",
      "However, adding as many entries as possible, like the rank `r` or the `base_model_name_or_path` (if it’s a Transformers model) is recommended. This information can help others understand the model better and share it more easily. To check which keys and values are expected, check out the [config.py](https://github.com\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.load.load import loads\n",
    "from bm25retriever import TrainWiseBM25Retriever\n",
    "\n",
    "docs = []\n",
    "with open(\"../chunks.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        docs.append(loads(line.strip()))\n",
    "\n",
    "bm25_retriever = TrainWiseBM25Retriever.from_documents(docs)\n",
    "bm25_retriever.k = 5\n",
    "\n",
    "query = \"What is LoRA?\"\n",
    "filter = {\"source\": \"hf\"}\n",
    "retrieved_docs = bm25_retriever.invoke(query, filter=filter)\n",
    "for doc, score in retrieved_docs:\n",
    "    print(doc.page_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trainwise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
